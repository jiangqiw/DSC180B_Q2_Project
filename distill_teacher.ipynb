{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bREGbUT_Y8F"
      },
      "source": [
        "### Import required packages and limit GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhcqwxR8_Y8I",
        "outputId": "26140e84-9279-479e-cc80-7c9a0b1a8fd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import argparse\n",
        "import time\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import csv\n",
        "\n",
        "# Import the module\n",
        "import networks\n",
        "import utils\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n73sKlgl_Y8K"
      },
      "outputs": [],
      "source": [
        "use_gpu = True    # set use_gpu to True if system has gpu\n",
        "gpu_id = 0        # id of gpu to be used\n",
        "cpu_device = torch.device('cpu')\n",
        "# fast_device is where computation (training, inference) happens\n",
        "fast_device = torch.device('cpu')\n",
        "if use_gpu:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'    # set visible devices depending on system configuration\n",
        "    fast_device = torch.device('cuda:' + str(gpu_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DAfPa7mw_Y8L"
      },
      "outputs": [],
      "source": [
        "def reproducibilitySeed():\n",
        "    \"\"\"\n",
        "    Ensure reproducibility of results; Seeds to 0\n",
        "    \"\"\"\n",
        "    torch_init_seed = 0\n",
        "    torch.manual_seed(torch_init_seed)\n",
        "    numpy_init_seed = 0\n",
        "    np.random.seed(numpy_init_seed)\n",
        "    if use_gpu:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reproducibilitySeed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o0sxuxkJbUEI"
      },
      "outputs": [],
      "source": [
        "checkpoints_path_teacher = 'checkpoints_teacher/'\n",
        "checkpoints_path_student = 'checkpoints_student_QAT/'\n",
        "if not os.path.exists(checkpoints_path_teacher):\n",
        "    os.makedirs(checkpoints_path_teacher)\n",
        "if not os.path.exists(checkpoints_path_student):\n",
        "    os.makedirs(checkpoints_path_student)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCz4Pup_Y8M"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import PIL\n",
        "\n",
        "# Set up transformations for CIFAR-10\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        #transforms.RandomCrop(32, padding=4),  # Augment training data by padding 4 and random cropping\n",
        "        transforms.RandomHorizontalFlip(),     # Randomly flip images horizontally\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "import torchvision as tv\n",
        "preprocess_train = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((160, 160), interpolation=PIL.Image.BILINEAR),  # It's the default, just being explicit for the reader.\n",
        "    tv.transforms.RandomCrop((128, 128)),\n",
        "    tv.transforms.RandomHorizontalFlip(),\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "])\n",
        "\n",
        "preprocess_eval = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((128, 128), interpolation=PIL.Image.BILINEAR),\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=False,\n",
        "                                            download=True, transform=transform_test)\n",
        "\n",
        "# Split the training dataset into training and validation\n",
        "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
        "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
        "\n",
        "# DataLoader setup\n",
        "batch_size = 128\n",
        "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load pre-trained teacher network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9225\n"
          ]
        }
      ],
      "source": [
        "import detectors\n",
        "import timm\n",
        "\n",
        "model = timm.create_model(\"resnet50_cifar10\", pretrained=True)\n",
        "\n",
        "model = model.to(fast_device)\n",
        "# pre-trained teacher accuracy\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(model, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_10944\\1025170478.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9225\n"
          ]
        }
      ],
      "source": [
        "# another way load teacher\n",
        "teacher_net = networks.TeacherNetwork50()\n",
        "\n",
        "checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
        "\n",
        "teacher_net.model.load_state_dict(checkpoint)\n",
        "teacher_net.to(fast_device)\n",
        "\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rayk6sDh7UXz"
      },
      "source": [
        "### Student Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (with training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hlvtw4Xkxhoh"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200\n",
        "print_every = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the total number of trainable parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters.\n",
        "    \"\"\"\n",
        "    return sum((p.data != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def count_zero_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the number of trainable parameters that are exactly zero in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose zero parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters that are exactly zero.\n",
        "    \"\"\"\n",
        "    return sum((p.data == 0).sum().item() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsT=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0 and pruning factor 0.0\n",
            "0.0 11173962 23520842\n",
            "[Epoch 1, Batch 100/372] Train Loss: 12.237, Train Accuracy: 0.086\n",
            "[Epoch 1, Batch 200/372] Train Loss: 4.760, Train Accuracy: 0.234\n",
            "[Epoch 1, Batch 300/372] Train Loss: 7.098, Train Accuracy: 0.602\n",
            "Epoch 1 Validation Accuracy: 0.640\n",
            "[Epoch 2, Batch 100/372] Train Loss: 5.812, Train Accuracy: 0.727\n",
            "[Epoch 2, Batch 200/372] Train Loss: 4.924, Train Accuracy: 0.086\n",
            "[Epoch 2, Batch 300/372] Train Loss: 3.587, Train Accuracy: 0.328\n",
            "Epoch 2 Validation Accuracy: 0.739\n",
            "[Epoch 3, Batch 100/372] Train Loss: 2.799, Train Accuracy: 0.844\n",
            "[Epoch 3, Batch 200/372] Train Loss: 2.914, Train Accuracy: 0.664\n",
            "[Epoch 3, Batch 300/372] Train Loss: 2.755, Train Accuracy: 0.797\n",
            "Epoch 3 Validation Accuracy: 0.800\n",
            "[Epoch 4, Batch 100/372] Train Loss: 3.705, Train Accuracy: 0.336\n",
            "[Epoch 4, Batch 200/372] Train Loss: 2.659, Train Accuracy: 0.719\n",
            "[Epoch 4, Batch 300/372] Train Loss: 3.255, Train Accuracy: 0.102\n",
            "Epoch 4 Validation Accuracy: 0.816\n",
            "[Epoch 5, Batch 100/372] Train Loss: 3.493, Train Accuracy: 0.078\n",
            "[Epoch 5, Batch 200/372] Train Loss: 2.840, Train Accuracy: 0.750\n",
            "[Epoch 5, Batch 300/372] Train Loss: 2.968, Train Accuracy: 0.109\n",
            "Epoch 5 Validation Accuracy: 0.832\n",
            "[Epoch 6, Batch 100/372] Train Loss: 2.474, Train Accuracy: 0.133\n",
            "[Epoch 6, Batch 200/372] Train Loss: 2.951, Train Accuracy: 0.148\n",
            "[Epoch 6, Batch 300/372] Train Loss: 2.703, Train Accuracy: 0.117\n",
            "Epoch 6 Validation Accuracy: 0.847\n",
            "[Epoch 7, Batch 100/372] Train Loss: 2.365, Train Accuracy: 0.133\n",
            "[Epoch 7, Batch 200/372] Train Loss: 2.690, Train Accuracy: 0.086\n",
            "[Epoch 7, Batch 300/372] Train Loss: 2.829, Train Accuracy: 0.609\n",
            "Epoch 7 Validation Accuracy: 0.846\n",
            "[Epoch 8, Batch 100/372] Train Loss: 2.501, Train Accuracy: 0.664\n",
            "[Epoch 8, Batch 200/372] Train Loss: 2.072, Train Accuracy: 0.133\n",
            "[Epoch 8, Batch 300/372] Train Loss: 2.237, Train Accuracy: 0.773\n",
            "Epoch 8 Validation Accuracy: 0.858\n",
            "[Epoch 9, Batch 100/372] Train Loss: 2.503, Train Accuracy: 0.195\n",
            "[Epoch 9, Batch 200/372] Train Loss: 2.395, Train Accuracy: 0.117\n",
            "[Epoch 9, Batch 300/372] Train Loss: 2.954, Train Accuracy: 0.781\n",
            "Epoch 9 Validation Accuracy: 0.862\n",
            "[Epoch 10, Batch 100/372] Train Loss: 2.271, Train Accuracy: 0.125\n",
            "[Epoch 10, Batch 200/372] Train Loss: 2.810, Train Accuracy: 0.203\n",
            "[Epoch 10, Batch 300/372] Train Loss: 2.727, Train Accuracy: 0.188\n",
            "Epoch 10 Validation Accuracy: 0.857\n",
            "[Epoch 11, Batch 100/372] Train Loss: 2.880, Train Accuracy: 0.266\n",
            "[Epoch 11, Batch 200/372] Train Loss: 2.778, Train Accuracy: 0.320\n",
            "[Epoch 11, Batch 300/372] Train Loss: 2.210, Train Accuracy: 0.109\n",
            "Epoch 11 Validation Accuracy: 0.854\n",
            "[Epoch 12, Batch 100/372] Train Loss: 2.048, Train Accuracy: 0.867\n",
            "[Epoch 12, Batch 200/372] Train Loss: 2.627, Train Accuracy: 0.375\n",
            "[Epoch 12, Batch 300/372] Train Loss: 2.132, Train Accuracy: 0.820\n",
            "Epoch 12 Validation Accuracy: 0.872\n",
            "[Epoch 13, Batch 100/372] Train Loss: 3.036, Train Accuracy: 0.516\n",
            "[Epoch 13, Batch 200/372] Train Loss: 2.286, Train Accuracy: 0.805\n",
            "[Epoch 13, Batch 300/372] Train Loss: 2.326, Train Accuracy: 0.250\n",
            "Epoch 13 Validation Accuracy: 0.819\n",
            "[Epoch 14, Batch 100/372] Train Loss: 2.032, Train Accuracy: 0.133\n",
            "[Epoch 14, Batch 200/372] Train Loss: 1.988, Train Accuracy: 0.141\n",
            "[Epoch 14, Batch 300/372] Train Loss: 2.261, Train Accuracy: 0.188\n",
            "Epoch 14 Validation Accuracy: 0.854\n",
            "[Epoch 15, Batch 100/372] Train Loss: 1.913, Train Accuracy: 0.844\n",
            "[Epoch 15, Batch 200/372] Train Loss: 2.234, Train Accuracy: 0.781\n",
            "[Epoch 15, Batch 300/372] Train Loss: 2.005, Train Accuracy: 0.820\n",
            "Epoch 15 Validation Accuracy: 0.897\n",
            "[Epoch 16, Batch 100/372] Train Loss: 2.560, Train Accuracy: 0.453\n",
            "[Epoch 16, Batch 200/372] Train Loss: 1.639, Train Accuracy: 0.094\n",
            "[Epoch 16, Batch 300/372] Train Loss: 2.221, Train Accuracy: 0.906\n",
            "Epoch 16 Validation Accuracy: 0.884\n",
            "[Epoch 17, Batch 100/372] Train Loss: 1.812, Train Accuracy: 0.938\n",
            "[Epoch 17, Batch 200/372] Train Loss: 1.685, Train Accuracy: 0.156\n",
            "[Epoch 17, Batch 300/372] Train Loss: 2.732, Train Accuracy: 0.359\n",
            "Epoch 17 Validation Accuracy: 0.878\n",
            "[Epoch 18, Batch 100/372] Train Loss: 2.440, Train Accuracy: 0.922\n",
            "[Epoch 18, Batch 200/372] Train Loss: 1.868, Train Accuracy: 0.930\n",
            "[Epoch 18, Batch 300/372] Train Loss: 2.236, Train Accuracy: 0.773\n",
            "Epoch 18 Validation Accuracy: 0.884\n",
            "[Epoch 19, Batch 100/372] Train Loss: 2.006, Train Accuracy: 0.930\n",
            "[Epoch 19, Batch 200/372] Train Loss: 1.517, Train Accuracy: 0.930\n",
            "[Epoch 19, Batch 300/372] Train Loss: 3.134, Train Accuracy: 0.312\n",
            "Epoch 19 Validation Accuracy: 0.893\n",
            "[Epoch 20, Batch 100/372] Train Loss: 1.734, Train Accuracy: 0.133\n",
            "[Epoch 20, Batch 200/372] Train Loss: 2.236, Train Accuracy: 0.211\n",
            "[Epoch 20, Batch 300/372] Train Loss: 1.574, Train Accuracy: 0.086\n",
            "Epoch 20 Validation Accuracy: 0.876\n",
            "[Epoch 21, Batch 100/372] Train Loss: 1.343, Train Accuracy: 0.930\n",
            "[Epoch 21, Batch 200/372] Train Loss: 1.786, Train Accuracy: 0.898\n",
            "[Epoch 21, Batch 300/372] Train Loss: 2.339, Train Accuracy: 0.633\n",
            "Epoch 21 Validation Accuracy: 0.888\n",
            "[Epoch 22, Batch 100/372] Train Loss: 2.811, Train Accuracy: 0.594\n",
            "[Epoch 22, Batch 200/372] Train Loss: 2.793, Train Accuracy: 0.562\n",
            "[Epoch 22, Batch 300/372] Train Loss: 2.586, Train Accuracy: 0.367\n",
            "Epoch 22 Validation Accuracy: 0.903\n",
            "[Epoch 23, Batch 100/372] Train Loss: 2.499, Train Accuracy: 0.414\n",
            "[Epoch 23, Batch 200/372] Train Loss: 1.912, Train Accuracy: 0.656\n",
            "[Epoch 23, Batch 300/372] Train Loss: 1.706, Train Accuracy: 0.164\n",
            "Epoch 23 Validation Accuracy: 0.895\n",
            "[Epoch 24, Batch 100/372] Train Loss: 1.645, Train Accuracy: 0.148\n",
            "[Epoch 24, Batch 200/372] Train Loss: 2.376, Train Accuracy: 0.562\n",
            "[Epoch 24, Batch 300/372] Train Loss: 2.171, Train Accuracy: 0.781\n",
            "Epoch 24 Validation Accuracy: 0.893\n",
            "[Epoch 25, Batch 100/372] Train Loss: 1.082, Train Accuracy: 0.164\n",
            "[Epoch 25, Batch 200/372] Train Loss: 1.820, Train Accuracy: 0.906\n",
            "[Epoch 25, Batch 300/372] Train Loss: 1.292, Train Accuracy: 0.930\n",
            "Epoch 25 Validation Accuracy: 0.899\n",
            "Checkpoint saved at epoch 25: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_25.tar\n",
            "[Epoch 26, Batch 100/372] Train Loss: 1.946, Train Accuracy: 0.188\n",
            "[Epoch 26, Batch 200/372] Train Loss: 2.002, Train Accuracy: 0.148\n",
            "[Epoch 26, Batch 300/372] Train Loss: 1.368, Train Accuracy: 0.117\n",
            "Epoch 26 Validation Accuracy: 0.893\n",
            "[Epoch 27, Batch 100/372] Train Loss: 2.281, Train Accuracy: 0.727\n",
            "[Epoch 27, Batch 200/372] Train Loss: 1.367, Train Accuracy: 0.141\n",
            "[Epoch 27, Batch 300/372] Train Loss: 1.547, Train Accuracy: 0.906\n",
            "Epoch 27 Validation Accuracy: 0.895\n",
            "[Epoch 28, Batch 100/372] Train Loss: 1.605, Train Accuracy: 0.891\n",
            "[Epoch 28, Batch 200/372] Train Loss: 2.274, Train Accuracy: 0.633\n",
            "[Epoch 28, Batch 300/372] Train Loss: 1.222, Train Accuracy: 0.148\n",
            "Epoch 28 Validation Accuracy: 0.897\n",
            "[Epoch 29, Batch 100/372] Train Loss: 2.291, Train Accuracy: 0.320\n",
            "[Epoch 29, Batch 200/372] Train Loss: 1.204, Train Accuracy: 0.953\n",
            "[Epoch 29, Batch 300/372] Train Loss: 1.694, Train Accuracy: 0.125\n",
            "Epoch 29 Validation Accuracy: 0.902\n",
            "[Epoch 30, Batch 100/372] Train Loss: 1.634, Train Accuracy: 0.906\n",
            "[Epoch 30, Batch 200/372] Train Loss: 1.216, Train Accuracy: 0.117\n",
            "[Epoch 30, Batch 300/372] Train Loss: 1.456, Train Accuracy: 0.898\n",
            "Epoch 30 Validation Accuracy: 0.902\n",
            "[Epoch 31, Batch 100/372] Train Loss: 1.542, Train Accuracy: 0.164\n",
            "[Epoch 31, Batch 200/372] Train Loss: 1.811, Train Accuracy: 0.812\n",
            "[Epoch 31, Batch 300/372] Train Loss: 1.157, Train Accuracy: 0.969\n",
            "Epoch 31 Validation Accuracy: 0.904\n",
            "[Epoch 32, Batch 100/372] Train Loss: 1.211, Train Accuracy: 0.070\n",
            "[Epoch 32, Batch 200/372] Train Loss: 1.196, Train Accuracy: 0.961\n",
            "[Epoch 32, Batch 300/372] Train Loss: 1.467, Train Accuracy: 0.914\n",
            "Epoch 32 Validation Accuracy: 0.905\n",
            "[Epoch 33, Batch 100/372] Train Loss: 2.258, Train Accuracy: 0.492\n",
            "[Epoch 33, Batch 200/372] Train Loss: 1.410, Train Accuracy: 0.898\n",
            "[Epoch 33, Batch 300/372] Train Loss: 1.233, Train Accuracy: 0.039\n",
            "Epoch 33 Validation Accuracy: 0.911\n",
            "[Epoch 34, Batch 100/372] Train Loss: 1.097, Train Accuracy: 0.156\n",
            "[Epoch 34, Batch 200/372] Train Loss: 1.694, Train Accuracy: 0.180\n",
            "[Epoch 34, Batch 300/372] Train Loss: 1.953, Train Accuracy: 0.758\n",
            "Epoch 34 Validation Accuracy: 0.908\n",
            "[Epoch 35, Batch 100/372] Train Loss: 2.360, Train Accuracy: 0.453\n",
            "[Epoch 35, Batch 200/372] Train Loss: 1.338, Train Accuracy: 0.945\n",
            "[Epoch 35, Batch 300/372] Train Loss: 1.282, Train Accuracy: 0.977\n",
            "Epoch 35 Validation Accuracy: 0.906\n",
            "[Epoch 36, Batch 100/372] Train Loss: 1.046, Train Accuracy: 0.977\n",
            "[Epoch 36, Batch 200/372] Train Loss: 1.071, Train Accuracy: 0.086\n",
            "[Epoch 36, Batch 300/372] Train Loss: 2.080, Train Accuracy: 0.203\n",
            "Epoch 36 Validation Accuracy: 0.900\n",
            "[Epoch 37, Batch 100/372] Train Loss: 1.121, Train Accuracy: 0.945\n",
            "[Epoch 37, Batch 200/372] Train Loss: 1.511, Train Accuracy: 0.117\n",
            "[Epoch 37, Batch 300/372] Train Loss: 1.013, Train Accuracy: 0.945\n",
            "Epoch 37 Validation Accuracy: 0.902\n",
            "[Epoch 38, Batch 100/372] Train Loss: 1.390, Train Accuracy: 0.938\n",
            "[Epoch 38, Batch 200/372] Train Loss: 2.567, Train Accuracy: 0.484\n",
            "[Epoch 38, Batch 300/372] Train Loss: 1.645, Train Accuracy: 0.117\n",
            "Epoch 38 Validation Accuracy: 0.905\n",
            "[Epoch 39, Batch 100/372] Train Loss: 0.964, Train Accuracy: 0.969\n",
            "[Epoch 39, Batch 200/372] Train Loss: 1.208, Train Accuracy: 0.062\n",
            "[Epoch 39, Batch 300/372] Train Loss: 1.260, Train Accuracy: 0.125\n",
            "Epoch 39 Validation Accuracy: 0.910\n",
            "[Epoch 40, Batch 100/372] Train Loss: 1.087, Train Accuracy: 0.984\n",
            "[Epoch 40, Batch 200/372] Train Loss: 1.659, Train Accuracy: 0.086\n",
            "[Epoch 40, Batch 300/372] Train Loss: 1.188, Train Accuracy: 0.086\n",
            "Epoch 40 Validation Accuracy: 0.909\n",
            "[Epoch 41, Batch 100/372] Train Loss: 0.911, Train Accuracy: 0.102\n",
            "[Epoch 41, Batch 200/372] Train Loss: 0.890, Train Accuracy: 0.969\n",
            "[Epoch 41, Batch 300/372] Train Loss: 0.985, Train Accuracy: 0.086\n",
            "Epoch 41 Validation Accuracy: 0.912\n",
            "[Epoch 42, Batch 100/372] Train Loss: 1.030, Train Accuracy: 0.930\n",
            "[Epoch 42, Batch 200/372] Train Loss: 1.337, Train Accuracy: 0.117\n",
            "[Epoch 42, Batch 300/372] Train Loss: 2.260, Train Accuracy: 0.414\n",
            "Epoch 42 Validation Accuracy: 0.910\n",
            "[Epoch 43, Batch 100/372] Train Loss: 0.921, Train Accuracy: 0.094\n",
            "[Epoch 43, Batch 200/372] Train Loss: 2.014, Train Accuracy: 0.516\n",
            "[Epoch 43, Batch 300/372] Train Loss: 1.899, Train Accuracy: 0.742\n",
            "Epoch 43 Validation Accuracy: 0.908\n",
            "[Epoch 44, Batch 100/372] Train Loss: 0.872, Train Accuracy: 0.094\n",
            "[Epoch 44, Batch 200/372] Train Loss: 0.926, Train Accuracy: 0.055\n",
            "[Epoch 44, Batch 300/372] Train Loss: 1.410, Train Accuracy: 0.914\n",
            "Epoch 44 Validation Accuracy: 0.908\n",
            "[Epoch 45, Batch 100/372] Train Loss: 0.997, Train Accuracy: 0.938\n",
            "[Epoch 45, Batch 200/372] Train Loss: 1.000, Train Accuracy: 0.930\n",
            "[Epoch 45, Batch 300/372] Train Loss: 1.305, Train Accuracy: 0.891\n",
            "Epoch 45 Validation Accuracy: 0.918\n",
            "[Epoch 46, Batch 100/372] Train Loss: 1.435, Train Accuracy: 0.156\n",
            "[Epoch 46, Batch 200/372] Train Loss: 1.172, Train Accuracy: 0.914\n",
            "[Epoch 46, Batch 300/372] Train Loss: 1.712, Train Accuracy: 0.156\n",
            "Epoch 46 Validation Accuracy: 0.916\n",
            "[Epoch 47, Batch 100/372] Train Loss: 0.888, Train Accuracy: 0.961\n",
            "[Epoch 47, Batch 200/372] Train Loss: 0.799, Train Accuracy: 0.117\n",
            "[Epoch 47, Batch 300/372] Train Loss: 0.988, Train Accuracy: 0.977\n",
            "Epoch 47 Validation Accuracy: 0.912\n",
            "[Epoch 48, Batch 100/372] Train Loss: 0.818, Train Accuracy: 0.117\n",
            "[Epoch 48, Batch 200/372] Train Loss: 0.934, Train Accuracy: 0.953\n",
            "[Epoch 48, Batch 300/372] Train Loss: 1.457, Train Accuracy: 0.906\n",
            "Epoch 48 Validation Accuracy: 0.916\n",
            "[Epoch 49, Batch 100/372] Train Loss: 1.740, Train Accuracy: 0.398\n",
            "[Epoch 49, Batch 200/372] Train Loss: 2.062, Train Accuracy: 0.227\n",
            "[Epoch 49, Batch 300/372] Train Loss: 1.994, Train Accuracy: 0.734\n",
            "Epoch 49 Validation Accuracy: 0.922\n",
            "[Epoch 50, Batch 100/372] Train Loss: 1.438, Train Accuracy: 0.109\n",
            "[Epoch 50, Batch 200/372] Train Loss: 1.605, Train Accuracy: 0.922\n",
            "[Epoch 50, Batch 300/372] Train Loss: 2.022, Train Accuracy: 0.258\n",
            "Epoch 50 Validation Accuracy: 0.921\n",
            "Checkpoint saved at epoch 50: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_50.tar\n",
            "[Epoch 51, Batch 100/372] Train Loss: 0.823, Train Accuracy: 0.984\n",
            "[Epoch 51, Batch 200/372] Train Loss: 0.760, Train Accuracy: 0.977\n",
            "[Epoch 51, Batch 300/372] Train Loss: 1.705, Train Accuracy: 0.133\n",
            "Epoch 51 Validation Accuracy: 0.909\n",
            "[Epoch 52, Batch 100/372] Train Loss: 1.482, Train Accuracy: 0.898\n",
            "[Epoch 52, Batch 200/372] Train Loss: 1.467, Train Accuracy: 0.891\n",
            "[Epoch 52, Batch 300/372] Train Loss: 1.169, Train Accuracy: 0.141\n",
            "Epoch 52 Validation Accuracy: 0.920\n",
            "[Epoch 53, Batch 100/372] Train Loss: 1.063, Train Accuracy: 0.125\n",
            "[Epoch 53, Batch 200/372] Train Loss: 1.492, Train Accuracy: 0.195\n",
            "[Epoch 53, Batch 300/372] Train Loss: 2.579, Train Accuracy: 0.500\n",
            "Epoch 53 Validation Accuracy: 0.918\n",
            "[Epoch 54, Batch 100/372] Train Loss: 2.152, Train Accuracy: 0.227\n",
            "[Epoch 54, Batch 200/372] Train Loss: 0.950, Train Accuracy: 0.109\n",
            "[Epoch 54, Batch 300/372] Train Loss: 2.531, Train Accuracy: 0.594\n",
            "Epoch 54 Validation Accuracy: 0.920\n",
            "[Epoch 55, Batch 100/372] Train Loss: 1.221, Train Accuracy: 0.094\n",
            "[Epoch 55, Batch 200/372] Train Loss: 2.118, Train Accuracy: 0.680\n",
            "[Epoch 55, Batch 300/372] Train Loss: 1.176, Train Accuracy: 0.086\n",
            "Epoch 55 Validation Accuracy: 0.919\n",
            "[Epoch 56, Batch 100/372] Train Loss: 2.091, Train Accuracy: 0.711\n",
            "[Epoch 56, Batch 200/372] Train Loss: 1.042, Train Accuracy: 0.984\n",
            "[Epoch 56, Batch 300/372] Train Loss: 1.152, Train Accuracy: 0.938\n",
            "Epoch 56 Validation Accuracy: 0.922\n",
            "[Epoch 57, Batch 100/372] Train Loss: 0.669, Train Accuracy: 0.109\n",
            "[Epoch 57, Batch 200/372] Train Loss: 0.955, Train Accuracy: 0.969\n",
            "[Epoch 57, Batch 300/372] Train Loss: 1.543, Train Accuracy: 0.164\n",
            "Epoch 57 Validation Accuracy: 0.912\n",
            "[Epoch 58, Batch 100/372] Train Loss: 1.586, Train Accuracy: 0.867\n",
            "[Epoch 58, Batch 200/372] Train Loss: 0.862, Train Accuracy: 0.969\n",
            "[Epoch 58, Batch 300/372] Train Loss: 1.672, Train Accuracy: 0.859\n",
            "Epoch 58 Validation Accuracy: 0.922\n",
            "[Epoch 59, Batch 100/372] Train Loss: 1.675, Train Accuracy: 0.805\n",
            "[Epoch 59, Batch 200/372] Train Loss: 0.731, Train Accuracy: 0.969\n",
            "[Epoch 59, Batch 300/372] Train Loss: 2.181, Train Accuracy: 0.367\n",
            "Epoch 59 Validation Accuracy: 0.924\n",
            "[Epoch 60, Batch 100/372] Train Loss: 0.856, Train Accuracy: 0.984\n",
            "[Epoch 60, Batch 200/372] Train Loss: 1.006, Train Accuracy: 0.125\n",
            "[Epoch 60, Batch 300/372] Train Loss: 2.073, Train Accuracy: 0.164\n",
            "Epoch 60 Validation Accuracy: 0.908\n",
            "[Epoch 61, Batch 100/372] Train Loss: 0.922, Train Accuracy: 0.086\n",
            "[Epoch 61, Batch 200/372] Train Loss: 1.778, Train Accuracy: 0.164\n",
            "[Epoch 61, Batch 300/372] Train Loss: 1.584, Train Accuracy: 0.234\n",
            "Epoch 61 Validation Accuracy: 0.922\n",
            "[Epoch 62, Batch 100/372] Train Loss: 1.987, Train Accuracy: 0.453\n",
            "[Epoch 62, Batch 200/372] Train Loss: 1.551, Train Accuracy: 0.859\n",
            "[Epoch 62, Batch 300/372] Train Loss: 1.967, Train Accuracy: 0.312\n",
            "Epoch 62 Validation Accuracy: 0.922\n",
            "[Epoch 63, Batch 100/372] Train Loss: 0.808, Train Accuracy: 0.969\n",
            "[Epoch 63, Batch 200/372] Train Loss: 1.364, Train Accuracy: 0.906\n",
            "[Epoch 63, Batch 300/372] Train Loss: 1.323, Train Accuracy: 0.969\n",
            "Epoch 63 Validation Accuracy: 0.918\n",
            "[Epoch 64, Batch 100/372] Train Loss: 0.993, Train Accuracy: 0.086\n",
            "[Epoch 64, Batch 200/372] Train Loss: 0.863, Train Accuracy: 0.086\n",
            "[Epoch 64, Batch 300/372] Train Loss: 1.863, Train Accuracy: 0.477\n",
            "Epoch 64 Validation Accuracy: 0.914\n",
            "[Epoch 65, Batch 100/372] Train Loss: 0.707, Train Accuracy: 0.953\n",
            "[Epoch 65, Batch 200/372] Train Loss: 1.592, Train Accuracy: 0.898\n",
            "[Epoch 65, Batch 300/372] Train Loss: 2.036, Train Accuracy: 0.453\n",
            "Epoch 65 Validation Accuracy: 0.918\n",
            "[Epoch 66, Batch 100/372] Train Loss: 0.686, Train Accuracy: 0.125\n",
            "[Epoch 66, Batch 200/372] Train Loss: 1.737, Train Accuracy: 0.242\n",
            "[Epoch 66, Batch 300/372] Train Loss: 2.150, Train Accuracy: 0.609\n",
            "Epoch 66 Validation Accuracy: 0.922\n",
            "[Epoch 67, Batch 100/372] Train Loss: 1.424, Train Accuracy: 0.141\n",
            "[Epoch 67, Batch 200/372] Train Loss: 1.046, Train Accuracy: 0.984\n",
            "[Epoch 67, Batch 300/372] Train Loss: 0.867, Train Accuracy: 0.953\n",
            "Epoch 67 Validation Accuracy: 0.922\n",
            "[Epoch 68, Batch 100/372] Train Loss: 0.763, Train Accuracy: 0.078\n",
            "[Epoch 68, Batch 200/372] Train Loss: 0.909, Train Accuracy: 0.133\n",
            "[Epoch 68, Batch 300/372] Train Loss: 1.276, Train Accuracy: 0.102\n",
            "Epoch 68 Validation Accuracy: 0.924\n",
            "[Epoch 69, Batch 100/372] Train Loss: 0.755, Train Accuracy: 0.117\n",
            "[Epoch 69, Batch 200/372] Train Loss: 1.218, Train Accuracy: 0.141\n",
            "[Epoch 69, Batch 300/372] Train Loss: 0.780, Train Accuracy: 0.109\n",
            "Epoch 69 Validation Accuracy: 0.922\n",
            "[Epoch 70, Batch 100/372] Train Loss: 0.958, Train Accuracy: 0.070\n",
            "[Epoch 70, Batch 200/372] Train Loss: 1.692, Train Accuracy: 0.195\n",
            "[Epoch 70, Batch 300/372] Train Loss: 1.970, Train Accuracy: 0.453\n",
            "Epoch 70 Validation Accuracy: 0.925\n",
            "[Epoch 71, Batch 100/372] Train Loss: 1.352, Train Accuracy: 0.242\n",
            "[Epoch 71, Batch 200/372] Train Loss: 0.914, Train Accuracy: 0.961\n",
            "[Epoch 71, Batch 300/372] Train Loss: 0.628, Train Accuracy: 0.977\n",
            "Epoch 71 Validation Accuracy: 0.924\n",
            "[Epoch 72, Batch 100/372] Train Loss: 0.634, Train Accuracy: 0.984\n",
            "[Epoch 72, Batch 200/372] Train Loss: 1.581, Train Accuracy: 0.180\n",
            "[Epoch 72, Batch 300/372] Train Loss: 0.780, Train Accuracy: 0.086\n",
            "Epoch 72 Validation Accuracy: 0.927\n",
            "[Epoch 73, Batch 100/372] Train Loss: 0.975, Train Accuracy: 0.141\n",
            "[Epoch 73, Batch 200/372] Train Loss: 1.945, Train Accuracy: 0.828\n",
            "[Epoch 73, Batch 300/372] Train Loss: 0.540, Train Accuracy: 0.094\n",
            "Epoch 73 Validation Accuracy: 0.919\n",
            "[Epoch 74, Batch 100/372] Train Loss: 1.939, Train Accuracy: 0.609\n",
            "[Epoch 74, Batch 200/372] Train Loss: 0.701, Train Accuracy: 0.109\n",
            "[Epoch 74, Batch 300/372] Train Loss: 1.788, Train Accuracy: 0.734\n",
            "Epoch 74 Validation Accuracy: 0.917\n",
            "[Epoch 75, Batch 100/372] Train Loss: 1.448, Train Accuracy: 0.914\n",
            "[Epoch 75, Batch 200/372] Train Loss: 0.678, Train Accuracy: 0.117\n",
            "[Epoch 75, Batch 300/372] Train Loss: 1.742, Train Accuracy: 0.156\n",
            "Epoch 75 Validation Accuracy: 0.917\n",
            "Checkpoint saved at epoch 75: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_75.tar\n",
            "[Epoch 76, Batch 100/372] Train Loss: 1.886, Train Accuracy: 0.180\n",
            "[Epoch 76, Batch 200/372] Train Loss: 1.205, Train Accuracy: 0.906\n",
            "[Epoch 76, Batch 300/372] Train Loss: 1.468, Train Accuracy: 0.156\n",
            "Epoch 76 Validation Accuracy: 0.928\n",
            "[Epoch 77, Batch 100/372] Train Loss: 1.922, Train Accuracy: 0.555\n",
            "[Epoch 77, Batch 200/372] Train Loss: 1.906, Train Accuracy: 0.305\n",
            "[Epoch 77, Batch 300/372] Train Loss: 0.703, Train Accuracy: 0.055\n",
            "Epoch 77 Validation Accuracy: 0.918\n",
            "[Epoch 78, Batch 100/372] Train Loss: 2.050, Train Accuracy: 0.406\n",
            "[Epoch 78, Batch 200/372] Train Loss: 0.740, Train Accuracy: 0.945\n",
            "[Epoch 78, Batch 300/372] Train Loss: 0.636, Train Accuracy: 0.984\n",
            "Epoch 78 Validation Accuracy: 0.926\n",
            "[Epoch 79, Batch 100/372] Train Loss: 0.674, Train Accuracy: 0.164\n",
            "[Epoch 79, Batch 200/372] Train Loss: 0.565, Train Accuracy: 0.070\n",
            "[Epoch 79, Batch 300/372] Train Loss: 0.706, Train Accuracy: 0.984\n",
            "Epoch 79 Validation Accuracy: 0.930\n",
            "[Epoch 80, Batch 100/372] Train Loss: 0.559, Train Accuracy: 0.172\n",
            "[Epoch 80, Batch 200/372] Train Loss: 1.258, Train Accuracy: 0.102\n",
            "[Epoch 80, Batch 300/372] Train Loss: 1.856, Train Accuracy: 0.477\n",
            "Epoch 80 Validation Accuracy: 0.893\n",
            "[Epoch 81, Batch 100/372] Train Loss: 1.078, Train Accuracy: 0.930\n",
            "[Epoch 81, Batch 200/372] Train Loss: 0.619, Train Accuracy: 0.078\n",
            "[Epoch 81, Batch 300/372] Train Loss: 2.111, Train Accuracy: 0.461\n",
            "Epoch 81 Validation Accuracy: 0.925\n",
            "[Epoch 82, Batch 100/372] Train Loss: 2.187, Train Accuracy: 0.375\n",
            "[Epoch 82, Batch 200/372] Train Loss: 0.521, Train Accuracy: 1.000\n",
            "[Epoch 82, Batch 300/372] Train Loss: 1.996, Train Accuracy: 0.672\n",
            "Epoch 82 Validation Accuracy: 0.929\n",
            "[Epoch 83, Batch 100/372] Train Loss: 2.048, Train Accuracy: 0.734\n",
            "[Epoch 83, Batch 200/372] Train Loss: 1.896, Train Accuracy: 0.672\n",
            "[Epoch 83, Batch 300/372] Train Loss: 0.761, Train Accuracy: 0.102\n",
            "Epoch 83 Validation Accuracy: 0.923\n",
            "[Epoch 84, Batch 100/372] Train Loss: 1.567, Train Accuracy: 0.258\n",
            "[Epoch 84, Batch 200/372] Train Loss: 1.248, Train Accuracy: 0.148\n",
            "[Epoch 84, Batch 300/372] Train Loss: 0.634, Train Accuracy: 0.094\n",
            "Epoch 84 Validation Accuracy: 0.926\n",
            "[Epoch 85, Batch 100/372] Train Loss: 0.558, Train Accuracy: 0.062\n",
            "[Epoch 85, Batch 200/372] Train Loss: 1.457, Train Accuracy: 0.156\n",
            "[Epoch 85, Batch 300/372] Train Loss: 1.490, Train Accuracy: 0.852\n",
            "Epoch 85 Validation Accuracy: 0.917\n",
            "[Epoch 86, Batch 100/372] Train Loss: 0.653, Train Accuracy: 0.055\n",
            "[Epoch 86, Batch 200/372] Train Loss: 0.752, Train Accuracy: 0.992\n",
            "[Epoch 86, Batch 300/372] Train Loss: 0.760, Train Accuracy: 0.984\n",
            "Epoch 86 Validation Accuracy: 0.926\n",
            "[Epoch 87, Batch 100/372] Train Loss: 0.659, Train Accuracy: 0.992\n",
            "[Epoch 87, Batch 200/372] Train Loss: 1.414, Train Accuracy: 0.141\n",
            "[Epoch 87, Batch 300/372] Train Loss: 0.529, Train Accuracy: 0.086\n",
            "Epoch 87 Validation Accuracy: 0.919\n",
            "[Epoch 88, Batch 100/372] Train Loss: 0.822, Train Accuracy: 0.945\n",
            "[Epoch 88, Batch 200/372] Train Loss: 0.643, Train Accuracy: 0.117\n",
            "[Epoch 88, Batch 300/372] Train Loss: 0.637, Train Accuracy: 0.094\n",
            "Epoch 88 Validation Accuracy: 0.927\n",
            "[Epoch 89, Batch 100/372] Train Loss: 1.407, Train Accuracy: 0.172\n",
            "[Epoch 89, Batch 200/372] Train Loss: 0.661, Train Accuracy: 0.180\n",
            "[Epoch 89, Batch 300/372] Train Loss: 0.935, Train Accuracy: 0.969\n",
            "Epoch 89 Validation Accuracy: 0.925\n",
            "[Epoch 90, Batch 100/372] Train Loss: 0.521, Train Accuracy: 0.984\n",
            "[Epoch 90, Batch 200/372] Train Loss: 2.179, Train Accuracy: 0.352\n",
            "[Epoch 90, Batch 300/372] Train Loss: 0.819, Train Accuracy: 0.102\n",
            "Epoch 90 Validation Accuracy: 0.922\n",
            "[Epoch 91, Batch 100/372] Train Loss: 2.275, Train Accuracy: 0.531\n",
            "[Epoch 91, Batch 200/372] Train Loss: 0.804, Train Accuracy: 0.984\n",
            "[Epoch 91, Batch 300/372] Train Loss: 0.741, Train Accuracy: 0.961\n",
            "Epoch 91 Validation Accuracy: 0.924\n",
            "[Epoch 92, Batch 100/372] Train Loss: 1.757, Train Accuracy: 0.836\n",
            "[Epoch 92, Batch 200/372] Train Loss: 1.977, Train Accuracy: 0.695\n",
            "[Epoch 92, Batch 300/372] Train Loss: 1.599, Train Accuracy: 0.203\n",
            "Epoch 92 Validation Accuracy: 0.930\n",
            "[Epoch 93, Batch 100/372] Train Loss: 0.607, Train Accuracy: 0.039\n",
            "[Epoch 93, Batch 200/372] Train Loss: 1.404, Train Accuracy: 0.820\n",
            "[Epoch 93, Batch 300/372] Train Loss: 0.497, Train Accuracy: 0.117\n",
            "Epoch 93 Validation Accuracy: 0.921\n",
            "[Epoch 94, Batch 100/372] Train Loss: 0.455, Train Accuracy: 0.992\n",
            "[Epoch 94, Batch 200/372] Train Loss: 1.694, Train Accuracy: 0.453\n",
            "[Epoch 94, Batch 300/372] Train Loss: 0.567, Train Accuracy: 0.094\n",
            "Epoch 94 Validation Accuracy: 0.923\n",
            "[Epoch 95, Batch 100/372] Train Loss: 0.619, Train Accuracy: 0.977\n",
            "[Epoch 95, Batch 200/372] Train Loss: 0.610, Train Accuracy: 0.078\n",
            "[Epoch 95, Batch 300/372] Train Loss: 0.593, Train Accuracy: 0.992\n",
            "Epoch 95 Validation Accuracy: 0.929\n",
            "[Epoch 96, Batch 100/372] Train Loss: 0.875, Train Accuracy: 0.117\n",
            "[Epoch 96, Batch 200/372] Train Loss: 1.293, Train Accuracy: 0.930\n",
            "[Epoch 96, Batch 300/372] Train Loss: 0.747, Train Accuracy: 0.125\n",
            "Epoch 96 Validation Accuracy: 0.928\n",
            "[Epoch 97, Batch 100/372] Train Loss: 0.814, Train Accuracy: 0.945\n",
            "[Epoch 97, Batch 200/372] Train Loss: 0.617, Train Accuracy: 0.977\n",
            "[Epoch 97, Batch 300/372] Train Loss: 1.890, Train Accuracy: 0.242\n",
            "Epoch 97 Validation Accuracy: 0.925\n",
            "[Epoch 98, Batch 100/372] Train Loss: 0.600, Train Accuracy: 0.984\n",
            "[Epoch 98, Batch 200/372] Train Loss: 1.781, Train Accuracy: 0.367\n",
            "[Epoch 98, Batch 300/372] Train Loss: 0.564, Train Accuracy: 0.992\n",
            "Epoch 98 Validation Accuracy: 0.920\n",
            "[Epoch 99, Batch 100/372] Train Loss: 2.004, Train Accuracy: 0.438\n",
            "[Epoch 99, Batch 200/372] Train Loss: 0.778, Train Accuracy: 0.148\n",
            "[Epoch 99, Batch 300/372] Train Loss: 0.784, Train Accuracy: 0.109\n",
            "Epoch 99 Validation Accuracy: 0.927\n",
            "[Epoch 100, Batch 100/372] Train Loss: 0.892, Train Accuracy: 0.953\n",
            "[Epoch 100, Batch 200/372] Train Loss: 0.690, Train Accuracy: 0.117\n",
            "[Epoch 100, Batch 300/372] Train Loss: 0.635, Train Accuracy: 0.031\n",
            "Epoch 100 Validation Accuracy: 0.924\n",
            "Checkpoint saved at epoch 100: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_100.tar\n",
            "[Epoch 101, Batch 100/372] Train Loss: 1.516, Train Accuracy: 0.156\n",
            "[Epoch 101, Batch 200/372] Train Loss: 0.983, Train Accuracy: 0.961\n",
            "[Epoch 101, Batch 300/372] Train Loss: 1.387, Train Accuracy: 0.875\n",
            "Epoch 101 Validation Accuracy: 0.927\n",
            "[Epoch 102, Batch 100/372] Train Loss: 0.976, Train Accuracy: 0.945\n",
            "[Epoch 102, Batch 200/372] Train Loss: 0.517, Train Accuracy: 0.094\n",
            "[Epoch 102, Batch 300/372] Train Loss: 0.633, Train Accuracy: 0.992\n",
            "Epoch 102 Validation Accuracy: 0.922\n",
            "[Epoch 103, Batch 100/372] Train Loss: 0.604, Train Accuracy: 0.977\n",
            "[Epoch 103, Batch 200/372] Train Loss: 0.823, Train Accuracy: 0.117\n",
            "[Epoch 103, Batch 300/372] Train Loss: 0.644, Train Accuracy: 0.992\n",
            "Epoch 103 Validation Accuracy: 0.925\n",
            "[Epoch 104, Batch 100/372] Train Loss: 0.781, Train Accuracy: 0.125\n",
            "[Epoch 104, Batch 200/372] Train Loss: 0.592, Train Accuracy: 0.102\n",
            "[Epoch 104, Batch 300/372] Train Loss: 0.452, Train Accuracy: 0.102\n",
            "Epoch 104 Validation Accuracy: 0.914\n",
            "[Epoch 105, Batch 100/372] Train Loss: 0.848, Train Accuracy: 0.961\n",
            "[Epoch 105, Batch 200/372] Train Loss: 2.109, Train Accuracy: 0.250\n",
            "[Epoch 105, Batch 300/372] Train Loss: 0.482, Train Accuracy: 0.977\n",
            "Epoch 105 Validation Accuracy: 0.928\n",
            "[Epoch 106, Batch 100/372] Train Loss: 1.653, Train Accuracy: 0.656\n",
            "[Epoch 106, Batch 200/372] Train Loss: 1.915, Train Accuracy: 0.570\n",
            "[Epoch 106, Batch 300/372] Train Loss: 0.573, Train Accuracy: 0.102\n",
            "Epoch 106 Validation Accuracy: 0.927\n",
            "[Epoch 107, Batch 100/372] Train Loss: 0.664, Train Accuracy: 0.977\n",
            "[Epoch 107, Batch 200/372] Train Loss: 1.696, Train Accuracy: 0.172\n",
            "[Epoch 107, Batch 300/372] Train Loss: 0.622, Train Accuracy: 0.156\n",
            "Epoch 107 Validation Accuracy: 0.920\n",
            "[Epoch 108, Batch 100/372] Train Loss: 1.554, Train Accuracy: 0.180\n",
            "[Epoch 108, Batch 200/372] Train Loss: 0.474, Train Accuracy: 0.102\n",
            "[Epoch 108, Batch 300/372] Train Loss: 1.763, Train Accuracy: 0.727\n",
            "Epoch 108 Validation Accuracy: 0.929\n",
            "[Epoch 109, Batch 100/372] Train Loss: 0.499, Train Accuracy: 0.984\n",
            "[Epoch 109, Batch 200/372] Train Loss: 0.446, Train Accuracy: 0.992\n",
            "[Epoch 109, Batch 300/372] Train Loss: 1.440, Train Accuracy: 0.195\n",
            "Epoch 109 Validation Accuracy: 0.927\n",
            "[Epoch 110, Batch 100/372] Train Loss: 0.671, Train Accuracy: 0.992\n",
            "[Epoch 110, Batch 200/372] Train Loss: 0.633, Train Accuracy: 0.078\n",
            "[Epoch 110, Batch 300/372] Train Loss: 1.030, Train Accuracy: 0.938\n",
            "Epoch 110 Validation Accuracy: 0.909\n",
            "[Epoch 111, Batch 100/372] Train Loss: 0.985, Train Accuracy: 0.945\n",
            "[Epoch 111, Batch 200/372] Train Loss: 1.719, Train Accuracy: 0.180\n",
            "[Epoch 111, Batch 300/372] Train Loss: 0.566, Train Accuracy: 0.109\n",
            "Epoch 111 Validation Accuracy: 0.921\n",
            "[Epoch 112, Batch 100/372] Train Loss: 1.623, Train Accuracy: 0.438\n",
            "[Epoch 112, Batch 200/372] Train Loss: 1.045, Train Accuracy: 0.094\n",
            "[Epoch 112, Batch 300/372] Train Loss: 0.746, Train Accuracy: 0.977\n",
            "Epoch 112 Validation Accuracy: 0.928\n",
            "[Epoch 113, Batch 100/372] Train Loss: 1.603, Train Accuracy: 0.250\n",
            "[Epoch 113, Batch 200/372] Train Loss: 0.500, Train Accuracy: 0.070\n",
            "[Epoch 113, Batch 300/372] Train Loss: 1.281, Train Accuracy: 0.883\n",
            "Epoch 113 Validation Accuracy: 0.930\n",
            "[Epoch 114, Batch 100/372] Train Loss: 0.491, Train Accuracy: 1.000\n",
            "[Epoch 114, Batch 200/372] Train Loss: 1.003, Train Accuracy: 0.898\n",
            "[Epoch 114, Batch 300/372] Train Loss: 1.956, Train Accuracy: 0.227\n",
            "Epoch 114 Validation Accuracy: 0.924\n",
            "[Epoch 115, Batch 100/372] Train Loss: 1.741, Train Accuracy: 0.305\n",
            "[Epoch 115, Batch 200/372] Train Loss: 1.500, Train Accuracy: 0.836\n",
            "[Epoch 115, Batch 300/372] Train Loss: 0.807, Train Accuracy: 0.977\n",
            "Epoch 115 Validation Accuracy: 0.930\n",
            "[Epoch 116, Batch 100/372] Train Loss: 1.748, Train Accuracy: 0.180\n",
            "[Epoch 116, Batch 200/372] Train Loss: 1.292, Train Accuracy: 0.906\n",
            "[Epoch 116, Batch 300/372] Train Loss: 1.279, Train Accuracy: 0.164\n",
            "Epoch 116 Validation Accuracy: 0.927\n",
            "[Epoch 117, Batch 100/372] Train Loss: 0.500, Train Accuracy: 0.070\n",
            "[Epoch 117, Batch 200/372] Train Loss: 1.337, Train Accuracy: 0.883\n",
            "[Epoch 117, Batch 300/372] Train Loss: 2.028, Train Accuracy: 0.547\n",
            "Epoch 117 Validation Accuracy: 0.921\n",
            "[Epoch 118, Batch 100/372] Train Loss: 0.469, Train Accuracy: 0.977\n",
            "[Epoch 118, Batch 200/372] Train Loss: 1.976, Train Accuracy: 0.484\n",
            "[Epoch 118, Batch 300/372] Train Loss: 1.656, Train Accuracy: 0.344\n",
            "Epoch 118 Validation Accuracy: 0.930\n",
            "[Epoch 119, Batch 100/372] Train Loss: 1.423, Train Accuracy: 0.219\n",
            "[Epoch 119, Batch 200/372] Train Loss: 0.436, Train Accuracy: 0.102\n",
            "[Epoch 119, Batch 300/372] Train Loss: 0.803, Train Accuracy: 0.969\n",
            "Epoch 119 Validation Accuracy: 0.933\n",
            "[Epoch 120, Batch 100/372] Train Loss: 1.599, Train Accuracy: 0.297\n",
            "[Epoch 120, Batch 200/372] Train Loss: 0.964, Train Accuracy: 0.961\n",
            "[Epoch 120, Batch 300/372] Train Loss: 0.502, Train Accuracy: 0.086\n",
            "Epoch 120 Validation Accuracy: 0.926\n",
            "[Epoch 121, Batch 100/372] Train Loss: 0.464, Train Accuracy: 0.055\n",
            "[Epoch 121, Batch 200/372] Train Loss: 0.530, Train Accuracy: 0.992\n",
            "[Epoch 121, Batch 300/372] Train Loss: 1.372, Train Accuracy: 0.172\n",
            "Epoch 121 Validation Accuracy: 0.933\n",
            "[Epoch 122, Batch 100/372] Train Loss: 0.545, Train Accuracy: 0.133\n",
            "[Epoch 122, Batch 200/372] Train Loss: 0.489, Train Accuracy: 0.133\n",
            "[Epoch 122, Batch 300/372] Train Loss: 0.411, Train Accuracy: 0.125\n",
            "Epoch 122 Validation Accuracy: 0.940\n",
            "[Epoch 123, Batch 100/372] Train Loss: 1.299, Train Accuracy: 0.125\n",
            "[Epoch 123, Batch 200/372] Train Loss: 1.286, Train Accuracy: 0.164\n",
            "[Epoch 123, Batch 300/372] Train Loss: 0.995, Train Accuracy: 0.977\n",
            "Epoch 123 Validation Accuracy: 0.931\n",
            "[Epoch 124, Batch 100/372] Train Loss: 0.605, Train Accuracy: 0.977\n",
            "[Epoch 124, Batch 200/372] Train Loss: 0.520, Train Accuracy: 0.102\n",
            "[Epoch 124, Batch 300/372] Train Loss: 0.495, Train Accuracy: 0.062\n",
            "Epoch 124 Validation Accuracy: 0.924\n",
            "[Epoch 125, Batch 100/372] Train Loss: 0.568, Train Accuracy: 0.062\n",
            "[Epoch 125, Batch 200/372] Train Loss: 0.484, Train Accuracy: 0.125\n",
            "[Epoch 125, Batch 300/372] Train Loss: 0.420, Train Accuracy: 0.992\n",
            "Epoch 125 Validation Accuracy: 0.929\n",
            "Checkpoint saved at epoch 125: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_125.tar\n",
            "[Epoch 126, Batch 100/372] Train Loss: 1.965, Train Accuracy: 0.750\n",
            "[Epoch 126, Batch 200/372] Train Loss: 0.595, Train Accuracy: 0.977\n",
            "[Epoch 126, Batch 300/372] Train Loss: 0.521, Train Accuracy: 0.086\n",
            "Epoch 126 Validation Accuracy: 0.922\n",
            "[Epoch 127, Batch 100/372] Train Loss: 0.491, Train Accuracy: 1.000\n",
            "[Epoch 127, Batch 200/372] Train Loss: 1.727, Train Accuracy: 0.664\n",
            "[Epoch 127, Batch 300/372] Train Loss: 0.600, Train Accuracy: 0.969\n",
            "Epoch 127 Validation Accuracy: 0.926\n",
            "[Epoch 128, Batch 100/372] Train Loss: 0.685, Train Accuracy: 0.094\n",
            "[Epoch 128, Batch 200/372] Train Loss: 1.275, Train Accuracy: 0.148\n",
            "[Epoch 128, Batch 300/372] Train Loss: 1.781, Train Accuracy: 0.289\n",
            "Epoch 128 Validation Accuracy: 0.929\n",
            "[Epoch 129, Batch 100/372] Train Loss: 0.961, Train Accuracy: 0.133\n",
            "[Epoch 129, Batch 200/372] Train Loss: 0.437, Train Accuracy: 0.109\n",
            "[Epoch 129, Batch 300/372] Train Loss: 1.754, Train Accuracy: 0.500\n",
            "Epoch 129 Validation Accuracy: 0.934\n",
            "[Epoch 130, Batch 100/372] Train Loss: 0.660, Train Accuracy: 0.070\n",
            "[Epoch 130, Batch 200/372] Train Loss: 1.262, Train Accuracy: 0.234\n",
            "[Epoch 130, Batch 300/372] Train Loss: 1.675, Train Accuracy: 0.227\n",
            "Epoch 130 Validation Accuracy: 0.926\n",
            "[Epoch 131, Batch 100/372] Train Loss: 0.528, Train Accuracy: 0.102\n",
            "[Epoch 131, Batch 200/372] Train Loss: 1.533, Train Accuracy: 0.578\n",
            "[Epoch 131, Batch 300/372] Train Loss: 0.849, Train Accuracy: 0.961\n",
            "Epoch 131 Validation Accuracy: 0.932\n",
            "[Epoch 132, Batch 100/372] Train Loss: 1.414, Train Accuracy: 0.211\n",
            "[Epoch 132, Batch 200/372] Train Loss: 0.505, Train Accuracy: 0.109\n",
            "[Epoch 132, Batch 300/372] Train Loss: 0.327, Train Accuracy: 0.992\n",
            "Epoch 132 Validation Accuracy: 0.929\n",
            "[Epoch 133, Batch 100/372] Train Loss: 0.702, Train Accuracy: 0.102\n",
            "[Epoch 133, Batch 200/372] Train Loss: 1.144, Train Accuracy: 0.906\n",
            "[Epoch 133, Batch 300/372] Train Loss: 0.788, Train Accuracy: 0.141\n",
            "Epoch 133 Validation Accuracy: 0.928\n",
            "[Epoch 134, Batch 100/372] Train Loss: 0.594, Train Accuracy: 0.977\n",
            "[Epoch 134, Batch 200/372] Train Loss: 0.685, Train Accuracy: 0.070\n",
            "[Epoch 134, Batch 300/372] Train Loss: 0.896, Train Accuracy: 0.914\n",
            "Epoch 134 Validation Accuracy: 0.934\n",
            "[Epoch 135, Batch 100/372] Train Loss: 0.498, Train Accuracy: 0.125\n",
            "[Epoch 135, Batch 200/372] Train Loss: 1.886, Train Accuracy: 0.391\n",
            "[Epoch 135, Batch 300/372] Train Loss: 0.655, Train Accuracy: 0.977\n",
            "Epoch 135 Validation Accuracy: 0.928\n",
            "[Epoch 136, Batch 100/372] Train Loss: 0.640, Train Accuracy: 0.062\n",
            "[Epoch 136, Batch 200/372] Train Loss: 0.681, Train Accuracy: 1.000\n",
            "[Epoch 136, Batch 300/372] Train Loss: 0.471, Train Accuracy: 0.984\n",
            "Epoch 136 Validation Accuracy: 0.932\n",
            "[Epoch 137, Batch 100/372] Train Loss: 2.020, Train Accuracy: 0.352\n",
            "[Epoch 137, Batch 200/372] Train Loss: 1.208, Train Accuracy: 0.172\n",
            "[Epoch 137, Batch 300/372] Train Loss: 1.919, Train Accuracy: 0.633\n",
            "Epoch 137 Validation Accuracy: 0.929\n",
            "[Epoch 138, Batch 100/372] Train Loss: 0.459, Train Accuracy: 0.984\n",
            "[Epoch 138, Batch 200/372] Train Loss: 1.774, Train Accuracy: 0.297\n",
            "[Epoch 138, Batch 300/372] Train Loss: 0.620, Train Accuracy: 0.148\n",
            "Epoch 138 Validation Accuracy: 0.932\n",
            "[Epoch 139, Batch 100/372] Train Loss: 0.513, Train Accuracy: 0.977\n",
            "[Epoch 139, Batch 200/372] Train Loss: 0.970, Train Accuracy: 0.938\n",
            "[Epoch 139, Batch 300/372] Train Loss: 0.630, Train Accuracy: 0.102\n",
            "Epoch 139 Validation Accuracy: 0.927\n",
            "[Epoch 140, Batch 100/372] Train Loss: 0.470, Train Accuracy: 0.992\n",
            "[Epoch 140, Batch 200/372] Train Loss: 0.867, Train Accuracy: 0.969\n",
            "[Epoch 140, Batch 300/372] Train Loss: 0.419, Train Accuracy: 0.109\n",
            "Epoch 140 Validation Accuracy: 0.936\n",
            "[Epoch 141, Batch 100/372] Train Loss: 0.669, Train Accuracy: 1.000\n",
            "[Epoch 141, Batch 200/372] Train Loss: 0.974, Train Accuracy: 0.945\n",
            "[Epoch 141, Batch 300/372] Train Loss: 0.454, Train Accuracy: 0.078\n",
            "Epoch 141 Validation Accuracy: 0.926\n",
            "[Epoch 142, Batch 100/372] Train Loss: 0.414, Train Accuracy: 0.070\n",
            "[Epoch 142, Batch 200/372] Train Loss: 0.461, Train Accuracy: 0.992\n",
            "[Epoch 142, Batch 300/372] Train Loss: 0.401, Train Accuracy: 0.094\n",
            "Epoch 142 Validation Accuracy: 0.927\n",
            "[Epoch 143, Batch 100/372] Train Loss: 1.489, Train Accuracy: 0.859\n",
            "[Epoch 143, Batch 200/372] Train Loss: 0.424, Train Accuracy: 0.992\n",
            "[Epoch 143, Batch 300/372] Train Loss: 1.810, Train Accuracy: 0.305\n",
            "Epoch 143 Validation Accuracy: 0.930\n",
            "[Epoch 144, Batch 100/372] Train Loss: 0.748, Train Accuracy: 0.094\n",
            "[Epoch 144, Batch 200/372] Train Loss: 0.429, Train Accuracy: 0.117\n",
            "[Epoch 144, Batch 300/372] Train Loss: 2.002, Train Accuracy: 0.672\n",
            "Epoch 144 Validation Accuracy: 0.928\n",
            "[Epoch 145, Batch 100/372] Train Loss: 1.765, Train Accuracy: 0.250\n",
            "[Epoch 145, Batch 200/372] Train Loss: 1.802, Train Accuracy: 0.453\n",
            "[Epoch 145, Batch 300/372] Train Loss: 1.655, Train Accuracy: 0.797\n",
            "Epoch 145 Validation Accuracy: 0.928\n",
            "[Epoch 146, Batch 100/372] Train Loss: 1.765, Train Accuracy: 0.352\n",
            "[Epoch 146, Batch 200/372] Train Loss: 0.438, Train Accuracy: 0.125\n",
            "[Epoch 146, Batch 300/372] Train Loss: 0.984, Train Accuracy: 0.094\n",
            "Epoch 146 Validation Accuracy: 0.922\n",
            "[Epoch 147, Batch 100/372] Train Loss: 0.508, Train Accuracy: 1.000\n",
            "[Epoch 147, Batch 200/372] Train Loss: 0.492, Train Accuracy: 0.992\n",
            "[Epoch 147, Batch 300/372] Train Loss: 1.851, Train Accuracy: 0.250\n",
            "Epoch 147 Validation Accuracy: 0.928\n",
            "[Epoch 148, Batch 100/372] Train Loss: 0.510, Train Accuracy: 0.188\n",
            "[Epoch 148, Batch 200/372] Train Loss: 1.295, Train Accuracy: 0.156\n",
            "[Epoch 148, Batch 300/372] Train Loss: 1.084, Train Accuracy: 0.156\n",
            "Epoch 148 Validation Accuracy: 0.928\n",
            "[Epoch 149, Batch 100/372] Train Loss: 0.867, Train Accuracy: 0.094\n",
            "[Epoch 149, Batch 200/372] Train Loss: 0.453, Train Accuracy: 0.086\n",
            "[Epoch 149, Batch 300/372] Train Loss: 0.302, Train Accuracy: 0.984\n",
            "Epoch 149 Validation Accuracy: 0.929\n",
            "[Epoch 150, Batch 100/372] Train Loss: 0.619, Train Accuracy: 0.984\n",
            "[Epoch 150, Batch 200/372] Train Loss: 1.037, Train Accuracy: 0.164\n",
            "[Epoch 150, Batch 300/372] Train Loss: 1.325, Train Accuracy: 0.148\n",
            "Epoch 150 Validation Accuracy: 0.921\n",
            "Checkpoint saved at epoch 150: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_150.tar\n",
            "[Epoch 151, Batch 100/372] Train Loss: 1.488, Train Accuracy: 0.789\n",
            "[Epoch 151, Batch 200/372] Train Loss: 0.544, Train Accuracy: 0.977\n",
            "[Epoch 151, Batch 300/372] Train Loss: 0.409, Train Accuracy: 1.000\n",
            "Epoch 151 Validation Accuracy: 0.929\n",
            "[Epoch 152, Batch 100/372] Train Loss: 0.418, Train Accuracy: 1.000\n",
            "[Epoch 152, Batch 200/372] Train Loss: 1.495, Train Accuracy: 0.180\n",
            "[Epoch 152, Batch 300/372] Train Loss: 0.548, Train Accuracy: 0.070\n",
            "Epoch 152 Validation Accuracy: 0.930\n",
            "[Epoch 153, Batch 100/372] Train Loss: 1.038, Train Accuracy: 0.945\n",
            "[Epoch 153, Batch 200/372] Train Loss: 1.279, Train Accuracy: 0.906\n",
            "[Epoch 153, Batch 300/372] Train Loss: 1.663, Train Accuracy: 0.273\n",
            "Epoch 153 Validation Accuracy: 0.931\n",
            "[Epoch 154, Batch 100/372] Train Loss: 0.362, Train Accuracy: 0.969\n",
            "[Epoch 154, Batch 200/372] Train Loss: 1.767, Train Accuracy: 0.289\n",
            "[Epoch 154, Batch 300/372] Train Loss: 1.492, Train Accuracy: 0.234\n",
            "Epoch 154 Validation Accuracy: 0.920\n",
            "[Epoch 155, Batch 100/372] Train Loss: 1.286, Train Accuracy: 0.867\n",
            "[Epoch 155, Batch 200/372] Train Loss: 1.898, Train Accuracy: 0.594\n",
            "[Epoch 155, Batch 300/372] Train Loss: 1.307, Train Accuracy: 0.828\n",
            "Epoch 155 Validation Accuracy: 0.931\n",
            "[Epoch 156, Batch 100/372] Train Loss: 0.383, Train Accuracy: 0.977\n",
            "[Epoch 156, Batch 200/372] Train Loss: 1.417, Train Accuracy: 0.883\n",
            "[Epoch 156, Batch 300/372] Train Loss: 1.793, Train Accuracy: 0.688\n",
            "Epoch 156 Validation Accuracy: 0.929\n",
            "[Epoch 157, Batch 100/372] Train Loss: 0.369, Train Accuracy: 0.984\n",
            "[Epoch 157, Batch 200/372] Train Loss: 0.578, Train Accuracy: 0.086\n",
            "[Epoch 157, Batch 300/372] Train Loss: 1.777, Train Accuracy: 0.375\n",
            "Epoch 157 Validation Accuracy: 0.935\n",
            "[Epoch 158, Batch 100/372] Train Loss: 0.329, Train Accuracy: 0.086\n",
            "[Epoch 158, Batch 200/372] Train Loss: 0.503, Train Accuracy: 0.961\n",
            "[Epoch 158, Batch 300/372] Train Loss: 0.343, Train Accuracy: 0.102\n",
            "Epoch 158 Validation Accuracy: 0.928\n",
            "[Epoch 159, Batch 100/372] Train Loss: 1.137, Train Accuracy: 0.125\n",
            "[Epoch 159, Batch 200/372] Train Loss: 0.379, Train Accuracy: 0.109\n",
            "[Epoch 159, Batch 300/372] Train Loss: 0.391, Train Accuracy: 0.992\n",
            "Epoch 159 Validation Accuracy: 0.932\n",
            "[Epoch 160, Batch 100/372] Train Loss: 1.769, Train Accuracy: 0.250\n",
            "[Epoch 160, Batch 200/372] Train Loss: 1.505, Train Accuracy: 0.172\n",
            "[Epoch 160, Batch 300/372] Train Loss: 0.404, Train Accuracy: 1.000\n",
            "Epoch 160 Validation Accuracy: 0.922\n",
            "[Epoch 161, Batch 100/372] Train Loss: 1.474, Train Accuracy: 0.812\n",
            "[Epoch 161, Batch 200/372] Train Loss: 0.796, Train Accuracy: 0.125\n",
            "[Epoch 161, Batch 300/372] Train Loss: 0.370, Train Accuracy: 0.992\n",
            "Epoch 161 Validation Accuracy: 0.925\n",
            "[Epoch 162, Batch 100/372] Train Loss: 0.653, Train Accuracy: 0.961\n",
            "[Epoch 162, Batch 200/372] Train Loss: 0.555, Train Accuracy: 0.094\n",
            "[Epoch 162, Batch 300/372] Train Loss: 0.619, Train Accuracy: 0.977\n",
            "Epoch 162 Validation Accuracy: 0.927\n",
            "[Epoch 163, Batch 100/372] Train Loss: 1.628, Train Accuracy: 0.250\n",
            "[Epoch 163, Batch 200/372] Train Loss: 0.557, Train Accuracy: 0.977\n",
            "[Epoch 163, Batch 300/372] Train Loss: 0.450, Train Accuracy: 0.992\n",
            "Epoch 163 Validation Accuracy: 0.928\n",
            "[Epoch 164, Batch 100/372] Train Loss: 0.413, Train Accuracy: 0.117\n",
            "[Epoch 164, Batch 200/372] Train Loss: 1.266, Train Accuracy: 0.914\n",
            "[Epoch 164, Batch 300/372] Train Loss: 1.066, Train Accuracy: 0.078\n",
            "Epoch 164 Validation Accuracy: 0.928\n",
            "[Epoch 165, Batch 100/372] Train Loss: 0.982, Train Accuracy: 0.938\n",
            "[Epoch 165, Batch 200/372] Train Loss: 0.956, Train Accuracy: 0.164\n",
            "[Epoch 165, Batch 300/372] Train Loss: 0.707, Train Accuracy: 0.984\n",
            "Epoch 165 Validation Accuracy: 0.927\n",
            "[Epoch 166, Batch 100/372] Train Loss: 0.382, Train Accuracy: 0.977\n",
            "[Epoch 166, Batch 200/372] Train Loss: 1.653, Train Accuracy: 0.477\n",
            "[Epoch 166, Batch 300/372] Train Loss: 1.100, Train Accuracy: 0.141\n",
            "Epoch 166 Validation Accuracy: 0.932\n",
            "[Epoch 167, Batch 100/372] Train Loss: 0.422, Train Accuracy: 0.086\n",
            "[Epoch 167, Batch 200/372] Train Loss: 1.182, Train Accuracy: 0.141\n",
            "[Epoch 167, Batch 300/372] Train Loss: 0.726, Train Accuracy: 0.109\n",
            "Epoch 167 Validation Accuracy: 0.925\n",
            "[Epoch 168, Batch 100/372] Train Loss: 1.757, Train Accuracy: 0.609\n",
            "[Epoch 168, Batch 200/372] Train Loss: 0.470, Train Accuracy: 0.078\n",
            "[Epoch 168, Batch 300/372] Train Loss: 0.715, Train Accuracy: 0.961\n",
            "Epoch 168 Validation Accuracy: 0.926\n",
            "[Epoch 169, Batch 100/372] Train Loss: 0.463, Train Accuracy: 0.094\n",
            "[Epoch 169, Batch 200/372] Train Loss: 0.828, Train Accuracy: 0.977\n",
            "[Epoch 169, Batch 300/372] Train Loss: 1.755, Train Accuracy: 0.406\n",
            "Epoch 169 Validation Accuracy: 0.930\n",
            "[Epoch 170, Batch 100/372] Train Loss: 1.689, Train Accuracy: 0.719\n",
            "[Epoch 170, Batch 200/372] Train Loss: 1.880, Train Accuracy: 0.664\n",
            "[Epoch 170, Batch 300/372] Train Loss: 0.344, Train Accuracy: 0.070\n",
            "Epoch 170 Validation Accuracy: 0.928\n",
            "[Epoch 171, Batch 100/372] Train Loss: 1.960, Train Accuracy: 0.414\n",
            "[Epoch 171, Batch 200/372] Train Loss: 1.912, Train Accuracy: 0.312\n",
            "[Epoch 171, Batch 300/372] Train Loss: 0.438, Train Accuracy: 0.984\n",
            "Epoch 171 Validation Accuracy: 0.926\n",
            "[Epoch 172, Batch 100/372] Train Loss: 1.550, Train Accuracy: 0.172\n",
            "[Epoch 172, Batch 200/372] Train Loss: 0.498, Train Accuracy: 0.977\n",
            "[Epoch 172, Batch 300/372] Train Loss: 1.282, Train Accuracy: 0.852\n",
            "Epoch 172 Validation Accuracy: 0.931\n",
            "[Epoch 173, Batch 100/372] Train Loss: 0.965, Train Accuracy: 0.094\n",
            "[Epoch 173, Batch 200/372] Train Loss: 1.148, Train Accuracy: 0.141\n",
            "[Epoch 173, Batch 300/372] Train Loss: 1.439, Train Accuracy: 0.734\n",
            "Epoch 173 Validation Accuracy: 0.928\n",
            "[Epoch 174, Batch 100/372] Train Loss: 0.978, Train Accuracy: 0.930\n",
            "[Epoch 174, Batch 200/372] Train Loss: 0.850, Train Accuracy: 0.086\n",
            "[Epoch 174, Batch 300/372] Train Loss: 0.442, Train Accuracy: 0.984\n",
            "Epoch 174 Validation Accuracy: 0.931\n",
            "[Epoch 175, Batch 100/372] Train Loss: 1.125, Train Accuracy: 0.172\n",
            "[Epoch 175, Batch 200/372] Train Loss: 0.599, Train Accuracy: 0.148\n",
            "[Epoch 175, Batch 300/372] Train Loss: 1.648, Train Accuracy: 0.312\n",
            "Epoch 175 Validation Accuracy: 0.929\n",
            "Checkpoint saved at epoch 175: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_175.tar\n",
            "[Epoch 176, Batch 100/372] Train Loss: 0.688, Train Accuracy: 0.969\n",
            "[Epoch 176, Batch 200/372] Train Loss: 1.680, Train Accuracy: 0.570\n",
            "[Epoch 176, Batch 300/372] Train Loss: 0.376, Train Accuracy: 0.078\n",
            "Epoch 176 Validation Accuracy: 0.927\n",
            "[Epoch 177, Batch 100/372] Train Loss: 0.739, Train Accuracy: 0.102\n",
            "[Epoch 177, Batch 200/372] Train Loss: 1.740, Train Accuracy: 0.305\n",
            "[Epoch 177, Batch 300/372] Train Loss: 1.073, Train Accuracy: 0.195\n",
            "Epoch 177 Validation Accuracy: 0.930\n",
            "[Epoch 178, Batch 100/372] Train Loss: 1.365, Train Accuracy: 0.234\n",
            "[Epoch 178, Batch 200/372] Train Loss: 0.383, Train Accuracy: 0.992\n",
            "[Epoch 178, Batch 300/372] Train Loss: 0.319, Train Accuracy: 0.992\n",
            "Epoch 178 Validation Accuracy: 0.930\n",
            "[Epoch 179, Batch 100/372] Train Loss: 1.475, Train Accuracy: 0.875\n",
            "[Epoch 179, Batch 200/372] Train Loss: 1.688, Train Accuracy: 0.227\n",
            "[Epoch 179, Batch 300/372] Train Loss: 0.597, Train Accuracy: 0.984\n",
            "Epoch 179 Validation Accuracy: 0.927\n",
            "[Epoch 180, Batch 100/372] Train Loss: 1.345, Train Accuracy: 0.141\n",
            "[Epoch 180, Batch 200/372] Train Loss: 1.711, Train Accuracy: 0.695\n",
            "[Epoch 180, Batch 300/372] Train Loss: 1.940, Train Accuracy: 0.328\n",
            "Epoch 180 Validation Accuracy: 0.928\n",
            "[Epoch 181, Batch 100/372] Train Loss: 0.373, Train Accuracy: 0.125\n",
            "[Epoch 181, Batch 200/372] Train Loss: 0.399, Train Accuracy: 0.148\n",
            "[Epoch 181, Batch 300/372] Train Loss: 0.474, Train Accuracy: 0.086\n",
            "Epoch 181 Validation Accuracy: 0.929\n",
            "[Epoch 182, Batch 100/372] Train Loss: 0.754, Train Accuracy: 0.109\n",
            "[Epoch 182, Batch 200/372] Train Loss: 0.718, Train Accuracy: 0.148\n",
            "[Epoch 182, Batch 300/372] Train Loss: 1.608, Train Accuracy: 0.297\n",
            "Epoch 182 Validation Accuracy: 0.928\n",
            "[Epoch 183, Batch 100/372] Train Loss: 0.456, Train Accuracy: 0.992\n",
            "[Epoch 183, Batch 200/372] Train Loss: 0.480, Train Accuracy: 0.992\n",
            "[Epoch 183, Batch 300/372] Train Loss: 0.347, Train Accuracy: 0.148\n",
            "Epoch 183 Validation Accuracy: 0.927\n",
            "[Epoch 184, Batch 100/372] Train Loss: 1.228, Train Accuracy: 0.914\n",
            "[Epoch 184, Batch 200/372] Train Loss: 0.302, Train Accuracy: 0.117\n",
            "[Epoch 184, Batch 300/372] Train Loss: 0.674, Train Accuracy: 0.992\n",
            "Epoch 184 Validation Accuracy: 0.926\n",
            "[Epoch 185, Batch 100/372] Train Loss: 0.843, Train Accuracy: 0.945\n",
            "[Epoch 185, Batch 200/372] Train Loss: 0.553, Train Accuracy: 0.156\n",
            "[Epoch 185, Batch 300/372] Train Loss: 1.258, Train Accuracy: 0.898\n",
            "Epoch 185 Validation Accuracy: 0.932\n",
            "[Epoch 186, Batch 100/372] Train Loss: 0.824, Train Accuracy: 0.945\n",
            "[Epoch 186, Batch 200/372] Train Loss: 1.384, Train Accuracy: 0.656\n",
            "[Epoch 186, Batch 300/372] Train Loss: 1.067, Train Accuracy: 0.078\n",
            "Epoch 186 Validation Accuracy: 0.928\n",
            "[Epoch 187, Batch 100/372] Train Loss: 1.184, Train Accuracy: 0.109\n",
            "[Epoch 187, Batch 200/372] Train Loss: 0.387, Train Accuracy: 0.125\n",
            "[Epoch 187, Batch 300/372] Train Loss: 1.601, Train Accuracy: 0.289\n",
            "Epoch 187 Validation Accuracy: 0.931\n",
            "[Epoch 188, Batch 100/372] Train Loss: 1.758, Train Accuracy: 0.328\n",
            "[Epoch 188, Batch 200/372] Train Loss: 0.368, Train Accuracy: 0.078\n",
            "[Epoch 188, Batch 300/372] Train Loss: 1.914, Train Accuracy: 0.516\n",
            "Epoch 188 Validation Accuracy: 0.928\n",
            "[Epoch 189, Batch 100/372] Train Loss: 0.505, Train Accuracy: 0.977\n",
            "[Epoch 189, Batch 200/372] Train Loss: 0.344, Train Accuracy: 1.000\n",
            "[Epoch 189, Batch 300/372] Train Loss: 1.129, Train Accuracy: 0.117\n",
            "Epoch 189 Validation Accuracy: 0.928\n",
            "[Epoch 190, Batch 100/372] Train Loss: 0.332, Train Accuracy: 1.000\n",
            "[Epoch 190, Batch 200/372] Train Loss: 1.747, Train Accuracy: 0.648\n",
            "[Epoch 190, Batch 300/372] Train Loss: 0.379, Train Accuracy: 0.977\n",
            "Epoch 190 Validation Accuracy: 0.928\n",
            "[Epoch 191, Batch 100/372] Train Loss: 0.505, Train Accuracy: 0.125\n",
            "[Epoch 191, Batch 200/372] Train Loss: 0.511, Train Accuracy: 0.125\n",
            "[Epoch 191, Batch 300/372] Train Loss: 0.434, Train Accuracy: 1.000\n",
            "Epoch 191 Validation Accuracy: 0.928\n",
            "[Epoch 192, Batch 100/372] Train Loss: 1.678, Train Accuracy: 0.281\n",
            "[Epoch 192, Batch 200/372] Train Loss: 1.631, Train Accuracy: 0.695\n",
            "[Epoch 192, Batch 300/372] Train Loss: 1.138, Train Accuracy: 0.906\n",
            "Epoch 192 Validation Accuracy: 0.928\n",
            "[Epoch 193, Batch 100/372] Train Loss: 0.363, Train Accuracy: 0.102\n",
            "[Epoch 193, Batch 200/372] Train Loss: 0.965, Train Accuracy: 0.141\n",
            "[Epoch 193, Batch 300/372] Train Loss: 0.547, Train Accuracy: 1.000\n",
            "Epoch 193 Validation Accuracy: 0.926\n",
            "[Epoch 194, Batch 100/372] Train Loss: 0.413, Train Accuracy: 0.977\n",
            "[Epoch 194, Batch 200/372] Train Loss: 1.588, Train Accuracy: 0.203\n",
            "[Epoch 194, Batch 300/372] Train Loss: 1.280, Train Accuracy: 0.820\n",
            "Epoch 194 Validation Accuracy: 0.927\n",
            "[Epoch 195, Batch 100/372] Train Loss: 1.791, Train Accuracy: 0.672\n",
            "[Epoch 195, Batch 200/372] Train Loss: 1.182, Train Accuracy: 0.188\n",
            "[Epoch 195, Batch 300/372] Train Loss: 1.528, Train Accuracy: 0.195\n",
            "Epoch 195 Validation Accuracy: 0.932\n",
            "[Epoch 196, Batch 100/372] Train Loss: 0.357, Train Accuracy: 0.117\n",
            "[Epoch 196, Batch 200/372] Train Loss: 1.680, Train Accuracy: 0.234\n",
            "[Epoch 196, Batch 300/372] Train Loss: 0.349, Train Accuracy: 0.117\n",
            "Epoch 196 Validation Accuracy: 0.930\n",
            "[Epoch 197, Batch 100/372] Train Loss: 0.360, Train Accuracy: 0.141\n",
            "[Epoch 197, Batch 200/372] Train Loss: 1.972, Train Accuracy: 0.414\n",
            "[Epoch 197, Batch 300/372] Train Loss: 0.688, Train Accuracy: 0.938\n",
            "Epoch 197 Validation Accuracy: 0.928\n",
            "[Epoch 198, Batch 100/372] Train Loss: 0.334, Train Accuracy: 0.102\n",
            "[Epoch 198, Batch 200/372] Train Loss: 1.345, Train Accuracy: 0.898\n",
            "[Epoch 198, Batch 300/372] Train Loss: 0.243, Train Accuracy: 0.984\n",
            "Epoch 198 Validation Accuracy: 0.931\n",
            "[Epoch 199, Batch 100/372] Train Loss: 0.667, Train Accuracy: 0.117\n",
            "[Epoch 199, Batch 200/372] Train Loss: 0.452, Train Accuracy: 0.984\n",
            "[Epoch 199, Batch 300/372] Train Loss: 1.180, Train Accuracy: 0.914\n",
            "Epoch 199 Validation Accuracy: 0.932\n",
            "[Epoch 200, Batch 100/372] Train Loss: 1.799, Train Accuracy: 0.461\n",
            "[Epoch 200, Batch 200/372] Train Loss: 1.002, Train Accuracy: 0.953\n",
            "[Epoch 200, Batch 300/372] Train Loss: 0.649, Train Accuracy: 0.984\n",
            "Epoch 200 Validation Accuracy: 0.928\n",
            "Checkpoint saved at epoch 200: checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_200.tar\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unsupported qscheme: per_channel_affine",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[43], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     63\u001b[0m prepared_student\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 65\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_student\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Final model save\u001b[39;00m\n\u001b[0;32m     68\u001b[0m final_save_path \u001b[38;5;241m=\u001b[39m checkpoints_path_student \u001b[38;5;241m+\u001b[39m utils\u001b[38;5;241m.\u001b[39mhparamToString(hparam) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:659\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m    658\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m--> 659\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[0;32m    668\u001b[0m     _remove_qconfig(module)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:716\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[0;32m    715\u001b[0m     ):\n\u001b[1;32m--> 716\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(\n\u001b[0;32m    725\u001b[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[0;32m    726\u001b[0m     )\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:716\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[0;32m    715\u001b[0m     ):\n\u001b[1;32m--> 716\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[0;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(\n\u001b[0;32m    725\u001b[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[0;32m    726\u001b[0m     )\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:724\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[0;32m    715\u001b[0m     ):\n\u001b[0;32m    716\u001b[0m         _convert(\n\u001b[0;32m    717\u001b[0m             mod,\n\u001b[0;32m    718\u001b[0m             mapping,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m             use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[0;32m    723\u001b[0m         )\n\u001b[1;32m--> 724\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    729\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:766\u001b[0m, in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    764\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m--> 766\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    770\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:172\u001b[0m, in \u001b[0;36mConvReLU2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     mod\u001b[38;5;241m.\u001b[39mweight, mod\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m fuse_conv_bn_weights(\n\u001b[0;32m    164\u001b[0m         mod\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    165\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:607\u001b[0m, in \u001b[0;36mConv2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m          utilities or provided by the user\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConvNd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:323\u001b[0m, in \u001b[0;36m_ConvNd.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    321\u001b[0m         mod \u001b[38;5;241m=\u001b[39m mod[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    322\u001b[0m     weight_post_process \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mqconfig\u001b[38;5;241m.\u001b[39mweight()\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_post_process\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:267\u001b[0m, in \u001b[0;36m_ConvNd.get_qconv\u001b[1;34m(cls, mod, activation_post_process, weight_post_process)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001b[39;00m\n\u001b[0;32m    256\u001b[0m qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    257\u001b[0m     mod\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[0;32m    258\u001b[0m     mod\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     mod\u001b[38;5;241m.\u001b[39mpadding_mode,\n\u001b[0;32m    266\u001b[0m )\n\u001b[1;32m--> 267\u001b[0m \u001b[43mqconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    269\u001b[0m     activation_post_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m activation_post_process\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[0;32m    271\u001b[0m ):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qconv  \u001b[38;5;66;03m# dynamic quantization doesn't need scale/zero_point\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:568\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[1;34m(self, w, b)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_prepack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    573\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    574\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
          ]
        }
      ],
      "source": [
        "# Hypothetical setup, please adjust according to actual import paths and methods\n",
        "temperatures = [4]\n",
        "alphas = [1.0]\n",
        "learning_rates = [1e-3]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [0.0]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "\n",
        "\n",
        "checkpoints_path_student = 'checkpoints_student_QAT/'\n",
        "\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {}\n",
        "    hparam['alpha'] = hparam_tuple[0]\n",
        "    hparam['T'] = hparam_tuple[1]\n",
        "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
        "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
        "    hparam['weight_decay'] = hparam_tuple[3]\n",
        "    hparam['lr_decay'] = hparam_tuple[4]\n",
        "    hparam['momentum'] = hparam_tuple[5]\n",
        "    hparam['lr'] = hparam_tuple[6]\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "results_distill = {}\n",
        "pruning_factors = [0]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = checkpoints_path_student + \"results_student.csv\"\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for hparam in hparams_list:\n",
        "    print('Training with hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    reproducibilitySeed()\n",
        "    student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "    student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
        "    prepared_student = torch.quantization.prepare_qat(student_net)\n",
        "    prepared_student.to(fast_device)\n",
        "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "    # Count parameters\n",
        "    student_params_num = count_parameters(prepared_student)\n",
        "    \n",
        "    print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "    results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparamMixup(\n",
        "            teacher_net, prepared_student, hparam, num_epochs,\n",
        "            train_loader, val_loader,\n",
        "            print_every=print_every,\n",
        "            fast_device=fast_device, quant=True, checkpoint_save_path= checkpoints_path_student, resume_checkpoint=False,\n",
        "            optimizer_choice='sgd'\n",
        "        )\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    prepared_student.eval()\n",
        "    \n",
        "    quantized_model = torch.quantization.convert(prepared_student)\n",
        "\n",
        "    # Final model save\n",
        "    final_save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
        "    torch.save({\n",
        "        'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "        'model_state_dict': quantized_model.state_dict(),\n",
        "        'epoch': num_epochs\n",
        "    }, final_save_path)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, fast_device) # 'cpu'\n",
        "    print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "    # Write results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            hparam['alpha'], hparam['T'], hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "            hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "            test_accuracy, training_time\n",
        "        ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_21196\\3476845874.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  info = torch.load('checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_25.tar')\n"
          ]
        }
      ],
      "source": [
        "info = torch.load('checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_25.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1bbc2bb3b20>]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGeCAYAAAA0WWMxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQvBJREFUeJzt3Xl4VOX5//FPFhLWhE0CgUQQFWQRURBBXCj5ihQRRK1QRESrVUFFLCK2oP6sBrW1bohbEVQWlwpVVChlp+wgCIosghDBAIpkCEuA5Pz+eEgyk8wkM2HmnJnM+3Vd55qZc55zzp2ZwNx51hjLsiwBAADYJNbpAAAAQHQh+QAAALYi+QAAALYi+QAAALYi+QAAALYi+QAAALYi+QAAALYi+QAAALYi+QAAALaKdzqAkgoKCrR3717VqlVLMTExTocDAAD8YFmWDh8+rNTUVMXGllO3YQVo0aJF1nXXXWc1atTIkmTNmDGjVJlvv/3W6t27t5WUlGRVr17d6tChg7Vr1y6/rp+VlWVJYmNjY2NjY4vALSsrq9zv+oBrPo4cOaJ27drpjjvuUL9+/Uod//7779W1a1fdeeedevLJJ5WUlKRvvvlGVatW9ev6tWrVkiRlZWUpKSkp0PAAAIADXC6X0tLSir7HyxJjWRVfWC4mJkYzZsxQ3759i/b1799fVapU0XvvvVeha7pcLiUnJysnJ4fkAwCACBHI93dQO5wWFBTo888/1/nnn68ePXqoQYMG6tSpk2bOnOnznLy8PLlcLo8NAABUXkFNPvbv36/c3FyNGzdO1157rf7zn//ohhtuUL9+/bRo0SKv52RmZio5ObloS0tLC2ZIAAAgzAS12WXv3r1q3LixBgwYoKlTpxaVu/7661WjRg1Nmzat1DXy8vKUl5dX9LqwzYhmFwAAIkcgzS5BHWpbv359xcfHq1WrVh77L7jgAi1dutTrOYmJiUpMTAxmGAAAIIwFtdklISFBHTt21JYtWzz2b926VWeffXYwbwUAACJUwDUfubm52r59e9HrnTt3av369apbt67S09M1cuRI3XLLLbryyivVrVs3zZ49W5999pkWLlwYzLgBAECECrjPx8KFC9WtW7dS+wcPHqxJkyZJkiZOnKjMzEz9+OOPatGihZ588kn16dPHr+sz1BYAgMgTyPf3GXU4DQWSDwAAIo9j83wAAACUh+QDAADYiuQDAADYiuQDAADYKrqSj193Sf97STrO+jEAADglqDOchr03r5KO/Srt+1bq94bT0QAAEJWiq+bj2K/m8YclzsYBAEAUi67kAwAAOI7kAwAA2IrkAwAA2Co6k4/wmlEeAICoEp3JBwAAcAzJBwAAsBXJBwAAsBXJBwAAsFV0Jh8xMU5HAABA1IrO5IPRLgAAOCY6kw8AAOAYkg8AAGArkg8AAGArkg8AAGCr6Ew+Du91OgIAAKJWdCYfkpR32OkIAACIStGbfAAAAEeQfAAAAFuRfAAAAFuRfAAAAFuRfAAAAFtFcfLB4nIAADghipMPAADghOhNPnJ+dDoCAACiUvQmH691cjoCAACiUvQmHwAAwBEkHwAAwFYBJx+LFy9W7969lZqaqpiYGM2cOdNn2XvuuUcxMTF68cUXzyBEAABQmQScfBw5ckTt2rXT+PHjyyw3Y8YMrVixQqmpqRUODgAAVD7xgZ7Qs2dP9ezZs8wye/bs0f333685c+aoV69eFQ4OAABUPgEnH+UpKCjQoEGDNHLkSLVu3brc8nl5ecrLyyt67XK5gh0SAAAII0HvcPrss88qPj5eDzzwgF/lMzMzlZycXLSlpaUFOyQAABBGgpp8rF27Vi+99JImTZqkmBj/pi8fPXq0cnJyirasrKxghgQAAMJMUJOPJUuWaP/+/UpPT1d8fLzi4+O1a9cuPfzww2ratKnXcxITE5WUlOSxAQCAyiuofT4GDRqkjIwMj309evTQoEGDNGTIkGDeCgAARKiAk4/c3Fxt37696PXOnTu1fv161a1bV+np6apXr55H+SpVqqhhw4Zq0aLFmUcLAAAiXsDNLmvWrFH79u3Vvn17SdKIESPUvn17jR07NujBhdyedU5HAABA1Am45uPqq6+WZVl+l//hhx8CvYV93uomPZHjdBQAAEQV1nYBAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvmYeK20+G9ORwEAQNQg+di9XJr/lNNRAAAQNUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArUg+AACArQJOPhYvXqzevXsrNTVVMTExmjlzZtGxkydPatSoUWrbtq1q1Kih1NRU3Xbbbdq7d28wYwYAABEs4OTjyJEjateuncaPH1/q2NGjR7Vu3TqNGTNG69at0yeffKItW7bo+uuvD0qwAAAg8sUHekLPnj3Vs2dPr8eSk5M1d+5cj32vvvqqLr30Uu3evVvp6ekVixIAAFQaAScfgcrJyVFMTIxq167t9XheXp7y8vKKXrtcrlCHBAAAHBTSDqfHjx/XqFGjNGDAACUlJXktk5mZqeTk5KItLS0tlCEBAACHhSz5OHnypH73u9/JsixNmDDBZ7nRo0crJyenaMvKygpVSAAAIAyEpNmlMPHYtWuX5s+f77PWQ5ISExOVmJgYijAAAEAYCnryUZh4bNu2TQsWLFC9evWCfQsAABDBAk4+cnNztX379qLXO3fu1Pr161W3bl01atRIN910k9atW6dZs2YpPz9f2dnZkqS6desqISEheJEDAICIFHDysWbNGnXr1q3o9YgRIyRJgwcP1hNPPKFPP/1UknTRRRd5nLdgwQJdffXVFY8UAABUCgEnH1dffbUsy/J5vKxjAAAArO0CAABsRfIBAABsRfJR6OhBpyMAACAqkHwUmvOY0xEAABAVSD4KHdjidAQAAEQFko9Ce9dJ8/6flH/S6UgAAKjUSD7cLfm7tGai01EAAFCpkXyUdHCH0xEAAFCpkXwAAABbkXwAAABbkXwAAABbkXwAAABbkXyUxMJ4AACEFMkHAACwFckHAACwFclHSTExTkcAAEClRvIBAABsRfIBAABsRfJR0olcpyMAAKBSI/ko6av3pTwSEAAAQoXkw5sD3zkdAQAAlRbJBwAAsBXJBwAAsBXJBwAAsBXJBwAAsBXJBwAAsBXJBwAAsBXJh1es7wIAQKiQfAAAAFuRfHhlOR0AAACVFskHAACwFckHAACwFckHAACwFckHAACwVcDJx+LFi9W7d2+lpqYqJiZGM2fO9DhuWZbGjh2rRo0aqVq1asrIyNC2bduCFa9NGGoLAECoBJx8HDlyRO3atdP48eO9Hn/uuef08ssv6/XXX9fKlStVo0YN9ejRQ8ePHz/jYAEAQOSLD/SEnj17qmfPnl6PWZalF198UX/5y1/Up08fSdK7776rlJQUzZw5U/379z+zaAEAQMQLap+PnTt3Kjs7WxkZGUX7kpOT1alTJy1fvtzrOXl5eXK5XB4bAACovIKafGRnZ0uSUlJSPPanpKQUHSspMzNTycnJRVtaWlowQwIAAGHG8dEuo0ePVk5OTtGWlZXldEhihlMAAEInqMlHw4YNJUn79u3z2L9v376iYyUlJiYqKSnJYwMAAJVXUJOPZs2aqWHDhpo3b17RPpfLpZUrV6pz587BvFWIMdQWAIBQCXi0S25urrZv3170eufOnVq/fr3q1q2r9PR0DR8+XH/961913nnnqVmzZhozZoxSU1PVt2/fYMYNAAAiVMDJx5o1a9StW7ei1yNGjJAkDR48WJMmTdIjjzyiI0eO6O6779ahQ4fUtWtXzZ49W1WrVg1e1AAAIGLFWJYVVr0rXS6XkpOTlZOTE/z+H08k+1fuD/OlJpcE994AAFRigXx/Oz7aBQAARBeSDwAAYCuSD28Y7AIAQMiQfAAAAFuRfHjz+cPSouecjgIAgEqJ5MObvV9JC56Wft3ldCQAAFQ6JB9l2fih0xEAAFDpkHwAAABbkXyUZedipyMAAKDSIfkoy87F0nefOx0FAACVCslHedZMdDoCAAAqFZIPAABgK5IPAABgK5IPAABgK5IPAABgK5KP8liW0xEAAFCpkHwAAABbkXwAAABbkXwAAABbkXyUJybG6QgAAKhUSD7KQ4dTAACCiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuQDAADYiuSjXKztAgBAMJF8AAAAW5F8AAAAWwU9+cjPz9eYMWPUrFkzVatWTc2bN9dTTz0lK1KXps/50ekIAACoVOKDfcFnn31WEyZM0OTJk9W6dWutWbNGQ4YMUXJysh544IFg3y70Dmc7HQEAAJVK0JOPZcuWqU+fPurVq5ckqWnTppo2bZpWrVoV7FvZI1JrbAAACFNBb3bp0qWL5s2bp61bt0qSNmzYoKVLl6pnz55ey+fl5cnlcnlsYeXEYWn3CqejAACg0gh6zcejjz4ql8ulli1bKi4uTvn5+Xr66ac1cOBAr+UzMzP15JNPBjuM4JrYQ3oix+koAACoFIJe8/Hhhx9qypQpmjp1qtatW6fJkyfrb3/7myZPnuy1/OjRo5WTk1O0ZWVlBTskAAAQRoJe8zFy5Eg9+uij6t+/vySpbdu22rVrlzIzMzV48OBS5RMTE5WYmBjsMAAAQJgKes3H0aNHFRvredm4uDgVFBQE+1YAACACBb3mo3fv3nr66aeVnp6u1q1b66uvvtILL7ygO+64I9i3spdlSTExTkcBAEDEC3ry8corr2jMmDG67777tH//fqWmpuqPf/yjxo4dG+xb2WvDNOmi3zsdBQAAES/GCrOpR10ul5KTk5WTk6OkpKTgXvyJ5Iqf2/gS6a75wYsFAIBKJJDvb9Z2AQAAtiL5AAAAtiL58Fd4tU4BABCxSD78VXDS6QgAAKgUSD78lb1ROnrQ6SgAAIh40ZN8BKPZ5MPbzvwaAABEuehJPoLhhyXSkV+k/70sHd7ndDQAAEQkko9AfXy7NHeMNPVmpyMBACAikXwEaudi8/jTBmfjAAAgQpF8AAAAW0VP8sE8HQAAhIXoST4AAEBYIPkAAAC2IvkAAAC2IvkAAAC2iqLkw+YOp66fpC2z6egKAEAJUZR82OwfraVpt0gbP3Y6EgAAwgrJx5nY/Jk08Vpp079KH7PyzeOOBfbGBABAmIt3OoCI9sGt5nH3cqnNjc7GAgBAhKDmI+RinA4AAICwEj3JBx0/AQAIC9GTfAAAgLBA8hFqtLoAAOCB5AMAANiK5AMAANgqipKPEHc4/e5zackLUkFBiQO0uwAA4I55PoJl+u/NY52mUpt+joYCAEA4i6KaD5v8utPpCAAACGskHwAAwFYkH8FWUCCte8/pKAAACFvR0+fDrhlO174jufYUv46hwykAAO6o+Qg298QDAACUQvJhh1MnpH/dJa2f6nQkAAA4juQj5GKk9VOkjR9KM+91OhgAABwXkuRjz549uvXWW1WvXj1Vq1ZNbdu21Zo1a0Jxq8hw7KDTEQAAEDaC3uH0119/1eWXX65u3brpyy+/1FlnnaVt27apTp06wb4VAACIQEFPPp599lmlpaXpnXfeKdrXrFmzYN8mcPEJztyX0S4AAHgIerPLp59+qg4dOujmm29WgwYN1L59e7311ls+y+fl5cnlcnlslUrOj9LP25yOAgCAsBH05GPHjh2aMGGCzjvvPM2ZM0f33nuvHnjgAU2ePNlr+czMTCUnJxdtaWlpwQ7JWdv/K22Y5nQUAACEjRjLCu7sWwkJCerQoYOWLVtWtO+BBx7Q6tWrtXz58lLl8/LylJeXV/Ta5XIpLS1NOTk5SkpKCmZo0hPJwb1ehWLIcToCAACCzuVyKTk52a/v76DXfDRq1EitWrXy2HfBBRdo9+7dXssnJiYqKSnJY4sauQekN66SVv/T6UgAALBN0JOPyy+/XFu2bPHYt3XrVp199tnBvlXgej7vdASeFjwt/bRe+nyE05EAAGCboCcfDz30kFasWKFnnnlG27dv19SpU/Xmm29q6NChwb5V4Drd7XQE0tT+xc9PHnUuDgAAHBL05KNjx46aMWOGpk2bpjZt2uipp57Siy++qIEDBwb7VpFp65fSyeNORwEAgGNCsqrtddddp+uuuy4Ul64cmPsDABDFWNsFAADYiuQDAADYiuQDAADYKvqSj/vXOR2B8eUo6esPnI4CAADbRV/yUa+50xFIp/Kkla87HQUAAI6IvuQjHFgFTkcAAIBjSD6csHy80xEAAOAYkg8nLH6u9L4jP9sfBwAADiD5CBebP3U6AgAAbEHyESm2/Vf6/E+msyoAABEsJNOroyLKmXJ9yo3msXaadPmDoQ8HAIAQoeYj0uT86HQEAACcEZIPAABgK5KPcJF/QprYU1rkZSQMAACVSHQmH3+YL53d1ekoPG2YJu1eJi142ulIAAAIqehMPppcIg35XOr9ktORFDuTUSwF+dLOxVJebvDiAQAgRKIz+Sh0ye1OR1Bs/7fFzy0rsHOXvSxN7i293y+4MQEAEALRnXyEqydrS58/7H/5de+ax6yVIQkHAIBgIvkIV6vflgoKpO++kLbPK96/6k3nYgIAIAiYZCycHf1Fmj6g9P7c/VLNBvbHAwBAEFDzUaWG0xH4duyg9/0nj5rHggL7YoGxZ530YlvpmxlORwIAEYvkI7mJ0xFUQIzk+kn6+/nS3MedDia6fHS7dGi3eQQAVAjJR5VqTkcQuFPHpSV/l44ckP73otPRRJf8k05HAAARj+Sj8zCnI/DN15Db8Zd6vj64I/SxwIirUvx83zfOxQEAEYzko+1NTkfg27/+4PvYL9vsiwPFDu0qfv7fJxwLAwAiGclHTDlL2Ttp30bfx3YstC0M+BLGvzsAEMZIPiqbFROkDdM99+XlSu/fJH31vjMxVVbHD1XsvDOZSh8AKgGSj8pm9qPSjD+a55ZlEo/l46Xtc6V/D5W2/kc6nuNsjJVF1krpxzWBnfPvodJfG9BPB0BUI/mozD64VcpsLP24qnjf1Juld/uUf+7+zdLEa82CdfBtxYTAyhfWPgV6HgBUIiQfktT+VqcjCI3vZpnH7f/13L/3q/LPndZf2r3cLFiHMgS4CCAAgORDklSlutMRhJ/c/U5HULkFunIxAFQiJB8lNWrndASIJCQRABAwko+SEpOcjsA+uQekXcu8H3P/Ul35pvTjWntiijgBJB8njwX/9gsypbczpBNHg39tAAgRkg9JSmntdATBt+698su82EZ6p6e0bW7Z5b4cKb39m+DEFW1cP0lfPGJGHD3d8MyuVZAvHS2x2OCicdKPq6UNU8/s2gBgI5IPSWo/SLrmr9JdCypPNfqnfkwbf+q4eSwv+fDHhg+kF1pJe9ef+bUiSXm/Lx/fIa16Q5rzWMkTA7/X5Oul55pJ+78rfYw1Z0r7foH0Qmtp+zynIwFQQsiTj3HjxikmJkbDhw8P9a0qLjZO6nK/1PhipyOJXDPullx7zJdtNDmc7X3/iSPS9/PNXCDBsmupeVzPZHF+ea+v5PpRer+f05EAKCGkycfq1av1xhtv6MILLwzlbYKsktR8BKLglPT5w9KmT4JwLR9/gc//q7R28plfP9z4mrBt+kDpvRskK9/eeAAgAoQs+cjNzdXAgQP11ltvqU6dOj7L5eXlyeVyeWywwTczip9v/Eha/bb08ZDQ3GvvV9Li56XPHjCvjx0yfVKOHQrN/WzlI1ndsSCE92RNGQCRLWTJx9ChQ9WrVy9lZGSUWS4zM1PJyclFW1paWqhC8k/dc5y9v10+ur34eZ5bwrd7xek+IF6+VJ9INiNfAlWyduDjO0yfFG9NNAe2SgueiZzE5MB30qTrpJPHAzuvsvQtAoAKCEnyMX36dK1bt06ZmZnllh09erRycnKKtqysrFCE5L9rnpIuvk1KaeNsHE6Z2EOaclNxZ9SSvhx55vf4fp7no7vxl0qLnpW+HHXm97HLD0ukr6eXX87dej9Gp+Sfqlg8ABDmgp58ZGVl6cEHH9SUKVNUtWrVcssnJiYqKSnJY3NUtTrS9a9I6Zc5G0ck2fjxmV/DssxQ0sIaF/f1aCLBqRMBli9nzo9lr5oF6HavqHhMABCmgp58rF27Vvv379fFF1+s+Ph4xcfHa9GiRXr55ZcVHx+v/Hw64FU6/7qz9L7sTdLCZ/2b/Grbf6Una0vPn1u8L9KaJWKC3A/jP382nVU/vT+41/XmmxmVszMwgLAVH+wLdu/eXRs3bvTYN2TIELVs2VKjRo1SXFxcsG+JcPT65ebx5BGpudsEZXm5pctOudE8HjtY+li0syMJK+z/0/w3Um2H+1wBiApBTz5q1aqlNm08+0vUqFFD9erVK7U/YvzuXTMiZPNnTkcS/g7t9ny9d71n8vHfxz2PR1oNhy/BrvlwwvFDkkg+AIQeM5z6o1Uf6RYmdgqKH/7n+doq8O+8WQ+ZNUzCdSbPaJvZFQDOQNBrPrxZuHChHbcJvXP/T9oehKnIK5Pyai52LjJboZI1BPm+OmqWuO6aieZxx0LpvP8LJEJ7fPWe1OfVEFz49PtQ8n0+8ovpHA0AEYiaj0Dc9E+nIwg/U24qvW9izzJOKJF8+Fps7dcfzLwiOXs89/tbU1KZ5B6Q/t6y+PW3/5aeP6fEe1fBZp/K0uwFIKKQfPhydpfS+6omS+0GeO7rPtaeeMJRQYG0/b+l9+9e5vucmAB/5eb/NbDyldHyV6VctzVkDu0yj/l5boUcSiIO7jRr2ABAAEg+fGndz3Q0fXCD5/5rx0m104tfX/Gw1LiDvbE5Lf+kSQq2fBH6e5VaG6Wcv/B3LZNce0MWTlBZltmO/WpeF/gahu5HYuHPpGW+YjgTL19k1rDZHcQF9EJh8yynIwDgxpY+HxEpJsZ0NC2pWm1p2FozPLHZFWZfpz9Kn6yxMzpnvXeDmdWzIvZtLL9MRexYKE0bIJ08Pa/IEz4WfAul/JPSTxukJS/4V/7t7lKNBtLWL6VmV5k1cO79n2dy66/srwM/J5j2rJXSOzkbQ1k+GCiN2CwlpTodCQCRfFRMfII0wO0vzbY3S4qRPvmDYyHZqqKJR0Vk+TnT6bteEkW7ZW80CYW/9qwtfl7YKXfx81LXh4r3V7Rm4rPhponruvISoSA110TCUOMjB0g+gDBBs0swxMRIDS5wOorK6dedpvNpRViWGRUimenPf9pwZs0M2eXU2rzVreLXLrTuXenl9hU/f+cSU4Oy9h1pzT9LL+oXMhGQfAAIGyQfwRKNozDs8lI738d+/UH65fvS+zdMl2beZ0aFbJtrmsneuFJaMaHse1mW6UCZu7/0sde7BhK1/XavlCZfJ715dfE+q8Dsf/9G6edtpc/xSMbOIIGIhJoPAGGD5CNYSD7ss22uGYa79EWTmLxycekyM/4obTjdNLboOWnL5+b58vHFZVw/SdMHmv4ihb6ZYfq0nEntQ1AFUFOT5WMRuonXmFFJ039f+pjLbSizzzlX/FHJko+s1aYJLNgrC2/8WPpHG1M7BUQxko9gIfmwT+HcIiWnavfFfYVc148meZGkWcOl72aZ/iLffmr2bfuPeTzhZQ0aJxzcIf3vJf/Kbvqk7OMl50yRpHlPFj9fP8X3uV+MlKb2N8OrvalsNR//zDAjuta+E9zr/utOKSdL+vC24F4XiDAkH8FSXl+Cq0bZE0dll+c682tMucnUBGydXbzvw0HSitc99/24Rpp4rWfH0HD203ovO92SAm8Jwsnjxc9Xv1087LekVW+aUTlOvxcnjtrYj0XSz1tDc91wXSagpF++N814hck5ECQkH8FSXs1Ht8fsiaOy+2lD+WX88f6NpffNHuX55ft2d2n3cumt35hmnkjnbYI314+erxf/rexr/DPDfCGte0/6YJDbtQOs+fjodumjId6PWZb5nE95aQZ69mxpXLp04ohn+Z2LzUyw/tqzVpp8ffm/T9E+A+zMe00T0YeDyi8LBIDkI2gC/E/qvB5StbqhCaUyW/ay0xFEMC8JQskvX39qll65WPp0mLTZ/a9hH8nH0YPS2kmetRVHfjZ9a775pHg0krtVb5nOwR/c6rnfsor7pbjXSGz+TJrc20x45o/8kyah3LlImnSd9K+7pC8e8V52Vxmz9ZZ08pjvZqlSKthMlXe4+Pmu5d47WwfTsUOhvT6iFslHsPjT5+Nht/8wr39ZOqtF6OIBSnL/vss7LI2/rHQZy5K+HCWtecd8SXurffB6bbeLu9dATBsgffagGXlUyNtMrv97SVr5pnm+4nSn4G1zpFNuU8hv/sz7vbfNMY/u/XTczytpzp+Ln+e5pI0fSqve8J447P9Gev48adYI89qyTPPcjyUmFTx60Ky1M/k63/c9U99+KmU2kRY+K/28XXrnWu+drYPKjz+qDu8zzZMbPqjYLY79avrXeBuNFQmOu8x6SyeOOh1JRCH5CJb4ql72VTOPXe43j7VSpLG/mtk3azWUbnzbvviA4zmmeeLEUemrKdKBzaXL/LBEWvm66Yz7Ylvp7y38HPFxOvn430vS386Vlr1iXheOwPlulmnmmD26xGkx0uFsae5Y6cuRpZOdZxpLOxZJh3Z7dhx2z6Tcz9nypZmPZcO00iEW/oGw6g3vP8KJw2bkU8nk6Mh+M2fKyeOmtmb2qNKTyX0x0jzu+p/3a/tyONts/vjsQfO48BmTFIWLuWNN8+SMuyt2/qyHzMii1zqfWRyr3jJJQDDt/cok4mU1v304yHQgrujPH6WY4TRYGrWTOtwp7d9cvLDaud2lG16XEmsVl4t1y/eSm9gbIzC5t3m8cqT34+4Tuh3+yfOxLIX9SeaeXmjxP3+RWvzWs8zORWYr2ZRxaHfx832bPGMoOCm9e715XpjEFzp2SNr0sam5KDStv3ls6aUGYtkr0k0Tff8M791g+oJ09zGK6ukUqfEl3o9t+rj4+Y6F0jlX+75PoVN5JrmTpDE/S3FVyj+niJdmm2O/mtqHNv2kmg38u0xBvvniTG1vFs3Mzfb8Gcv60v3pa5PkVXQSwEKFsxgXnDTNcFWTpbgAv5r2fSt98Sfz/OrHpKuD1MG/cM6cGmdJF/io1Socqr/5M9M37ILe0s2Tpdg4/+6Rf9L83qVebGbPjhLUfARLTIyZyvqOL4v3JdTwTDy8Kfkl0MvPdUEAf7zQyvv+xc/7f42DO8ov463Dqa8mAfdROdvnSf/8v+LX/s4SGxNj5i35/GHvx7/zspDcpn9572NSqHAkT1lDjv0Z7VM41X/hwoHexMR4TmTn19But2tleVnIb8Y9plbGW2dqX7bONu/V/Kekf7QyfWH2u9eIud3zfy97/vxvXCGteM1zfplfvi/7PS7P8+d4/j7464jbe7nwmYo3Afni/p7kHjCJhq8awc2feY6aK8/sR6WJPaTPHyq/bCVC8hEKvV82NSEZT5Zf9qpHpUEzpeGbTJ+QjneGPDxEkZNHyi9TnsKah7Ic3FmxaweyHtLW/3i+DrSJQzIJSLBs+dL09fCmIN98kb9/o/cEpCDfs4nguMuMIPI11FnyPLb81eLn66dKr15a/IUXyCKDJ4+V3vfjGhPzZw9Kv2wv3j93jPmZyvLKxSaBOBN715VfZsXrZv2iwve25Hv8nz+XOuWM5J+Qlv7DdNAu7Ay9sozZkt07Bpdn9enm96/eP7MYIwzJRyhcMlj642IpqVH5ZePipebdpNpppk8IEImW2lBj9/OWM7/Glz6am9x56xDrzbT+5i/WvetLH3u2mfkS/X6e97+Cc7OLV2CWpPf6mhFEhcOXt82VflhqnntLENzNvLfs98ZX7cupPGmOjykAsjeaUUoV9dZvfHf6zdkj/eBn4pi7X3q3b+l5RmaPMhPAFb5HJR05YEYD+cNbDcbOxdJLFxW/Xvm69N8nTOJxeK/Zt9lL7Vqh7I3+/x4F26+7pN0+ZjsOIyQfAILD1rlQQjij6q8B1OL8vNX7HBh5bkOLp/U3/SL2legk6j6dfWHT1g9LTLX+lJukSb2kZa+aUTRlfdGVJfeAaXr77xOlj61+W8rdV3r/sldMk4ov/3vZe8Llbs9a0/yw/ztpys2ezTX/aCVN+q1/K1bP+bO0Y4F5j/d+Ja2Z6JlMrXhNmuujj84715Z//cXPS39tUHq6+8m9PX8PvA1BL2uE4/JXzRIP7n7e7jmpn2T6GbkrOWLGW83asldNrZSvpPKlC01SXPL3LczQ4RRA5Nn4kdMRFHPvMOvLhK5mNI07X/1uVritP1TYfPDBwMDj2veNNKGLeb70H1LXEVLVJPM6/6T0tY9+EeXVMM0d49/9v3rPdAQ9st8sW3DfCun7BcXHdy+X0i4t+xpH3IZtF3b+rOqW5G75wmxlNVd5U5BvRnSte9e8/vA2aXg5q1aXVN70Chs/Mos6trnBdEB+7wapQWup/UDTufTszmYRS3dZK6Tmp5u2NnxgRtBc8Sepu9t7Xvg7cWF/cw1fvhwlZTwhNekQ2M9lE2o+wtmVI6UB00vvHxDkzlRApIm0yeZKJh5lWfqP4NyzMPEoNC6t+C/vOY8Fb7ZgX3Ys9OwI+tpl0hy3odZzx5Y9msayvC/A9/Edpfetm+z9GuM7SVvneO7LP2X6/hQmHpJJIP0d8lwcYPkz4ObsNsPP15xeI2j/N+a9L6tW5tghE/Os4eb1Eh+zDh85IOXl+u7k+8MSMyR82u9Nc9GMe0ytlr/NUSFGzUc4a9nLe7thixK/uCO/l55vbk9MAMKfy8fw6AldpFZ9zFo94cBX59gdi/zr6FyeA99JU38n3TLFLOiX86P09YeeSVGhH1ebYbL+sgqkj30sEVDS5k/LL1Po3TKm/Xfvn/KvO6XEJOnoz+b10FVmLp2SClf0dr/mH+Y5XiNC8hFJbpkiJTf23Nfu91KN+s7EAyA8veJjPpKD39vTOdhfx3Mkl5fVloOReLjzp9nqp6+l2mebCSD9cfK4WSagoiZ7SXRK9gEptPFjqe1NxU1PkukzVJh4SNL4cpqw3O1e4XjyQbNLOKuZIp3V0jxPqGkmuUlt71mmcH6FtE72xgYgfAVjiLUdyhvJY6fFz5mOtu/386+8txmCA7Fzsf9l/3Wn5Nor7QuwX4ovwR6KXAExlhVeyza6XC4lJycrJydHSUlJTofjjF3LzLj/wuaV4y4pLkGq4jaF+3+fkFb/U7pniVSnqflHfGCLaQsP5lwG8K7ldd4nsgKASPBETvllAhTI9zc1H+Ho7C6e/TqqJnkmHpLpxTzqB5N4SFKValLqRfIYgtiilzTWy1Ct5LTS++5aUHoffOv7mtMRAEDF5fkzq27okHxEMm9rB7g3ywyYWrrMrZ9ID23y3JfxpNS4nNUxu4+tWIyVVVU757QAgCArXPzRIXQ4rWw6nZ7YxtvCVtXqmMXuSuo6vOxrxiVIZ3c908gAAOHimI+lAWxC8lHZxFWRugwL7jUTaspjgSkAAM4AzS7R4IY3TK1H/2luOwOYnnrgx+WXiUbNvdQiAQDKRfIRDdr1lx7Z6TkV7x2zpbMukAZ/Vva59c+XmviYM6Ckpm7rQZx1gXTVKDOlcmWVepHTEQBABYVwfSQ/kHxEi5gSv2jpl0lDV0jNriznvNO/IoXzjZTFfQTI9a9I3R6TGlwQWJyR5ILTEyElNZGGrnY2FgCIIPT5QNlS2pjHarWlh7eaIb9L/m7WK5BMzUjnoVLeYal2uimTk+U5e97oPVJm41KX9iqtk5S1Mqg/QsikXiQ9sN5MBpdQ3eloAMB/q96QfvucY7en5gPe3fCm1Ole6bduK2/WSjFDTP/v/xXvi4mVLrld6nJ/cZmS0/Ym1vT/vje9Y6aMz3iiopHbq24z34lHrVTpogqsRgoAlVzQk4/MzEx17NhRtWrVUoMGDdS3b19t2VLOEs0IP40ulHqOk6rXLadggO2GVz0qVa3t+3hyY+mGCVLaZT5u52Vuk3BxfokF/+5ZYpqiYkr8M6t7zpnfq3b6mV8DABwS9ORj0aJFGjp0qFasWKG5c+fq5MmTuuaaa3TkSISsNQD/pJ9ervuS2wM7LzZOSqhR/Po3Y8xU5dXrS4PcFmlKv8zMLyJJV/xJatDKPF47rrhMsyvN6oxlOTfDNPvYoY9bn5e0TsUL/g1dJdVoUHyscQfpQR+reQJAFAh6n4/Zs2d7vJ40aZIaNGigtWvX6sory+nciPDhniB4c+u/pOyNUpOOgV03LkFyX07oyj+ZR8vy7BQbEyONOVD8uvsY83gqT9r2HzOJWuF8Jq37Sd98ItVtbl4f/L74vIEfm2s99K30j1aesfR+WfrsgcDiL0uNet731z9Pumue9GLb0/H2ZYZUAFEt5H0+cnLM4jV163qvvs/Ly5PL5fLY4LC0TuVX6ydUl9I7SbF+/gp1f9z8xd/xD/I6YVnJ0Ti+xCdKt37sOZFa39ekgf+S7l12+vqndbq3+LrJjU0ti7tLBpeY+8RP1z5bfpmGF3q+rp0uDVsr3f6F1OK3pgNv6xukC7wsq+0P95oUAIgwIU0+CgoKNHz4cF1++eVq06aN1zKZmZlKTk4u2tLSvCx6Bnt1CWJtQKErRpi//hNrSh3uMPvc5wU5E1WqSedlmJE4tRoW7+85zrNcYS2Lu5a/LX7e6V7/7nfxbb6P3bNUuuJhKePx0sfqnys1vbw4Ibp5knTL+9KoXf7dV5LOvtw0N934tv/nAECYCelQ26FDh2rTpk1aunSpzzKjR4/WiBEjil67XC4SEMeFeCr1riNMn47Gfk5eFohWfaUuX5naG3+N/VU6vFfKPyGtnODHCWW8Pw3bmi0Q1Wp733/HHGlij+LXfV6T2jN6BkDkC1nNx7BhwzRr1iwtWLBATZo08VkuMTFRSUlJHhscZoU4+YiLN3+9l9evpCJiY6VrnpIuuC6wc5KbmFEo9VuUXz4U788d//Gys0RT1EW/93zdd4JU7zyzUjEARJCgJx+WZWnYsGGaMWOG5s+fr2bNmgX7FkDFXfWoeexwp/fjw1ZJjx8q5yIhSD7SvdTUpLT2fF2yX8xFv5fuX1N6peKzLy/7Xt3Hlt43eJZ0/zrf5/jTz6WirnsxdNcGEJaC3uwydOhQTZ06Vf/+979Vq1YtZWdnS5KSk5NVrVq1YN8OCMzVj5rRJmXVcHjr/NrrBenz082DsSGeGLjPeKlVn8AmZ3N326dS7j5pw1Rp/l9LHPu3GSadl2uaey68xYwgqnO27+udd4102T3SspclVwiGLZ9ztff9dc+RDu4I/v0AOC7o/4tOmGDazK+++mqP/e+8845uv/32YN8OIRHiZhcnxcQEtt5Mqz6miajDHebcuATTwTWU4hKlxFoVO/eigaZZK7mxdOVIafMs6af1Zl6R+KpmBlrJe4dYSbrxn2bq/OzT85D87t3iNWwe+Eo6cUR67nRtZkJN6URuxeIs1Ge8mSXWmz8uMVNAz/t/3o8DiFhBTz6sUPcXQOgl+e6jEzUatjXzmFz3YvEsr4WjdOzU9ArphyX+l2/+G8/Xf5hnEgRfnVpLanuT2fasMwnIBdcX1wTFJ5rtzrnSpk+kzvcVz13iy4W3SF9/4Pt4+1u977/yEVPz0+XBspOPC/tLX08vOwYAYYeF5VBs0Ezp1x+kJiEYhRJp7l4knToemk6x5XG/Z4ML/Es+HvrWJAslp3iPi/c/8XDX+GKzeZN2qdkk8zvzXt8S90wwI4ckqUdmcfKR8YS0/zv/koXCKenjyvkv6pqnSD6ACMTCcijWvJvUYYjTUYSHktPA2+Gav0ptbpTO71F+2ZKSG0stevo/WVuwNO8mdbzLNPEUikssfh6fICWens21/W3Sb/7i33Xdf46hq80w4xv/6a2gWQTR3X0RsCoyM9wiypF8AOGiy/3STRNN4lPoiofNbKZdH3IurvL0+ptnUpFYU+r9kpm+PrGW9PB30sgdvqefl6QWvTxfu3dCPet8M79J25tKn1c1WWp7s+e+Bi2936N+C8+ZYYOxOF/7QRU779HdbnGU0dm3PDdNrPi5gINIPoBwVquh9KetpskiUliWWXDwksHmdUJ174mH+6KAN78j3TVfenir2Z/uY1XjjncVP394i6lZiY2VziqjE/GFt0id7pHuW+E5UumB9aXLDvhAuuhWadQPZitPn1fLL1OeWo1MjFLpJQDKckFvqcmlZ35/wAH0+QDCnd1NKWfMz07nZ7nVUMQnFs94Wzgixxv3Wg33qfTvmifNvNeMTiqpn3uzjFts7jVMhVpca7ZCg2aa/ieNL5EyG/uOSzITvrXpJy0KcE6UK0ea5qsr/mQWIZz/lH/n9XtbOvxTYPcCwgQ1HwAiR+Gw35LT5yfUMMOC29xY9vmn8jxfe51Z1k3zbtI5V5mmpG5/9jyWcnqkT68XTO3FLe9J3R6TqpTTV6haHc/XqReZROis88tONOOrer6uUlWyCsq+FxCmSD4ABFcoh9vXbCA9tlcaMrti55886vk6vZPUqJ1/517+oOe6Pbd/Zh473imN2Fz+/DEZT5oE5q4F5vXwTaZzbI36/t3/sZ9MXxp3tdOl6m7nX/+q9NA3/l2vPFWqFz+/5ungXLMyu3iw0xFEFJpdAARZWclHEBKTMxmF1PNZ6bMHPTvw+pssxSdKQ76UXusiNb/aswbDvcbC/fnjh6Q3rjTDoNsN8GxSqu1jAc3Cydv6jJc2TC8eah0bK6WUWB08ropJfGJipSMHpKRG/v0sha58RFr8nOe+xCTpsvukqx4xQ6azN5lmp6O/SEtfKC533T+k2CrSp8MCu2fL66T8k9K2OZ77q9c3CU/Obu/nBVN6F2n3suBdr3o96fqXTXL6hZfVs1EKyQcAZ4SyL8ugGdKsh0xNgLtLbjdzodR071cSQEKUWEsa/nXZsf/+A2n676Wez5tydy80NS7+zlr7wFcmWWneXTrwnec8L006SP2nSnXcZoWNTzCPZSUePTKlOaNL7IwxSZh78tGko1lNubA/TGw1Ka2jed59rHTxIGn7POn7+WY23fhEM4/Mslel618xsSbUlGbcXXzN9rdKl95tkjBJqtNU+mV76Rg7/sEM9f+7H4s7BqrGWSY5K1Te/DGB6jPePHa4k+TDTzS7AAiusmoS3JsI3OcDCbbmv5Ee3CA1u6L0sVoNPZOHwpFEl/7Rv2uXlzQ17So98oPU7vQIlti4wKbLr9lAOjfD3KfuOaWPt+wlpbQq+xqF865cNFD6y34zG22h338oPZEjPXHIjERKcWtK6jvBe0dcqTieS++SBkwziYdkRt3cOcf0Wel4p/m5k9w65/YZb5q2bvtUumSIWV/JXdPTn1G7W8xnk1ROx97ydLqn+Hlikhny3bRryR9GSg7CUGtJSrtMOvf/zPNYH1+psVXMhiLUfACwT0J109chNi74f31W1LkZZt6NxKTgXdPXl1Cg2t8mufZKza4K7LyrHzPNGyltit/ne5dJv+4qPYndXfPMqJk6TYMSsiTp5snS1Js9+4qcc5XZSrrtU+nE4eKJ10Z8K73aUfp5q3/3at1P+uaT0+d+Z2qAVr5uXt/6iam5SW4sfTPD87xef5Om/s48r5liZgl+ysdcNPVbSD9vKb2/++PSFSPKjzEmxrNzcIc7pDXRPUdLmPzrB1B5lNOM4auvg5PCdcbRuHj/Z4V1FxtrRtG4S2lttpLiE4ObeEjmC/+Rnb5riS64Xto626wjFRtb+v2/b6X001fSW25rFblP2z/4Myl3v6nhql7XzH7rLeErvH/z7tK1z0qzRxXvj3OriXjoG/NeX3q3tOr00OwB06Vp/c3zQZ+Y/ihfvW8mtYtPlKrW9n7PWo28D4EuOFX8/LKh5Scf9c4zNWZ715VdLkLR7AIguFhcElLZzVPtBpgaj3t8rFsUG1s874tkmkgGflT8utmVZsbbwkUfSyYB519ran0aXVQcy2VuzTGKKdEE6KVJpEVP05n33uVSchNzr8sfMDUr1ev6rt364xKp31vSn7OL91WvZzr3Fqp/rufx866RBs/yvM79a6Rb3pdqNlQpQ740zWlnom7zMzv/DFHzAQCwV2ys9yYYX2JiTNNTp3vKH9IsmVqLwvN8aXShGf7sXhNX/3zPMkmpZgtEzbOkC0835wyaaVZl7v2SGQnT9ubifjxVqpnOuDsXSze9Y+aSKSm5sVmeoOCUtPqfxTU3Z3c5fa8UKXefed7vLemT0zMA93hGmvNY8XX+tE06sMX8rFvnSIv/Jt3kba0k+5B8AAgyaj4QJO1+L22YajqpxsSYodL+KK9TcGHC0XW45/5LhkhHD3quLXQmmnczW6GzSiQ3fcabmsLCeP8wT3q7u3S5W1yFTUQd/yBt/sxzrhl3F/6uOPmISzC1NpOvN/1LajYwmyR1+qNpXnJ45uQYywqvOlKXy6Xk5GTl5OQoKSmIHcAAhNYTp9vtq9Xxb10UoDwFBZLrx+AsAiiZIcIbpkvXjitusgk3p/KKRxKV52/nF9d8PJEjLXhG2jZXuv1z07nbZoF8f0ddn48l2w5ofdYhncpnWmIgJMLr7xlEstjY4CUekumg2u/N8E08JP8TD8nMvSKZGhvJTO9/9wJHEo9ARVWzy0drsjTy468lSa8NvFi/bRvgbIAA/EDyAdii/a2miehM50ZxQFTVfBQmHpLU+Rwf47kBVExqe/PY5iZn4wCiSXITx/tvVERU1Xy4q1MjwekQgMrl1k9Mm3rLXk5HAiDMRWXy8fKA9k6HAFQ+1euauRcAoBxR1ewSe7pm6rJmYdzZCACASi5qko+CAksFp/vBxcdFzY8NAEDYiZpv4ZMFxUNr4+Mir3MOAACVRdT0+YiNidH9vzlXpwosVY33sWQ0AAAIuahJPqrExerha1o4HQYAAFEvappdAABAeCD5AAAAtiL5AAAAtiL5AAAAtiL5AAAAtiL5AAAAtiL5AAAAtiL5AAAAtgpZ8jF+/Hg1bdpUVatWVadOnbRq1apQ3QoAAESQkCQfH3zwgUaMGKHHH39c69atU7t27dSjRw/t378/FLcDAAARJCTJxwsvvKC77rpLQ4YMUatWrfT666+revXqmjhxYihuBwAAIkjQk48TJ05o7dq1ysjIKL5JbKwyMjK0fPnyUuXz8vLkcrk8NgAAUHkFPfn4+eeflZ+fr5SUFI/9KSkpys7OLlU+MzNTycnJRVtaWlqwQwIAAGHE8VVtR48erREjRhS9zsnJUXp6OjUgAABEkMLvbcuyyi0b9OSjfv36iouL0759+zz279u3Tw0bNixVPjExUYmJiUWvC4OnBgQAgMhz+PBhJScnl1km6MlHQkKCLrnkEs2bN099+/aVJBUUFGjevHkaNmxYueenpqYqKytLtWrVUkxMTFBjc7lcSktLU1ZWlpKSkoJ6bfiHz8B5fAbO4zNwHp9B8FmWpcOHDys1NbXcsiFpdhkxYoQGDx6sDh066NJLL9WLL76oI0eOaMiQIeWeGxsbqyZNmoQirCJJSUn8sjmMz8B5fAbO4zNwHp9BcJVX41EoJMnHLbfcogMHDmjs2LHKzs7WRRddpNmzZ5fqhAoAAKJPyDqcDhs2zK9mFgAAEF2iam2XxMREPf744x4dXGEvPgPn8Rk4j8/AeXwGzoqx/BkTAwAAECRRVfMBAACcR/IBAABsRfIBAABsRfIBAABsRfIBAABsFVXJx/jx49W0aVNVrVpVnTp10qpVq5wOKeJkZmaqY8eOqlWrlho0aKC+fftqy5YtHmWOHz+uoUOHql69eqpZs6ZuvPHGUmv97N69W7169VL16tXVoEEDjRw5UqdOnfIos3DhQl188cVKTEzUueeeq0mTJoX6x4tI48aNU0xMjIYPH160j88g9Pbs2aNbb71V9erVU7Vq1dS2bVutWbOm6LhlWRo7dqwaNWqkatWqKSMjQ9u2bfO4xsGDBzVw4EAlJSWpdu3auvPOO5Wbm+tR5uuvv9YVV1yhqlWrKi0tTc8995wtP1+4y8/P15gxY9SsWTNVq1ZNzZs311NPPeWxqBmfQRizosT06dOthIQEa+LEidY333xj3XXXXVbt2rWtffv2OR1aROnRo4f1zjvvWJs2bbLWr19v/fa3v7XS09Ot3NzcojL33HOPlZaWZs2bN89as2aNddlll1ldunQpOn7q1CmrTZs2VkZGhvXVV19ZX3zxhVW/fn1r9OjRRWV27NhhVa9e3RoxYoT17bffWq+88ooVFxdnzZ4929afN9ytWrXKatq0qXXhhRdaDz74YNF+PoPQOnjwoHX22Wdbt99+u7Vy5Uprx44d1pw5c6zt27cXlRk3bpyVnJxszZw509qwYYN1/fXXW82aNbOOHTtWVObaa6+12rVrZ61YscJasmSJde6551oDBgwoOp6Tk2OlpKRYAwcOtDZt2mRNmzbNqlatmvXGG2/Y+vOGo6efftqqV6+eNWvWLGvnzp3WRx99ZNWsWdN66aWXisrwGYSvqEk+Lr30Umvo0KFFr/Pz863U1FQrMzPTwagi3/79+y1J1qJFiyzLsqxDhw5ZVapUsT766KOiMps3b7YkWcuXL7csy7K++OILKzY21srOzi4qM2HCBCspKcnKy8uzLMuyHnnkEat169Ye97rlllusHj16hPpHihiHDx+2zjvvPGvu3LnWVVddVZR88BmE3qhRo6yuXbv6PF5QUGA1bNjQev7554v2HTp0yEpMTLSmTZtmWZZlffvtt5Yka/Xq1UVlvvzySysmJsbas2ePZVmW9dprr1l16tQp+kwK792iRYtg/0gRp1evXtYdd9zhsa9fv37WwIEDLcviMwh3UdHscuLECa1du1YZGRlF+2JjY5WRkaHly5c7GFnky8nJkSTVrVtXkrR27VqdPHnS471u2bKl0tPTi97r5cuXq23bth5r/fTo0UMul0vffPNNURn3axSW4fMqNnToUPXq1avU+8RnEHqffvqpOnTooJtvvlkNGjRQ+/bt9dZbbxUd37lzp7Kzsz3ev+TkZHXq1MnjM6hdu7Y6dOhQVCYjI0OxsbFauXJlUZkrr7xSCQkJRWV69OihLVu26Ndffw31jxnWunTponnz5mnr1q2SpA0bNmjp0qXq2bOnJD6DcBeytV3Cyc8//6z8/PxSC9ulpKTou+++cyiqyFdQUKDhw4fr8ssvV5s2bSRJ2dnZSkhIUO3atT3KpqSkKDs7u6iMt8+i8FhZZVwul44dO6Zq1aqF4keKGNOnT9e6deu0evXqUsf4DEJvx44dmjBhgkaMGKHHHntMq1ev1gMPPKCEhAQNHjy46D309v65v78NGjTwOB4fH6+6det6lGnWrFmpaxQeq1OnTkh+vkjw6KOPyuVyqWXLloqLi1N+fr6efvppDRw4UJL4DMJcVCQfCI2hQ4dq06ZNWrp0qdOhRJWsrCw9+OCDmjt3rqpWrep0OFGpoKBAHTp00DPPPCNJat++vTZt2qTXX39dgwcPdji66PDhhx9qypQpmjp1qlq3bq3169dr+PDhSk1N5TOIAFHR7FK/fn3FxcWV6u2/b98+NWzY0KGoItuwYcM0a9YsLViwQE2aNCna37BhQ504cUKHDh3yKO/+Xjds2NDrZ1F4rKwySUlJUf0Xt2SaVfbv36+LL75Y8fHxio+P16JFi/Tyyy8rPj5eKSkpfAYh1qhRI7Vq1cpj3wUXXKDdu3dLKn4Py/o/p2HDhtq/f7/H8VOnTungwYMBfU7RauTIkXr00UfVv39/tW3bVoMGDdJDDz2kzMxMSXwG4S4qko+EhARdcsklmjdvXtG+goICzZs3T507d3YwsshjWZaGDRumGTNmaP78+aWqIy+55BJVqVLF473esmWLdu/eXfRed+7cWRs3bvT4Rz937lwlJSUV/YfeuXNnj2sUluHzkrp3766NGzdq/fr1RVuHDh00cODAoud8BqF1+eWXlxpivnXrVp199tmSpGbNmqlhw4Ye75/L5dLKlSs9PoNDhw5p7dq1RWXmz5+vgoICderUqajM4sWLdfLkyaIyc+fOVYsWLaK+uv/o0aOKjfX8CouLi1NBQYEkPoOw53SPV7tMnz7dSkxMtCZNmmR9++231t13323Vrl3bo7c/ynfvvfdaycnJ1sKFC62ffvqpaDt69GhRmXvuucdKT0+35s+fb61Zs8bq3Lmz1blz56LjhcM8r7nmGmv9+vXW7NmzrbPOOsvrMM+RI0damzdvtsaPH88wzzK4j3axLD6DUFu1apUVHx9vPf3009a2bdusKVOmWNWrV7fef//9ojLjxo2zateubf373/+2vv76a6tPnz5eh3m2b9/eWrlypbV06VLrvPPO8xjmeejQISslJcUaNGiQtWnTJmv69OlW9erVGeZpWdbgwYOtxo0bFw21/eSTT6z69etbjzzySFEZPoPwFTXJh2VZ1iuvvGKlp6dbCQkJ1qWXXmqtWLHC6ZAijiSv2zvvvFNU5tixY9Z9991n1alTx6pevbp1ww03WD/99JPHdX744QerZ8+eVrVq1az69etbDz/8sHXy5EmPMgsWLLAuuugiKyEhwTrnnHM87gFPJZMPPoPQ++yzz6w2bdpYiYmJVsuWLa0333zT43hBQYE1ZswYKyUlxUpMTLS6d+9ubdmyxaPML7/8Yg0YMMCqWbOmlZSUZA0ZMsQ6fPiwR5kNGzZYXbt2tRITE63GjRtb48aNC/nPFglcLpf14IMPWunp6VbVqlWtc845x/rzn//sMSSWzyB8xViW23RwAAAAIRYVfT4AAED4IPkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2IvkAAAC2+v9vT7tCq8P3EgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(info['val_acc'])\n",
        "plt.plot(info['train_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_21196\\1265263037.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f'checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_{cp}.tar')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.896\n",
            "test accuracy:  0.9161\n",
            "test accuracy:  0.9178\n",
            "test accuracy:  0.9196\n",
            "test accuracy:  0.9227\n",
            "test accuracy:  0.9195\n",
            "test accuracy:  0.9226\n",
            "test accuracy:  0.921\n"
          ]
        }
      ],
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torch.quantization import fuse_modules\n",
        "\n",
        "checkpoints = [x for x in range(25, 201, 25)]\n",
        "test_accs = []\n",
        "\n",
        "for cp in checkpoints:\n",
        "    student_net = networks.StudentNetwork(Apruning_factor=0.0, teacher_net = teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "    checkpoint = torch.load(f'checkpoints_student_QAT/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_{cp}.tar')\n",
        "    student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
        "    student_net = torch.quantization.prepare_qat(student_net)\n",
        "    \n",
        "    student_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    student_net.to(fast_device)\n",
        "    student_net.eval()\n",
        "\n",
        "    student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "\n",
        "    student_net_prepared = torch.ao.quantization.prepare(student_net)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in train_loader:\n",
        "            inputs = inputs.to(fast_device)\n",
        "            student_net_prepared(inputs)  # Run a forward pass to collect activation statistics\n",
        "    student_net_prepared.to('cpu')\n",
        "\n",
        "    student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
        "    reproducibilitySeed()\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_int8, test_loader, 'cpu')\n",
        "    print('test accuracy: ', test_accuracy)\n",
        "    test_accs.append(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encountered a RuntimeError: Error(s) in loading state_dict for StudentNetwork:\n",
            "\tMissing key(s) in state_dict: \"model.model.conv1.0.weight\", \"model.model.conv1.1.weight\", \"model.model.conv1.1.bias\", \"model.model.conv1.1.running_mean\", \"model.model.conv1.1.running_var\", \"model.model.layer1.0.conv1.0.weight\", \"model.model.layer1.0.conv1.1.weight\", \"model.model.layer1.0.conv1.1.bias\", \"model.model.layer1.0.conv1.1.running_mean\", \"model.model.layer1.0.conv1.1.running_var\", \"model.model.layer1.0.conv2.0.weight\", \"model.model.layer1.0.conv2.1.weight\", \"model.model.layer1.0.conv2.1.bias\", \"model.model.layer1.0.conv2.1.running_mean\", \"model.model.layer1.0.conv2.1.running_var\", \"model.model.layer1.1.conv1.0.weight\", \"model.model.layer1.1.conv1.1.weight\", \"model.model.layer1.1.conv1.1.bias\", \"model.model.layer1.1.conv1.1.running_mean\", \"model.model.layer1.1.conv1.1.running_var\", \"model.model.layer1.1.conv2.0.weight\", \"model.model.layer1.1.conv2.1.weight\", \"model.model.layer1.1.conv2.1.bias\", \"model.model.layer1.1.conv2.1.running_mean\", \"model.model.layer1.1.conv2.1.running_var\", \"model.model.layer2.0.conv1.0.weight\", \"model.model.layer2.0.conv1.1.weight\", \"model.model.layer2.0.conv1.1.bias\", \"model.model.layer2.0.conv1.1.running_mean\", \"model.model.layer2.0.conv1.1.running_var\", \"model.model.layer2.0.conv2.0.weight\", \"model.model.layer2.0.conv2.1.weight\", \"model.model.layer2.0.conv2.1.bias\", \"model.model.layer2.0.conv2.1.running_mean\", \"model.model.layer2.0.conv2.1.running_var\", \"model.model.layer2.0.downsample.0.0.weight\", \"model.model.layer2.0.downsample.0.1.weight\", \"model.model.layer2.0.downsample.0.1.bias\", \"model.model.layer2.0.downsample.0.1.running_mean\", \"model.model.layer2.0.downsample.0.1.running_var\", \"model.model.layer2.1.conv1.0.weight\", \"model.model.layer2.1.conv1.1.weight\", \"model.model.layer2.1.conv1.1.bias\", \"model.model.layer2.1.conv1.1.running_mean\", \"model.model.layer2.1.conv1.1.running_var\", \"model.model.layer2.1.conv2.0.weight\", \"model.model.layer2.1.conv2.1.weight\", \"model.model.layer2.1.conv2.1.bias\", \"model.model.layer2.1.conv2.1.running_mean\", \"model.model.layer2.1.conv2.1.running_var\", \"model.model.layer3.0.conv1.0.weight\", \"model.model.layer3.0.conv1.1.weight\", \"model.model.layer3.0.conv1.1.bias\", \"model.model.layer3.0.conv1.1.running_mean\", \"model.model.layer3.0.conv1.1.running_var\", \"model.model.layer3.0.conv2.0.weight\", \"model.model.layer3.0.conv2.1.weight\", \"model.model.layer3.0.conv2.1.bias\", \"model.model.layer3.0.conv2.1.running_mean\", \"model.model.layer3.0.conv2.1.running_var\", \"model.model.layer3.0.downsample.0.0.weight\", \"model.model.layer3.0.downsample.0.1.weight\", \"model.model.layer3.0.downsample.0.1.bias\", \"model.model.layer3.0.downsample.0.1.running_mean\", \"model.model.layer3.0.downsample.0.1.running_var\", \"model.model.layer3.1.conv1.0.weight\", \"model.model.layer3.1.conv1.1.weight\", \"model.model.layer3.1.conv1.1.bias\", \"model.model.layer3.1.conv1.1.running_mean\", \"model.model.layer3.1.conv1.1.running_var\", \"model.model.layer3.1.conv2.0.weight\", \"model.model.layer3.1.conv2.1.weight\", \"model.model.layer3.1.conv2.1.bias\", \"model.model.layer3.1.conv2.1.running_mean\", \"model.model.layer3.1.conv2.1.running_var\", \"model.model.layer4.0.conv1.0.weight\", \"model.model.layer4.0.conv1.1.weight\", \"model.model.layer4.0.conv1.1.bias\", \"model.model.layer4.0.conv1.1.running_mean\", \"model.model.layer4.0.conv1.1.running_var\", \"model.model.layer4.0.conv2.0.weight\", \"model.model.layer4.0.conv2.1.weight\", \"model.model.layer4.0.conv2.1.bias\", \"model.model.layer4.0.conv2.1.running_mean\", \"model.model.layer4.0.conv2.1.running_var\", \"model.model.layer4.0.downsample.0.0.weight\", \"model.model.layer4.0.downsample.0.1.weight\", \"model.model.layer4.0.downsample.0.1.bias\", \"model.model.layer4.0.downsample.0.1.running_mean\", \"model.model.layer4.0.downsample.0.1.running_var\", \"model.model.layer4.1.conv1.0.weight\", \"model.model.layer4.1.conv1.1.weight\", \"model.model.layer4.1.conv1.1.bias\", \"model.model.layer4.1.conv1.1.running_mean\", \"model.model.layer4.1.conv1.1.running_var\", \"model.model.layer4.1.conv2.0.weight\", \"model.model.layer4.1.conv2.1.weight\", \"model.model.layer4.1.conv2.1.bias\", \"model.model.layer4.1.conv2.1.running_mean\", \"model.model.layer4.1.conv2.1.running_var\", \"conv1.weight\". \n",
            "\tUnexpected key(s) in state_dict: \"model.model.conv1.weight\", \"model.model.conv1.bn.weight\", \"model.model.conv1.bn.bias\", \"model.model.conv1.bn.running_mean\", \"model.model.conv1.bn.running_var\", \"model.model.conv1.bn.num_batches_tracked\", \"model.model.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.conv1.weight_fake_quant.observer_enabled\", \"model.model.conv1.weight_fake_quant.scale\", \"model.model.conv1.weight_fake_quant.zero_point\", \"model.model.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.conv1.activation_post_process.fake_quant_enabled\", \"model.model.conv1.activation_post_process.observer_enabled\", \"model.model.conv1.activation_post_process.scale\", \"model.model.conv1.activation_post_process.zero_point\", \"model.model.conv1.activation_post_process.activation_post_process.eps\", \"model.model.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv1.weight\", \"model.model.layer1.0.conv1.bn.weight\", \"model.model.layer1.0.conv1.bn.bias\", \"model.model.layer1.0.conv1.bn.running_mean\", \"model.model.layer1.0.conv1.bn.running_var\", \"model.model.layer1.0.conv1.bn.num_batches_tracked\", \"model.model.layer1.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.scale\", \"model.model.layer1.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv1.activation_post_process.scale\", \"model.model.layer1.0.conv1.activation_post_process.zero_point\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv2.weight\", \"model.model.layer1.0.conv2.bn.weight\", \"model.model.layer1.0.conv2.bn.bias\", \"model.model.layer1.0.conv2.bn.running_mean\", \"model.model.layer1.0.conv2.bn.running_var\", \"model.model.layer1.0.conv2.bn.num_batches_tracked\", \"model.model.layer1.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.scale\", \"model.model.layer1.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv2.activation_post_process.scale\", \"model.model.layer1.0.conv2.activation_post_process.zero_point\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.0.add_func.activation_post_process.scale\", \"model.model.layer1.0.add_func.activation_post_process.zero_point\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv1.weight\", \"model.model.layer1.1.conv1.bn.weight\", \"model.model.layer1.1.conv1.bn.bias\", \"model.model.layer1.1.conv1.bn.running_mean\", \"model.model.layer1.1.conv1.bn.running_var\", \"model.model.layer1.1.conv1.bn.num_batches_tracked\", \"model.model.layer1.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.scale\", \"model.model.layer1.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv1.activation_post_process.scale\", \"model.model.layer1.1.conv1.activation_post_process.zero_point\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv2.weight\", \"model.model.layer1.1.conv2.bn.weight\", \"model.model.layer1.1.conv2.bn.bias\", \"model.model.layer1.1.conv2.bn.running_mean\", \"model.model.layer1.1.conv2.bn.running_var\", \"model.model.layer1.1.conv2.bn.num_batches_tracked\", \"model.model.layer1.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.scale\", \"model.model.layer1.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv2.activation_post_process.scale\", \"model.model.layer1.1.conv2.activation_post_process.zero_point\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.1.add_func.activation_post_process.scale\", \"model.model.layer1.1.add_func.activation_post_process.zero_point\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv1.weight\", \"model.model.layer2.0.conv1.bn.weight\", \"model.model.layer2.0.conv1.bn.bias\", \"model.model.layer2.0.conv1.bn.running_mean\", \"model.model.layer2.0.conv1.bn.running_var\", \"model.model.layer2.0.conv1.bn.num_batches_tracked\", \"model.model.layer2.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.scale\", \"model.model.layer2.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv1.activation_post_process.scale\", \"model.model.layer2.0.conv1.activation_post_process.zero_point\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv2.weight\", \"model.model.layer2.0.conv2.bn.weight\", \"model.model.layer2.0.conv2.bn.bias\", \"model.model.layer2.0.conv2.bn.running_mean\", \"model.model.layer2.0.conv2.bn.running_var\", \"model.model.layer2.0.conv2.bn.num_batches_tracked\", \"model.model.layer2.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.scale\", \"model.model.layer2.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv2.activation_post_process.scale\", \"model.model.layer2.0.conv2.activation_post_process.zero_point\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.weight\", \"model.model.layer2.0.downsample.0.bn.weight\", \"model.model.layer2.0.downsample.0.bn.bias\", \"model.model.layer2.0.downsample.0.bn.running_mean\", \"model.model.layer2.0.downsample.0.bn.running_var\", \"model.model.layer2.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer2.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.scale\", \"model.model.layer2.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.0.add_func.activation_post_process.scale\", \"model.model.layer2.0.add_func.activation_post_process.zero_point\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv1.weight\", \"model.model.layer2.1.conv1.bn.weight\", \"model.model.layer2.1.conv1.bn.bias\", \"model.model.layer2.1.conv1.bn.running_mean\", \"model.model.layer2.1.conv1.bn.running_var\", \"model.model.layer2.1.conv1.bn.num_batches_tracked\", \"model.model.layer2.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.scale\", \"model.model.layer2.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv1.activation_post_process.scale\", \"model.model.layer2.1.conv1.activation_post_process.zero_point\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv2.weight\", \"model.model.layer2.1.conv2.bn.weight\", \"model.model.layer2.1.conv2.bn.bias\", \"model.model.layer2.1.conv2.bn.running_mean\", \"model.model.layer2.1.conv2.bn.running_var\", \"model.model.layer2.1.conv2.bn.num_batches_tracked\", \"model.model.layer2.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.scale\", \"model.model.layer2.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv2.activation_post_process.scale\", \"model.model.layer2.1.conv2.activation_post_process.zero_point\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.1.add_func.activation_post_process.scale\", \"model.model.layer2.1.add_func.activation_post_process.zero_point\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv1.weight\", \"model.model.layer3.0.conv1.bn.weight\", \"model.model.layer3.0.conv1.bn.bias\", \"model.model.layer3.0.conv1.bn.running_mean\", \"model.model.layer3.0.conv1.bn.running_var\", \"model.model.layer3.0.conv1.bn.num_batches_tracked\", \"model.model.layer3.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.scale\", \"model.model.layer3.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv1.activation_post_process.scale\", \"model.model.layer3.0.conv1.activation_post_process.zero_point\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv2.weight\", \"model.model.layer3.0.conv2.bn.weight\", \"model.model.layer3.0.conv2.bn.bias\", \"model.model.layer3.0.conv2.bn.running_mean\", \"model.model.layer3.0.conv2.bn.running_var\", \"model.model.layer3.0.conv2.bn.num_batches_tracked\", \"model.model.layer3.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.scale\", \"model.model.layer3.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv2.activation_post_process.scale\", \"model.model.layer3.0.conv2.activation_post_process.zero_point\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.weight\", \"model.model.layer3.0.downsample.0.bn.weight\", \"model.model.layer3.0.downsample.0.bn.bias\", \"model.model.layer3.0.downsample.0.bn.running_mean\", \"model.model.layer3.0.downsample.0.bn.running_var\", \"model.model.layer3.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer3.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.scale\", \"model.model.layer3.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.0.add_func.activation_post_process.scale\", \"model.model.layer3.0.add_func.activation_post_process.zero_point\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv1.weight\", \"model.model.layer3.1.conv1.bn.weight\", \"model.model.layer3.1.conv1.bn.bias\", \"model.model.layer3.1.conv1.bn.running_mean\", \"model.model.layer3.1.conv1.bn.running_var\", \"model.model.layer3.1.conv1.bn.num_batches_tracked\", \"model.model.layer3.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.scale\", \"model.model.layer3.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv1.activation_post_process.scale\", \"model.model.layer3.1.conv1.activation_post_process.zero_point\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv2.weight\", \"model.model.layer3.1.conv2.bn.weight\", \"model.model.layer3.1.conv2.bn.bias\", \"model.model.layer3.1.conv2.bn.running_mean\", \"model.model.layer3.1.conv2.bn.running_var\", \"model.model.layer3.1.conv2.bn.num_batches_tracked\", \"model.model.layer3.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.scale\", \"model.model.layer3.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv2.activation_post_process.scale\", \"model.model.layer3.1.conv2.activation_post_process.zero_point\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.1.add_func.activation_post_process.scale\", \"model.model.layer3.1.add_func.activation_post_process.zero_point\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv1.weight\", \"model.model.layer4.0.conv1.bn.weight\", \"model.model.layer4.0.conv1.bn.bias\", \"model.model.layer4.0.conv1.bn.running_mean\", \"model.model.layer4.0.conv1.bn.running_var\", \"model.model.layer4.0.conv1.bn.num_batches_tracked\", \"model.model.layer4.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.scale\", \"model.model.layer4.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv1.activation_post_process.scale\", \"model.model.layer4.0.conv1.activation_post_process.zero_point\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv2.weight\", \"model.model.layer4.0.conv2.bn.weight\", \"model.model.layer4.0.conv2.bn.bias\", \"model.model.layer4.0.conv2.bn.running_mean\", \"model.model.layer4.0.conv2.bn.running_var\", \"model.model.layer4.0.conv2.bn.num_batches_tracked\", \"model.model.layer4.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.scale\", \"model.model.layer4.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv2.activation_post_process.scale\", \"model.model.layer4.0.conv2.activation_post_process.zero_point\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.weight\", \"model.model.layer4.0.downsample.0.bn.weight\", \"model.model.layer4.0.downsample.0.bn.bias\", \"model.model.layer4.0.downsample.0.bn.running_mean\", \"model.model.layer4.0.downsample.0.bn.running_var\", \"model.model.layer4.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer4.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.scale\", \"model.model.layer4.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.0.add_func.activation_post_process.scale\", \"model.model.layer4.0.add_func.activation_post_process.zero_point\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv1.weight\", \"model.model.layer4.1.conv1.bn.weight\", \"model.model.layer4.1.conv1.bn.bias\", \"model.model.layer4.1.conv1.bn.running_mean\", \"model.model.layer4.1.conv1.bn.running_var\", \"model.model.layer4.1.conv1.bn.num_batches_tracked\", \"model.model.layer4.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.scale\", \"model.model.layer4.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv1.activation_post_process.scale\", \"model.model.layer4.1.conv1.activation_post_process.zero_point\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv2.weight\", \"model.model.layer4.1.conv2.bn.weight\", \"model.model.layer4.1.conv2.bn.bias\", \"model.model.layer4.1.conv2.bn.running_mean\", \"model.model.layer4.1.conv2.bn.running_var\", \"model.model.layer4.1.conv2.bn.num_batches_tracked\", \"model.model.layer4.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.scale\", \"model.model.layer4.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv2.activation_post_process.scale\", \"model.model.layer4.1.conv2.activation_post_process.zero_point\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.1.add_func.activation_post_process.scale\", \"model.model.layer4.1.add_func.activation_post_process.zero_point\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.fc.weight_fake_quant.fake_quant_enabled\", \"model.model.fc.weight_fake_quant.observer_enabled\", \"model.model.fc.weight_fake_quant.scale\", \"model.model.fc.weight_fake_quant.zero_point\", \"model.model.fc.weight_fake_quant.activation_post_process.eps\", \"model.model.fc.weight_fake_quant.activation_post_process.min_val\", \"model.model.fc.weight_fake_quant.activation_post_process.max_val\", \"model.model.fc.activation_post_process.fake_quant_enabled\", \"model.model.fc.activation_post_process.observer_enabled\", \"model.model.fc.activation_post_process.scale\", \"model.model.fc.activation_post_process.zero_point\", \"model.model.fc.activation_post_process.activation_post_process.eps\", \"model.model.fc.activation_post_process.activation_post_process.min_val\", \"model.model.fc.activation_post_process.activation_post_process.max_val\", \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\". \n",
            "Attempting to load state dict with strict=False\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    student_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "except RuntimeError as e:\n",
        "    print(\"Encountered a RuntimeError:\", e)\n",
        "    print(\"Attempting to load state dict with strict=False\")\n",
        "    student_net.load_state_dict(checkpoint['model_state_dict'], strict=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsT=4, alpha=1.0, angle_ratio=0.5, dist_ratio=(0.5,), dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0 and pruning factor 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 11173962 23520842\n",
            "Epoch 1, Batch 100: Loss = 10.8462\n",
            "Epoch 1, Batch 200: Loss = 7.9141\n",
            "Epoch 1, Batch 300: Loss = 6.9008\n",
            "Epoch 1: Validation Loss = 1.0877, Validation Accuracy = 0.63%\n",
            "Epoch 2, Batch 100: Loss = 5.4554\n",
            "Epoch 2, Batch 200: Loss = 4.1714\n",
            "Epoch 2, Batch 300: Loss = 4.5779\n",
            "Epoch 2: Validation Loss = 0.7812, Validation Accuracy = 0.72%\n",
            "Epoch 3, Batch 100: Loss = 4.5319\n",
            "Epoch 3, Batch 200: Loss = 3.5200\n",
            "Epoch 3, Batch 300: Loss = 3.8894\n",
            "Epoch 3: Validation Loss = 0.6203, Validation Accuracy = 0.79%\n",
            "Epoch 4, Batch 100: Loss = 3.0337\n",
            "Epoch 4, Batch 200: Loss = 3.8023\n",
            "Epoch 4, Batch 300: Loss = 3.1403\n",
            "Epoch 4: Validation Loss = 0.5890, Validation Accuracy = 0.80%\n",
            "Epoch 5, Batch 100: Loss = 3.4799\n",
            "Epoch 5, Batch 200: Loss = 3.7533\n",
            "Epoch 5, Batch 300: Loss = 3.2624\n",
            "Epoch 5: Validation Loss = 0.5326, Validation Accuracy = 0.82%\n",
            "Epoch 6, Batch 100: Loss = 4.0533\n",
            "Epoch 6, Batch 200: Loss = 3.9168\n",
            "Epoch 6, Batch 300: Loss = 4.1164\n",
            "Epoch 6: Validation Loss = 0.4839, Validation Accuracy = 0.83%\n",
            "Epoch 7, Batch 100: Loss = 3.2820\n",
            "Epoch 7, Batch 200: Loss = 3.0447\n",
            "Epoch 7, Batch 300: Loss = 2.5394\n",
            "Epoch 7: Validation Loss = 0.4810, Validation Accuracy = 0.84%\n",
            "Epoch 8, Batch 100: Loss = 3.3147\n",
            "Epoch 8, Batch 200: Loss = 4.3885\n",
            "Epoch 8, Batch 300: Loss = 2.5549\n",
            "Epoch 8: Validation Loss = 0.4714, Validation Accuracy = 0.84%\n",
            "Epoch 9, Batch 100: Loss = 3.7395\n",
            "Epoch 9, Batch 200: Loss = 3.2032\n",
            "Epoch 9, Batch 300: Loss = 3.8698\n",
            "Epoch 9: Validation Loss = 0.5384, Validation Accuracy = 0.82%\n",
            "Epoch 10, Batch 100: Loss = 3.1211\n",
            "Epoch 10, Batch 200: Loss = 2.8608\n",
            "Epoch 10, Batch 300: Loss = 4.8699\n",
            "Epoch 10: Validation Loss = 0.5050, Validation Accuracy = 0.83%\n",
            "Epoch 11, Batch 100: Loss = 3.2219\n",
            "Epoch 11, Batch 200: Loss = 3.9728\n",
            "Epoch 11, Batch 300: Loss = 3.2008\n",
            "Epoch 11: Validation Loss = 0.4929, Validation Accuracy = 0.83%\n",
            "Epoch 12, Batch 100: Loss = 3.5293\n",
            "Epoch 12, Batch 200: Loss = 2.5384\n",
            "Epoch 12, Batch 300: Loss = 3.8110\n",
            "Epoch 12: Validation Loss = 0.4319, Validation Accuracy = 0.87%\n",
            "Epoch 13, Batch 100: Loss = 3.6581\n",
            "Epoch 13, Batch 200: Loss = 2.1645\n",
            "Epoch 13, Batch 300: Loss = 2.0195\n",
            "Epoch 13: Validation Loss = 0.4552, Validation Accuracy = 0.86%\n",
            "Epoch 14, Batch 100: Loss = 3.0390\n",
            "Epoch 14, Batch 200: Loss = 4.1890\n",
            "Epoch 14, Batch 300: Loss = 3.4142\n",
            "Epoch 14: Validation Loss = 0.4270, Validation Accuracy = 0.87%\n",
            "Epoch 15, Batch 100: Loss = 2.7701\n",
            "Epoch 15, Batch 200: Loss = 3.5085\n",
            "Epoch 15, Batch 300: Loss = 2.9880\n",
            "Epoch 15: Validation Loss = 0.4668, Validation Accuracy = 0.87%\n",
            "Epoch 16, Batch 100: Loss = 2.7172\n",
            "Epoch 16, Batch 200: Loss = 2.1474\n",
            "Epoch 16, Batch 300: Loss = 2.8031\n",
            "Epoch 16: Validation Loss = 0.4758, Validation Accuracy = 0.86%\n",
            "Epoch 17, Batch 100: Loss = 2.7576\n",
            "Epoch 17, Batch 200: Loss = 2.5447\n",
            "Epoch 17, Batch 300: Loss = 2.9399\n",
            "Epoch 17: Validation Loss = 0.4764, Validation Accuracy = 0.86%\n",
            "Epoch 18, Batch 100: Loss = 2.2464\n",
            "Epoch 18, Batch 200: Loss = 2.7398\n",
            "Epoch 18, Batch 300: Loss = 3.0850\n",
            "Epoch 18: Validation Loss = 0.4846, Validation Accuracy = 0.87%\n",
            "Epoch 19, Batch 100: Loss = 2.2945\n",
            "Epoch 19, Batch 200: Loss = 1.9478\n",
            "Epoch 19, Batch 300: Loss = 1.6040\n",
            "Epoch 19: Validation Loss = 0.4518, Validation Accuracy = 0.88%\n",
            "Epoch 20, Batch 100: Loss = 1.8708\n",
            "Epoch 20, Batch 200: Loss = 2.3696\n",
            "Epoch 20, Batch 300: Loss = 1.6139\n",
            "Epoch 20: Validation Loss = 0.4526, Validation Accuracy = 0.88%\n",
            "Epoch 21, Batch 100: Loss = 1.7257\n",
            "Epoch 21, Batch 200: Loss = 1.9110\n",
            "Epoch 21, Batch 300: Loss = 2.4245\n",
            "Epoch 21: Validation Loss = 0.5032, Validation Accuracy = 0.88%\n",
            "Epoch 22, Batch 100: Loss = 2.1402\n",
            "Epoch 22, Batch 200: Loss = 2.4432\n",
            "Epoch 22, Batch 300: Loss = 2.0286\n",
            "Epoch 22: Validation Loss = 0.5253, Validation Accuracy = 0.88%\n",
            "Epoch 23, Batch 100: Loss = 1.0289\n",
            "Epoch 23, Batch 200: Loss = 2.4412\n",
            "Epoch 23, Batch 300: Loss = 2.0596\n",
            "Epoch 23: Validation Loss = 0.5339, Validation Accuracy = 0.88%\n",
            "Epoch 24, Batch 100: Loss = 1.7521\n",
            "Epoch 24, Batch 200: Loss = 1.8299\n",
            "Epoch 24, Batch 300: Loss = 1.9561\n",
            "Epoch 24: Validation Loss = 0.5350, Validation Accuracy = 0.88%\n",
            "Epoch 25, Batch 100: Loss = 1.1484\n",
            "Epoch 25, Batch 200: Loss = 0.8184\n",
            "Epoch 25, Batch 300: Loss = 1.9237\n",
            "Epoch 25: Validation Loss = 0.5482, Validation Accuracy = 0.88%\n",
            "Checkpoint saved at: checkpoints_student/checkpoint_epoch_25.pth\n",
            "Epoch 26, Batch 100: Loss = 1.7324\n",
            "Epoch 26, Batch 200: Loss = 1.6223\n",
            "Epoch 26, Batch 300: Loss = 1.7353\n",
            "Epoch 26: Validation Loss = 0.6126, Validation Accuracy = 0.88%\n",
            "Epoch 27, Batch 100: Loss = 1.9053\n",
            "Epoch 27, Batch 200: Loss = 2.3277\n",
            "Epoch 27, Batch 300: Loss = 2.0487\n",
            "Epoch 27: Validation Loss = 0.5941, Validation Accuracy = 0.88%\n",
            "Epoch 28, Batch 100: Loss = 1.3187\n",
            "Epoch 28, Batch 200: Loss = 2.0089\n",
            "Epoch 28, Batch 300: Loss = 1.2275\n",
            "Epoch 28: Validation Loss = 0.5580, Validation Accuracy = 0.89%\n",
            "Epoch 29, Batch 100: Loss = 2.4250\n",
            "Epoch 29, Batch 200: Loss = 1.4135\n",
            "Epoch 29, Batch 300: Loss = 1.5065\n",
            "Epoch 29: Validation Loss = 0.5836, Validation Accuracy = 0.88%\n",
            "Epoch 30, Batch 100: Loss = 1.2179\n",
            "Epoch 30, Batch 200: Loss = 1.8056\n",
            "Epoch 30, Batch 300: Loss = 1.2113\n",
            "Epoch 30: Validation Loss = 0.6212, Validation Accuracy = 0.88%\n",
            "Epoch 31, Batch 100: Loss = 1.8033\n",
            "Epoch 31, Batch 200: Loss = 1.8045\n",
            "Epoch 31, Batch 300: Loss = 1.7103\n",
            "Epoch 31: Validation Loss = 0.6317, Validation Accuracy = 0.89%\n",
            "Epoch 32, Batch 100: Loss = 1.9026\n",
            "Epoch 32, Batch 200: Loss = 1.6407\n",
            "Epoch 32, Batch 300: Loss = 0.9113\n",
            "Epoch 32: Validation Loss = 0.6177, Validation Accuracy = 0.89%\n",
            "Epoch 33, Batch 100: Loss = 2.1022\n",
            "Epoch 33, Batch 200: Loss = 1.7019\n",
            "Epoch 33, Batch 300: Loss = 2.2042\n",
            "Epoch 33: Validation Loss = 0.6031, Validation Accuracy = 0.89%\n",
            "Epoch 34, Batch 100: Loss = 2.0035\n",
            "Epoch 34, Batch 200: Loss = 2.0024\n",
            "Epoch 34, Batch 300: Loss = 1.4056\n",
            "Epoch 34: Validation Loss = 0.6109, Validation Accuracy = 0.89%\n",
            "Epoch 35, Batch 100: Loss = 2.0010\n",
            "Epoch 35, Batch 200: Loss = 1.9029\n",
            "Epoch 35, Batch 300: Loss = 1.3049\n",
            "Epoch 35: Validation Loss = 0.6087, Validation Accuracy = 0.90%\n",
            "Epoch 36, Batch 100: Loss = 1.4057\n",
            "Epoch 36, Batch 200: Loss = 1.4005\n",
            "Epoch 36, Batch 300: Loss = 1.1005\n",
            "Epoch 36: Validation Loss = 0.6365, Validation Accuracy = 0.89%\n",
            "Epoch 37, Batch 100: Loss = 0.9006\n",
            "Epoch 37, Batch 200: Loss = 1.3005\n",
            "Epoch 37, Batch 300: Loss = 1.3005\n",
            "Epoch 37: Validation Loss = 0.6140, Validation Accuracy = 0.90%\n",
            "Epoch 38, Batch 100: Loss = 1.3003\n",
            "Epoch 38, Batch 200: Loss = 1.6005\n",
            "Epoch 38, Batch 300: Loss = 1.1005\n",
            "Epoch 38: Validation Loss = 0.6325, Validation Accuracy = 0.90%\n",
            "Epoch 39, Batch 100: Loss = 1.4018\n",
            "Epoch 39, Batch 200: Loss = 1.1012\n",
            "Epoch 39, Batch 300: Loss = 0.8001\n",
            "Epoch 39: Validation Loss = 0.6533, Validation Accuracy = 0.90%\n",
            "Epoch 40, Batch 100: Loss = 2.3013\n",
            "Epoch 40, Batch 200: Loss = 1.4004\n",
            "Epoch 40, Batch 300: Loss = 1.3006\n",
            "Epoch 40: Validation Loss = 0.6519, Validation Accuracy = 0.89%\n",
            "Epoch 41, Batch 100: Loss = 0.9009\n",
            "Epoch 41, Batch 200: Loss = 1.2008\n",
            "Epoch 41, Batch 300: Loss = 1.4001\n",
            "Epoch 41: Validation Loss = 0.6806, Validation Accuracy = 0.89%\n",
            "Epoch 42, Batch 100: Loss = 1.2003\n",
            "Epoch 42, Batch 200: Loss = 1.6002\n",
            "Epoch 42, Batch 300: Loss = 1.5013\n",
            "Epoch 42: Validation Loss = 0.6758, Validation Accuracy = 0.90%\n",
            "Epoch 43, Batch 100: Loss = 0.6003\n",
            "Epoch 43, Batch 200: Loss = 0.7001\n",
            "Epoch 43, Batch 300: Loss = 1.1003\n",
            "Epoch 43: Validation Loss = 0.6623, Validation Accuracy = 0.90%\n",
            "Epoch 44, Batch 100: Loss = 1.7001\n",
            "Epoch 44, Batch 200: Loss = 1.3002\n",
            "Epoch 44, Batch 300: Loss = 1.2004\n",
            "Epoch 44: Validation Loss = 0.6890, Validation Accuracy = 0.90%\n",
            "Epoch 45, Batch 100: Loss = 1.5001\n",
            "Epoch 45, Batch 200: Loss = 1.2002\n",
            "Epoch 45, Batch 300: Loss = 1.0001\n",
            "Epoch 45: Validation Loss = 0.7015, Validation Accuracy = 0.89%\n",
            "Epoch 46, Batch 100: Loss = 1.4002\n",
            "Epoch 46, Batch 200: Loss = 0.5007\n",
            "Epoch 46, Batch 300: Loss = 1.1001\n",
            "Epoch 46: Validation Loss = 0.7076, Validation Accuracy = 0.89%\n",
            "Epoch 47, Batch 100: Loss = 1.0007\n",
            "Epoch 47, Batch 200: Loss = 1.0003\n",
            "Epoch 47, Batch 300: Loss = 0.4003\n",
            "Epoch 47: Validation Loss = 0.7168, Validation Accuracy = 0.90%\n",
            "Epoch 48, Batch 100: Loss = 1.3003\n",
            "Epoch 48, Batch 200: Loss = 0.8002\n",
            "Epoch 48, Batch 300: Loss = 0.8002\n",
            "Epoch 48: Validation Loss = 0.7032, Validation Accuracy = 0.90%\n",
            "Epoch 49, Batch 100: Loss = 1.1001\n",
            "Epoch 49, Batch 200: Loss = 1.4002\n",
            "Epoch 49, Batch 300: Loss = 1.0002\n",
            "Epoch 49: Validation Loss = 0.7065, Validation Accuracy = 0.90%\n",
            "Epoch 50, Batch 100: Loss = 1.1003\n",
            "Epoch 50, Batch 200: Loss = 0.8002\n",
            "Epoch 50, Batch 300: Loss = 1.6002\n",
            "Epoch 50: Validation Loss = 0.6811, Validation Accuracy = 0.90%\n",
            "Checkpoint saved at: checkpoints_student/checkpoint_epoch_50.pth\n",
            "Epoch 51, Batch 100: Loss = 1.5001\n",
            "Epoch 51, Batch 200: Loss = 1.0002\n",
            "Epoch 51, Batch 300: Loss = 1.0001\n",
            "Epoch 51: Validation Loss = 0.7011, Validation Accuracy = 0.90%\n",
            "Epoch 52, Batch 100: Loss = 1.0001\n",
            "Epoch 52, Batch 200: Loss = 1.4003\n",
            "Epoch 52, Batch 300: Loss = 1.3002\n",
            "Epoch 52: Validation Loss = 0.7318, Validation Accuracy = 0.90%\n",
            "Epoch 53, Batch 100: Loss = 0.9002\n",
            "Epoch 53, Batch 200: Loss = 1.0002\n",
            "Epoch 53, Batch 300: Loss = 1.1003\n",
            "Epoch 53: Validation Loss = 0.6826, Validation Accuracy = 0.90%\n",
            "Epoch 54, Batch 100: Loss = 1.4001\n",
            "Epoch 54, Batch 200: Loss = 1.5001\n",
            "Epoch 54, Batch 300: Loss = 0.6002\n",
            "Epoch 54: Validation Loss = 0.7347, Validation Accuracy = 0.89%\n",
            "Epoch 55, Batch 100: Loss = 1.3002\n",
            "Epoch 55, Batch 200: Loss = 1.2002\n",
            "Epoch 55, Batch 300: Loss = 1.4001\n",
            "Epoch 55: Validation Loss = 0.7445, Validation Accuracy = 0.90%\n",
            "Epoch 56, Batch 100: Loss = 1.5001\n",
            "Epoch 56, Batch 200: Loss = 1.1002\n",
            "Epoch 56, Batch 300: Loss = 1.4002\n",
            "Epoch 56: Validation Loss = 0.7692, Validation Accuracy = 0.89%\n",
            "Epoch 57, Batch 100: Loss = 1.0002\n",
            "Epoch 57, Batch 200: Loss = 0.7002\n",
            "Epoch 57, Batch 300: Loss = 0.6002\n",
            "Epoch 57: Validation Loss = 0.7272, Validation Accuracy = 0.89%\n",
            "Epoch 58, Batch 100: Loss = 0.7001\n",
            "Epoch 58, Batch 200: Loss = 1.1002\n",
            "Epoch 58, Batch 300: Loss = 0.9001\n",
            "Epoch 58: Validation Loss = 0.7283, Validation Accuracy = 0.90%\n",
            "Epoch 59, Batch 100: Loss = 1.1001\n",
            "Epoch 59, Batch 200: Loss = 1.0002\n",
            "Epoch 59, Batch 300: Loss = 1.3002\n",
            "Epoch 59: Validation Loss = 0.7650, Validation Accuracy = 0.90%\n",
            "Epoch 60, Batch 100: Loss = 1.6001\n",
            "Epoch 60, Batch 200: Loss = 1.4002\n",
            "Epoch 60, Batch 300: Loss = 1.2001\n",
            "Epoch 60: Validation Loss = 0.7307, Validation Accuracy = 0.90%\n",
            "Epoch 61, Batch 100: Loss = 0.9002\n",
            "Epoch 61, Batch 200: Loss = 1.2001\n",
            "Epoch 61, Batch 300: Loss = 1.6001\n",
            "Epoch 61: Validation Loss = 0.7414, Validation Accuracy = 0.90%\n",
            "Epoch 62, Batch 100: Loss = 1.2001\n",
            "Epoch 62, Batch 200: Loss = 1.0001\n",
            "Epoch 62, Batch 300: Loss = 1.7001\n",
            "Epoch 62: Validation Loss = 0.7355, Validation Accuracy = 0.90%\n",
            "Epoch 63, Batch 100: Loss = 1.1002\n",
            "Epoch 63, Batch 200: Loss = 0.8002\n",
            "Epoch 63, Batch 300: Loss = 1.7000\n",
            "Epoch 63: Validation Loss = 0.7315, Validation Accuracy = 0.89%\n",
            "Epoch 64, Batch 100: Loss = 1.0001\n",
            "Epoch 64, Batch 200: Loss = 1.4001\n",
            "Epoch 64, Batch 300: Loss = 0.8001\n",
            "Epoch 64: Validation Loss = 0.7468, Validation Accuracy = 0.90%\n",
            "Epoch 65, Batch 100: Loss = 0.9002\n",
            "Epoch 65, Batch 200: Loss = 0.8001\n",
            "Epoch 65, Batch 300: Loss = 1.0001\n",
            "Epoch 65: Validation Loss = 0.7548, Validation Accuracy = 0.90%\n",
            "Epoch 66, Batch 100: Loss = 1.0002\n",
            "Epoch 66, Batch 200: Loss = 0.7002\n",
            "Epoch 66, Batch 300: Loss = 0.9000\n",
            "Epoch 66: Validation Loss = 0.7893, Validation Accuracy = 0.89%\n",
            "Epoch 67, Batch 100: Loss = 1.6002\n",
            "Epoch 67, Batch 200: Loss = 1.5001\n",
            "Epoch 67, Batch 300: Loss = 1.2001\n",
            "Epoch 67: Validation Loss = 0.7711, Validation Accuracy = 0.89%\n",
            "Epoch 68, Batch 100: Loss = 0.8000\n",
            "Epoch 68, Batch 200: Loss = 1.3001\n",
            "Epoch 68, Batch 300: Loss = 0.8001\n",
            "Epoch 68: Validation Loss = 0.7603, Validation Accuracy = 0.90%\n",
            "Epoch 69, Batch 100: Loss = 0.5001\n",
            "Epoch 69, Batch 200: Loss = 0.9001\n",
            "Epoch 69, Batch 300: Loss = 0.5001\n",
            "Epoch 69: Validation Loss = 0.7392, Validation Accuracy = 0.90%\n",
            "Epoch 70, Batch 100: Loss = 1.1001\n",
            "Epoch 70, Batch 200: Loss = 1.0002\n",
            "Epoch 70, Batch 300: Loss = 0.9001\n",
            "Epoch 70: Validation Loss = 0.8044, Validation Accuracy = 0.89%\n",
            "Epoch 71, Batch 100: Loss = 1.2000\n",
            "Epoch 71, Batch 200: Loss = 1.2001\n",
            "Epoch 71, Batch 300: Loss = 1.3002\n",
            "Epoch 71: Validation Loss = 0.7770, Validation Accuracy = 0.89%\n",
            "Epoch 72, Batch 100: Loss = 0.9001\n",
            "Epoch 72, Batch 200: Loss = 0.2001\n",
            "Epoch 72, Batch 300: Loss = 1.5001\n",
            "Epoch 72: Validation Loss = 0.7748, Validation Accuracy = 0.89%\n",
            "Epoch 73, Batch 100: Loss = 1.1002\n",
            "Epoch 73, Batch 200: Loss = 0.7001\n",
            "Epoch 73, Batch 300: Loss = 0.7001\n",
            "Epoch 73: Validation Loss = 0.7703, Validation Accuracy = 0.90%\n",
            "Epoch 74, Batch 100: Loss = 1.3001\n",
            "Epoch 74, Batch 200: Loss = 0.9001\n",
            "Epoch 74, Batch 300: Loss = 0.9001\n",
            "Epoch 74: Validation Loss = 0.7814, Validation Accuracy = 0.89%\n",
            "Epoch 75, Batch 100: Loss = 1.0001\n",
            "Epoch 75, Batch 200: Loss = 1.3000\n",
            "Epoch 75, Batch 300: Loss = 1.0001\n",
            "Epoch 75: Validation Loss = 0.7791, Validation Accuracy = 0.89%\n",
            "Checkpoint saved at: checkpoints_student/checkpoint_epoch_75.pth\n",
            "Epoch 76, Batch 100: Loss = 1.3001\n",
            "Epoch 76, Batch 200: Loss = 1.1000\n",
            "Epoch 76, Batch 300: Loss = 1.2001\n",
            "Epoch 76: Validation Loss = 0.7750, Validation Accuracy = 0.90%\n",
            "Epoch 77, Batch 100: Loss = 0.7001\n",
            "Epoch 77, Batch 200: Loss = 0.9001\n",
            "Epoch 77, Batch 300: Loss = 0.7001\n",
            "Epoch 77: Validation Loss = 0.7349, Validation Accuracy = 0.90%\n",
            "Epoch 78, Batch 100: Loss = 0.7001\n",
            "Epoch 78, Batch 200: Loss = 1.1001\n",
            "Epoch 78, Batch 300: Loss = 1.1001\n",
            "Epoch 78: Validation Loss = 0.7628, Validation Accuracy = 0.89%\n",
            "Epoch 79, Batch 100: Loss = 1.3001\n",
            "Epoch 79, Batch 200: Loss = 0.8000\n",
            "Epoch 79, Batch 300: Loss = 0.4000\n",
            "Epoch 79: Validation Loss = 0.7567, Validation Accuracy = 0.90%\n",
            "Epoch 80, Batch 100: Loss = 1.2002\n",
            "Epoch 80, Batch 200: Loss = 1.1001\n",
            "Epoch 80, Batch 300: Loss = 0.7000\n",
            "Epoch 80: Validation Loss = 0.7726, Validation Accuracy = 0.90%\n",
            "Epoch 81, Batch 100: Loss = 0.9002\n",
            "Epoch 81, Batch 200: Loss = 1.0001\n",
            "Epoch 81, Batch 300: Loss = 1.1001\n",
            "Epoch 81: Validation Loss = 0.7846, Validation Accuracy = 0.90%\n",
            "Epoch 82, Batch 100: Loss = 1.2001\n",
            "Epoch 82, Batch 200: Loss = 0.3001\n",
            "Epoch 82, Batch 300: Loss = 1.0001\n",
            "Epoch 82: Validation Loss = 0.7595, Validation Accuracy = 0.90%\n",
            "Epoch 83, Batch 100: Loss = 1.2001\n",
            "Epoch 83, Batch 200: Loss = 1.0001\n",
            "Epoch 83, Batch 300: Loss = 1.0001\n",
            "Epoch 83: Validation Loss = 0.7536, Validation Accuracy = 0.89%\n",
            "Epoch 84, Batch 100: Loss = 0.8001\n",
            "Epoch 84, Batch 200: Loss = 1.3001\n",
            "Epoch 84, Batch 300: Loss = 0.7001\n",
            "Epoch 84: Validation Loss = 0.7969, Validation Accuracy = 0.89%\n",
            "Epoch 85, Batch 100: Loss = 1.1000\n",
            "Epoch 85, Batch 200: Loss = 0.7001\n",
            "Epoch 85, Batch 300: Loss = 0.9001\n",
            "Epoch 85: Validation Loss = 0.7934, Validation Accuracy = 0.89%\n",
            "Epoch 86, Batch 100: Loss = 0.9000\n",
            "Epoch 86, Batch 200: Loss = 0.9001\n",
            "Epoch 86, Batch 300: Loss = 0.8000\n",
            "Epoch 86: Validation Loss = 0.7526, Validation Accuracy = 0.90%\n",
            "Epoch 87, Batch 100: Loss = 0.7002\n",
            "Epoch 87, Batch 200: Loss = 1.3001\n",
            "Epoch 87, Batch 300: Loss = 1.4001\n",
            "Epoch 87: Validation Loss = 0.7654, Validation Accuracy = 0.90%\n",
            "Epoch 88, Batch 100: Loss = 1.2001\n",
            "Epoch 88, Batch 200: Loss = 0.5001\n",
            "Epoch 88, Batch 300: Loss = 0.6002\n",
            "Epoch 88: Validation Loss = 0.7464, Validation Accuracy = 0.90%\n",
            "Epoch 89, Batch 100: Loss = 0.9001\n",
            "Epoch 89, Batch 200: Loss = 0.8000\n",
            "Epoch 89, Batch 300: Loss = 1.0001\n",
            "Epoch 89: Validation Loss = 0.7599, Validation Accuracy = 0.90%\n",
            "Epoch 90, Batch 100: Loss = 1.0001\n",
            "Epoch 90, Batch 200: Loss = 0.7001\n",
            "Epoch 90, Batch 300: Loss = 0.6001\n",
            "Epoch 90: Validation Loss = 0.7818, Validation Accuracy = 0.89%\n",
            "Epoch 91, Batch 100: Loss = 0.9001\n",
            "Epoch 91, Batch 200: Loss = 0.6001\n",
            "Epoch 91, Batch 300: Loss = 1.7000\n",
            "Epoch 91: Validation Loss = 0.7744, Validation Accuracy = 0.89%\n",
            "Epoch 92, Batch 100: Loss = 1.1001\n",
            "Epoch 92, Batch 200: Loss = 0.8000\n",
            "Epoch 92, Batch 300: Loss = 0.9001\n",
            "Epoch 92: Validation Loss = 0.7783, Validation Accuracy = 0.89%\n",
            "Epoch 93, Batch 100: Loss = 1.1001\n",
            "Epoch 93, Batch 200: Loss = 1.4001\n",
            "Epoch 93, Batch 300: Loss = 0.4000\n",
            "Epoch 93: Validation Loss = 0.7721, Validation Accuracy = 0.89%\n",
            "Epoch 94, Batch 100: Loss = 0.9001\n",
            "Epoch 94, Batch 200: Loss = 0.9001\n",
            "Epoch 94, Batch 300: Loss = 1.1001\n",
            "Epoch 94: Validation Loss = 0.7879, Validation Accuracy = 0.89%\n",
            "Epoch 95, Batch 100: Loss = 0.8001\n",
            "Epoch 95, Batch 200: Loss = 1.0001\n",
            "Epoch 95, Batch 300: Loss = 1.2000\n",
            "Epoch 95: Validation Loss = 0.8231, Validation Accuracy = 0.89%\n",
            "Epoch 96, Batch 100: Loss = 1.5001\n",
            "Epoch 96, Batch 200: Loss = 0.7001\n",
            "Epoch 96, Batch 300: Loss = 1.3001\n",
            "Epoch 96: Validation Loss = 0.7862, Validation Accuracy = 0.89%\n",
            "Epoch 97, Batch 100: Loss = 1.0001\n",
            "Epoch 97, Batch 200: Loss = 0.9001\n",
            "Epoch 97, Batch 300: Loss = 1.5001\n",
            "Epoch 97: Validation Loss = 0.7729, Validation Accuracy = 0.90%\n",
            "Epoch 98, Batch 100: Loss = 1.3000\n",
            "Epoch 98, Batch 200: Loss = 1.5001\n",
            "Epoch 98, Batch 300: Loss = 0.6001\n",
            "Epoch 98: Validation Loss = 0.7584, Validation Accuracy = 0.90%\n",
            "Epoch 99, Batch 100: Loss = 0.7001\n",
            "Epoch 99, Batch 200: Loss = 0.8000\n",
            "Epoch 99, Batch 300: Loss = 0.7000\n",
            "Epoch 99: Validation Loss = 0.7761, Validation Accuracy = 0.89%\n",
            "Epoch 100, Batch 100: Loss = 0.6000\n",
            "Epoch 100, Batch 200: Loss = 0.9001\n",
            "Epoch 100, Batch 300: Loss = 0.7001\n",
            "Epoch 100: Validation Loss = 0.7736, Validation Accuracy = 0.90%\n",
            "Checkpoint saved at: checkpoints_student/checkpoint_epoch_100.pth\n",
            "Epoch 101, Batch 100: Loss = 1.3001\n",
            "Epoch 101, Batch 200: Loss = 0.9001\n",
            "Epoch 101, Batch 300: Loss = 0.7000\n",
            "Epoch 101: Validation Loss = 0.7943, Validation Accuracy = 0.90%\n",
            "Epoch 102, Batch 100: Loss = 0.9000\n",
            "Epoch 102, Batch 200: Loss = 0.5001\n",
            "Epoch 102, Batch 300: Loss = 1.1000\n",
            "Epoch 102: Validation Loss = 0.8054, Validation Accuracy = 0.90%\n",
            "Epoch 103, Batch 100: Loss = 0.8000\n",
            "Epoch 103, Batch 200: Loss = 0.9001\n",
            "Epoch 103, Batch 300: Loss = 1.3000\n",
            "Epoch 103: Validation Loss = 0.7745, Validation Accuracy = 0.89%\n",
            "Epoch 104, Batch 100: Loss = 1.0001\n",
            "Epoch 104, Batch 200: Loss = 1.0000\n",
            "Epoch 104, Batch 300: Loss = 0.6001\n",
            "Epoch 104: Validation Loss = 0.8237, Validation Accuracy = 0.89%\n",
            "Epoch 105, Batch 100: Loss = 0.5000\n",
            "Epoch 105, Batch 200: Loss = 0.7001\n",
            "Epoch 105, Batch 300: Loss = 0.9000\n",
            "Epoch 105: Validation Loss = 0.8142, Validation Accuracy = 0.89%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m student_params_num \u001b[38;5;241m=\u001b[39m count_parameters(prepared_student)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(pruning_factor, student_params_num, count_parameters(teacher_net))\n\u001b[1;32m---> 57\u001b[0m results_distill[(hparam_tuple, pruning_factor)] \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainStudentOnHparamAT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepared_student\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfast_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcheckpoints_path_student\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     66\u001b[0m prepared_student\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\17598\\Downloads\\DSC180B_Q2_Project\\utils.py:841\u001b[0m, in \u001b[0;36mtrainStudentOnHparamAT\u001b[1;34m(teacher_net, student_net, hparam, num_epochs, train_loader, val_loader, print_every, fast_device, quant, checkpoint_save_path, resume_checkpoint, optimizer_choice)\u001b[0m\n\u001b[0;32m    838\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(fast_device), labels\u001b[38;5;241m.\u001b[39mto(fast_device)\n\u001b[0;32m    839\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 841\u001b[0m student_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mstudent_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    843\u001b[0m     teacher_outputs \u001b[38;5;241m=\u001b[39m teacher_net(inputs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\Downloads\\DSC180B_Q2_Project\\networks.py:105\u001b[0m, in \u001b[0;36mStudentNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[1;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(x)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\Downloads\\DSC180B_Q2_Project\\networks.py:27\u001b[0m, in \u001b[0;36mTeacherNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\Downloads\\DSC180B_Q2_Project\\quantized_resnet18.py:17\u001b[0m, in \u001b[0;36mQuantizedBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1803\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1801\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1803\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1806\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:147\u001b[0m, in \u001b[0;36m_observer_forward_hook\u001b[1;34m(self, input, output)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_observer_forward_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[0;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward hook that calls observer on the output\"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\fake_quantize.py:408\u001b[0m, in \u001b[0;36mFusedMovingAvgObsFakeQuantize.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfused_moving_avg_obs_fake_quant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserver_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_quant_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maveraging_constant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mch_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_per_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_symmetric_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Hypothetical setup, please adjust according to actual import paths and methods\n",
        "temperatures = [4]\n",
        "alphas = [1.0]\n",
        "learning_rates = [1e-3]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [0.0]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "pruning_factor = 0\n",
        "\n",
        "\n",
        "checkpoints_path_student = 'checkpoints_student/'\n",
        "\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {}\n",
        "    hparam['alpha'] = hparam_tuple[0]\n",
        "    hparam['T'] = hparam_tuple[1]\n",
        "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
        "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
        "    hparam['weight_decay'] = hparam_tuple[3]\n",
        "    hparam['lr_decay'] = hparam_tuple[4]\n",
        "    hparam['momentum'] = hparam_tuple[5]\n",
        "    hparam['lr'] = hparam_tuple[6]\n",
        "    hparam['dist_ratio'] = 0.5,\n",
        "    hparam['angle_ratio'] = 0.5\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "results_distill = {}\n",
        "pruning_factors = [0]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = checkpoints_path_student + \"results_student.csv\"\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for hparam in hparams_list:\n",
        "    print('Training with hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    reproducibilitySeed()\n",
        "    student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "    student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
        "    prepared_student = torch.quantization.prepare_qat(student_net)\n",
        "    prepared_student.to(fast_device)\n",
        "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "    # Count parameters\n",
        "    student_params_num = count_parameters(prepared_student)\n",
        "    \n",
        "    print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "    results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparamAT(\n",
        "            teacher_net, prepared_student, hparam, num_epochs,\n",
        "            train_loader, val_loader,\n",
        "            print_every=print_every,\n",
        "            fast_device=fast_device, quant=True, checkpoint_save_path= checkpoints_path_student, resume_checkpoint=False,\n",
        "            optimizer_choice='sgd'\n",
        "        )\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    prepared_student.eval()\n",
        "    \n",
        "    quantized_model = torch.quantization.convert(prepared_student)\n",
        "\n",
        "    # Final model save\n",
        "    final_save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
        "    torch.save({\n",
        "        'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "        'model_state_dict': quantized_model.state_dict(),\n",
        "        'epoch': num_epochs\n",
        "    }, final_save_path)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, fast_device) # 'cpu'\n",
        "    print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "    # Write results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            hparam['alpha'], hparam['T'], hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "            hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "            test_accuracy, training_time\n",
        "        ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_10944\\538120021.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f'checkpoints_student/checkpoint_epoch_{cp}.pth')\n",
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.8745\n",
            "test accuracy:  0.8901\n",
            "test accuracy:  0.8906\n",
            "test accuracy:  0.89\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torch.quantization import fuse_modules\n",
        "\n",
        "checkpoints = [x for x in range(25, 101, 25)]\n",
        "test_accs = []\n",
        "\n",
        "for cp in checkpoints:\n",
        "    student_net = networks.StudentNetwork(pruning_factor=0.0, teacher_net = teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "    checkpoint = torch.load(f'checkpoints_student/checkpoint_epoch_{cp}.pth')\n",
        "    student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
        "    student_net = torch.quantization.prepare_qat(student_net)\n",
        "    \n",
        "    student_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    student_net.to(fast_device)\n",
        "    student_net.eval()\n",
        "\n",
        "    student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "\n",
        "    student_net_prepared = torch.ao.quantization.prepare(student_net)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in train_loader:\n",
        "            inputs = inputs.to(fast_device)\n",
        "            student_net_prepared(inputs)  # Run a forward pass to collect activation statistics\n",
        "    student_net_prepared.to(fast_device)\n",
        "\n",
        "    #student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
        "    reproducibilitySeed()\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_prepared, test_loader, fast_device)\n",
        "    print('test accuracy: ', test_accuracy)\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "# Plotting the test accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(checkpoints, test_accs, marker='o', linestyle='-', color='b')\n",
        "plt.title('Test Accuracy of Quantized Models at Different Checkpoints')\n",
        "plt.xlabel('Checkpoint Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(checkpoints)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved for Student 0 at Epoch 25: checkpoints_student/student_0_epoch_24.pth\n",
            "Checkpoint saved for Student 1 at Epoch 25: checkpoints_student/student_1_epoch_24.pth\n",
            "Checkpoint saved for Student 0 at Epoch 50: checkpoints_student/student_0_epoch_49.pth\n",
            "Checkpoint saved for Student 1 at Epoch 50: checkpoints_student/student_1_epoch_49.pth\n",
            "Checkpoint saved for Student 0 at Epoch 75: checkpoints_student/student_0_epoch_74.pth\n",
            "Checkpoint saved for Student 1 at Epoch 75: checkpoints_student/student_1_epoch_74.pth\n",
            "Checkpoint saved for Student 0 at Epoch 100: checkpoints_student/student_0_epoch_99.pth\n",
            "Checkpoint saved for Student 1 at Epoch 100: checkpoints_student/student_1_epoch_99.pth\n",
            "Checkpoint saved for Student 0 at Epoch 125: checkpoints_student/student_0_epoch_124.pth\n",
            "Checkpoint saved for Student 1 at Epoch 125: checkpoints_student/student_1_epoch_124.pth\n",
            "Checkpoint saved for Student 0 at Epoch 150: checkpoints_student/student_0_epoch_149.pth\n",
            "Checkpoint saved for Student 1 at Epoch 150: checkpoints_student/student_1_epoch_149.pth\n",
            "Checkpoint saved for Student 0 at Epoch 175: checkpoints_student/student_0_epoch_174.pth\n",
            "Checkpoint saved for Student 1 at Epoch 175: checkpoints_student/student_1_epoch_174.pth\n",
            "Checkpoint saved for Student 0 at Epoch 200: checkpoints_student/student_0_epoch_199.pth\n",
            "Checkpoint saved for Student 1 at Epoch 200: checkpoints_student/student_1_epoch_199.pth\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup for the hyperparameters and training configurations\n",
        "temperatures = [4]\n",
        "alphas = [1.0]\n",
        "learning_rates = [1e-3]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [0.0]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "pruning_factor = 0\n",
        "\n",
        "checkpoints_path_student = 'checkpoints_student/'\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'alpha': hparam_tuple[0],\n",
        "        'T': hparam_tuple[1],\n",
        "        'dropout_input': hparam_tuple[2][0],\n",
        "        'dropout_hidden': hparam_tuple[2][1],\n",
        "        'weight_decay': hparam_tuple[3],\n",
        "        'lr_decay': hparam_tuple[4],\n",
        "        'momentum': hparam_tuple[5],\n",
        "        'lr': hparam_tuple[6]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Training and logging setup\n",
        "csv_file = os.path.join(checkpoints_path_student, \"results_student.csv\")\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Create multiple student models and their optimizers\n",
        "student_models = [networks.StudentNetwork(pruning_factor=0.0, teacher_net = teacher_net, q=True, fuse=True, qat=True, dif_arch=True),  networks.StudentNetwork(pruning_factor=0.0, teacher_net = teacher_net, q=True, fuse=True, qat=True, dif_arch=True)]  # Create multiple instances for DML\n",
        "optimizers = [optim.SGD(student.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0) for student in student_models]\n",
        "\n",
        "def trainDML(student_models, hparam, num_epochs, train_loader, val_loader, optimizers, fast_device=torch.device('cuda:0')):\n",
        "    for student in student_models:\n",
        "        student.to(fast_device).train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(fast_device), labels.to(fast_device)\n",
        "\n",
        "            # Zero the parameter gradients for all optimizers at the start of the batch processing\n",
        "            for optimizer in optimizers:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            outputs = [student(data) for student in student_models]\n",
        "            losses = []\n",
        "\n",
        "            # Calculate losses for all students but do not yet backpropagate\n",
        "            for i, student_output in enumerate(outputs):\n",
        "                loss = criterion(student_output, labels)  # Standard classification loss\n",
        "                dml_loss = sum(F.mse_loss(F.softmax(student_output, dim=1), F.softmax(other_output, dim=1))\n",
        "                               for j, other_output in enumerate(outputs) if i != j)\n",
        "                total_loss = loss + dml_loss / (len(student_models) - 1)  # Normalize DML loss\n",
        "                losses.append(total_loss)\n",
        "\n",
        "            # Now backpropagate for all students\n",
        "            for i, loss in enumerate(losses):\n",
        "                loss.backward(retain_graph=True if i < len(student_models) - 1 else False)  # Retain graph for all but last loss\n",
        "\n",
        "            # Step the optimizers after all gradients are calculated\n",
        "            for optimizer in optimizers:\n",
        "                optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 25 == 0:\n",
        "            for idx, student in enumerate(student_models):\n",
        "                torch.save(student.state_dict(), f\"{checkpoints_path_student}student_{idx}_epoch_{epoch}.pth\")\n",
        "                print(f\"Checkpoint saved for Student {idx} at Epoch {epoch + 1}: {checkpoints_path_student}student_{idx}_epoch_{epoch}.pth\")\n",
        "\n",
        "        # Add validation and additional logging as necessary\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# Make sure to call this function with the correct parameters and dataloaders\n",
        "trainDML(student_models, hparam, 200, train_loader, val_loader, optimizers)\n",
        "# Further steps such as evaluation and logging results to CSV as shown above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating checkpoint at epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_13036\\869396217.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=fast_device)  # Ensure it loads on the correct device\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student 1 test accuracy at epoch 25: 0.8892%\n",
            "Student 2 test accuracy at epoch 25: 0.8851%\n",
            "Evaluating checkpoint at epoch 50\n",
            "Student 1 test accuracy at epoch 50: 0.8898%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000014BB5711630>\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
            "    if self._persistent_workers or self._workers_status[worker_id]:\n",
            "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student 2 test accuracy at epoch 50: 0.8902%\n",
            "Evaluating checkpoint at epoch 75\n",
            "Student 1 test accuracy at epoch 75: 0.894%\n",
            "Student 2 test accuracy at epoch 75: 0.893%\n",
            "Evaluating checkpoint at epoch 100\n",
            "Student 1 test accuracy at epoch 100: 0.8985%\n",
            "Student 2 test accuracy at epoch 100: 0.8973%\n",
            "Evaluating checkpoint at epoch 125\n",
            "Student 1 test accuracy at epoch 125: 0.8934%\n",
            "Student 2 test accuracy at epoch 125: 0.8891%\n",
            "Evaluating checkpoint at epoch 150\n",
            "Student 1 test accuracy at epoch 150: 0.8973%\n",
            "Student 2 test accuracy at epoch 150: 0.8937%\n",
            "Evaluating checkpoint at epoch 175\n",
            "Student 1 test accuracy at epoch 175: 0.8978%\n",
            "Student 2 test accuracy at epoch 175: 0.8981%\n",
            "Evaluating checkpoint at epoch 200\n",
            "Student 1 test accuracy at epoch 200: 0.8968%\n",
            "Student 2 test accuracy at epoch 200: 0.8982%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torch.quantization import fuse_modules\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "checkpoints = [x for x in range(25, 201, 25)]\n",
        "students_count = 2  # Assuming there are two students in DML\n",
        "test_accs = {i: [] for i in range(students_count)}  # Dictionary to store accuracies for each student\n",
        "\n",
        "for cp in checkpoints:\n",
        "    print(f\"Evaluating checkpoint at epoch {cp}\")\n",
        "    for student_index in range(students_count):\n",
        "        student_net = networks.StudentNetwork(pruning_factor=0.0, teacher_net=teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "        checkpoint_path = f'checkpoints_student/student_{student_index}_epoch_{cp-1}.pth'\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=fast_device)  # Ensure it loads on the correct device\n",
        "        \n",
        "        # Load the state dictionary directly since it's not under 'model_state_dict'\n",
        "        student_net.load_state_dict(checkpoint)\n",
        "        \n",
        "        student_net.to(fast_device)\n",
        "        student_net.eval()\n",
        "\n",
        "        # Prepare and convert the network using the appropriate quantization configuration\n",
        "        student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "        student_net_prepared = torch.ao.quantization.prepare(student_net, inplace=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in train_loader:\n",
        "                inputs = inputs.to(fast_device)\n",
        "                student_net_prepared(inputs)  # Calibrate with some forward passes\n",
        "\n",
        "        #student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
        "        reproducibilitySeed()\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_prepared, test_loader, fast_device)\n",
        "        print(f'Student {student_index + 1} test accuracy at epoch {cp}: {test_accuracy}%')\n",
        "        test_accs[student_index].append(test_accuracy)\n",
        "\n",
        "# Plotting accuracies for each student\n",
        "for student_index, accuracies in test_accs.items():\n",
        "    plt.plot(checkpoints, accuracies, label=f'Student {student_index + 1}')\n",
        "\n",
        "plt.title('Test Accuracy Over Checkpoints')\n",
        "plt.xlabel('Checkpoint Epoch')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_13036\\4265574020.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error during conversion: Unsupported qscheme: per_channel_affine\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you've defined a model class 'StudentNetwork'\n",
        "def load_and_quantize_model(checkpoint_path, device):\n",
        "    # Instantiate the model\n",
        "    student_net = networks.StudentNetwork(pruning_factor=0.0, teacher_net=teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "    \n",
        "    # Load the model state dictionary\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    student_net.load_state_dict(checkpoint)\n",
        "    \n",
        "    student_net.to(device)\n",
        "    student_net.eval()\n",
        "\n",
        "    # Prepare for quantization\n",
        "    student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "    student_net_prepared = torch.ao.quantization.prepare(student_net, inplace=False)\n",
        "\n",
        "    # Convert the model to quantized version\n",
        "    try:\n",
        "        student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error during conversion: {e}\")\n",
        "        return None\n",
        "\n",
        "    return student_net_int8\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path to the saved model checkpoint\n",
        "checkpoint_path = 'checkpoints_student/student_0_epoch_24.pth'\n",
        "\n",
        "# Load and quantize the model\n",
        "quantized_model = load_and_quantize_model(checkpoint_path, device)\n",
        "\n",
        "if quantized_model:\n",
        "    # You can now perform evaluation or further processing with your quantized model\n",
        "    print(\"Quantization successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (without training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.05 10623029 11181642\n",
            "Test accuracy:  0.8617\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.1\n",
            "0.1 10064428 11181642\n",
            "Test accuracy:  0.8622\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.15\n",
            "0.15 9505825 11181642\n",
            "Test accuracy:  0.862\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.2\n",
            "0.2 8947224 11181642\n",
            "Test accuracy:  0.8614\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.25\n",
            "0.25 8388631 11181642\n",
            "Test accuracy:  0.8595\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.3\n",
            "0.3 7830025 11181642\n",
            "Test accuracy:  0.8583\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.35\n",
            "0.35 7271420 11181642\n",
            "Test accuracy:  0.8582\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.4\n",
            "0.4 6712818 11181642\n",
            "Test accuracy:  0.8565\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.45\n",
            "0.45 6154222 11181642\n",
            "Test accuracy:  0.8553\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.5\n",
            "0.5 5595620 11181642\n",
            "Test accuracy:  0.8534\n",
            "Results saved to checkpoints_student/results_student_wo\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter ranges\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.90]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "\n",
        "# Prepare the list of hyperparameters\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'dropout_input': hparam_tuple[0][0],\n",
        "        'dropout_hidden': hparam_tuple[0][1],\n",
        "        'weight_decay': hparam_tuple[1],\n",
        "        'lr_decay': hparam_tuple[2],\n",
        "        'momentum': hparam_tuple[3],\n",
        "        'lr': hparam_tuple[4]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Results dictionary\n",
        "results = {}\n",
        "pruning_factors = [i/20 for i in range(1, 20)]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_student/results_student_wo.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Trainable Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training without hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net_wo = networks.StudentNetwork(pruning_factor, teacher_net)\n",
        "        student_net_wo = student_net_wo.to(fast_device)\n",
        "        #hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(student_net_wo)\n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "\n",
        "        # Train the student network\n",
        "        #results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs,\n",
        "                                                                                    #train_val_loader, None,\n",
        "                                                                                    #print_every=print_every,\n",
        "                                                                                    #fast_device=fast_device)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        #save_path = checkpoints_path_student + utils.hparamToString(hparam) + f'_pruning_{pruning_factor}_w/o_final.tar'\n",
        "        #torch.save({\n",
        "            #'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            #'model_state_dict': student_net.state_dict(),\n",
        "            #'epoch': num_epochs\n",
        "        #}, save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_wo, test_loader, fast_device)\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
