{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bREGbUT_Y8F"
      },
      "source": [
        "### Import required packages and limit GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhcqwxR8_Y8I",
        "outputId": "26140e84-9279-479e-cc80-7c9a0b1a8fd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import argparse\n",
        "import time\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "#sys.path.append('/content/KD')\n",
        "# Import the module\n",
        "import networks\n",
        "import utils\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n73sKlgl_Y8K"
      },
      "outputs": [],
      "source": [
        "use_gpu = True    # set use_gpu to True if system has gpu\n",
        "gpu_id = 0        # id of gpu to be used\n",
        "cpu_device = torch.device('cpu')\n",
        "# fast_device is where computation (training, inference) happens\n",
        "fast_device = torch.device('cpu')\n",
        "if use_gpu:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'    # set visible devices depending on system configuration\n",
        "    fast_device = torch.device('cuda:' + str(gpu_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DAfPa7mw_Y8L"
      },
      "outputs": [],
      "source": [
        "def reproducibilitySeed():\n",
        "    \"\"\"\n",
        "    Ensure reproducibility of results; Seeds to 0\n",
        "    \"\"\"\n",
        "    torch_init_seed = 0\n",
        "    torch.manual_seed(torch_init_seed)\n",
        "    numpy_init_seed = 0\n",
        "    np.random.seed(numpy_init_seed)\n",
        "    if use_gpu:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reproducibilitySeed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o0sxuxkJbUEI"
      },
      "outputs": [],
      "source": [
        "checkpoints_path_teacher = 'checkpoints_teacher/'\n",
        "checkpoints_path_student = 'checkpoints_student_QAT/'\n",
        "if not os.path.exists(checkpoints_path_teacher):\n",
        "    os.makedirs(checkpoints_path_teacher)\n",
        "if not os.path.exists(checkpoints_path_student):\n",
        "    os.makedirs(checkpoints_path_student)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCz4Pup_Y8M"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import PIL\n",
        "\n",
        "# Set up transformations for CIFAR-10\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),  # Augment training data by padding 4 and random cropping\n",
        "        transforms.RandomHorizontalFlip(),     # Randomly flip images horizontally\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "import torchvision as tv\n",
        "preprocess_train = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((160, 160), interpolation=PIL.Image.BILINEAR),  # It's the default, just being explicit for the reader.\n",
        "    tv.transforms.RandomCrop((128, 128)),\n",
        "    tv.transforms.RandomHorizontalFlip(),\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "])\n",
        "\n",
        "preprocess_eval = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((128, 128), interpolation=PIL.Image.BILINEAR),\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=False,\n",
        "                                            download=True, transform=transform_test)\n",
        "\n",
        "# Split the training dataset into training and validation\n",
        "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
        "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
        "\n",
        "# DataLoader setup\n",
        "batch_size = 128\n",
        "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO8vAtxV_Y8N",
        "outputId": "797e7d9e-fa75-47c3-977a-22b6ef78b28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set up transformations for CIFAR-100\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),  # Randomly crop with padding of 4 pixels\n",
        "        transforms.RandomHorizontalFlip(),     # Random horizontal flip\n",
        "        transforms.ToTensor(),                 # Convert to PyTorch tensor\n",
        "        transforms.Normalize(\n",
        "            (0.5071, 0.4867, 0.4408),         # Mean for CIFAR-100\n",
        "            (0.2675, 0.2565, 0.2761)          # Standard deviation for CIFAR-100\n",
        "        )  # Normalize images\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),                 # Convert to PyTorch tensor\n",
        "        transforms.Normalize(\n",
        "            (0.5071, 0.4867, 0.4408),         # Mean for CIFAR-100\n",
        "            (0.2675, 0.2565, 0.2761)          # Standard deviation for CIFAR-100\n",
        "        )  # Normalize images\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_val_dataset = torchvision.datasets.CIFAR100(root='./CIFAR100_dataset/', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./CIFAR100_dataset/', train=False,\n",
        "                                            download=True, transform=transform_test)\n",
        "\n",
        "# Split the training dataset into training and validation\n",
        "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
        "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
        "\n",
        "# DataLoader setup\n",
        "batch_size = 512\n",
        "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jywk1DS_Y8O"
      },
      "source": [
        "### Train teacher network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6wDSA64C_Y8R"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "print_every = 100    # Interval size for which to print statistics of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnEnJ1A1_Y8T",
        "outputId": "3d5687b1-f814-41bf-9be6-8bd6d3c86577",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m reproducibilitySeed()\n\u001b[0;32m     38\u001b[0m teacher_net \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mTeacherNetwork()\n\u001b[1;32m---> 39\u001b[0m teacher_net \u001b[38;5;241m=\u001b[39m \u001b[43mteacher_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m hparam_tuple \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mhparamDictToTuple(hparam)\n\u001b[0;32m     42\u001b[0m results[hparam_tuple] \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtrainTeacherOnHparam(teacher_net, hparam, num_epochs, train_val_loader, \u001b[38;5;28;01mNone\u001b[39;00m, print_every\u001b[38;5;241m=\u001b[39mprint_every, fast_device\u001b[38;5;241m=\u001b[39mfast_device)\n",
            "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define hyperparameter ranges\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.90]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "\n",
        "# Prepare the list of hyperparameters\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'dropout_input': hparam_tuple[0][0],\n",
        "        'dropout_hidden': hparam_tuple[0][1],\n",
        "        'weight_decay': hparam_tuple[1],\n",
        "        'lr_decay': hparam_tuple[2],\n",
        "        'momentum': hparam_tuple[3],\n",
        "        'lr': hparam_tuple[4]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Results dictionary\n",
        "results = {}\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_teacher/results_teacher\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for hparam in hparams_list:\n",
        "    print('Training with hparams' + utils.hparamToString(hparam))\n",
        "\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    reproducibilitySeed()\n",
        "    teacher_net = networks.TeacherNetwork()\n",
        "    teacher_net = teacher_net.to(fast_device)\n",
        "\n",
        "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "    results[hparam_tuple] = utils.trainTeacherOnHparam(teacher_net, hparam, num_epochs, train_val_loader, None, print_every=print_every, fast_device=fast_device)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Save model\n",
        "    save_path = checkpoints_path_teacher + utils.hparamToString(hparam) + '_final.tar'\n",
        "    torch.save({\n",
        "        'results': results[hparam_tuple],\n",
        "        'model_state_dict': teacher_net.state_dict(),\n",
        "        'epoch': num_epochs\n",
        "    }, save_path)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "    print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "    # Write results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "            hparam['lr_decay'], hparam['momentum'], hparam['lr'],\n",
        "            test_accuracy, training_time\n",
        "        ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load pre-trained teacher network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9752\n"
          ]
        }
      ],
      "source": [
        "# Path to the saved model\n",
        "teacher_path = \"checkpoints_teacher/dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_final.tar\"\n",
        "\n",
        "# Initialize the network\n",
        "teacher_net = networks.TeacherNetworkBiT()\n",
        "teacher_net = teacher_net.to(fast_device)\n",
        "\n",
        "\n",
        "# pre-trained teacher accuracy\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_15720\\1847993978.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9616\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the teacher networks\n",
        "teacher_net_1 = networks.TeacherNetwork50()\n",
        "checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
        "\n",
        "\n",
        "teacher_net_1.model.load_state_dict(checkpoint)\n",
        "teacher_net_2 = networks.TeacherNetworkBiT()\n",
        "\n",
        "# Create the ensemble model\n",
        "teacher_net = networks.EnsembleModel(teacher_net_1, teacher_net_2)\n",
        "\n",
        "# Move the ensemble model to the appropriate device (e.g., GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher_net = teacher_net.to(device)\n",
        "\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_50604\\1802089175.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('best_resnet50_cifar10.pth')\n"
          ]
        }
      ],
      "source": [
        "teacher_net = networks.TeacherNetwork50()\n",
        "checkpoint = torch.load('best_resnet50_cifar10.pth')\n",
        "\n",
        "\n",
        "teacher_net.model.load_state_dict(checkpoint)\n",
        "\n",
        "teacher_net = teacher_net.to(fast_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(model, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rayk6sDh7UXz"
      },
      "source": [
        "### Student Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (with training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hlvtw4Xkxhoh"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200\n",
        "print_every = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the total number of trainable parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters.\n",
        "    \"\"\"\n",
        "    return sum((p.data != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def count_zero_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the number of trainable parameters that are exactly zero in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose zero parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters that are exactly zero.\n",
        "    \"\"\"\n",
        "    return sum((p.data == 0).sum().item() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'teacher_net' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m count_parameters(\u001b[43mteacher_net\u001b[49m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'teacher_net' is not defined"
          ]
        }
      ],
      "source": [
        "count_parameters(teacher_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\dsc180b\\DSC180B_Q2_Project\\networks.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('BiT-M-R101x1_step5000.tar')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9799\n"
          ]
        }
      ],
      "source": [
        "teacher_net = networks.TeacherNetworkBiT(size='R101x1')\n",
        "teacher_net = teacher_net.to(fast_device)\n",
        "\n",
        "\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "probs = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(fast_device)\n",
        "        outputs = teacher_net(images)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        max_probs, _ = torch.max(probabilities, dim=1)\n",
        "        probs.extend(max_probs.cpu().numpy())\n",
        "        \n",
        "# Plot histogram of highest predicted class probabilities\n",
        "plt.hist(probs, bins=np.linspace(0, 1, 21), alpha=0.7, color='b', edgecolor='black')\n",
        "plt.xlabel('Highest Predicted Class Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Highest Predicted Class Probabilities on CIFAR-10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_77628\\469792112.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('vit_cifar10_finetune.t7')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import timm\n",
        "net = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
        "net.head = nn.Linear(net.head.in_features, 10)\n",
        "checkpoint = torch.load('vit_cifar10_finetune.t7')\n",
        "\n",
        "net.load_state_dict(checkpoint['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
            "ERROR: No matching distribution found for triton\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.0\n",
            "[notice] To update, run: C:\\Users\\daniel\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_77628\\1954639066.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('vit_cifar10_finetune.t7', map_location=fast_device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.9866\n"
          ]
        }
      ],
      "source": [
        "class TeacherNetworkViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherNetworkViT, self).__init__()\n",
        "        self.model = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
        "        self.model.head = nn.Linear(net.head.in_features, 10)\n",
        "        checkpoint = torch.load('vit_cifar10_finetune.t7', map_location=fast_device)\n",
        "        self.model.load_state_dict(checkpoint['model'])\n",
        "        self.resize = transforms.Resize(384)\n",
        "        self.model.half().to(fast_device)\n",
        "        self.model.to(memory_format=torch.channels_last)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resize(x)\n",
        "        x = x.to(memory_format=torch.channels_last).half()\n",
        "        return self.model(x)\n",
        "    \n",
        "\n",
        "teacher_net = TeacherNetworkViT()\n",
        "teacher_net.to(fast_device)\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAXvRE-7a9bC",
        "outputId": "86382da5-7836-4b1f-e4e2-a1dd2ec8f8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsT=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001 and pruning factor 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 11173962 86095762\n",
            "[Epoch 1, Batch 100/372] Train Loss: 1.935, Train Accuracy: 0.102\n",
            "[Epoch 1, Batch 200/372] Train Loss: 0.794, Train Accuracy: 0.156\n",
            "[Epoch 1, Batch 300/372] Train Loss: 1.421, Train Accuracy: 0.602\n",
            "Epoch 1 Validation Accuracy: 0.671\n",
            "[Epoch 2, Batch 100/372] Train Loss: 1.145, Train Accuracy: 0.602\n",
            "[Epoch 2, Batch 200/372] Train Loss: 1.066, Train Accuracy: 0.094\n",
            "[Epoch 2, Batch 300/372] Train Loss: 0.705, Train Accuracy: 0.250\n",
            "Epoch 2 Validation Accuracy: 0.760\n",
            "[Epoch 3, Batch 100/372] Train Loss: 0.782, Train Accuracy: 0.680\n",
            "[Epoch 3, Batch 200/372] Train Loss: 0.639, Train Accuracy: 0.695\n",
            "[Epoch 3, Batch 300/372] Train Loss: 0.599, Train Accuracy: 0.766\n",
            "Epoch 3 Validation Accuracy: 0.800\n",
            "[Epoch 4, Batch 100/372] Train Loss: 0.601, Train Accuracy: 0.391\n",
            "[Epoch 4, Batch 200/372] Train Loss: 0.535, Train Accuracy: 0.750\n",
            "[Epoch 4, Batch 300/372] Train Loss: 0.723, Train Accuracy: 0.102\n",
            "Epoch 4 Validation Accuracy: 0.823\n",
            "[Epoch 5, Batch 100/372] Train Loss: 0.731, Train Accuracy: 0.109\n",
            "[Epoch 5, Batch 200/372] Train Loss: 0.584, Train Accuracy: 0.812\n",
            "[Epoch 5, Batch 300/372] Train Loss: 0.558, Train Accuracy: 0.125\n",
            "Epoch 5 Validation Accuracy: 0.843\n",
            "[Epoch 6, Batch 100/372] Train Loss: 0.527, Train Accuracy: 0.117\n",
            "[Epoch 6, Batch 200/372] Train Loss: 0.501, Train Accuracy: 0.133\n",
            "[Epoch 6, Batch 300/372] Train Loss: 0.564, Train Accuracy: 0.102\n",
            "Epoch 6 Validation Accuracy: 0.850\n",
            "[Epoch 7, Batch 100/372] Train Loss: 0.502, Train Accuracy: 0.133\n",
            "[Epoch 7, Batch 200/372] Train Loss: 0.443, Train Accuracy: 0.148\n",
            "[Epoch 7, Batch 300/372] Train Loss: 0.531, Train Accuracy: 0.664\n",
            "Epoch 7 Validation Accuracy: 0.866\n",
            "[Epoch 8, Batch 100/372] Train Loss: 0.551, Train Accuracy: 0.617\n",
            "[Epoch 8, Batch 200/372] Train Loss: 0.413, Train Accuracy: 0.117\n",
            "[Epoch 8, Batch 300/372] Train Loss: 0.576, Train Accuracy: 0.680\n",
            "Epoch 8 Validation Accuracy: 0.872\n",
            "[Epoch 9, Batch 100/372] Train Loss: 0.489, Train Accuracy: 0.258\n",
            "[Epoch 9, Batch 200/372] Train Loss: 0.393, Train Accuracy: 0.102\n",
            "[Epoch 9, Batch 300/372] Train Loss: 0.368, Train Accuracy: 0.875\n",
            "Epoch 9 Validation Accuracy: 0.884\n",
            "[Epoch 10, Batch 100/372] Train Loss: 0.351, Train Accuracy: 0.148\n",
            "[Epoch 10, Batch 200/372] Train Loss: 0.482, Train Accuracy: 0.250\n",
            "[Epoch 10, Batch 300/372] Train Loss: 0.479, Train Accuracy: 0.172\n",
            "Epoch 10 Validation Accuracy: 0.886\n",
            "[Epoch 11, Batch 100/372] Train Loss: 0.509, Train Accuracy: 0.258\n",
            "[Epoch 11, Batch 200/372] Train Loss: 0.509, Train Accuracy: 0.344\n",
            "[Epoch 11, Batch 300/372] Train Loss: 0.398, Train Accuracy: 0.117\n",
            "Epoch 11 Validation Accuracy: 0.895\n",
            "[Epoch 12, Batch 100/372] Train Loss: 0.371, Train Accuracy: 0.875\n",
            "[Epoch 12, Batch 200/372] Train Loss: 0.501, Train Accuracy: 0.273\n",
            "[Epoch 12, Batch 300/372] Train Loss: 0.375, Train Accuracy: 0.859\n",
            "Epoch 12 Validation Accuracy: 0.895\n",
            "[Epoch 13, Batch 100/372] Train Loss: 0.549, Train Accuracy: 0.617\n",
            "[Epoch 13, Batch 200/372] Train Loss: 0.460, Train Accuracy: 0.797\n",
            "[Epoch 13, Batch 300/372] Train Loss: 0.468, Train Accuracy: 0.289\n",
            "Epoch 13 Validation Accuracy: 0.891\n",
            "[Epoch 14, Batch 100/372] Train Loss: 0.501, Train Accuracy: 0.117\n",
            "[Epoch 14, Batch 200/372] Train Loss: 0.399, Train Accuracy: 0.133\n",
            "[Epoch 14, Batch 300/372] Train Loss: 0.446, Train Accuracy: 0.109\n",
            "Epoch 14 Validation Accuracy: 0.896\n",
            "[Epoch 15, Batch 100/372] Train Loss: 0.454, Train Accuracy: 0.797\n",
            "[Epoch 15, Batch 200/372] Train Loss: 0.466, Train Accuracy: 0.703\n",
            "[Epoch 15, Batch 300/372] Train Loss: 0.406, Train Accuracy: 0.852\n",
            "Epoch 15 Validation Accuracy: 0.903\n",
            "[Epoch 16, Batch 100/372] Train Loss: 0.486, Train Accuracy: 0.531\n",
            "[Epoch 16, Batch 200/372] Train Loss: 0.308, Train Accuracy: 0.078\n",
            "[Epoch 16, Batch 300/372] Train Loss: 0.394, Train Accuracy: 0.906\n",
            "Epoch 16 Validation Accuracy: 0.906\n",
            "[Epoch 17, Batch 100/372] Train Loss: 0.264, Train Accuracy: 0.938\n",
            "[Epoch 17, Batch 200/372] Train Loss: 0.405, Train Accuracy: 0.156\n",
            "[Epoch 17, Batch 300/372] Train Loss: 0.528, Train Accuracy: 0.422\n",
            "Epoch 17 Validation Accuracy: 0.902\n",
            "[Epoch 18, Batch 100/372] Train Loss: 0.477, Train Accuracy: 0.891\n",
            "[Epoch 18, Batch 200/372] Train Loss: 0.338, Train Accuracy: 0.875\n",
            "[Epoch 18, Batch 300/372] Train Loss: 0.389, Train Accuracy: 0.828\n",
            "Epoch 18 Validation Accuracy: 0.907\n",
            "[Epoch 19, Batch 100/372] Train Loss: 0.282, Train Accuracy: 0.938\n",
            "[Epoch 19, Batch 200/372] Train Loss: 0.315, Train Accuracy: 0.930\n",
            "[Epoch 19, Batch 300/372] Train Loss: 0.503, Train Accuracy: 0.375\n",
            "Epoch 19 Validation Accuracy: 0.916\n",
            "[Epoch 20, Batch 100/372] Train Loss: 0.265, Train Accuracy: 0.125\n",
            "[Epoch 20, Batch 200/372] Train Loss: 0.475, Train Accuracy: 0.273\n",
            "[Epoch 20, Batch 300/372] Train Loss: 0.294, Train Accuracy: 0.078\n",
            "Epoch 20 Validation Accuracy: 0.916\n",
            "[Epoch 21, Batch 100/372] Train Loss: 0.335, Train Accuracy: 0.875\n",
            "[Epoch 21, Batch 200/372] Train Loss: 0.355, Train Accuracy: 0.898\n",
            "[Epoch 21, Batch 300/372] Train Loss: 0.472, Train Accuracy: 0.570\n",
            "Epoch 21 Validation Accuracy: 0.914\n",
            "[Epoch 22, Batch 100/372] Train Loss: 0.514, Train Accuracy: 0.633\n",
            "[Epoch 22, Batch 200/372] Train Loss: 0.516, Train Accuracy: 0.523\n",
            "[Epoch 22, Batch 300/372] Train Loss: 0.471, Train Accuracy: 0.406\n",
            "Epoch 22 Validation Accuracy: 0.916\n",
            "[Epoch 23, Batch 100/372] Train Loss: 0.515, Train Accuracy: 0.422\n",
            "[Epoch 23, Batch 200/372] Train Loss: 0.541, Train Accuracy: 0.500\n",
            "[Epoch 23, Batch 300/372] Train Loss: 0.357, Train Accuracy: 0.117\n",
            "Epoch 23 Validation Accuracy: 0.914\n",
            "[Epoch 24, Batch 100/372] Train Loss: 0.314, Train Accuracy: 0.141\n",
            "[Epoch 24, Batch 200/372] Train Loss: 0.456, Train Accuracy: 0.625\n",
            "[Epoch 24, Batch 300/372] Train Loss: 0.390, Train Accuracy: 0.781\n",
            "Epoch 24 Validation Accuracy: 0.918\n",
            "[Epoch 25, Batch 100/372] Train Loss: 0.217, Train Accuracy: 0.156\n",
            "[Epoch 25, Batch 200/372] Train Loss: 0.285, Train Accuracy: 0.961\n",
            "[Epoch 25, Batch 300/372] Train Loss: 0.295, Train Accuracy: 0.906\n",
            "Epoch 25 Validation Accuracy: 0.922\n",
            "Checkpoint saved at epoch 25: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_25.tar\n",
            "[Epoch 26, Batch 100/372] Train Loss: 0.302, Train Accuracy: 0.094\n",
            "[Epoch 26, Batch 200/372] Train Loss: 0.269, Train Accuracy: 0.102\n",
            "[Epoch 26, Batch 300/372] Train Loss: 0.230, Train Accuracy: 0.141\n",
            "Epoch 26 Validation Accuracy: 0.922\n",
            "[Epoch 27, Batch 100/372] Train Loss: 0.390, Train Accuracy: 0.758\n",
            "[Epoch 27, Batch 200/372] Train Loss: 0.290, Train Accuracy: 0.125\n",
            "[Epoch 27, Batch 300/372] Train Loss: 0.348, Train Accuracy: 0.891\n",
            "Epoch 27 Validation Accuracy: 0.919\n",
            "[Epoch 28, Batch 100/372] Train Loss: 0.322, Train Accuracy: 0.867\n",
            "[Epoch 28, Batch 200/372] Train Loss: 0.429, Train Accuracy: 0.695\n",
            "[Epoch 28, Batch 300/372] Train Loss: 0.236, Train Accuracy: 0.062\n",
            "Epoch 28 Validation Accuracy: 0.914\n",
            "[Epoch 29, Batch 100/372] Train Loss: 0.430, Train Accuracy: 0.391\n",
            "[Epoch 29, Batch 200/372] Train Loss: 0.184, Train Accuracy: 0.984\n",
            "[Epoch 29, Batch 300/372] Train Loss: 0.289, Train Accuracy: 0.148\n",
            "Epoch 29 Validation Accuracy: 0.924\n",
            "[Epoch 30, Batch 100/372] Train Loss: 0.344, Train Accuracy: 0.883\n",
            "[Epoch 30, Batch 200/372] Train Loss: 0.267, Train Accuracy: 0.086\n",
            "[Epoch 30, Batch 300/372] Train Loss: 0.335, Train Accuracy: 0.891\n",
            "Epoch 30 Validation Accuracy: 0.928\n",
            "[Epoch 31, Batch 100/372] Train Loss: 0.312, Train Accuracy: 0.195\n",
            "[Epoch 31, Batch 200/372] Train Loss: 0.321, Train Accuracy: 0.875\n",
            "[Epoch 31, Batch 300/372] Train Loss: 0.167, Train Accuracy: 0.953\n",
            "Epoch 31 Validation Accuracy: 0.925\n",
            "[Epoch 32, Batch 100/372] Train Loss: 0.208, Train Accuracy: 0.117\n",
            "[Epoch 32, Batch 200/372] Train Loss: 0.208, Train Accuracy: 0.969\n",
            "[Epoch 32, Batch 300/372] Train Loss: 0.208, Train Accuracy: 0.969\n",
            "Epoch 32 Validation Accuracy: 0.921\n",
            "[Epoch 33, Batch 100/372] Train Loss: 0.522, Train Accuracy: 0.445\n",
            "[Epoch 33, Batch 200/372] Train Loss: 0.262, Train Accuracy: 0.914\n",
            "[Epoch 33, Batch 300/372] Train Loss: 0.261, Train Accuracy: 0.164\n",
            "Epoch 33 Validation Accuracy: 0.920\n",
            "[Epoch 34, Batch 100/372] Train Loss: 0.256, Train Accuracy: 0.086\n",
            "[Epoch 34, Batch 200/372] Train Loss: 0.387, Train Accuracy: 0.188\n",
            "[Epoch 34, Batch 300/372] Train Loss: 0.428, Train Accuracy: 0.680\n",
            "Epoch 34 Validation Accuracy: 0.923\n",
            "[Epoch 35, Batch 100/372] Train Loss: 0.426, Train Accuracy: 0.477\n",
            "[Epoch 35, Batch 200/372] Train Loss: 0.282, Train Accuracy: 0.953\n",
            "[Epoch 35, Batch 300/372] Train Loss: 0.216, Train Accuracy: 0.969\n",
            "Epoch 35 Validation Accuracy: 0.930\n",
            "[Epoch 36, Batch 100/372] Train Loss: 0.249, Train Accuracy: 0.945\n",
            "[Epoch 36, Batch 200/372] Train Loss: 0.155, Train Accuracy: 0.102\n",
            "[Epoch 36, Batch 300/372] Train Loss: 0.311, Train Accuracy: 0.164\n",
            "Epoch 36 Validation Accuracy: 0.932\n",
            "[Epoch 37, Batch 100/372] Train Loss: 0.187, Train Accuracy: 0.977\n",
            "[Epoch 37, Batch 200/372] Train Loss: 0.278, Train Accuracy: 0.117\n",
            "[Epoch 37, Batch 300/372] Train Loss: 0.230, Train Accuracy: 0.938\n",
            "Epoch 37 Validation Accuracy: 0.932\n",
            "[Epoch 38, Batch 100/372] Train Loss: 0.244, Train Accuracy: 0.922\n",
            "[Epoch 38, Batch 200/372] Train Loss: 0.456, Train Accuracy: 0.516\n",
            "[Epoch 38, Batch 300/372] Train Loss: 0.247, Train Accuracy: 0.102\n",
            "Epoch 38 Validation Accuracy: 0.924\n",
            "[Epoch 39, Batch 100/372] Train Loss: 0.168, Train Accuracy: 0.977\n",
            "[Epoch 39, Batch 200/372] Train Loss: 0.266, Train Accuracy: 0.094\n",
            "[Epoch 39, Batch 300/372] Train Loss: 0.167, Train Accuracy: 0.109\n",
            "Epoch 39 Validation Accuracy: 0.926\n",
            "[Epoch 40, Batch 100/372] Train Loss: 0.206, Train Accuracy: 0.961\n",
            "[Epoch 40, Batch 200/372] Train Loss: 0.267, Train Accuracy: 0.148\n",
            "[Epoch 40, Batch 300/372] Train Loss: 0.211, Train Accuracy: 0.070\n",
            "Epoch 40 Validation Accuracy: 0.930\n",
            "[Epoch 41, Batch 100/372] Train Loss: 0.210, Train Accuracy: 0.117\n",
            "[Epoch 41, Batch 200/372] Train Loss: 0.228, Train Accuracy: 0.945\n",
            "[Epoch 41, Batch 300/372] Train Loss: 0.148, Train Accuracy: 0.102\n",
            "Epoch 41 Validation Accuracy: 0.924\n",
            "[Epoch 42, Batch 100/372] Train Loss: 0.207, Train Accuracy: 0.953\n",
            "[Epoch 42, Batch 200/372] Train Loss: 0.157, Train Accuracy: 0.125\n",
            "[Epoch 42, Batch 300/372] Train Loss: 0.473, Train Accuracy: 0.516\n",
            "Epoch 42 Validation Accuracy: 0.928\n",
            "[Epoch 43, Batch 100/372] Train Loss: 0.202, Train Accuracy: 0.094\n",
            "[Epoch 43, Batch 200/372] Train Loss: 0.525, Train Accuracy: 0.547\n",
            "[Epoch 43, Batch 300/372] Train Loss: 0.378, Train Accuracy: 0.781\n",
            "Epoch 43 Validation Accuracy: 0.931\n",
            "[Epoch 44, Batch 100/372] Train Loss: 0.177, Train Accuracy: 0.094\n",
            "[Epoch 44, Batch 200/372] Train Loss: 0.171, Train Accuracy: 0.094\n",
            "[Epoch 44, Batch 300/372] Train Loss: 0.223, Train Accuracy: 0.945\n",
            "Epoch 44 Validation Accuracy: 0.936\n",
            "[Epoch 45, Batch 100/372] Train Loss: 0.186, Train Accuracy: 0.961\n",
            "[Epoch 45, Batch 200/372] Train Loss: 0.281, Train Accuracy: 0.906\n",
            "[Epoch 45, Batch 300/372] Train Loss: 0.273, Train Accuracy: 0.867\n",
            "Epoch 45 Validation Accuracy: 0.936\n",
            "[Epoch 46, Batch 100/372] Train Loss: 0.327, Train Accuracy: 0.195\n",
            "[Epoch 46, Batch 200/372] Train Loss: 0.207, Train Accuracy: 0.961\n",
            "[Epoch 46, Batch 300/372] Train Loss: 0.345, Train Accuracy: 0.172\n",
            "Epoch 46 Validation Accuracy: 0.934\n",
            "[Epoch 47, Batch 100/372] Train Loss: 0.187, Train Accuracy: 0.961\n",
            "[Epoch 47, Batch 200/372] Train Loss: 0.169, Train Accuracy: 0.109\n",
            "[Epoch 47, Batch 300/372] Train Loss: 0.190, Train Accuracy: 0.977\n",
            "Epoch 47 Validation Accuracy: 0.932\n",
            "[Epoch 48, Batch 100/372] Train Loss: 0.210, Train Accuracy: 0.070\n",
            "[Epoch 48, Batch 200/372] Train Loss: 0.178, Train Accuracy: 0.961\n",
            "[Epoch 48, Batch 300/372] Train Loss: 0.301, Train Accuracy: 0.914\n",
            "Epoch 48 Validation Accuracy: 0.933\n",
            "[Epoch 49, Batch 100/372] Train Loss: 0.440, Train Accuracy: 0.445\n",
            "[Epoch 49, Batch 200/372] Train Loss: 0.436, Train Accuracy: 0.156\n",
            "[Epoch 49, Batch 300/372] Train Loss: 0.392, Train Accuracy: 0.758\n",
            "Epoch 49 Validation Accuracy: 0.938\n",
            "[Epoch 50, Batch 100/372] Train Loss: 0.257, Train Accuracy: 0.117\n",
            "[Epoch 50, Batch 200/372] Train Loss: 0.305, Train Accuracy: 0.906\n",
            "[Epoch 50, Batch 300/372] Train Loss: 0.401, Train Accuracy: 0.258\n",
            "Epoch 50 Validation Accuracy: 0.937\n",
            "Checkpoint saved at epoch 50: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_50.tar\n",
            "[Epoch 51, Batch 100/372] Train Loss: 0.173, Train Accuracy: 0.969\n",
            "[Epoch 51, Batch 200/372] Train Loss: 0.194, Train Accuracy: 0.969\n",
            "[Epoch 51, Batch 300/372] Train Loss: 0.357, Train Accuracy: 0.188\n",
            "Epoch 51 Validation Accuracy: 0.932\n",
            "[Epoch 52, Batch 100/372] Train Loss: 0.262, Train Accuracy: 0.922\n",
            "[Epoch 52, Batch 200/372] Train Loss: 0.294, Train Accuracy: 0.859\n",
            "[Epoch 52, Batch 300/372] Train Loss: 0.181, Train Accuracy: 0.117\n",
            "Epoch 52 Validation Accuracy: 0.935\n",
            "[Epoch 53, Batch 100/372] Train Loss: 0.175, Train Accuracy: 0.102\n",
            "[Epoch 53, Batch 200/372] Train Loss: 0.358, Train Accuracy: 0.133\n",
            "[Epoch 53, Batch 300/372] Train Loss: 0.382, Train Accuracy: 0.508\n",
            "Epoch 53 Validation Accuracy: 0.937\n",
            "[Epoch 54, Batch 100/372] Train Loss: 0.364, Train Accuracy: 0.305\n",
            "[Epoch 54, Batch 200/372] Train Loss: 0.206, Train Accuracy: 0.070\n",
            "[Epoch 54, Batch 300/372] Train Loss: 0.421, Train Accuracy: 0.609\n",
            "Epoch 54 Validation Accuracy: 0.938\n",
            "[Epoch 55, Batch 100/372] Train Loss: 0.203, Train Accuracy: 0.109\n",
            "[Epoch 55, Batch 200/372] Train Loss: 0.383, Train Accuracy: 0.633\n",
            "[Epoch 55, Batch 300/372] Train Loss: 0.213, Train Accuracy: 0.117\n",
            "Epoch 55 Validation Accuracy: 0.937\n",
            "[Epoch 56, Batch 100/372] Train Loss: 0.380, Train Accuracy: 0.688\n",
            "[Epoch 56, Batch 200/372] Train Loss: 0.105, Train Accuracy: 0.992\n",
            "[Epoch 56, Batch 300/372] Train Loss: 0.212, Train Accuracy: 0.945\n",
            "Epoch 56 Validation Accuracy: 0.938\n",
            "[Epoch 57, Batch 100/372] Train Loss: 0.163, Train Accuracy: 0.102\n",
            "[Epoch 57, Batch 200/372] Train Loss: 0.149, Train Accuracy: 0.969\n",
            "[Epoch 57, Batch 300/372] Train Loss: 0.296, Train Accuracy: 0.164\n",
            "Epoch 57 Validation Accuracy: 0.938\n",
            "[Epoch 58, Batch 100/372] Train Loss: 0.348, Train Accuracy: 0.805\n",
            "[Epoch 58, Batch 200/372] Train Loss: 0.201, Train Accuracy: 0.938\n",
            "[Epoch 58, Batch 300/372] Train Loss: 0.311, Train Accuracy: 0.891\n",
            "Epoch 58 Validation Accuracy: 0.938\n",
            "[Epoch 59, Batch 100/372] Train Loss: 0.356, Train Accuracy: 0.789\n",
            "[Epoch 59, Batch 200/372] Train Loss: 0.147, Train Accuracy: 0.969\n",
            "[Epoch 59, Batch 300/372] Train Loss: 0.357, Train Accuracy: 0.320\n",
            "Epoch 59 Validation Accuracy: 0.942\n",
            "[Epoch 60, Batch 100/372] Train Loss: 0.136, Train Accuracy: 1.000\n",
            "[Epoch 60, Batch 200/372] Train Loss: 0.182, Train Accuracy: 0.109\n",
            "[Epoch 60, Batch 300/372] Train Loss: 0.397, Train Accuracy: 0.227\n",
            "Epoch 60 Validation Accuracy: 0.941\n",
            "[Epoch 61, Batch 100/372] Train Loss: 0.191, Train Accuracy: 0.117\n",
            "[Epoch 61, Batch 200/372] Train Loss: 0.348, Train Accuracy: 0.148\n",
            "[Epoch 61, Batch 300/372] Train Loss: 0.345, Train Accuracy: 0.203\n",
            "Epoch 61 Validation Accuracy: 0.942\n",
            "[Epoch 62, Batch 100/372] Train Loss: 0.420, Train Accuracy: 0.469\n",
            "[Epoch 62, Batch 200/372] Train Loss: 0.328, Train Accuracy: 0.820\n",
            "[Epoch 62, Batch 300/372] Train Loss: 0.392, Train Accuracy: 0.305\n",
            "Epoch 62 Validation Accuracy: 0.940\n",
            "[Epoch 63, Batch 100/372] Train Loss: 0.158, Train Accuracy: 0.969\n",
            "[Epoch 63, Batch 200/372] Train Loss: 0.263, Train Accuracy: 0.930\n",
            "[Epoch 63, Batch 300/372] Train Loss: 0.143, Train Accuracy: 0.984\n",
            "Epoch 63 Validation Accuracy: 0.937\n",
            "[Epoch 64, Batch 100/372] Train Loss: 0.187, Train Accuracy: 0.062\n",
            "[Epoch 64, Batch 200/372] Train Loss: 0.187, Train Accuracy: 0.117\n",
            "[Epoch 64, Batch 300/372] Train Loss: 0.426, Train Accuracy: 0.438\n",
            "Epoch 64 Validation Accuracy: 0.938\n",
            "[Epoch 65, Batch 100/372] Train Loss: 0.119, Train Accuracy: 0.992\n",
            "[Epoch 65, Batch 200/372] Train Loss: 0.223, Train Accuracy: 0.977\n",
            "[Epoch 65, Batch 300/372] Train Loss: 0.480, Train Accuracy: 0.461\n",
            "Epoch 65 Validation Accuracy: 0.939\n",
            "[Epoch 66, Batch 100/372] Train Loss: 0.101, Train Accuracy: 0.117\n",
            "[Epoch 66, Batch 200/372] Train Loss: 0.400, Train Accuracy: 0.141\n",
            "[Epoch 66, Batch 300/372] Train Loss: 0.401, Train Accuracy: 0.609\n",
            "Epoch 66 Validation Accuracy: 0.936\n",
            "[Epoch 67, Batch 100/372] Train Loss: 0.260, Train Accuracy: 0.180\n",
            "[Epoch 67, Batch 200/372] Train Loss: 0.240, Train Accuracy: 0.922\n",
            "[Epoch 67, Batch 300/372] Train Loss: 0.164, Train Accuracy: 0.969\n",
            "Epoch 67 Validation Accuracy: 0.938\n",
            "[Epoch 68, Batch 100/372] Train Loss: 0.147, Train Accuracy: 0.117\n",
            "[Epoch 68, Batch 200/372] Train Loss: 0.147, Train Accuracy: 0.078\n",
            "[Epoch 68, Batch 300/372] Train Loss: 0.237, Train Accuracy: 0.109\n",
            "Epoch 68 Validation Accuracy: 0.935\n",
            "[Epoch 69, Batch 100/372] Train Loss: 0.106, Train Accuracy: 0.070\n",
            "[Epoch 69, Batch 200/372] Train Loss: 0.250, Train Accuracy: 0.148\n",
            "[Epoch 69, Batch 300/372] Train Loss: 0.149, Train Accuracy: 0.062\n",
            "Epoch 69 Validation Accuracy: 0.941\n",
            "[Epoch 70, Batch 100/372] Train Loss: 0.152, Train Accuracy: 0.094\n",
            "[Epoch 70, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.125\n",
            "[Epoch 70, Batch 300/372] Train Loss: 0.397, Train Accuracy: 0.539\n",
            "Epoch 70 Validation Accuracy: 0.940\n",
            "[Epoch 71, Batch 100/372] Train Loss: 0.291, Train Accuracy: 0.148\n",
            "[Epoch 71, Batch 200/372] Train Loss: 0.183, Train Accuracy: 0.984\n",
            "[Epoch 71, Batch 300/372] Train Loss: 0.108, Train Accuracy: 0.992\n",
            "Epoch 71 Validation Accuracy: 0.944\n",
            "[Epoch 72, Batch 100/372] Train Loss: 0.099, Train Accuracy: 1.000\n",
            "[Epoch 72, Batch 200/372] Train Loss: 0.330, Train Accuracy: 0.109\n",
            "[Epoch 72, Batch 300/372] Train Loss: 0.148, Train Accuracy: 0.062\n",
            "Epoch 72 Validation Accuracy: 0.934\n",
            "[Epoch 73, Batch 100/372] Train Loss: 0.208, Train Accuracy: 0.086\n",
            "[Epoch 73, Batch 200/372] Train Loss: 0.329, Train Accuracy: 0.789\n",
            "[Epoch 73, Batch 300/372] Train Loss: 0.112, Train Accuracy: 0.055\n",
            "Epoch 73 Validation Accuracy: 0.940\n",
            "[Epoch 74, Batch 100/372] Train Loss: 0.413, Train Accuracy: 0.531\n",
            "[Epoch 74, Batch 200/372] Train Loss: 0.137, Train Accuracy: 0.125\n",
            "[Epoch 74, Batch 300/372] Train Loss: 0.377, Train Accuracy: 0.703\n",
            "Epoch 74 Validation Accuracy: 0.941\n",
            "[Epoch 75, Batch 100/372] Train Loss: 0.303, Train Accuracy: 0.852\n",
            "[Epoch 75, Batch 200/372] Train Loss: 0.148, Train Accuracy: 0.133\n",
            "[Epoch 75, Batch 300/372] Train Loss: 0.322, Train Accuracy: 0.227\n",
            "Epoch 75 Validation Accuracy: 0.942\n",
            "Checkpoint saved at epoch 75: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_75.tar\n",
            "[Epoch 76, Batch 100/372] Train Loss: 0.311, Train Accuracy: 0.234\n",
            "[Epoch 76, Batch 200/372] Train Loss: 0.335, Train Accuracy: 0.852\n",
            "[Epoch 76, Batch 300/372] Train Loss: 0.288, Train Accuracy: 0.133\n",
            "Epoch 76 Validation Accuracy: 0.941\n",
            "[Epoch 77, Batch 100/372] Train Loss: 0.425, Train Accuracy: 0.523\n",
            "[Epoch 77, Batch 200/372] Train Loss: 0.416, Train Accuracy: 0.297\n",
            "[Epoch 77, Batch 300/372] Train Loss: 0.132, Train Accuracy: 0.062\n",
            "Epoch 77 Validation Accuracy: 0.943\n",
            "[Epoch 78, Batch 100/372] Train Loss: 0.421, Train Accuracy: 0.344\n",
            "[Epoch 78, Batch 200/372] Train Loss: 0.144, Train Accuracy: 0.977\n",
            "[Epoch 78, Batch 300/372] Train Loss: 0.083, Train Accuracy: 1.000\n",
            "Epoch 78 Validation Accuracy: 0.935\n",
            "[Epoch 79, Batch 100/372] Train Loss: 0.122, Train Accuracy: 0.125\n",
            "[Epoch 79, Batch 200/372] Train Loss: 0.126, Train Accuracy: 0.125\n",
            "[Epoch 79, Batch 300/372] Train Loss: 0.170, Train Accuracy: 0.961\n",
            "Epoch 79 Validation Accuracy: 0.946\n",
            "[Epoch 80, Batch 100/372] Train Loss: 0.171, Train Accuracy: 0.133\n",
            "[Epoch 80, Batch 200/372] Train Loss: 0.271, Train Accuracy: 0.156\n",
            "[Epoch 80, Batch 300/372] Train Loss: 0.412, Train Accuracy: 0.438\n",
            "Epoch 80 Validation Accuracy: 0.943\n",
            "[Epoch 81, Batch 100/372] Train Loss: 0.232, Train Accuracy: 0.938\n",
            "[Epoch 81, Batch 200/372] Train Loss: 0.168, Train Accuracy: 0.148\n",
            "[Epoch 81, Batch 300/372] Train Loss: 0.410, Train Accuracy: 0.492\n",
            "Epoch 81 Validation Accuracy: 0.940\n",
            "[Epoch 82, Batch 100/372] Train Loss: 0.353, Train Accuracy: 0.375\n",
            "[Epoch 82, Batch 200/372] Train Loss: 0.118, Train Accuracy: 0.992\n",
            "[Epoch 82, Batch 300/372] Train Loss: 0.384, Train Accuracy: 0.719\n",
            "Epoch 82 Validation Accuracy: 0.948\n",
            "[Epoch 83, Batch 100/372] Train Loss: 0.382, Train Accuracy: 0.727\n",
            "[Epoch 83, Batch 200/372] Train Loss: 0.373, Train Accuracy: 0.727\n",
            "[Epoch 83, Batch 300/372] Train Loss: 0.102, Train Accuracy: 0.078\n",
            "Epoch 83 Validation Accuracy: 0.941\n",
            "[Epoch 84, Batch 100/372] Train Loss: 0.330, Train Accuracy: 0.281\n",
            "[Epoch 84, Batch 200/372] Train Loss: 0.290, Train Accuracy: 0.164\n",
            "[Epoch 84, Batch 300/372] Train Loss: 0.080, Train Accuracy: 0.102\n",
            "Epoch 84 Validation Accuracy: 0.945\n",
            "[Epoch 85, Batch 100/372] Train Loss: 0.076, Train Accuracy: 0.062\n",
            "[Epoch 85, Batch 200/372] Train Loss: 0.332, Train Accuracy: 0.148\n",
            "[Epoch 85, Batch 300/372] Train Loss: 0.277, Train Accuracy: 0.852\n",
            "Epoch 85 Validation Accuracy: 0.948\n",
            "[Epoch 86, Batch 100/372] Train Loss: 0.132, Train Accuracy: 0.078\n",
            "[Epoch 86, Batch 200/372] Train Loss: 0.141, Train Accuracy: 0.969\n",
            "[Epoch 86, Batch 300/372] Train Loss: 0.163, Train Accuracy: 0.977\n",
            "Epoch 86 Validation Accuracy: 0.949\n",
            "[Epoch 87, Batch 100/372] Train Loss: 0.141, Train Accuracy: 0.984\n",
            "[Epoch 87, Batch 200/372] Train Loss: 0.272, Train Accuracy: 0.141\n",
            "[Epoch 87, Batch 300/372] Train Loss: 0.088, Train Accuracy: 0.078\n",
            "Epoch 87 Validation Accuracy: 0.942\n",
            "[Epoch 88, Batch 100/372] Train Loss: 0.159, Train Accuracy: 1.000\n",
            "[Epoch 88, Batch 200/372] Train Loss: 0.098, Train Accuracy: 0.078\n",
            "[Epoch 88, Batch 300/372] Train Loss: 0.167, Train Accuracy: 0.094\n",
            "Epoch 88 Validation Accuracy: 0.948\n",
            "[Epoch 89, Batch 100/372] Train Loss: 0.302, Train Accuracy: 0.164\n",
            "[Epoch 89, Batch 200/372] Train Loss: 0.102, Train Accuracy: 0.102\n",
            "[Epoch 89, Batch 300/372] Train Loss: 0.241, Train Accuracy: 0.922\n",
            "Epoch 89 Validation Accuracy: 0.951\n",
            "[Epoch 90, Batch 100/372] Train Loss: 0.099, Train Accuracy: 1.000\n",
            "[Epoch 90, Batch 200/372] Train Loss: 0.442, Train Accuracy: 0.320\n",
            "[Epoch 90, Batch 300/372] Train Loss: 0.112, Train Accuracy: 0.133\n",
            "Epoch 90 Validation Accuracy: 0.946\n",
            "[Epoch 91, Batch 100/372] Train Loss: 0.385, Train Accuracy: 0.547\n",
            "[Epoch 91, Batch 200/372] Train Loss: 0.188, Train Accuracy: 0.945\n",
            "[Epoch 91, Batch 300/372] Train Loss: 0.147, Train Accuracy: 0.984\n",
            "Epoch 91 Validation Accuracy: 0.942\n",
            "[Epoch 92, Batch 100/372] Train Loss: 0.323, Train Accuracy: 0.859\n",
            "[Epoch 92, Batch 200/372] Train Loss: 0.369, Train Accuracy: 0.664\n",
            "[Epoch 92, Batch 300/372] Train Loss: 0.280, Train Accuracy: 0.117\n",
            "Epoch 92 Validation Accuracy: 0.948\n",
            "[Epoch 93, Batch 100/372] Train Loss: 0.144, Train Accuracy: 0.078\n",
            "[Epoch 93, Batch 200/372] Train Loss: 0.335, Train Accuracy: 0.828\n",
            "[Epoch 93, Batch 300/372] Train Loss: 0.096, Train Accuracy: 0.094\n",
            "Epoch 93 Validation Accuracy: 0.939\n",
            "[Epoch 94, Batch 100/372] Train Loss: 0.122, Train Accuracy: 0.977\n",
            "[Epoch 94, Batch 200/372] Train Loss: 0.386, Train Accuracy: 0.516\n",
            "[Epoch 94, Batch 300/372] Train Loss: 0.123, Train Accuracy: 0.125\n",
            "Epoch 94 Validation Accuracy: 0.949\n",
            "[Epoch 95, Batch 100/372] Train Loss: 0.110, Train Accuracy: 0.984\n",
            "[Epoch 95, Batch 200/372] Train Loss: 0.114, Train Accuracy: 0.109\n",
            "[Epoch 95, Batch 300/372] Train Loss: 0.090, Train Accuracy: 0.984\n",
            "Epoch 95 Validation Accuracy: 0.942\n",
            "[Epoch 96, Batch 100/372] Train Loss: 0.180, Train Accuracy: 0.125\n",
            "[Epoch 96, Batch 200/372] Train Loss: 0.337, Train Accuracy: 0.867\n",
            "[Epoch 96, Batch 300/372] Train Loss: 0.184, Train Accuracy: 0.125\n",
            "Epoch 96 Validation Accuracy: 0.947\n",
            "[Epoch 97, Batch 100/372] Train Loss: 0.166, Train Accuracy: 0.977\n",
            "[Epoch 97, Batch 200/372] Train Loss: 0.083, Train Accuracy: 0.984\n",
            "[Epoch 97, Batch 300/372] Train Loss: 0.290, Train Accuracy: 0.234\n",
            "Epoch 97 Validation Accuracy: 0.942\n",
            "[Epoch 98, Batch 100/372] Train Loss: 0.081, Train Accuracy: 1.000\n",
            "[Epoch 98, Batch 200/372] Train Loss: 0.426, Train Accuracy: 0.328\n",
            "[Epoch 98, Batch 300/372] Train Loss: 0.101, Train Accuracy: 1.000\n",
            "Epoch 98 Validation Accuracy: 0.942\n",
            "[Epoch 99, Batch 100/372] Train Loss: 0.434, Train Accuracy: 0.500\n",
            "[Epoch 99, Batch 200/372] Train Loss: 0.130, Train Accuracy: 0.133\n",
            "[Epoch 99, Batch 300/372] Train Loss: 0.164, Train Accuracy: 0.109\n",
            "Epoch 99 Validation Accuracy: 0.947\n",
            "[Epoch 100, Batch 100/372] Train Loss: 0.173, Train Accuracy: 0.969\n",
            "[Epoch 100, Batch 200/372] Train Loss: 0.117, Train Accuracy: 0.117\n",
            "[Epoch 100, Batch 300/372] Train Loss: 0.134, Train Accuracy: 0.062\n",
            "Epoch 100 Validation Accuracy: 0.947\n",
            "Checkpoint saved at epoch 100: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_100.tar\n",
            "[Epoch 101, Batch 100/372] Train Loss: 0.256, Train Accuracy: 0.148\n",
            "[Epoch 101, Batch 200/372] Train Loss: 0.202, Train Accuracy: 0.938\n",
            "[Epoch 101, Batch 300/372] Train Loss: 0.269, Train Accuracy: 0.867\n",
            "Epoch 101 Validation Accuracy: 0.943\n",
            "[Epoch 102, Batch 100/372] Train Loss: 0.220, Train Accuracy: 0.938\n",
            "[Epoch 102, Batch 200/372] Train Loss: 0.094, Train Accuracy: 0.102\n",
            "[Epoch 102, Batch 300/372] Train Loss: 0.097, Train Accuracy: 0.992\n",
            "Epoch 102 Validation Accuracy: 0.946\n",
            "[Epoch 103, Batch 100/372] Train Loss: 0.105, Train Accuracy: 0.992\n",
            "[Epoch 103, Batch 200/372] Train Loss: 0.166, Train Accuracy: 0.055\n",
            "[Epoch 103, Batch 300/372] Train Loss: 0.128, Train Accuracy: 0.977\n",
            "Epoch 103 Validation Accuracy: 0.944\n",
            "[Epoch 104, Batch 100/372] Train Loss: 0.188, Train Accuracy: 0.094\n",
            "[Epoch 104, Batch 200/372] Train Loss: 0.149, Train Accuracy: 0.211\n",
            "[Epoch 104, Batch 300/372] Train Loss: 0.092, Train Accuracy: 0.109\n",
            "Epoch 104 Validation Accuracy: 0.944\n",
            "[Epoch 105, Batch 100/372] Train Loss: 0.193, Train Accuracy: 0.953\n",
            "[Epoch 105, Batch 200/372] Train Loss: 0.335, Train Accuracy: 0.250\n",
            "[Epoch 105, Batch 300/372] Train Loss: 0.085, Train Accuracy: 1.000\n",
            "Epoch 105 Validation Accuracy: 0.945\n",
            "[Epoch 106, Batch 100/372] Train Loss: 0.348, Train Accuracy: 0.625\n",
            "[Epoch 106, Batch 200/372] Train Loss: 0.410, Train Accuracy: 0.531\n",
            "[Epoch 106, Batch 300/372] Train Loss: 0.102, Train Accuracy: 0.141\n",
            "Epoch 106 Validation Accuracy: 0.949\n",
            "[Epoch 107, Batch 100/372] Train Loss: 0.105, Train Accuracy: 0.992\n",
            "[Epoch 107, Batch 200/372] Train Loss: 0.342, Train Accuracy: 0.164\n",
            "[Epoch 107, Batch 300/372] Train Loss: 0.098, Train Accuracy: 0.070\n",
            "Epoch 107 Validation Accuracy: 0.940\n",
            "[Epoch 108, Batch 100/372] Train Loss: 0.313, Train Accuracy: 0.219\n",
            "[Epoch 108, Batch 200/372] Train Loss: 0.070, Train Accuracy: 0.078\n",
            "[Epoch 108, Batch 300/372] Train Loss: 0.393, Train Accuracy: 0.656\n",
            "Epoch 108 Validation Accuracy: 0.946\n",
            "[Epoch 109, Batch 100/372] Train Loss: 0.100, Train Accuracy: 0.984\n",
            "[Epoch 109, Batch 200/372] Train Loss: 0.088, Train Accuracy: 0.992\n",
            "[Epoch 109, Batch 300/372] Train Loss: 0.305, Train Accuracy: 0.203\n",
            "Epoch 109 Validation Accuracy: 0.945\n",
            "[Epoch 110, Batch 100/372] Train Loss: 0.138, Train Accuracy: 0.984\n",
            "[Epoch 110, Batch 200/372] Train Loss: 0.122, Train Accuracy: 0.117\n",
            "[Epoch 110, Batch 300/372] Train Loss: 0.206, Train Accuracy: 0.953\n",
            "Epoch 110 Validation Accuracy: 0.944\n",
            "[Epoch 111, Batch 100/372] Train Loss: 0.137, Train Accuracy: 1.000\n",
            "[Epoch 111, Batch 200/372] Train Loss: 0.356, Train Accuracy: 0.188\n",
            "[Epoch 111, Batch 300/372] Train Loss: 0.111, Train Accuracy: 0.078\n",
            "Epoch 111 Validation Accuracy: 0.947\n",
            "[Epoch 112, Batch 100/372] Train Loss: 0.423, Train Accuracy: 0.398\n",
            "[Epoch 112, Batch 200/372] Train Loss: 0.224, Train Accuracy: 0.086\n",
            "[Epoch 112, Batch 300/372] Train Loss: 0.127, Train Accuracy: 1.000\n",
            "Epoch 112 Validation Accuracy: 0.943\n",
            "[Epoch 113, Batch 100/372] Train Loss: 0.296, Train Accuracy: 0.195\n",
            "[Epoch 113, Batch 200/372] Train Loss: 0.101, Train Accuracy: 0.148\n",
            "[Epoch 113, Batch 300/372] Train Loss: 0.265, Train Accuracy: 0.914\n",
            "Epoch 113 Validation Accuracy: 0.950\n",
            "[Epoch 114, Batch 100/372] Train Loss: 0.069, Train Accuracy: 0.992\n",
            "[Epoch 114, Batch 200/372] Train Loss: 0.192, Train Accuracy: 0.961\n",
            "[Epoch 114, Batch 300/372] Train Loss: 0.343, Train Accuracy: 0.211\n",
            "Epoch 114 Validation Accuracy: 0.941\n",
            "[Epoch 115, Batch 100/372] Train Loss: 0.380, Train Accuracy: 0.336\n",
            "[Epoch 115, Batch 200/372] Train Loss: 0.267, Train Accuracy: 0.891\n",
            "[Epoch 115, Batch 300/372] Train Loss: 0.210, Train Accuracy: 0.945\n",
            "Epoch 115 Validation Accuracy: 0.946\n",
            "[Epoch 116, Batch 100/372] Train Loss: 0.379, Train Accuracy: 0.289\n",
            "[Epoch 116, Batch 200/372] Train Loss: 0.319, Train Accuracy: 0.875\n",
            "[Epoch 116, Batch 300/372] Train Loss: 0.259, Train Accuracy: 0.172\n",
            "Epoch 116 Validation Accuracy: 0.944\n",
            "[Epoch 117, Batch 100/372] Train Loss: 0.133, Train Accuracy: 0.117\n",
            "[Epoch 117, Batch 200/372] Train Loss: 0.259, Train Accuracy: 0.891\n",
            "[Epoch 117, Batch 300/372] Train Loss: 0.447, Train Accuracy: 0.430\n",
            "Epoch 117 Validation Accuracy: 0.947\n",
            "[Epoch 118, Batch 100/372] Train Loss: 0.071, Train Accuracy: 0.992\n",
            "[Epoch 118, Batch 200/372] Train Loss: 0.369, Train Accuracy: 0.438\n",
            "[Epoch 118, Batch 300/372] Train Loss: 0.378, Train Accuracy: 0.383\n",
            "Epoch 118 Validation Accuracy: 0.942\n",
            "[Epoch 119, Batch 100/372] Train Loss: 0.258, Train Accuracy: 0.164\n",
            "[Epoch 119, Batch 200/372] Train Loss: 0.089, Train Accuracy: 0.102\n",
            "[Epoch 119, Batch 300/372] Train Loss: 0.135, Train Accuracy: 0.984\n",
            "Epoch 119 Validation Accuracy: 0.948\n",
            "[Epoch 120, Batch 100/372] Train Loss: 0.337, Train Accuracy: 0.281\n",
            "[Epoch 120, Batch 200/372] Train Loss: 0.193, Train Accuracy: 0.945\n",
            "[Epoch 120, Batch 300/372] Train Loss: 0.126, Train Accuracy: 0.141\n",
            "Epoch 120 Validation Accuracy: 0.942\n",
            "[Epoch 121, Batch 100/372] Train Loss: 0.108, Train Accuracy: 0.148\n",
            "[Epoch 121, Batch 200/372] Train Loss: 0.088, Train Accuracy: 1.000\n",
            "[Epoch 121, Batch 300/372] Train Loss: 0.244, Train Accuracy: 0.164\n",
            "Epoch 121 Validation Accuracy: 0.943\n",
            "[Epoch 122, Batch 100/372] Train Loss: 0.073, Train Accuracy: 0.078\n",
            "[Epoch 122, Batch 200/372] Train Loss: 0.068, Train Accuracy: 0.062\n",
            "[Epoch 122, Batch 300/372] Train Loss: 0.074, Train Accuracy: 0.109\n",
            "Epoch 122 Validation Accuracy: 0.948\n",
            "[Epoch 123, Batch 100/372] Train Loss: 0.279, Train Accuracy: 0.172\n",
            "[Epoch 123, Batch 200/372] Train Loss: 0.271, Train Accuracy: 0.156\n",
            "[Epoch 123, Batch 300/372] Train Loss: 0.215, Train Accuracy: 0.930\n",
            "Epoch 123 Validation Accuracy: 0.948\n",
            "[Epoch 124, Batch 100/372] Train Loss: 0.109, Train Accuracy: 1.000\n",
            "[Epoch 124, Batch 200/372] Train Loss: 0.071, Train Accuracy: 0.109\n",
            "[Epoch 124, Batch 300/372] Train Loss: 0.115, Train Accuracy: 0.094\n",
            "Epoch 124 Validation Accuracy: 0.949\n",
            "[Epoch 125, Batch 100/372] Train Loss: 0.094, Train Accuracy: 0.109\n",
            "[Epoch 125, Batch 200/372] Train Loss: 0.103, Train Accuracy: 0.094\n",
            "[Epoch 125, Batch 300/372] Train Loss: 0.138, Train Accuracy: 0.977\n",
            "Epoch 125 Validation Accuracy: 0.950\n",
            "Checkpoint saved at epoch 125: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_125.tar\n",
            "[Epoch 126, Batch 100/372] Train Loss: 0.305, Train Accuracy: 0.773\n",
            "[Epoch 126, Batch 200/372] Train Loss: 0.114, Train Accuracy: 0.977\n",
            "[Epoch 126, Batch 300/372] Train Loss: 0.091, Train Accuracy: 0.125\n",
            "Epoch 126 Validation Accuracy: 0.948\n",
            "[Epoch 127, Batch 100/372] Train Loss: 0.070, Train Accuracy: 1.000\n",
            "[Epoch 127, Batch 200/372] Train Loss: 0.351, Train Accuracy: 0.625\n",
            "[Epoch 127, Batch 300/372] Train Loss: 0.084, Train Accuracy: 1.000\n",
            "Epoch 127 Validation Accuracy: 0.948\n",
            "[Epoch 128, Batch 100/372] Train Loss: 0.114, Train Accuracy: 0.102\n",
            "[Epoch 128, Batch 200/372] Train Loss: 0.252, Train Accuracy: 0.164\n",
            "[Epoch 128, Batch 300/372] Train Loss: 0.351, Train Accuracy: 0.344\n",
            "Epoch 128 Validation Accuracy: 0.952\n",
            "[Epoch 129, Batch 100/372] Train Loss: 0.192, Train Accuracy: 0.180\n",
            "[Epoch 129, Batch 200/372] Train Loss: 0.119, Train Accuracy: 0.102\n",
            "[Epoch 129, Batch 300/372] Train Loss: 0.384, Train Accuracy: 0.500\n",
            "Epoch 129 Validation Accuracy: 0.946\n",
            "[Epoch 130, Batch 100/372] Train Loss: 0.112, Train Accuracy: 0.102\n",
            "[Epoch 130, Batch 200/372] Train Loss: 0.321, Train Accuracy: 0.203\n",
            "[Epoch 130, Batch 300/372] Train Loss: 0.282, Train Accuracy: 0.211\n",
            "Epoch 130 Validation Accuracy: 0.944\n",
            "[Epoch 131, Batch 100/372] Train Loss: 0.073, Train Accuracy: 0.148\n",
            "[Epoch 131, Batch 200/372] Train Loss: 0.364, Train Accuracy: 0.617\n",
            "[Epoch 131, Batch 300/372] Train Loss: 0.180, Train Accuracy: 0.930\n",
            "Epoch 131 Validation Accuracy: 0.945\n",
            "[Epoch 132, Batch 100/372] Train Loss: 0.318, Train Accuracy: 0.195\n",
            "[Epoch 132, Batch 200/372] Train Loss: 0.097, Train Accuracy: 0.086\n",
            "[Epoch 132, Batch 300/372] Train Loss: 0.096, Train Accuracy: 0.984\n",
            "Epoch 132 Validation Accuracy: 0.949\n",
            "[Epoch 133, Batch 100/372] Train Loss: 0.094, Train Accuracy: 0.078\n",
            "[Epoch 133, Batch 200/372] Train Loss: 0.230, Train Accuracy: 0.906\n",
            "[Epoch 133, Batch 300/372] Train Loss: 0.205, Train Accuracy: 0.164\n",
            "Epoch 133 Validation Accuracy: 0.952\n",
            "[Epoch 134, Batch 100/372] Train Loss: 0.109, Train Accuracy: 0.992\n",
            "[Epoch 134, Batch 200/372] Train Loss: 0.162, Train Accuracy: 0.148\n",
            "[Epoch 134, Batch 300/372] Train Loss: 0.219, Train Accuracy: 0.930\n",
            "Epoch 134 Validation Accuracy: 0.946\n",
            "[Epoch 135, Batch 100/372] Train Loss: 0.077, Train Accuracy: 0.094\n",
            "[Epoch 135, Batch 200/372] Train Loss: 0.408, Train Accuracy: 0.352\n",
            "[Epoch 135, Batch 300/372] Train Loss: 0.121, Train Accuracy: 1.000\n",
            "Epoch 135 Validation Accuracy: 0.948\n",
            "[Epoch 136, Batch 100/372] Train Loss: 0.134, Train Accuracy: 0.047\n",
            "[Epoch 136, Batch 200/372] Train Loss: 0.194, Train Accuracy: 0.945\n",
            "[Epoch 136, Batch 300/372] Train Loss: 0.122, Train Accuracy: 1.000\n",
            "Epoch 136 Validation Accuracy: 0.950\n",
            "[Epoch 137, Batch 100/372] Train Loss: 0.414, Train Accuracy: 0.453\n",
            "[Epoch 137, Batch 200/372] Train Loss: 0.215, Train Accuracy: 0.125\n",
            "[Epoch 137, Batch 300/372] Train Loss: 0.395, Train Accuracy: 0.594\n",
            "Epoch 137 Validation Accuracy: 0.944\n",
            "[Epoch 138, Batch 100/372] Train Loss: 0.091, Train Accuracy: 0.992\n",
            "[Epoch 138, Batch 200/372] Train Loss: 0.335, Train Accuracy: 0.273\n",
            "[Epoch 138, Batch 300/372] Train Loss: 0.127, Train Accuracy: 0.102\n",
            "Epoch 138 Validation Accuracy: 0.945\n",
            "[Epoch 139, Batch 100/372] Train Loss: 0.096, Train Accuracy: 0.992\n",
            "[Epoch 139, Batch 200/372] Train Loss: 0.184, Train Accuracy: 0.969\n",
            "[Epoch 139, Batch 300/372] Train Loss: 0.125, Train Accuracy: 0.102\n",
            "Epoch 139 Validation Accuracy: 0.949\n",
            "[Epoch 140, Batch 100/372] Train Loss: 0.134, Train Accuracy: 0.969\n",
            "[Epoch 140, Batch 200/372] Train Loss: 0.168, Train Accuracy: 0.961\n",
            "[Epoch 140, Batch 300/372] Train Loss: 0.079, Train Accuracy: 0.148\n",
            "Epoch 140 Validation Accuracy: 0.948\n",
            "[Epoch 141, Batch 100/372] Train Loss: 0.145, Train Accuracy: 0.992\n",
            "[Epoch 141, Batch 200/372] Train Loss: 0.193, Train Accuracy: 0.977\n",
            "[Epoch 141, Batch 300/372] Train Loss: 0.089, Train Accuracy: 0.102\n",
            "Epoch 141 Validation Accuracy: 0.946\n",
            "[Epoch 142, Batch 100/372] Train Loss: 0.075, Train Accuracy: 0.062\n",
            "[Epoch 142, Batch 200/372] Train Loss: 0.104, Train Accuracy: 0.992\n",
            "[Epoch 142, Batch 300/372] Train Loss: 0.068, Train Accuracy: 0.086\n",
            "Epoch 142 Validation Accuracy: 0.953\n",
            "[Epoch 143, Batch 100/372] Train Loss: 0.295, Train Accuracy: 0.875\n",
            "[Epoch 143, Batch 200/372] Train Loss: 0.078, Train Accuracy: 0.992\n",
            "[Epoch 143, Batch 300/372] Train Loss: 0.310, Train Accuracy: 0.336\n",
            "Epoch 143 Validation Accuracy: 0.956\n",
            "[Epoch 144, Batch 100/372] Train Loss: 0.153, Train Accuracy: 0.062\n",
            "[Epoch 144, Batch 200/372] Train Loss: 0.088, Train Accuracy: 0.102\n",
            "[Epoch 144, Batch 300/372] Train Loss: 0.399, Train Accuracy: 0.664\n",
            "Epoch 144 Validation Accuracy: 0.953\n",
            "[Epoch 145, Batch 100/372] Train Loss: 0.340, Train Accuracy: 0.211\n",
            "[Epoch 145, Batch 200/372] Train Loss: 0.358, Train Accuracy: 0.469\n",
            "[Epoch 145, Batch 300/372] Train Loss: 0.298, Train Accuracy: 0.859\n",
            "Epoch 145 Validation Accuracy: 0.950\n",
            "[Epoch 146, Batch 100/372] Train Loss: 0.364, Train Accuracy: 0.398\n",
            "[Epoch 146, Batch 200/372] Train Loss: 0.108, Train Accuracy: 0.086\n",
            "[Epoch 146, Batch 300/372] Train Loss: 0.211, Train Accuracy: 0.125\n",
            "Epoch 146 Validation Accuracy: 0.949\n",
            "[Epoch 147, Batch 100/372] Train Loss: 0.094, Train Accuracy: 1.000\n",
            "[Epoch 147, Batch 200/372] Train Loss: 0.111, Train Accuracy: 0.992\n",
            "[Epoch 147, Batch 300/372] Train Loss: 0.355, Train Accuracy: 0.242\n",
            "Epoch 147 Validation Accuracy: 0.949\n",
            "[Epoch 148, Batch 100/372] Train Loss: 0.103, Train Accuracy: 0.078\n",
            "[Epoch 148, Batch 200/372] Train Loss: 0.248, Train Accuracy: 0.195\n",
            "[Epoch 148, Batch 300/372] Train Loss: 0.241, Train Accuracy: 0.117\n",
            "Epoch 148 Validation Accuracy: 0.948\n",
            "[Epoch 149, Batch 100/372] Train Loss: 0.171, Train Accuracy: 0.133\n",
            "[Epoch 149, Batch 200/372] Train Loss: 0.071, Train Accuracy: 0.102\n",
            "[Epoch 149, Batch 300/372] Train Loss: 0.107, Train Accuracy: 0.992\n",
            "Epoch 149 Validation Accuracy: 0.947\n",
            "[Epoch 150, Batch 100/372] Train Loss: 0.101, Train Accuracy: 1.000\n",
            "[Epoch 150, Batch 200/372] Train Loss: 0.199, Train Accuracy: 0.156\n",
            "[Epoch 150, Batch 300/372] Train Loss: 0.253, Train Accuracy: 0.227\n",
            "Epoch 150 Validation Accuracy: 0.950\n",
            "Checkpoint saved at epoch 150: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_150.tar\n",
            "[Epoch 151, Batch 100/372] Train Loss: 0.315, Train Accuracy: 0.812\n",
            "[Epoch 151, Batch 200/372] Train Loss: 0.120, Train Accuracy: 0.977\n",
            "[Epoch 151, Batch 300/372] Train Loss: 0.064, Train Accuracy: 1.000\n",
            "Epoch 151 Validation Accuracy: 0.952\n",
            "[Epoch 152, Batch 100/372] Train Loss: 0.093, Train Accuracy: 0.984\n",
            "[Epoch 152, Batch 200/372] Train Loss: 0.351, Train Accuracy: 0.188\n",
            "[Epoch 152, Batch 300/372] Train Loss: 0.146, Train Accuracy: 0.109\n",
            "Epoch 152 Validation Accuracy: 0.946\n",
            "[Epoch 153, Batch 100/372] Train Loss: 0.197, Train Accuracy: 0.977\n",
            "[Epoch 153, Batch 200/372] Train Loss: 0.268, Train Accuracy: 0.906\n",
            "[Epoch 153, Batch 300/372] Train Loss: 0.320, Train Accuracy: 0.297\n",
            "Epoch 153 Validation Accuracy: 0.950\n",
            "[Epoch 154, Batch 100/372] Train Loss: 0.097, Train Accuracy: 0.992\n",
            "[Epoch 154, Batch 200/372] Train Loss: 0.313, Train Accuracy: 0.359\n",
            "[Epoch 154, Batch 300/372] Train Loss: 0.304, Train Accuracy: 0.258\n",
            "Epoch 154 Validation Accuracy: 0.953\n",
            "[Epoch 155, Batch 100/372] Train Loss: 0.294, Train Accuracy: 0.891\n",
            "[Epoch 155, Batch 200/372] Train Loss: 0.337, Train Accuracy: 0.617\n",
            "[Epoch 155, Batch 300/372] Train Loss: 0.341, Train Accuracy: 0.695\n",
            "Epoch 155 Validation Accuracy: 0.949\n",
            "[Epoch 156, Batch 100/372] Train Loss: 0.066, Train Accuracy: 1.000\n",
            "[Epoch 156, Batch 200/372] Train Loss: 0.311, Train Accuracy: 0.859\n",
            "[Epoch 156, Batch 300/372] Train Loss: 0.395, Train Accuracy: 0.711\n",
            "Epoch 156 Validation Accuracy: 0.947\n",
            "[Epoch 157, Batch 100/372] Train Loss: 0.085, Train Accuracy: 0.992\n",
            "[Epoch 157, Batch 200/372] Train Loss: 0.098, Train Accuracy: 0.078\n",
            "[Epoch 157, Batch 300/372] Train Loss: 0.408, Train Accuracy: 0.398\n",
            "Epoch 157 Validation Accuracy: 0.952\n",
            "[Epoch 158, Batch 100/372] Train Loss: 0.071, Train Accuracy: 0.062\n",
            "[Epoch 158, Batch 200/372] Train Loss: 0.077, Train Accuracy: 0.992\n",
            "[Epoch 158, Batch 300/372] Train Loss: 0.068, Train Accuracy: 0.133\n",
            "Epoch 158 Validation Accuracy: 0.952\n",
            "[Epoch 159, Batch 100/372] Train Loss: 0.234, Train Accuracy: 0.117\n",
            "[Epoch 159, Batch 200/372] Train Loss: 0.057, Train Accuracy: 0.109\n",
            "[Epoch 159, Batch 300/372] Train Loss: 0.081, Train Accuracy: 1.000\n",
            "Epoch 159 Validation Accuracy: 0.946\n",
            "[Epoch 160, Batch 100/372] Train Loss: 0.322, Train Accuracy: 0.273\n",
            "[Epoch 160, Batch 200/372] Train Loss: 0.315, Train Accuracy: 0.203\n",
            "[Epoch 160, Batch 300/372] Train Loss: 0.058, Train Accuracy: 0.984\n",
            "Epoch 160 Validation Accuracy: 0.949\n",
            "[Epoch 161, Batch 100/372] Train Loss: 0.311, Train Accuracy: 0.789\n",
            "[Epoch 161, Batch 200/372] Train Loss: 0.164, Train Accuracy: 0.117\n",
            "[Epoch 161, Batch 300/372] Train Loss: 0.071, Train Accuracy: 1.000\n",
            "Epoch 161 Validation Accuracy: 0.947\n",
            "[Epoch 162, Batch 100/372] Train Loss: 0.095, Train Accuracy: 1.000\n",
            "[Epoch 162, Batch 200/372] Train Loss: 0.094, Train Accuracy: 0.125\n",
            "[Epoch 162, Batch 300/372] Train Loss: 0.122, Train Accuracy: 0.992\n",
            "Epoch 162 Validation Accuracy: 0.950\n",
            "[Epoch 163, Batch 100/372] Train Loss: 0.364, Train Accuracy: 0.266\n",
            "[Epoch 163, Batch 200/372] Train Loss: 0.128, Train Accuracy: 0.984\n",
            "[Epoch 163, Batch 300/372] Train Loss: 0.099, Train Accuracy: 1.000\n",
            "Epoch 163 Validation Accuracy: 0.948\n",
            "[Epoch 164, Batch 100/372] Train Loss: 0.108, Train Accuracy: 0.062\n",
            "[Epoch 164, Batch 200/372] Train Loss: 0.244, Train Accuracy: 0.898\n",
            "[Epoch 164, Batch 300/372] Train Loss: 0.185, Train Accuracy: 0.109\n",
            "Epoch 164 Validation Accuracy: 0.955\n",
            "[Epoch 165, Batch 100/372] Train Loss: 0.204, Train Accuracy: 0.984\n",
            "[Epoch 165, Batch 200/372] Train Loss: 0.173, Train Accuracy: 0.125\n",
            "[Epoch 165, Batch 300/372] Train Loss: 0.131, Train Accuracy: 0.992\n",
            "Epoch 165 Validation Accuracy: 0.944\n",
            "[Epoch 166, Batch 100/372] Train Loss: 0.071, Train Accuracy: 1.000\n",
            "[Epoch 166, Batch 200/372] Train Loss: 0.377, Train Accuracy: 0.453\n",
            "[Epoch 166, Batch 300/372] Train Loss: 0.270, Train Accuracy: 0.148\n",
            "Epoch 166 Validation Accuracy: 0.946\n",
            "[Epoch 167, Batch 100/372] Train Loss: 0.079, Train Accuracy: 0.062\n",
            "[Epoch 167, Batch 200/372] Train Loss: 0.230, Train Accuracy: 0.180\n",
            "[Epoch 167, Batch 300/372] Train Loss: 0.125, Train Accuracy: 0.117\n",
            "Epoch 167 Validation Accuracy: 0.949\n",
            "[Epoch 168, Batch 100/372] Train Loss: 0.398, Train Accuracy: 0.586\n",
            "[Epoch 168, Batch 200/372] Train Loss: 0.072, Train Accuracy: 0.102\n",
            "[Epoch 168, Batch 300/372] Train Loss: 0.149, Train Accuracy: 0.977\n",
            "Epoch 168 Validation Accuracy: 0.948\n",
            "[Epoch 169, Batch 100/372] Train Loss: 0.074, Train Accuracy: 0.086\n",
            "[Epoch 169, Batch 200/372] Train Loss: 0.157, Train Accuracy: 0.984\n",
            "[Epoch 169, Batch 300/372] Train Loss: 0.359, Train Accuracy: 0.336\n",
            "Epoch 169 Validation Accuracy: 0.950\n",
            "[Epoch 170, Batch 100/372] Train Loss: 0.337, Train Accuracy: 0.727\n",
            "[Epoch 170, Batch 200/372] Train Loss: 0.319, Train Accuracy: 0.719\n",
            "[Epoch 170, Batch 300/372] Train Loss: 0.088, Train Accuracy: 0.055\n",
            "Epoch 170 Validation Accuracy: 0.950\n",
            "[Epoch 171, Batch 100/372] Train Loss: 0.374, Train Accuracy: 0.375\n",
            "[Epoch 171, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.344\n",
            "[Epoch 171, Batch 300/372] Train Loss: 0.100, Train Accuracy: 0.992\n",
            "Epoch 171 Validation Accuracy: 0.953\n",
            "[Epoch 172, Batch 100/372] Train Loss: 0.296, Train Accuracy: 0.188\n",
            "[Epoch 172, Batch 200/372] Train Loss: 0.092, Train Accuracy: 0.992\n",
            "[Epoch 172, Batch 300/372] Train Loss: 0.316, Train Accuracy: 0.852\n",
            "Epoch 172 Validation Accuracy: 0.949\n",
            "[Epoch 173, Batch 100/372] Train Loss: 0.198, Train Accuracy: 0.078\n",
            "[Epoch 173, Batch 200/372] Train Loss: 0.243, Train Accuracy: 0.141\n",
            "[Epoch 173, Batch 300/372] Train Loss: 0.321, Train Accuracy: 0.734\n",
            "Epoch 173 Validation Accuracy: 0.954\n",
            "[Epoch 174, Batch 100/372] Train Loss: 0.194, Train Accuracy: 0.945\n",
            "[Epoch 174, Batch 200/372] Train Loss: 0.172, Train Accuracy: 0.141\n",
            "[Epoch 174, Batch 300/372] Train Loss: 0.086, Train Accuracy: 1.000\n",
            "Epoch 174 Validation Accuracy: 0.951\n",
            "[Epoch 175, Batch 100/372] Train Loss: 0.257, Train Accuracy: 0.141\n",
            "[Epoch 175, Batch 200/372] Train Loss: 0.122, Train Accuracy: 0.117\n",
            "[Epoch 175, Batch 300/372] Train Loss: 0.309, Train Accuracy: 0.273\n",
            "Epoch 175 Validation Accuracy: 0.948\n",
            "Checkpoint saved at epoch 175: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_175.tar\n",
            "[Epoch 176, Batch 100/372] Train Loss: 0.125, Train Accuracy: 0.992\n",
            "[Epoch 176, Batch 200/372] Train Loss: 0.390, Train Accuracy: 0.531\n",
            "[Epoch 176, Batch 300/372] Train Loss: 0.079, Train Accuracy: 0.070\n",
            "Epoch 176 Validation Accuracy: 0.952\n",
            "[Epoch 177, Batch 100/372] Train Loss: 0.164, Train Accuracy: 0.125\n",
            "[Epoch 177, Batch 200/372] Train Loss: 0.291, Train Accuracy: 0.312\n",
            "[Epoch 177, Batch 300/372] Train Loss: 0.201, Train Accuracy: 0.117\n",
            "Epoch 177 Validation Accuracy: 0.953\n",
            "[Epoch 178, Batch 100/372] Train Loss: 0.355, Train Accuracy: 0.250\n",
            "[Epoch 178, Batch 200/372] Train Loss: 0.067, Train Accuracy: 1.000\n",
            "[Epoch 178, Batch 300/372] Train Loss: 0.069, Train Accuracy: 1.000\n",
            "Epoch 178 Validation Accuracy: 0.952\n",
            "[Epoch 179, Batch 100/372] Train Loss: 0.294, Train Accuracy: 0.836\n",
            "[Epoch 179, Batch 200/372] Train Loss: 0.338, Train Accuracy: 0.367\n",
            "[Epoch 179, Batch 300/372] Train Loss: 0.131, Train Accuracy: 0.984\n",
            "Epoch 179 Validation Accuracy: 0.952\n",
            "[Epoch 180, Batch 100/372] Train Loss: 0.235, Train Accuracy: 0.156\n",
            "[Epoch 180, Batch 200/372] Train Loss: 0.382, Train Accuracy: 0.703\n",
            "[Epoch 180, Batch 300/372] Train Loss: 0.358, Train Accuracy: 0.297\n",
            "Epoch 180 Validation Accuracy: 0.948\n",
            "[Epoch 181, Batch 100/372] Train Loss: 0.071, Train Accuracy: 0.117\n",
            "[Epoch 181, Batch 200/372] Train Loss: 0.090, Train Accuracy: 0.102\n",
            "[Epoch 181, Batch 300/372] Train Loss: 0.084, Train Accuracy: 0.102\n",
            "Epoch 181 Validation Accuracy: 0.946\n",
            "[Epoch 182, Batch 100/372] Train Loss: 0.171, Train Accuracy: 0.109\n",
            "[Epoch 182, Batch 200/372] Train Loss: 0.147, Train Accuracy: 0.172\n",
            "[Epoch 182, Batch 300/372] Train Loss: 0.321, Train Accuracy: 0.320\n",
            "Epoch 182 Validation Accuracy: 0.951\n",
            "[Epoch 183, Batch 100/372] Train Loss: 0.065, Train Accuracy: 1.000\n",
            "[Epoch 183, Batch 200/372] Train Loss: 0.105, Train Accuracy: 0.977\n",
            "[Epoch 183, Batch 300/372] Train Loss: 0.066, Train Accuracy: 0.094\n",
            "Epoch 183 Validation Accuracy: 0.956\n",
            "[Epoch 184, Batch 100/372] Train Loss: 0.219, Train Accuracy: 0.938\n",
            "[Epoch 184, Batch 200/372] Train Loss: 0.087, Train Accuracy: 0.125\n",
            "[Epoch 184, Batch 300/372] Train Loss: 0.114, Train Accuracy: 0.992\n",
            "Epoch 184 Validation Accuracy: 0.953\n",
            "[Epoch 185, Batch 100/372] Train Loss: 0.171, Train Accuracy: 0.969\n",
            "[Epoch 185, Batch 200/372] Train Loss: 0.106, Train Accuracy: 0.102\n",
            "[Epoch 185, Batch 300/372] Train Loss: 0.292, Train Accuracy: 0.836\n",
            "Epoch 185 Validation Accuracy: 0.948\n",
            "[Epoch 186, Batch 100/372] Train Loss: 0.171, Train Accuracy: 0.969\n",
            "[Epoch 186, Batch 200/372] Train Loss: 0.364, Train Accuracy: 0.672\n",
            "[Epoch 186, Batch 300/372] Train Loss: 0.201, Train Accuracy: 0.086\n",
            "Epoch 186 Validation Accuracy: 0.950\n",
            "[Epoch 187, Batch 100/372] Train Loss: 0.241, Train Accuracy: 0.164\n",
            "[Epoch 187, Batch 200/372] Train Loss: 0.071, Train Accuracy: 0.156\n",
            "[Epoch 187, Batch 300/372] Train Loss: 0.363, Train Accuracy: 0.281\n",
            "Epoch 187 Validation Accuracy: 0.946\n",
            "[Epoch 188, Batch 100/372] Train Loss: 0.365, Train Accuracy: 0.242\n",
            "[Epoch 188, Batch 200/372] Train Loss: 0.053, Train Accuracy: 0.133\n",
            "[Epoch 188, Batch 300/372] Train Loss: 0.380, Train Accuracy: 0.484\n",
            "Epoch 188 Validation Accuracy: 0.951\n",
            "[Epoch 189, Batch 100/372] Train Loss: 0.099, Train Accuracy: 0.984\n",
            "[Epoch 189, Batch 200/372] Train Loss: 0.080, Train Accuracy: 1.000\n",
            "[Epoch 189, Batch 300/372] Train Loss: 0.209, Train Accuracy: 0.125\n",
            "Epoch 189 Validation Accuracy: 0.957\n",
            "[Epoch 190, Batch 100/372] Train Loss: 0.077, Train Accuracy: 0.992\n",
            "[Epoch 190, Batch 200/372] Train Loss: 0.433, Train Accuracy: 0.609\n",
            "[Epoch 190, Batch 300/372] Train Loss: 0.083, Train Accuracy: 0.992\n",
            "Epoch 190 Validation Accuracy: 0.951\n",
            "[Epoch 191, Batch 100/372] Train Loss: 0.095, Train Accuracy: 0.125\n",
            "[Epoch 191, Batch 200/372] Train Loss: 0.082, Train Accuracy: 0.055\n",
            "[Epoch 191, Batch 300/372] Train Loss: 0.080, Train Accuracy: 1.000\n",
            "Epoch 191 Validation Accuracy: 0.951\n",
            "[Epoch 192, Batch 100/372] Train Loss: 0.293, Train Accuracy: 0.367\n",
            "[Epoch 192, Batch 200/372] Train Loss: 0.304, Train Accuracy: 0.656\n",
            "[Epoch 192, Batch 300/372] Train Loss: 0.189, Train Accuracy: 0.961\n",
            "Epoch 192 Validation Accuracy: 0.955\n",
            "[Epoch 193, Batch 100/372] Train Loss: 0.066, Train Accuracy: 0.133\n",
            "[Epoch 193, Batch 200/372] Train Loss: 0.187, Train Accuracy: 0.203\n",
            "[Epoch 193, Batch 300/372] Train Loss: 0.088, Train Accuracy: 1.000\n",
            "Epoch 193 Validation Accuracy: 0.951\n",
            "[Epoch 194, Batch 100/372] Train Loss: 0.093, Train Accuracy: 1.000\n",
            "[Epoch 194, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.266\n",
            "[Epoch 194, Batch 300/372] Train Loss: 0.287, Train Accuracy: 0.812\n",
            "Epoch 194 Validation Accuracy: 0.953\n",
            "[Epoch 195, Batch 100/372] Train Loss: 0.305, Train Accuracy: 0.688\n",
            "[Epoch 195, Batch 200/372] Train Loss: 0.224, Train Accuracy: 0.117\n",
            "[Epoch 195, Batch 300/372] Train Loss: 0.376, Train Accuracy: 0.250\n",
            "Epoch 195 Validation Accuracy: 0.948\n",
            "[Epoch 196, Batch 100/372] Train Loss: 0.071, Train Accuracy: 0.055\n",
            "[Epoch 196, Batch 200/372] Train Loss: 0.274, Train Accuracy: 0.242\n",
            "[Epoch 196, Batch 300/372] Train Loss: 0.087, Train Accuracy: 0.094\n",
            "Epoch 196 Validation Accuracy: 0.956\n",
            "[Epoch 197, Batch 100/372] Train Loss: 0.074, Train Accuracy: 0.062\n",
            "[Epoch 197, Batch 200/372] Train Loss: 0.293, Train Accuracy: 0.484\n",
            "[Epoch 197, Batch 300/372] Train Loss: 0.121, Train Accuracy: 0.984\n",
            "Epoch 197 Validation Accuracy: 0.952\n",
            "[Epoch 198, Batch 100/372] Train Loss: 0.077, Train Accuracy: 0.102\n",
            "[Epoch 198, Batch 200/372] Train Loss: 0.266, Train Accuracy: 0.875\n",
            "[Epoch 198, Batch 300/372] Train Loss: 0.067, Train Accuracy: 0.992\n",
            "Epoch 198 Validation Accuracy: 0.952\n",
            "[Epoch 199, Batch 100/372] Train Loss: 0.125, Train Accuracy: 0.109\n",
            "[Epoch 199, Batch 200/372] Train Loss: 0.094, Train Accuracy: 1.000\n",
            "[Epoch 199, Batch 300/372] Train Loss: 0.215, Train Accuracy: 0.922\n",
            "Epoch 199 Validation Accuracy: 0.951\n",
            "[Epoch 200, Batch 100/372] Train Loss: 0.368, Train Accuracy: 0.477\n",
            "[Epoch 200, Batch 200/372] Train Loss: 0.156, Train Accuracy: 0.992\n",
            "[Epoch 200, Batch 300/372] Train Loss: 0.134, Train Accuracy: 0.977\n",
            "Epoch 200 Validation Accuracy: 0.949\n",
            "Checkpoint saved at epoch 200: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar\n",
            "[Epoch 201, Batch 100/372] Train Loss: 0.088, Train Accuracy: 1.000\n",
            "[Epoch 201, Batch 200/372] Train Loss: 0.077, Train Accuracy: 0.141\n",
            "[Epoch 201, Batch 300/372] Train Loss: 0.118, Train Accuracy: 0.141\n",
            "Epoch 201 Validation Accuracy: 0.948\n",
            "[Epoch 202, Batch 100/372] Train Loss: 0.076, Train Accuracy: 0.992\n",
            "[Epoch 202, Batch 200/372] Train Loss: 0.352, Train Accuracy: 0.445\n",
            "[Epoch 202, Batch 300/372] Train Loss: 0.282, Train Accuracy: 0.906\n",
            "Epoch 202 Validation Accuracy: 0.950\n",
            "[Epoch 203, Batch 100/372] Train Loss: 0.061, Train Accuracy: 0.133\n",
            "[Epoch 203, Batch 200/372] Train Loss: 0.160, Train Accuracy: 0.977\n",
            "[Epoch 203, Batch 300/372] Train Loss: 0.079, Train Accuracy: 0.156\n",
            "Epoch 203 Validation Accuracy: 0.954\n",
            "[Epoch 204, Batch 100/372] Train Loss: 0.249, Train Accuracy: 0.156\n",
            "[Epoch 204, Batch 200/372] Train Loss: 0.242, Train Accuracy: 0.922\n",
            "[Epoch 204, Batch 300/372] Train Loss: 0.249, Train Accuracy: 0.211\n",
            "Epoch 204 Validation Accuracy: 0.952\n",
            "[Epoch 205, Batch 100/372] Train Loss: 0.069, Train Accuracy: 1.000\n",
            "[Epoch 205, Batch 200/372] Train Loss: 0.414, Train Accuracy: 0.445\n",
            "[Epoch 205, Batch 300/372] Train Loss: 0.200, Train Accuracy: 0.102\n",
            "Epoch 205 Validation Accuracy: 0.954\n",
            "[Epoch 206, Batch 100/372] Train Loss: 0.110, Train Accuracy: 0.992\n",
            "[Epoch 206, Batch 200/372] Train Loss: 0.152, Train Accuracy: 0.984\n",
            "[Epoch 206, Batch 300/372] Train Loss: 0.124, Train Accuracy: 0.055\n",
            "Epoch 206 Validation Accuracy: 0.952\n",
            "[Epoch 207, Batch 100/372] Train Loss: 0.336, Train Accuracy: 0.711\n",
            "[Epoch 207, Batch 200/372] Train Loss: 0.185, Train Accuracy: 0.180\n",
            "[Epoch 207, Batch 300/372] Train Loss: 0.115, Train Accuracy: 0.102\n",
            "Epoch 207 Validation Accuracy: 0.953\n",
            "[Epoch 208, Batch 100/372] Train Loss: 0.402, Train Accuracy: 0.570\n",
            "[Epoch 208, Batch 200/372] Train Loss: 0.164, Train Accuracy: 0.961\n",
            "[Epoch 208, Batch 300/372] Train Loss: 0.302, Train Accuracy: 0.719\n",
            "Epoch 208 Validation Accuracy: 0.951\n",
            "[Epoch 209, Batch 100/372] Train Loss: 0.080, Train Accuracy: 1.000\n",
            "[Epoch 209, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.852\n",
            "[Epoch 209, Batch 300/372] Train Loss: 0.271, Train Accuracy: 0.836\n",
            "Epoch 209 Validation Accuracy: 0.947\n",
            "[Epoch 210, Batch 100/372] Train Loss: 0.386, Train Accuracy: 0.305\n",
            "[Epoch 210, Batch 200/372] Train Loss: 0.344, Train Accuracy: 0.688\n",
            "[Epoch 210, Batch 300/372] Train Loss: 0.130, Train Accuracy: 0.977\n",
            "Epoch 210 Validation Accuracy: 0.950\n",
            "[Epoch 211, Batch 100/372] Train Loss: 0.061, Train Accuracy: 0.992\n",
            "[Epoch 211, Batch 200/372] Train Loss: 0.058, Train Accuracy: 1.000\n",
            "[Epoch 211, Batch 300/372] Train Loss: 0.365, Train Accuracy: 0.484\n",
            "Epoch 211 Validation Accuracy: 0.949\n",
            "[Epoch 212, Batch 100/372] Train Loss: 0.290, Train Accuracy: 0.289\n",
            "[Epoch 212, Batch 200/372] Train Loss: 0.351, Train Accuracy: 0.461\n",
            "[Epoch 212, Batch 300/372] Train Loss: 0.224, Train Accuracy: 0.922\n",
            "Epoch 212 Validation Accuracy: 0.944\n",
            "[Epoch 213, Batch 100/372] Train Loss: 0.086, Train Accuracy: 0.141\n",
            "[Epoch 213, Batch 200/372] Train Loss: 0.107, Train Accuracy: 0.156\n",
            "[Epoch 213, Batch 300/372] Train Loss: 0.329, Train Accuracy: 0.445\n",
            "Epoch 213 Validation Accuracy: 0.952\n",
            "[Epoch 214, Batch 100/372] Train Loss: 0.283, Train Accuracy: 0.805\n",
            "[Epoch 214, Batch 200/372] Train Loss: 0.230, Train Accuracy: 0.141\n",
            "[Epoch 214, Batch 300/372] Train Loss: 0.078, Train Accuracy: 0.125\n",
            "Epoch 214 Validation Accuracy: 0.950\n",
            "[Epoch 215, Batch 100/372] Train Loss: 0.057, Train Accuracy: 0.055\n",
            "[Epoch 215, Batch 200/372] Train Loss: 0.199, Train Accuracy: 0.969\n",
            "[Epoch 215, Batch 300/372] Train Loss: 0.054, Train Accuracy: 0.117\n",
            "Epoch 215 Validation Accuracy: 0.951\n",
            "[Epoch 216, Batch 100/372] Train Loss: 0.178, Train Accuracy: 0.961\n",
            "[Epoch 216, Batch 200/372] Train Loss: 0.294, Train Accuracy: 0.281\n",
            "[Epoch 216, Batch 300/372] Train Loss: 0.157, Train Accuracy: 0.094\n",
            "Epoch 216 Validation Accuracy: 0.951\n",
            "[Epoch 217, Batch 100/372] Train Loss: 0.085, Train Accuracy: 0.992\n",
            "[Epoch 217, Batch 200/372] Train Loss: 0.245, Train Accuracy: 0.180\n",
            "[Epoch 217, Batch 300/372] Train Loss: 0.318, Train Accuracy: 0.305\n",
            "Epoch 217 Validation Accuracy: 0.949\n",
            "[Epoch 218, Batch 100/372] Train Loss: 0.071, Train Accuracy: 0.070\n",
            "[Epoch 218, Batch 200/372] Train Loss: 0.122, Train Accuracy: 0.109\n",
            "[Epoch 218, Batch 300/372] Train Loss: 0.060, Train Accuracy: 0.070\n",
            "Epoch 218 Validation Accuracy: 0.954\n",
            "[Epoch 219, Batch 100/372] Train Loss: 0.239, Train Accuracy: 0.172\n",
            "[Epoch 219, Batch 200/372] Train Loss: 0.093, Train Accuracy: 1.000\n",
            "[Epoch 219, Batch 300/372] Train Loss: 0.091, Train Accuracy: 0.148\n",
            "Epoch 219 Validation Accuracy: 0.953\n",
            "[Epoch 220, Batch 100/372] Train Loss: 0.330, Train Accuracy: 0.281\n",
            "[Epoch 220, Batch 200/372] Train Loss: 0.164, Train Accuracy: 0.094\n",
            "[Epoch 220, Batch 300/372] Train Loss: 0.244, Train Accuracy: 0.922\n",
            "Epoch 220 Validation Accuracy: 0.947\n",
            "[Epoch 221, Batch 100/372] Train Loss: 0.063, Train Accuracy: 0.992\n",
            "[Epoch 221, Batch 200/372] Train Loss: 0.347, Train Accuracy: 0.266\n",
            "[Epoch 221, Batch 300/372] Train Loss: 0.085, Train Accuracy: 1.000\n",
            "Epoch 221 Validation Accuracy: 0.951\n",
            "[Epoch 222, Batch 100/372] Train Loss: 0.051, Train Accuracy: 1.000\n",
            "[Epoch 222, Batch 200/372] Train Loss: 0.309, Train Accuracy: 0.234\n",
            "[Epoch 222, Batch 300/372] Train Loss: 0.170, Train Accuracy: 0.984\n",
            "Epoch 222 Validation Accuracy: 0.954\n",
            "[Epoch 223, Batch 100/372] Train Loss: 0.342, Train Accuracy: 0.539\n",
            "[Epoch 223, Batch 200/372] Train Loss: 0.079, Train Accuracy: 1.000\n",
            "[Epoch 223, Batch 300/372] Train Loss: 0.252, Train Accuracy: 0.812\n",
            "Epoch 223 Validation Accuracy: 0.950\n",
            "[Epoch 224, Batch 100/372] Train Loss: 0.351, Train Accuracy: 0.539\n",
            "[Epoch 224, Batch 200/372] Train Loss: 0.351, Train Accuracy: 0.445\n",
            "[Epoch 224, Batch 300/372] Train Loss: 0.093, Train Accuracy: 1.000\n",
            "Epoch 224 Validation Accuracy: 0.950\n",
            "[Epoch 225, Batch 100/372] Train Loss: 0.061, Train Accuracy: 0.078\n",
            "[Epoch 225, Batch 200/372] Train Loss: 0.059, Train Accuracy: 1.000\n",
            "[Epoch 225, Batch 300/372] Train Loss: 0.247, Train Accuracy: 0.234\n",
            "Epoch 225 Validation Accuracy: 0.954\n",
            "Checkpoint saved at epoch 225: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_225.tar\n",
            "[Epoch 226, Batch 100/372] Train Loss: 0.079, Train Accuracy: 0.133\n",
            "[Epoch 226, Batch 200/372] Train Loss: 0.228, Train Accuracy: 0.922\n",
            "[Epoch 226, Batch 300/372] Train Loss: 0.343, Train Accuracy: 0.711\n",
            "Epoch 226 Validation Accuracy: 0.948\n",
            "[Epoch 227, Batch 100/372] Train Loss: 0.191, Train Accuracy: 0.953\n",
            "[Epoch 227, Batch 200/372] Train Loss: 0.178, Train Accuracy: 0.953\n",
            "[Epoch 227, Batch 300/372] Train Loss: 0.282, Train Accuracy: 0.320\n",
            "Epoch 227 Validation Accuracy: 0.950\n",
            "[Epoch 228, Batch 100/372] Train Loss: 0.226, Train Accuracy: 0.180\n",
            "[Epoch 228, Batch 200/372] Train Loss: 0.242, Train Accuracy: 0.211\n",
            "[Epoch 228, Batch 300/372] Train Loss: 0.072, Train Accuracy: 1.000\n",
            "Epoch 228 Validation Accuracy: 0.954\n",
            "[Epoch 229, Batch 100/372] Train Loss: 0.087, Train Accuracy: 1.000\n",
            "[Epoch 229, Batch 200/372] Train Loss: 0.210, Train Accuracy: 0.148\n",
            "[Epoch 229, Batch 300/372] Train Loss: 0.323, Train Accuracy: 0.477\n",
            "Epoch 229 Validation Accuracy: 0.952\n",
            "[Epoch 230, Batch 100/372] Train Loss: 0.338, Train Accuracy: 0.547\n",
            "[Epoch 230, Batch 200/372] Train Loss: 0.258, Train Accuracy: 0.875\n",
            "[Epoch 230, Batch 300/372] Train Loss: 0.150, Train Accuracy: 0.969\n",
            "Epoch 230 Validation Accuracy: 0.948\n",
            "[Epoch 231, Batch 100/372] Train Loss: 0.092, Train Accuracy: 0.977\n",
            "[Epoch 231, Batch 200/372] Train Loss: 0.106, Train Accuracy: 0.117\n",
            "[Epoch 231, Batch 300/372] Train Loss: 0.130, Train Accuracy: 0.102\n",
            "Epoch 231 Validation Accuracy: 0.951\n",
            "[Epoch 232, Batch 100/372] Train Loss: 0.063, Train Accuracy: 0.070\n",
            "[Epoch 232, Batch 200/372] Train Loss: 0.303, Train Accuracy: 0.680\n",
            "[Epoch 232, Batch 300/372] Train Loss: 0.297, Train Accuracy: 0.258\n",
            "Epoch 232 Validation Accuracy: 0.950\n",
            "[Epoch 233, Batch 100/372] Train Loss: 0.188, Train Accuracy: 0.164\n",
            "[Epoch 233, Batch 200/372] Train Loss: 0.218, Train Accuracy: 0.117\n",
            "[Epoch 233, Batch 300/372] Train Loss: 0.080, Train Accuracy: 0.992\n",
            "Epoch 233 Validation Accuracy: 0.955\n",
            "[Epoch 234, Batch 100/372] Train Loss: 0.193, Train Accuracy: 0.164\n",
            "[Epoch 234, Batch 200/372] Train Loss: 0.298, Train Accuracy: 0.297\n",
            "[Epoch 234, Batch 300/372] Train Loss: 0.062, Train Accuracy: 1.000\n",
            "Epoch 234 Validation Accuracy: 0.951\n",
            "[Epoch 235, Batch 100/372] Train Loss: 0.070, Train Accuracy: 0.141\n",
            "[Epoch 235, Batch 200/372] Train Loss: 0.078, Train Accuracy: 0.078\n",
            "[Epoch 235, Batch 300/372] Train Loss: 0.108, Train Accuracy: 0.109\n",
            "Epoch 235 Validation Accuracy: 0.946\n",
            "[Epoch 236, Batch 100/372] Train Loss: 0.283, Train Accuracy: 0.766\n",
            "[Epoch 236, Batch 200/372] Train Loss: 0.092, Train Accuracy: 0.102\n",
            "[Epoch 236, Batch 300/372] Train Loss: 0.352, Train Accuracy: 0.703\n",
            "Epoch 236 Validation Accuracy: 0.950\n",
            "[Epoch 237, Batch 100/372] Train Loss: 0.103, Train Accuracy: 0.992\n",
            "[Epoch 237, Batch 200/372] Train Loss: 0.056, Train Accuracy: 0.102\n",
            "[Epoch 237, Batch 300/372] Train Loss: 0.059, Train Accuracy: 1.000\n",
            "Epoch 237 Validation Accuracy: 0.948\n",
            "[Epoch 238, Batch 100/372] Train Loss: 0.163, Train Accuracy: 0.078\n",
            "[Epoch 238, Batch 200/372] Train Loss: 0.336, Train Accuracy: 0.508\n",
            "[Epoch 238, Batch 300/372] Train Loss: 0.077, Train Accuracy: 1.000\n",
            "Epoch 238 Validation Accuracy: 0.947\n",
            "[Epoch 239, Batch 100/372] Train Loss: 0.118, Train Accuracy: 0.992\n",
            "[Epoch 239, Batch 200/372] Train Loss: 0.336, Train Accuracy: 0.391\n",
            "[Epoch 239, Batch 300/372] Train Loss: 0.068, Train Accuracy: 1.000\n",
            "Epoch 239 Validation Accuracy: 0.954\n",
            "[Epoch 240, Batch 100/372] Train Loss: 0.269, Train Accuracy: 0.906\n",
            "[Epoch 240, Batch 200/372] Train Loss: 0.324, Train Accuracy: 0.789\n",
            "[Epoch 240, Batch 300/372] Train Loss: 0.107, Train Accuracy: 0.078\n",
            "Epoch 240 Validation Accuracy: 0.950\n",
            "[Epoch 241, Batch 100/372] Train Loss: 0.257, Train Accuracy: 0.141\n",
            "[Epoch 241, Batch 200/372] Train Loss: 0.092, Train Accuracy: 0.984\n",
            "[Epoch 241, Batch 300/372] Train Loss: 0.062, Train Accuracy: 0.094\n",
            "Epoch 241 Validation Accuracy: 0.949\n",
            "[Epoch 242, Batch 100/372] Train Loss: 0.243, Train Accuracy: 0.898\n",
            "[Epoch 242, Batch 200/372] Train Loss: 0.113, Train Accuracy: 0.109\n",
            "[Epoch 242, Batch 300/372] Train Loss: 0.276, Train Accuracy: 0.820\n",
            "Epoch 242 Validation Accuracy: 0.953\n",
            "[Epoch 243, Batch 100/372] Train Loss: 0.224, Train Accuracy: 0.141\n",
            "[Epoch 243, Batch 200/372] Train Loss: 0.117, Train Accuracy: 0.969\n",
            "[Epoch 243, Batch 300/372] Train Loss: 0.243, Train Accuracy: 0.156\n",
            "Epoch 243 Validation Accuracy: 0.952\n",
            "[Epoch 244, Batch 100/372] Train Loss: 0.125, Train Accuracy: 0.992\n",
            "[Epoch 244, Batch 200/372] Train Loss: 0.192, Train Accuracy: 0.945\n",
            "[Epoch 244, Batch 300/372] Train Loss: 0.064, Train Accuracy: 1.000\n",
            "Epoch 244 Validation Accuracy: 0.954\n",
            "[Epoch 245, Batch 100/372] Train Loss: 0.058, Train Accuracy: 1.000\n",
            "[Epoch 245, Batch 200/372] Train Loss: 0.321, Train Accuracy: 0.297\n",
            "[Epoch 245, Batch 300/372] Train Loss: 0.298, Train Accuracy: 0.789\n",
            "Epoch 245 Validation Accuracy: 0.954\n",
            "[Epoch 246, Batch 100/372] Train Loss: 0.227, Train Accuracy: 0.914\n",
            "[Epoch 246, Batch 200/372] Train Loss: 0.073, Train Accuracy: 0.086\n",
            "[Epoch 246, Batch 300/372] Train Loss: 0.341, Train Accuracy: 0.523\n",
            "Epoch 246 Validation Accuracy: 0.947\n",
            "[Epoch 247, Batch 100/372] Train Loss: 0.221, Train Accuracy: 0.148\n",
            "[Epoch 247, Batch 200/372] Train Loss: 0.169, Train Accuracy: 0.164\n",
            "[Epoch 247, Batch 300/372] Train Loss: 0.065, Train Accuracy: 0.086\n",
            "Epoch 247 Validation Accuracy: 0.950\n",
            "[Epoch 248, Batch 100/372] Train Loss: 0.328, Train Accuracy: 0.602\n",
            "[Epoch 248, Batch 200/372] Train Loss: 0.217, Train Accuracy: 0.156\n",
            "[Epoch 248, Batch 300/372] Train Loss: 0.068, Train Accuracy: 0.164\n",
            "Epoch 248 Validation Accuracy: 0.951\n",
            "[Epoch 249, Batch 100/372] Train Loss: 0.091, Train Accuracy: 0.992\n",
            "[Epoch 249, Batch 200/372] Train Loss: 0.066, Train Accuracy: 0.133\n",
            "[Epoch 249, Batch 300/372] Train Loss: 0.354, Train Accuracy: 0.289\n",
            "Epoch 249 Validation Accuracy: 0.954\n",
            "[Epoch 250, Batch 100/372] Train Loss: 0.076, Train Accuracy: 0.117\n",
            "[Epoch 250, Batch 200/372] Train Loss: 0.075, Train Accuracy: 1.000\n",
            "[Epoch 250, Batch 300/372] Train Loss: 0.085, Train Accuracy: 0.125\n",
            "Epoch 250 Validation Accuracy: 0.953\n",
            "Checkpoint saved at epoch 250: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_250.tar\n",
            "[Epoch 251, Batch 100/372] Train Loss: 0.213, Train Accuracy: 0.102\n",
            "[Epoch 251, Batch 200/372] Train Loss: 0.075, Train Accuracy: 0.148\n",
            "[Epoch 251, Batch 300/372] Train Loss: 0.130, Train Accuracy: 0.141\n",
            "Epoch 251 Validation Accuracy: 0.952\n",
            "[Epoch 252, Batch 100/372] Train Loss: 0.277, Train Accuracy: 0.219\n",
            "[Epoch 252, Batch 200/372] Train Loss: 0.275, Train Accuracy: 0.883\n",
            "[Epoch 252, Batch 300/372] Train Loss: 0.300, Train Accuracy: 0.656\n",
            "Epoch 252 Validation Accuracy: 0.952\n",
            "[Epoch 253, Batch 100/372] Train Loss: 0.084, Train Accuracy: 1.000\n",
            "[Epoch 253, Batch 200/372] Train Loss: 0.239, Train Accuracy: 0.945\n",
            "[Epoch 253, Batch 300/372] Train Loss: 0.131, Train Accuracy: 0.969\n",
            "Epoch 253 Validation Accuracy: 0.954\n",
            "[Epoch 254, Batch 100/372] Train Loss: 0.183, Train Accuracy: 0.141\n",
            "[Epoch 254, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.789\n",
            "[Epoch 254, Batch 300/372] Train Loss: 0.087, Train Accuracy: 1.000\n",
            "Epoch 254 Validation Accuracy: 0.956\n",
            "[Epoch 255, Batch 100/372] Train Loss: 0.250, Train Accuracy: 0.766\n",
            "[Epoch 255, Batch 200/372] Train Loss: 0.174, Train Accuracy: 0.969\n",
            "[Epoch 255, Batch 300/372] Train Loss: 0.263, Train Accuracy: 0.117\n",
            "Epoch 255 Validation Accuracy: 0.952\n",
            "[Epoch 256, Batch 100/372] Train Loss: 0.069, Train Accuracy: 0.078\n",
            "[Epoch 256, Batch 200/372] Train Loss: 0.191, Train Accuracy: 0.125\n",
            "[Epoch 256, Batch 300/372] Train Loss: 0.142, Train Accuracy: 0.984\n",
            "Epoch 256 Validation Accuracy: 0.952\n",
            "[Epoch 257, Batch 100/372] Train Loss: 0.306, Train Accuracy: 0.320\n",
            "[Epoch 257, Batch 200/372] Train Loss: 0.078, Train Accuracy: 1.000\n",
            "[Epoch 257, Batch 300/372] Train Loss: 0.236, Train Accuracy: 0.883\n",
            "Epoch 257 Validation Accuracy: 0.952\n",
            "[Epoch 258, Batch 100/372] Train Loss: 0.340, Train Accuracy: 0.695\n",
            "[Epoch 258, Batch 200/372] Train Loss: 0.061, Train Accuracy: 1.000\n",
            "[Epoch 258, Batch 300/372] Train Loss: 0.077, Train Accuracy: 0.992\n",
            "Epoch 258 Validation Accuracy: 0.957\n",
            "[Epoch 259, Batch 100/372] Train Loss: 0.290, Train Accuracy: 0.852\n",
            "[Epoch 259, Batch 200/372] Train Loss: 0.261, Train Accuracy: 0.898\n",
            "[Epoch 259, Batch 300/372] Train Loss: 0.196, Train Accuracy: 0.109\n",
            "Epoch 259 Validation Accuracy: 0.950\n",
            "[Epoch 260, Batch 100/372] Train Loss: 0.287, Train Accuracy: 0.320\n",
            "[Epoch 260, Batch 200/372] Train Loss: 0.147, Train Accuracy: 0.977\n",
            "[Epoch 260, Batch 300/372] Train Loss: 0.167, Train Accuracy: 0.078\n",
            "Epoch 260 Validation Accuracy: 0.956\n",
            "[Epoch 261, Batch 100/372] Train Loss: 0.064, Train Accuracy: 0.102\n",
            "[Epoch 261, Batch 200/372] Train Loss: 0.323, Train Accuracy: 0.344\n",
            "[Epoch 261, Batch 300/372] Train Loss: 0.082, Train Accuracy: 0.984\n",
            "Epoch 261 Validation Accuracy: 0.950\n",
            "[Epoch 262, Batch 100/372] Train Loss: 0.078, Train Accuracy: 1.000\n",
            "[Epoch 262, Batch 200/372] Train Loss: 0.263, Train Accuracy: 0.844\n",
            "[Epoch 262, Batch 300/372] Train Loss: 0.169, Train Accuracy: 0.969\n",
            "Epoch 262 Validation Accuracy: 0.956\n",
            "[Epoch 263, Batch 100/372] Train Loss: 0.290, Train Accuracy: 0.641\n",
            "[Epoch 263, Batch 200/372] Train Loss: 0.347, Train Accuracy: 0.484\n",
            "[Epoch 263, Batch 300/372] Train Loss: 0.059, Train Accuracy: 0.094\n",
            "Epoch 263 Validation Accuracy: 0.956\n",
            "[Epoch 264, Batch 100/372] Train Loss: 0.300, Train Accuracy: 0.586\n",
            "[Epoch 264, Batch 200/372] Train Loss: 0.323, Train Accuracy: 0.477\n",
            "[Epoch 264, Batch 300/372] Train Loss: 0.128, Train Accuracy: 0.977\n",
            "Epoch 264 Validation Accuracy: 0.956\n",
            "[Epoch 265, Batch 100/372] Train Loss: 0.153, Train Accuracy: 0.961\n",
            "[Epoch 265, Batch 200/372] Train Loss: 0.269, Train Accuracy: 0.344\n",
            "[Epoch 265, Batch 300/372] Train Loss: 0.315, Train Accuracy: 0.781\n",
            "Epoch 265 Validation Accuracy: 0.952\n",
            "[Epoch 266, Batch 100/372] Train Loss: 0.192, Train Accuracy: 0.953\n",
            "[Epoch 266, Batch 200/372] Train Loss: 0.254, Train Accuracy: 0.227\n",
            "[Epoch 266, Batch 300/372] Train Loss: 0.185, Train Accuracy: 0.969\n",
            "Epoch 266 Validation Accuracy: 0.954\n",
            "[Epoch 267, Batch 100/372] Train Loss: 0.234, Train Accuracy: 0.219\n",
            "[Epoch 267, Batch 200/372] Train Loss: 0.260, Train Accuracy: 0.891\n",
            "[Epoch 267, Batch 300/372] Train Loss: 0.064, Train Accuracy: 0.133\n",
            "Epoch 267 Validation Accuracy: 0.954\n",
            "[Epoch 268, Batch 100/372] Train Loss: 0.180, Train Accuracy: 0.945\n",
            "[Epoch 268, Batch 200/372] Train Loss: 0.056, Train Accuracy: 0.102\n",
            "[Epoch 268, Batch 300/372] Train Loss: 0.141, Train Accuracy: 0.094\n",
            "Epoch 268 Validation Accuracy: 0.956\n",
            "[Epoch 269, Batch 100/372] Train Loss: 0.202, Train Accuracy: 0.922\n",
            "[Epoch 269, Batch 200/372] Train Loss: 0.285, Train Accuracy: 0.211\n",
            "[Epoch 269, Batch 300/372] Train Loss: 0.157, Train Accuracy: 0.977\n",
            "Epoch 269 Validation Accuracy: 0.951\n",
            "[Epoch 270, Batch 100/372] Train Loss: 0.061, Train Accuracy: 0.992\n",
            "[Epoch 270, Batch 200/372] Train Loss: 0.063, Train Accuracy: 0.148\n",
            "[Epoch 270, Batch 300/372] Train Loss: 0.055, Train Accuracy: 0.070\n",
            "Epoch 270 Validation Accuracy: 0.953\n",
            "[Epoch 271, Batch 100/372] Train Loss: 0.331, Train Accuracy: 0.492\n",
            "[Epoch 271, Batch 200/372] Train Loss: 0.086, Train Accuracy: 0.078\n",
            "[Epoch 271, Batch 300/372] Train Loss: 0.350, Train Accuracy: 0.328\n",
            "Epoch 271 Validation Accuracy: 0.950\n",
            "[Epoch 272, Batch 100/372] Train Loss: 0.299, Train Accuracy: 0.656\n",
            "[Epoch 272, Batch 200/372] Train Loss: 0.286, Train Accuracy: 0.789\n",
            "[Epoch 272, Batch 300/372] Train Loss: 0.053, Train Accuracy: 0.992\n",
            "Epoch 272 Validation Accuracy: 0.954\n",
            "[Epoch 273, Batch 100/372] Train Loss: 0.116, Train Accuracy: 0.992\n",
            "[Epoch 273, Batch 200/372] Train Loss: 0.090, Train Accuracy: 0.117\n",
            "[Epoch 273, Batch 300/372] Train Loss: 0.208, Train Accuracy: 0.094\n",
            "Epoch 273 Validation Accuracy: 0.955\n",
            "[Epoch 274, Batch 100/372] Train Loss: 0.277, Train Accuracy: 0.305\n",
            "[Epoch 274, Batch 200/372] Train Loss: 0.183, Train Accuracy: 0.961\n",
            "[Epoch 274, Batch 300/372] Train Loss: 0.114, Train Accuracy: 0.062\n",
            "Epoch 274 Validation Accuracy: 0.954\n",
            "[Epoch 275, Batch 100/372] Train Loss: 0.064, Train Accuracy: 0.133\n",
            "[Epoch 275, Batch 200/372] Train Loss: 0.152, Train Accuracy: 0.102\n",
            "[Epoch 275, Batch 300/372] Train Loss: 0.099, Train Accuracy: 0.984\n",
            "Epoch 275 Validation Accuracy: 0.952\n",
            "Checkpoint saved at epoch 275: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_275.tar\n",
            "[Epoch 276, Batch 100/372] Train Loss: 0.318, Train Accuracy: 0.742\n",
            "[Epoch 276, Batch 200/372] Train Loss: 0.103, Train Accuracy: 0.984\n",
            "[Epoch 276, Batch 300/372] Train Loss: 0.080, Train Accuracy: 0.984\n",
            "Epoch 276 Validation Accuracy: 0.948\n",
            "[Epoch 277, Batch 100/372] Train Loss: 0.074, Train Accuracy: 0.148\n",
            "[Epoch 277, Batch 200/372] Train Loss: 0.060, Train Accuracy: 1.000\n",
            "[Epoch 277, Batch 300/372] Train Loss: 0.269, Train Accuracy: 0.789\n",
            "Epoch 277 Validation Accuracy: 0.950\n",
            "[Epoch 278, Batch 100/372] Train Loss: 0.326, Train Accuracy: 0.453\n",
            "[Epoch 278, Batch 200/372] Train Loss: 0.122, Train Accuracy: 0.125\n",
            "[Epoch 278, Batch 300/372] Train Loss: 0.326, Train Accuracy: 0.734\n",
            "Epoch 278 Validation Accuracy: 0.948\n",
            "[Epoch 279, Batch 100/372] Train Loss: 0.327, Train Accuracy: 0.414\n",
            "[Epoch 279, Batch 200/372] Train Loss: 0.343, Train Accuracy: 0.492\n",
            "[Epoch 279, Batch 300/372] Train Loss: 0.218, Train Accuracy: 0.164\n",
            "Epoch 279 Validation Accuracy: 0.952\n",
            "[Epoch 280, Batch 100/372] Train Loss: 0.325, Train Accuracy: 0.484\n",
            "[Epoch 280, Batch 200/372] Train Loss: 0.052, Train Accuracy: 1.000\n",
            "[Epoch 280, Batch 300/372] Train Loss: 0.197, Train Accuracy: 0.914\n",
            "Epoch 280 Validation Accuracy: 0.950\n",
            "[Epoch 281, Batch 100/372] Train Loss: 0.205, Train Accuracy: 0.938\n",
            "[Epoch 281, Batch 200/372] Train Loss: 0.067, Train Accuracy: 0.094\n",
            "[Epoch 281, Batch 300/372] Train Loss: 0.330, Train Accuracy: 0.773\n",
            "Epoch 281 Validation Accuracy: 0.952\n",
            "[Epoch 282, Batch 100/372] Train Loss: 0.294, Train Accuracy: 0.250\n",
            "[Epoch 282, Batch 200/372] Train Loss: 0.083, Train Accuracy: 1.000\n",
            "[Epoch 282, Batch 300/372] Train Loss: 0.211, Train Accuracy: 0.938\n",
            "Epoch 282 Validation Accuracy: 0.956\n",
            "[Epoch 283, Batch 100/372] Train Loss: 0.114, Train Accuracy: 0.109\n",
            "[Epoch 283, Batch 200/372] Train Loss: 0.291, Train Accuracy: 0.727\n",
            "[Epoch 283, Batch 300/372] Train Loss: 0.074, Train Accuracy: 1.000\n",
            "Epoch 283 Validation Accuracy: 0.952\n",
            "[Epoch 284, Batch 100/372] Train Loss: 0.283, Train Accuracy: 0.164\n",
            "[Epoch 284, Batch 200/372] Train Loss: 0.060, Train Accuracy: 1.000\n",
            "[Epoch 284, Batch 300/372] Train Loss: 0.152, Train Accuracy: 0.148\n",
            "Epoch 284 Validation Accuracy: 0.954\n",
            "[Epoch 285, Batch 100/372] Train Loss: 0.251, Train Accuracy: 0.156\n",
            "[Epoch 285, Batch 200/372] Train Loss: 0.305, Train Accuracy: 0.297\n",
            "[Epoch 285, Batch 300/372] Train Loss: 0.060, Train Accuracy: 1.000\n",
            "Epoch 285 Validation Accuracy: 0.949\n",
            "[Epoch 286, Batch 100/372] Train Loss: 0.265, Train Accuracy: 0.281\n",
            "[Epoch 286, Batch 200/372] Train Loss: 0.156, Train Accuracy: 0.984\n",
            "[Epoch 286, Batch 300/372] Train Loss: 0.068, Train Accuracy: 0.102\n",
            "Epoch 286 Validation Accuracy: 0.950\n",
            "[Epoch 287, Batch 100/372] Train Loss: 0.140, Train Accuracy: 0.133\n",
            "[Epoch 287, Batch 200/372] Train Loss: 0.062, Train Accuracy: 0.117\n",
            "[Epoch 287, Batch 300/372] Train Loss: 0.300, Train Accuracy: 0.383\n",
            "Epoch 287 Validation Accuracy: 0.954\n",
            "[Epoch 288, Batch 100/372] Train Loss: 0.072, Train Accuracy: 0.086\n",
            "[Epoch 288, Batch 200/372] Train Loss: 0.257, Train Accuracy: 0.297\n",
            "[Epoch 288, Batch 300/372] Train Loss: 0.205, Train Accuracy: 0.945\n",
            "Epoch 288 Validation Accuracy: 0.957\n",
            "[Epoch 289, Batch 100/372] Train Loss: 0.172, Train Accuracy: 0.961\n",
            "[Epoch 289, Batch 200/372] Train Loss: 0.071, Train Accuracy: 1.000\n",
            "[Epoch 289, Batch 300/372] Train Loss: 0.316, Train Accuracy: 0.797\n",
            "Epoch 289 Validation Accuracy: 0.953\n",
            "[Epoch 290, Batch 100/372] Train Loss: 0.095, Train Accuracy: 0.141\n",
            "[Epoch 290, Batch 200/372] Train Loss: 0.201, Train Accuracy: 0.141\n",
            "[Epoch 290, Batch 300/372] Train Loss: 0.153, Train Accuracy: 0.984\n",
            "Epoch 290 Validation Accuracy: 0.951\n",
            "[Epoch 291, Batch 100/372] Train Loss: 0.108, Train Accuracy: 0.992\n",
            "[Epoch 291, Batch 200/372] Train Loss: 0.087, Train Accuracy: 0.094\n",
            "[Epoch 291, Batch 300/372] Train Loss: 0.073, Train Accuracy: 0.156\n",
            "Epoch 291 Validation Accuracy: 0.950\n",
            "[Epoch 292, Batch 100/372] Train Loss: 0.322, Train Accuracy: 0.750\n",
            "[Epoch 292, Batch 200/372] Train Loss: 0.185, Train Accuracy: 0.109\n",
            "[Epoch 292, Batch 300/372] Train Loss: 0.076, Train Accuracy: 1.000\n",
            "Epoch 292 Validation Accuracy: 0.952\n",
            "[Epoch 293, Batch 100/372] Train Loss: 0.051, Train Accuracy: 0.094\n",
            "[Epoch 293, Batch 200/372] Train Loss: 0.055, Train Accuracy: 1.000\n",
            "[Epoch 293, Batch 300/372] Train Loss: 0.051, Train Accuracy: 1.000\n",
            "Epoch 293 Validation Accuracy: 0.953\n",
            "[Epoch 294, Batch 100/372] Train Loss: 0.165, Train Accuracy: 0.945\n",
            "[Epoch 294, Batch 200/372] Train Loss: 0.080, Train Accuracy: 1.000\n",
            "[Epoch 294, Batch 300/372] Train Loss: 0.171, Train Accuracy: 0.977\n",
            "Epoch 294 Validation Accuracy: 0.956\n",
            "[Epoch 295, Batch 100/372] Train Loss: 0.339, Train Accuracy: 0.203\n",
            "[Epoch 295, Batch 200/372] Train Loss: 0.156, Train Accuracy: 0.969\n",
            "[Epoch 295, Batch 300/372] Train Loss: 0.081, Train Accuracy: 1.000\n",
            "Epoch 295 Validation Accuracy: 0.956\n",
            "[Epoch 296, Batch 100/372] Train Loss: 0.219, Train Accuracy: 0.180\n",
            "[Epoch 296, Batch 200/372] Train Loss: 0.100, Train Accuracy: 0.992\n",
            "[Epoch 296, Batch 300/372] Train Loss: 0.272, Train Accuracy: 0.211\n",
            "Epoch 296 Validation Accuracy: 0.952\n",
            "[Epoch 297, Batch 100/372] Train Loss: 0.071, Train Accuracy: 1.000\n",
            "[Epoch 297, Batch 200/372] Train Loss: 0.091, Train Accuracy: 0.992\n",
            "[Epoch 297, Batch 300/372] Train Loss: 0.060, Train Accuracy: 0.117\n",
            "Epoch 297 Validation Accuracy: 0.958\n",
            "[Epoch 298, Batch 100/372] Train Loss: 0.081, Train Accuracy: 1.000\n",
            "[Epoch 298, Batch 200/372] Train Loss: 0.081, Train Accuracy: 1.000\n",
            "[Epoch 298, Batch 300/372] Train Loss: 0.067, Train Accuracy: 1.000\n",
            "Epoch 298 Validation Accuracy: 0.954\n",
            "[Epoch 299, Batch 100/372] Train Loss: 0.089, Train Accuracy: 0.992\n",
            "[Epoch 299, Batch 200/372] Train Loss: 0.341, Train Accuracy: 0.539\n",
            "[Epoch 299, Batch 300/372] Train Loss: 0.086, Train Accuracy: 1.000\n",
            "Epoch 299 Validation Accuracy: 0.953\n",
            "[Epoch 300, Batch 100/372] Train Loss: 0.281, Train Accuracy: 0.789\n",
            "[Epoch 300, Batch 200/372] Train Loss: 0.055, Train Accuracy: 1.000\n",
            "[Epoch 300, Batch 300/372] Train Loss: 0.182, Train Accuracy: 0.117\n",
            "Epoch 300 Validation Accuracy: 0.952\n",
            "Checkpoint saved at epoch 300: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_300.tar\n",
            "[Epoch 301, Batch 100/372] Train Loss: 0.078, Train Accuracy: 0.992\n",
            "[Epoch 301, Batch 200/372] Train Loss: 0.049, Train Accuracy: 0.133\n",
            "[Epoch 301, Batch 300/372] Train Loss: 0.226, Train Accuracy: 0.188\n",
            "Epoch 301 Validation Accuracy: 0.949\n",
            "[Epoch 302, Batch 100/372] Train Loss: 0.056, Train Accuracy: 1.000\n",
            "[Epoch 302, Batch 200/372] Train Loss: 0.342, Train Accuracy: 0.391\n",
            "[Epoch 302, Batch 300/372] Train Loss: 0.291, Train Accuracy: 0.469\n",
            "Epoch 302 Validation Accuracy: 0.953\n",
            "[Epoch 303, Batch 100/372] Train Loss: 0.208, Train Accuracy: 0.922\n",
            "[Epoch 303, Batch 200/372] Train Loss: 0.116, Train Accuracy: 0.992\n",
            "[Epoch 303, Batch 300/372] Train Loss: 0.187, Train Accuracy: 0.922\n",
            "Epoch 303 Validation Accuracy: 0.953\n",
            "[Epoch 304, Batch 100/372] Train Loss: 0.135, Train Accuracy: 0.133\n",
            "[Epoch 304, Batch 200/372] Train Loss: 0.050, Train Accuracy: 1.000\n",
            "[Epoch 304, Batch 300/372] Train Loss: 0.113, Train Accuracy: 0.133\n",
            "Epoch 304 Validation Accuracy: 0.951\n",
            "[Epoch 305, Batch 100/372] Train Loss: 0.069, Train Accuracy: 0.992\n",
            "[Epoch 305, Batch 200/372] Train Loss: 0.058, Train Accuracy: 0.070\n",
            "[Epoch 305, Batch 300/372] Train Loss: 0.309, Train Accuracy: 0.648\n",
            "Epoch 305 Validation Accuracy: 0.949\n",
            "[Epoch 306, Batch 100/372] Train Loss: 0.098, Train Accuracy: 0.078\n",
            "[Epoch 306, Batch 200/372] Train Loss: 0.050, Train Accuracy: 1.000\n",
            "[Epoch 306, Batch 300/372] Train Loss: 0.050, Train Accuracy: 0.070\n",
            "Epoch 306 Validation Accuracy: 0.956\n",
            "[Epoch 307, Batch 100/372] Train Loss: 0.341, Train Accuracy: 0.523\n",
            "[Epoch 307, Batch 200/372] Train Loss: 0.079, Train Accuracy: 0.984\n",
            "[Epoch 307, Batch 300/372] Train Loss: 0.117, Train Accuracy: 1.000\n",
            "Epoch 307 Validation Accuracy: 0.958\n",
            "[Epoch 308, Batch 100/372] Train Loss: 0.317, Train Accuracy: 0.711\n",
            "[Epoch 308, Batch 200/372] Train Loss: 0.332, Train Accuracy: 0.570\n",
            "[Epoch 308, Batch 300/372] Train Loss: 0.326, Train Accuracy: 0.484\n",
            "Epoch 308 Validation Accuracy: 0.950\n",
            "[Epoch 309, Batch 100/372] Train Loss: 0.064, Train Accuracy: 0.141\n",
            "[Epoch 309, Batch 200/372] Train Loss: 0.257, Train Accuracy: 0.781\n",
            "[Epoch 309, Batch 300/372] Train Loss: 0.301, Train Accuracy: 0.656\n",
            "Epoch 309 Validation Accuracy: 0.951\n",
            "[Epoch 310, Batch 100/372] Train Loss: 0.220, Train Accuracy: 0.234\n",
            "[Epoch 310, Batch 200/372] Train Loss: 0.100, Train Accuracy: 1.000\n",
            "[Epoch 310, Batch 300/372] Train Loss: 0.047, Train Accuracy: 1.000\n",
            "Epoch 310 Validation Accuracy: 0.958\n",
            "[Epoch 311, Batch 100/372] Train Loss: 0.249, Train Accuracy: 0.906\n",
            "[Epoch 311, Batch 200/372] Train Loss: 0.121, Train Accuracy: 0.969\n",
            "[Epoch 311, Batch 300/372] Train Loss: 0.229, Train Accuracy: 0.867\n",
            "Epoch 311 Validation Accuracy: 0.949\n",
            "[Epoch 312, Batch 100/372] Train Loss: 0.278, Train Accuracy: 0.242\n",
            "[Epoch 312, Batch 200/372] Train Loss: 0.292, Train Accuracy: 0.688\n",
            "[Epoch 312, Batch 300/372] Train Loss: 0.074, Train Accuracy: 1.000\n",
            "Epoch 312 Validation Accuracy: 0.952\n",
            "[Epoch 313, Batch 100/372] Train Loss: 0.251, Train Accuracy: 0.727\n",
            "[Epoch 313, Batch 200/372] Train Loss: 0.332, Train Accuracy: 0.398\n",
            "[Epoch 313, Batch 300/372] Train Loss: 0.110, Train Accuracy: 0.141\n",
            "Epoch 313 Validation Accuracy: 0.951\n",
            "[Epoch 314, Batch 100/372] Train Loss: 0.280, Train Accuracy: 0.234\n",
            "[Epoch 314, Batch 200/372] Train Loss: 0.202, Train Accuracy: 0.133\n",
            "[Epoch 314, Batch 300/372] Train Loss: 0.288, Train Accuracy: 0.242\n",
            "Epoch 314 Validation Accuracy: 0.957\n",
            "[Epoch 315, Batch 100/372] Train Loss: 0.324, Train Accuracy: 0.305\n",
            "[Epoch 315, Batch 200/372] Train Loss: 0.331, Train Accuracy: 0.578\n",
            "[Epoch 315, Batch 300/372] Train Loss: 0.155, Train Accuracy: 0.969\n",
            "Epoch 315 Validation Accuracy: 0.953\n",
            "[Epoch 316, Batch 100/372] Train Loss: 0.306, Train Accuracy: 0.797\n",
            "[Epoch 316, Batch 200/372] Train Loss: 0.280, Train Accuracy: 0.680\n",
            "[Epoch 316, Batch 300/372] Train Loss: 0.170, Train Accuracy: 0.117\n",
            "Epoch 316 Validation Accuracy: 0.954\n",
            "[Epoch 317, Batch 100/372] Train Loss: 0.303, Train Accuracy: 0.555\n",
            "[Epoch 317, Batch 200/372] Train Loss: 0.066, Train Accuracy: 0.109\n",
            "[Epoch 317, Batch 300/372] Train Loss: 0.247, Train Accuracy: 0.875\n",
            "Epoch 317 Validation Accuracy: 0.954\n",
            "[Epoch 318, Batch 100/372] Train Loss: 0.287, Train Accuracy: 0.750\n",
            "[Epoch 318, Batch 200/372] Train Loss: 0.122, Train Accuracy: 0.969\n",
            "[Epoch 318, Batch 300/372] Train Loss: 0.222, Train Accuracy: 0.164\n",
            "Epoch 318 Validation Accuracy: 0.960\n",
            "[Epoch 319, Batch 100/372] Train Loss: 0.084, Train Accuracy: 1.000\n",
            "[Epoch 319, Batch 200/372] Train Loss: 0.159, Train Accuracy: 0.062\n",
            "[Epoch 319, Batch 300/372] Train Loss: 0.305, Train Accuracy: 0.422\n",
            "Epoch 319 Validation Accuracy: 0.959\n",
            "[Epoch 320, Batch 100/372] Train Loss: 0.065, Train Accuracy: 1.000\n",
            "[Epoch 320, Batch 200/372] Train Loss: 0.125, Train Accuracy: 0.992\n",
            "[Epoch 320, Batch 300/372] Train Loss: 0.082, Train Accuracy: 0.117\n",
            "Epoch 320 Validation Accuracy: 0.954\n",
            "[Epoch 321, Batch 100/372] Train Loss: 0.103, Train Accuracy: 0.109\n",
            "[Epoch 321, Batch 200/372] Train Loss: 0.359, Train Accuracy: 0.453\n",
            "[Epoch 321, Batch 300/372] Train Loss: 0.246, Train Accuracy: 0.789\n",
            "Epoch 321 Validation Accuracy: 0.951\n",
            "[Epoch 322, Batch 100/372] Train Loss: 0.301, Train Accuracy: 0.438\n",
            "[Epoch 322, Batch 200/372] Train Loss: 0.298, Train Accuracy: 0.453\n",
            "[Epoch 322, Batch 300/372] Train Loss: 0.292, Train Accuracy: 0.305\n",
            "Epoch 322 Validation Accuracy: 0.951\n",
            "[Epoch 323, Batch 100/372] Train Loss: 0.073, Train Accuracy: 0.156\n",
            "[Epoch 323, Batch 200/372] Train Loss: 0.088, Train Accuracy: 0.109\n",
            "[Epoch 323, Batch 300/372] Train Loss: 0.236, Train Accuracy: 0.945\n",
            "Epoch 323 Validation Accuracy: 0.952\n",
            "[Epoch 324, Batch 100/372] Train Loss: 0.215, Train Accuracy: 0.180\n",
            "[Epoch 324, Batch 200/372] Train Loss: 0.059, Train Accuracy: 1.000\n",
            "[Epoch 324, Batch 300/372] Train Loss: 0.167, Train Accuracy: 0.984\n",
            "Epoch 324 Validation Accuracy: 0.954\n",
            "[Epoch 325, Batch 100/372] Train Loss: 0.188, Train Accuracy: 0.953\n",
            "[Epoch 325, Batch 200/372] Train Loss: 0.191, Train Accuracy: 0.164\n",
            "[Epoch 325, Batch 300/372] Train Loss: 0.053, Train Accuracy: 0.094\n",
            "Epoch 325 Validation Accuracy: 0.952\n",
            "Checkpoint saved at epoch 325: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_325.tar\n",
            "[Epoch 326, Batch 100/372] Train Loss: 0.130, Train Accuracy: 0.125\n",
            "[Epoch 326, Batch 200/372] Train Loss: 0.269, Train Accuracy: 0.836\n",
            "[Epoch 326, Batch 300/372] Train Loss: 0.205, Train Accuracy: 0.938\n",
            "Epoch 326 Validation Accuracy: 0.954\n",
            "[Epoch 327, Batch 100/372] Train Loss: 0.279, Train Accuracy: 0.812\n",
            "[Epoch 327, Batch 200/372] Train Loss: 0.064, Train Accuracy: 1.000\n",
            "[Epoch 327, Batch 300/372] Train Loss: 0.259, Train Accuracy: 0.375\n",
            "Epoch 327 Validation Accuracy: 0.954\n",
            "[Epoch 328, Batch 100/372] Train Loss: 0.302, Train Accuracy: 0.430\n",
            "[Epoch 328, Batch 200/372] Train Loss: 0.141, Train Accuracy: 0.086\n",
            "[Epoch 328, Batch 300/372] Train Loss: 0.290, Train Accuracy: 0.430\n",
            "Epoch 328 Validation Accuracy: 0.950\n",
            "[Epoch 329, Batch 100/372] Train Loss: 0.114, Train Accuracy: 1.000\n",
            "[Epoch 329, Batch 200/372] Train Loss: 0.338, Train Accuracy: 0.656\n",
            "[Epoch 329, Batch 300/372] Train Loss: 0.127, Train Accuracy: 0.148\n",
            "Epoch 329 Validation Accuracy: 0.956\n",
            "[Epoch 330, Batch 100/372] Train Loss: 0.179, Train Accuracy: 0.195\n",
            "[Epoch 330, Batch 200/372] Train Loss: 0.087, Train Accuracy: 0.133\n",
            "[Epoch 330, Batch 300/372] Train Loss: 0.225, Train Accuracy: 0.219\n",
            "Epoch 330 Validation Accuracy: 0.954\n",
            "[Epoch 331, Batch 100/372] Train Loss: 0.240, Train Accuracy: 0.195\n",
            "[Epoch 331, Batch 200/372] Train Loss: 0.092, Train Accuracy: 0.109\n",
            "[Epoch 331, Batch 300/372] Train Loss: 0.108, Train Accuracy: 0.984\n",
            "Epoch 331 Validation Accuracy: 0.948\n",
            "[Epoch 332, Batch 100/372] Train Loss: 0.040, Train Accuracy: 0.086\n",
            "[Epoch 332, Batch 200/372] Train Loss: 0.070, Train Accuracy: 0.133\n",
            "[Epoch 332, Batch 300/372] Train Loss: 0.060, Train Accuracy: 1.000\n",
            "Epoch 332 Validation Accuracy: 0.950\n",
            "[Epoch 333, Batch 100/372] Train Loss: 0.079, Train Accuracy: 0.109\n",
            "[Epoch 333, Batch 200/372] Train Loss: 0.065, Train Accuracy: 1.000\n",
            "[Epoch 333, Batch 300/372] Train Loss: 0.110, Train Accuracy: 0.984\n",
            "Epoch 333 Validation Accuracy: 0.959\n",
            "[Epoch 334, Batch 100/372] Train Loss: 0.070, Train Accuracy: 1.000\n",
            "[Epoch 334, Batch 200/372] Train Loss: 0.114, Train Accuracy: 0.094\n",
            "[Epoch 334, Batch 300/372] Train Loss: 0.298, Train Accuracy: 0.688\n",
            "Epoch 334 Validation Accuracy: 0.956\n",
            "[Epoch 335, Batch 100/372] Train Loss: 0.115, Train Accuracy: 0.992\n",
            "[Epoch 335, Batch 200/372] Train Loss: 0.079, Train Accuracy: 0.117\n",
            "[Epoch 335, Batch 300/372] Train Loss: 0.252, Train Accuracy: 0.195\n",
            "Epoch 335 Validation Accuracy: 0.952\n",
            "[Epoch 336, Batch 100/372] Train Loss: 0.085, Train Accuracy: 1.000\n",
            "[Epoch 336, Batch 200/372] Train Loss: 0.069, Train Accuracy: 1.000\n",
            "[Epoch 336, Batch 300/372] Train Loss: 0.295, Train Accuracy: 0.273\n",
            "Epoch 336 Validation Accuracy: 0.956\n",
            "[Epoch 337, Batch 100/372] Train Loss: 0.065, Train Accuracy: 1.000\n",
            "[Epoch 337, Batch 200/372] Train Loss: 0.066, Train Accuracy: 1.000\n",
            "[Epoch 337, Batch 300/372] Train Loss: 0.053, Train Accuracy: 0.094\n",
            "Epoch 337 Validation Accuracy: 0.956\n",
            "[Epoch 338, Batch 100/372] Train Loss: 0.195, Train Accuracy: 0.930\n",
            "[Epoch 338, Batch 200/372] Train Loss: 0.044, Train Accuracy: 1.000\n",
            "[Epoch 338, Batch 300/372] Train Loss: 0.255, Train Accuracy: 0.188\n",
            "Epoch 338 Validation Accuracy: 0.952\n",
            "[Epoch 339, Batch 100/372] Train Loss: 0.276, Train Accuracy: 0.352\n",
            "[Epoch 339, Batch 200/372] Train Loss: 0.325, Train Accuracy: 0.594\n",
            "[Epoch 339, Batch 300/372] Train Loss: 0.090, Train Accuracy: 0.992\n",
            "Epoch 339 Validation Accuracy: 0.953\n",
            "[Epoch 340, Batch 100/372] Train Loss: 0.153, Train Accuracy: 0.961\n",
            "[Epoch 340, Batch 200/372] Train Loss: 0.084, Train Accuracy: 0.992\n",
            "[Epoch 340, Batch 300/372] Train Loss: 0.263, Train Accuracy: 0.391\n",
            "Epoch 340 Validation Accuracy: 0.958\n",
            "[Epoch 341, Batch 100/372] Train Loss: 0.270, Train Accuracy: 0.773\n",
            "[Epoch 341, Batch 200/372] Train Loss: 0.055, Train Accuracy: 1.000\n",
            "[Epoch 341, Batch 300/372] Train Loss: 0.246, Train Accuracy: 0.344\n",
            "Epoch 341 Validation Accuracy: 0.951\n",
            "[Epoch 342, Batch 100/372] Train Loss: 0.275, Train Accuracy: 0.742\n",
            "[Epoch 342, Batch 200/372] Train Loss: 0.076, Train Accuracy: 1.000\n",
            "[Epoch 342, Batch 300/372] Train Loss: 0.255, Train Accuracy: 0.180\n",
            "Epoch 342 Validation Accuracy: 0.956\n",
            "[Epoch 343, Batch 100/372] Train Loss: 0.265, Train Accuracy: 0.742\n",
            "[Epoch 343, Batch 200/372] Train Loss: 0.313, Train Accuracy: 0.648\n",
            "[Epoch 343, Batch 300/372] Train Loss: 0.060, Train Accuracy: 0.133\n",
            "Epoch 343 Validation Accuracy: 0.955\n",
            "[Epoch 344, Batch 100/372] Train Loss: 0.285, Train Accuracy: 0.352\n",
            "[Epoch 344, Batch 200/372] Train Loss: 0.053, Train Accuracy: 1.000\n",
            "[Epoch 344, Batch 300/372] Train Loss: 0.305, Train Accuracy: 0.742\n",
            "Epoch 344 Validation Accuracy: 0.954\n",
            "[Epoch 345, Batch 100/372] Train Loss: 0.309, Train Accuracy: 0.328\n",
            "[Epoch 345, Batch 200/372] Train Loss: 0.070, Train Accuracy: 0.992\n",
            "[Epoch 345, Batch 300/372] Train Loss: 0.226, Train Accuracy: 0.242\n",
            "Epoch 345 Validation Accuracy: 0.951\n",
            "[Epoch 346, Batch 100/372] Train Loss: 0.281, Train Accuracy: 0.445\n",
            "[Epoch 346, Batch 200/372] Train Loss: 0.038, Train Accuracy: 1.000\n",
            "[Epoch 346, Batch 300/372] Train Loss: 0.300, Train Accuracy: 0.305\n",
            "Epoch 346 Validation Accuracy: 0.951\n",
            "[Epoch 347, Batch 100/372] Train Loss: 0.061, Train Accuracy: 0.086\n",
            "[Epoch 347, Batch 200/372] Train Loss: 0.071, Train Accuracy: 0.102\n",
            "[Epoch 347, Batch 300/372] Train Loss: 0.058, Train Accuracy: 0.141\n",
            "Epoch 347 Validation Accuracy: 0.954\n",
            "[Epoch 348, Batch 100/372] Train Loss: 0.043, Train Accuracy: 0.148\n",
            "[Epoch 348, Batch 200/372] Train Loss: 0.067, Train Accuracy: 1.000\n",
            "[Epoch 348, Batch 300/372] Train Loss: 0.101, Train Accuracy: 0.094\n",
            "Epoch 348 Validation Accuracy: 0.953\n",
            "[Epoch 349, Batch 100/372] Train Loss: 0.290, Train Accuracy: 0.383\n",
            "[Epoch 349, Batch 200/372] Train Loss: 0.064, Train Accuracy: 0.062\n",
            "[Epoch 349, Batch 300/372] Train Loss: 0.095, Train Accuracy: 0.992\n",
            "Epoch 349 Validation Accuracy: 0.957\n",
            "[Epoch 350, Batch 100/372] Train Loss: 0.333, Train Accuracy: 0.609\n",
            "[Epoch 350, Batch 200/372] Train Loss: 0.086, Train Accuracy: 0.125\n",
            "[Epoch 350, Batch 300/372] Train Loss: 0.074, Train Accuracy: 1.000\n",
            "Epoch 350 Validation Accuracy: 0.955\n",
            "Checkpoint saved at epoch 350: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_350.tar\n",
            "[Epoch 351, Batch 100/372] Train Loss: 0.073, Train Accuracy: 0.102\n",
            "[Epoch 351, Batch 200/372] Train Loss: 0.082, Train Accuracy: 0.992\n",
            "[Epoch 351, Batch 300/372] Train Loss: 0.040, Train Accuracy: 0.109\n",
            "Epoch 351 Validation Accuracy: 0.957\n",
            "[Epoch 352, Batch 100/372] Train Loss: 0.103, Train Accuracy: 0.977\n",
            "[Epoch 352, Batch 200/372] Train Loss: 0.233, Train Accuracy: 0.875\n",
            "[Epoch 352, Batch 300/372] Train Loss: 0.095, Train Accuracy: 1.000\n",
            "Epoch 352 Validation Accuracy: 0.950\n",
            "[Epoch 353, Batch 100/372] Train Loss: 0.273, Train Accuracy: 0.711\n",
            "[Epoch 353, Batch 200/372] Train Loss: 0.257, Train Accuracy: 0.492\n",
            "[Epoch 353, Batch 300/372] Train Loss: 0.321, Train Accuracy: 0.406\n",
            "Epoch 353 Validation Accuracy: 0.954\n",
            "[Epoch 354, Batch 100/372] Train Loss: 0.167, Train Accuracy: 0.188\n",
            "[Epoch 354, Batch 200/372] Train Loss: 0.251, Train Accuracy: 0.789\n",
            "[Epoch 354, Batch 300/372] Train Loss: 0.259, Train Accuracy: 0.750\n",
            "Epoch 354 Validation Accuracy: 0.955\n",
            "[Epoch 355, Batch 100/372] Train Loss: 0.098, Train Accuracy: 0.094\n",
            "[Epoch 355, Batch 200/372] Train Loss: 0.042, Train Accuracy: 0.133\n",
            "[Epoch 355, Batch 300/372] Train Loss: 0.163, Train Accuracy: 0.984\n",
            "Epoch 355 Validation Accuracy: 0.958\n",
            "[Epoch 356, Batch 100/372] Train Loss: 0.062, Train Accuracy: 0.070\n",
            "[Epoch 356, Batch 200/372] Train Loss: 0.068, Train Accuracy: 0.102\n",
            "[Epoch 356, Batch 300/372] Train Loss: 0.072, Train Accuracy: 0.117\n",
            "Epoch 356 Validation Accuracy: 0.956\n",
            "[Epoch 357, Batch 100/372] Train Loss: 0.041, Train Accuracy: 0.078\n",
            "[Epoch 357, Batch 200/372] Train Loss: 0.053, Train Accuracy: 1.000\n",
            "[Epoch 357, Batch 300/372] Train Loss: 0.072, Train Accuracy: 0.117\n",
            "Epoch 357 Validation Accuracy: 0.949\n",
            "[Epoch 358, Batch 100/372] Train Loss: 0.058, Train Accuracy: 0.156\n",
            "[Epoch 358, Batch 200/372] Train Loss: 0.224, Train Accuracy: 0.227\n",
            "[Epoch 358, Batch 300/372] Train Loss: 0.283, Train Accuracy: 0.328\n",
            "Epoch 358 Validation Accuracy: 0.954\n",
            "[Epoch 359, Batch 100/372] Train Loss: 0.309, Train Accuracy: 0.555\n",
            "[Epoch 359, Batch 200/372] Train Loss: 0.058, Train Accuracy: 0.070\n",
            "[Epoch 359, Batch 300/372] Train Loss: 0.097, Train Accuracy: 0.102\n",
            "Epoch 359 Validation Accuracy: 0.950\n",
            "[Epoch 360, Batch 100/372] Train Loss: 0.266, Train Accuracy: 0.273\n",
            "[Epoch 360, Batch 200/372] Train Loss: 0.219, Train Accuracy: 0.180\n",
            "[Epoch 360, Batch 300/372] Train Loss: 0.100, Train Accuracy: 0.109\n",
            "Epoch 360 Validation Accuracy: 0.954\n",
            "[Epoch 361, Batch 100/372] Train Loss: 0.282, Train Accuracy: 0.234\n",
            "[Epoch 361, Batch 200/372] Train Loss: 0.055, Train Accuracy: 1.000\n",
            "[Epoch 361, Batch 300/372] Train Loss: 0.094, Train Accuracy: 0.992\n",
            "Epoch 361 Validation Accuracy: 0.953\n",
            "[Epoch 362, Batch 100/372] Train Loss: 0.244, Train Accuracy: 0.242\n",
            "[Epoch 362, Batch 200/372] Train Loss: 0.144, Train Accuracy: 0.984\n",
            "[Epoch 362, Batch 300/372] Train Loss: 0.254, Train Accuracy: 0.859\n",
            "Epoch 362 Validation Accuracy: 0.952\n",
            "[Epoch 363, Batch 100/372] Train Loss: 0.317, Train Accuracy: 0.484\n",
            "[Epoch 363, Batch 200/372] Train Loss: 0.298, Train Accuracy: 0.453\n",
            "[Epoch 363, Batch 300/372] Train Loss: 0.185, Train Accuracy: 0.117\n",
            "Epoch 363 Validation Accuracy: 0.959\n",
            "[Epoch 364, Batch 100/372] Train Loss: 0.047, Train Accuracy: 1.000\n",
            "[Epoch 364, Batch 200/372] Train Loss: 0.064, Train Accuracy: 1.000\n",
            "[Epoch 364, Batch 300/372] Train Loss: 0.156, Train Accuracy: 0.977\n",
            "Epoch 364 Validation Accuracy: 0.952\n",
            "[Epoch 365, Batch 100/372] Train Loss: 0.191, Train Accuracy: 0.945\n",
            "[Epoch 365, Batch 200/372] Train Loss: 0.067, Train Accuracy: 0.992\n",
            "[Epoch 365, Batch 300/372] Train Loss: 0.075, Train Accuracy: 1.000\n",
            "Epoch 365 Validation Accuracy: 0.954\n",
            "[Epoch 366, Batch 100/372] Train Loss: 0.217, Train Accuracy: 0.156\n",
            "[Epoch 366, Batch 200/372] Train Loss: 0.163, Train Accuracy: 0.977\n",
            "[Epoch 366, Batch 300/372] Train Loss: 0.059, Train Accuracy: 0.070\n",
            "Epoch 366 Validation Accuracy: 0.958\n",
            "[Epoch 367, Batch 100/372] Train Loss: 0.213, Train Accuracy: 0.930\n",
            "[Epoch 367, Batch 200/372] Train Loss: 0.187, Train Accuracy: 0.961\n",
            "[Epoch 367, Batch 300/372] Train Loss: 0.151, Train Accuracy: 0.078\n",
            "Epoch 367 Validation Accuracy: 0.958\n",
            "[Epoch 368, Batch 100/372] Train Loss: 0.279, Train Accuracy: 0.859\n",
            "[Epoch 368, Batch 200/372] Train Loss: 0.046, Train Accuracy: 1.000\n",
            "[Epoch 368, Batch 300/372] Train Loss: 0.307, Train Accuracy: 0.594\n",
            "Epoch 368 Validation Accuracy: 0.954\n",
            "[Epoch 369, Batch 100/372] Train Loss: 0.102, Train Accuracy: 0.992\n",
            "[Epoch 369, Batch 200/372] Train Loss: 0.070, Train Accuracy: 0.117\n",
            "[Epoch 369, Batch 300/372] Train Loss: 0.118, Train Accuracy: 0.977\n",
            "Epoch 369 Validation Accuracy: 0.953\n",
            "[Epoch 370, Batch 100/372] Train Loss: 0.227, Train Accuracy: 0.891\n",
            "[Epoch 370, Batch 200/372] Train Loss: 0.339, Train Accuracy: 0.422\n",
            "[Epoch 370, Batch 300/372] Train Loss: 0.059, Train Accuracy: 1.000\n",
            "Epoch 370 Validation Accuracy: 0.953\n",
            "[Epoch 371, Batch 100/372] Train Loss: 0.192, Train Accuracy: 0.078\n",
            "[Epoch 371, Batch 200/372] Train Loss: 0.269, Train Accuracy: 0.844\n",
            "[Epoch 371, Batch 300/372] Train Loss: 0.222, Train Accuracy: 0.133\n",
            "Epoch 371 Validation Accuracy: 0.954\n",
            "[Epoch 372, Batch 100/372] Train Loss: 0.157, Train Accuracy: 0.969\n",
            "[Epoch 372, Batch 200/372] Train Loss: 0.182, Train Accuracy: 0.141\n",
            "[Epoch 372, Batch 300/372] Train Loss: 0.232, Train Accuracy: 0.891\n",
            "Epoch 372 Validation Accuracy: 0.957\n",
            "[Epoch 373, Batch 100/372] Train Loss: 0.058, Train Accuracy: 0.117\n",
            "[Epoch 373, Batch 200/372] Train Loss: 0.083, Train Accuracy: 0.992\n",
            "[Epoch 373, Batch 300/372] Train Loss: 0.257, Train Accuracy: 0.844\n",
            "Epoch 373 Validation Accuracy: 0.958\n",
            "[Epoch 374, Batch 100/372] Train Loss: 0.275, Train Accuracy: 0.227\n",
            "[Epoch 374, Batch 200/372] Train Loss: 0.075, Train Accuracy: 0.109\n",
            "[Epoch 374, Batch 300/372] Train Loss: 0.260, Train Accuracy: 0.781\n",
            "Epoch 374 Validation Accuracy: 0.958\n",
            "[Epoch 375, Batch 100/372] Train Loss: 0.254, Train Accuracy: 0.859\n",
            "[Epoch 375, Batch 200/372] Train Loss: 0.233, Train Accuracy: 0.922\n",
            "[Epoch 375, Batch 300/372] Train Loss: 0.302, Train Accuracy: 0.641\n",
            "Epoch 375 Validation Accuracy: 0.961\n",
            "Checkpoint saved at epoch 375: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_375.tar\n",
            "[Epoch 376, Batch 100/372] Train Loss: 0.187, Train Accuracy: 0.125\n",
            "[Epoch 376, Batch 200/372] Train Loss: 0.179, Train Accuracy: 0.172\n",
            "[Epoch 376, Batch 300/372] Train Loss: 0.071, Train Accuracy: 0.094\n",
            "Epoch 376 Validation Accuracy: 0.955\n",
            "[Epoch 377, Batch 100/372] Train Loss: 0.133, Train Accuracy: 0.102\n",
            "[Epoch 377, Batch 200/372] Train Loss: 0.199, Train Accuracy: 0.133\n",
            "[Epoch 377, Batch 300/372] Train Loss: 0.221, Train Accuracy: 0.914\n",
            "Epoch 377 Validation Accuracy: 0.958\n",
            "[Epoch 378, Batch 100/372] Train Loss: 0.173, Train Accuracy: 0.953\n",
            "[Epoch 378, Batch 200/372] Train Loss: 0.224, Train Accuracy: 0.148\n",
            "[Epoch 378, Batch 300/372] Train Loss: 0.070, Train Accuracy: 0.992\n",
            "Epoch 378 Validation Accuracy: 0.955\n",
            "[Epoch 379, Batch 100/372] Train Loss: 0.049, Train Accuracy: 0.133\n",
            "[Epoch 379, Batch 200/372] Train Loss: 0.068, Train Accuracy: 0.102\n",
            "[Epoch 379, Batch 300/372] Train Loss: 0.172, Train Accuracy: 0.141\n",
            "Epoch 379 Validation Accuracy: 0.956\n",
            "[Epoch 380, Batch 100/372] Train Loss: 0.049, Train Accuracy: 0.094\n",
            "[Epoch 380, Batch 200/372] Train Loss: 0.078, Train Accuracy: 0.117\n",
            "[Epoch 380, Batch 300/372] Train Loss: 0.078, Train Accuracy: 1.000\n",
            "Epoch 380 Validation Accuracy: 0.957\n",
            "[Epoch 381, Batch 100/372] Train Loss: 0.069, Train Accuracy: 1.000\n",
            "[Epoch 381, Batch 200/372] Train Loss: 0.038, Train Accuracy: 1.000\n",
            "[Epoch 381, Batch 300/372] Train Loss: 0.156, Train Accuracy: 0.945\n",
            "Epoch 381 Validation Accuracy: 0.952\n",
            "[Epoch 382, Batch 100/372] Train Loss: 0.041, Train Accuracy: 1.000\n",
            "[Epoch 382, Batch 200/372] Train Loss: 0.174, Train Accuracy: 0.078\n",
            "[Epoch 382, Batch 300/372] Train Loss: 0.076, Train Accuracy: 0.117\n",
            "Epoch 382 Validation Accuracy: 0.958\n",
            "[Epoch 383, Batch 100/372] Train Loss: 0.040, Train Accuracy: 0.203\n",
            "[Epoch 383, Batch 200/372] Train Loss: 0.074, Train Accuracy: 0.125\n",
            "[Epoch 383, Batch 300/372] Train Loss: 0.306, Train Accuracy: 0.742\n",
            "Epoch 383 Validation Accuracy: 0.954\n",
            "[Epoch 384, Batch 100/372] Train Loss: 0.125, Train Accuracy: 0.977\n",
            "[Epoch 384, Batch 200/372] Train Loss: 0.056, Train Accuracy: 0.086\n",
            "[Epoch 384, Batch 300/372] Train Loss: 0.168, Train Accuracy: 0.133\n",
            "Epoch 384 Validation Accuracy: 0.956\n",
            "[Epoch 385, Batch 100/372] Train Loss: 0.051, Train Accuracy: 0.070\n",
            "[Epoch 385, Batch 200/372] Train Loss: 0.279, Train Accuracy: 0.844\n",
            "[Epoch 385, Batch 300/372] Train Loss: 0.315, Train Accuracy: 0.742\n",
            "Epoch 385 Validation Accuracy: 0.955\n",
            "[Epoch 386, Batch 100/372] Train Loss: 0.247, Train Accuracy: 0.320\n",
            "[Epoch 386, Batch 200/372] Train Loss: 0.060, Train Accuracy: 1.000\n",
            "[Epoch 386, Batch 300/372] Train Loss: 0.176, Train Accuracy: 0.961\n",
            "Epoch 386 Validation Accuracy: 0.955\n",
            "[Epoch 387, Batch 100/372] Train Loss: 0.291, Train Accuracy: 0.617\n",
            "[Epoch 387, Batch 200/372] Train Loss: 0.062, Train Accuracy: 0.109\n",
            "[Epoch 387, Batch 300/372] Train Loss: 0.091, Train Accuracy: 0.094\n",
            "Epoch 387 Validation Accuracy: 0.953\n",
            "[Epoch 388, Batch 100/372] Train Loss: 0.059, Train Accuracy: 1.000\n",
            "[Epoch 388, Batch 200/372] Train Loss: 0.238, Train Accuracy: 0.148\n",
            "[Epoch 388, Batch 300/372] Train Loss: 0.099, Train Accuracy: 1.000\n",
            "Epoch 388 Validation Accuracy: 0.956\n",
            "[Epoch 389, Batch 100/372] Train Loss: 0.177, Train Accuracy: 0.117\n",
            "[Epoch 389, Batch 200/372] Train Loss: 0.388, Train Accuracy: 0.406\n",
            "[Epoch 389, Batch 300/372] Train Loss: 0.076, Train Accuracy: 0.992\n",
            "Epoch 389 Validation Accuracy: 0.957\n",
            "[Epoch 390, Batch 100/372] Train Loss: 0.097, Train Accuracy: 0.070\n",
            "[Epoch 390, Batch 200/372] Train Loss: 0.127, Train Accuracy: 0.117\n",
            "[Epoch 390, Batch 300/372] Train Loss: 0.113, Train Accuracy: 0.984\n",
            "Epoch 390 Validation Accuracy: 0.954\n",
            "[Epoch 391, Batch 100/372] Train Loss: 0.247, Train Accuracy: 0.883\n",
            "[Epoch 391, Batch 200/372] Train Loss: 0.149, Train Accuracy: 0.102\n",
            "[Epoch 391, Batch 300/372] Train Loss: 0.099, Train Accuracy: 0.125\n",
            "Epoch 391 Validation Accuracy: 0.961\n",
            "[Epoch 392, Batch 100/372] Train Loss: 0.170, Train Accuracy: 0.125\n",
            "[Epoch 392, Batch 200/372] Train Loss: 0.285, Train Accuracy: 0.320\n",
            "[Epoch 392, Batch 300/372] Train Loss: 0.199, Train Accuracy: 0.930\n",
            "Epoch 392 Validation Accuracy: 0.954\n",
            "[Epoch 393, Batch 100/372] Train Loss: 0.229, Train Accuracy: 0.914\n",
            "[Epoch 393, Batch 200/372] Train Loss: 0.288, Train Accuracy: 0.250\n",
            "[Epoch 393, Batch 300/372] Train Loss: 0.050, Train Accuracy: 1.000\n",
            "Epoch 393 Validation Accuracy: 0.954\n",
            "[Epoch 394, Batch 100/372] Train Loss: 0.149, Train Accuracy: 0.984\n",
            "[Epoch 394, Batch 200/372] Train Loss: 0.070, Train Accuracy: 0.102\n",
            "[Epoch 394, Batch 300/372] Train Loss: 0.047, Train Accuracy: 0.109\n",
            "Epoch 394 Validation Accuracy: 0.951\n",
            "[Epoch 395, Batch 100/372] Train Loss: 0.250, Train Accuracy: 0.344\n",
            "[Epoch 395, Batch 200/372] Train Loss: 0.284, Train Accuracy: 0.836\n",
            "[Epoch 395, Batch 300/372] Train Loss: 0.057, Train Accuracy: 1.000\n",
            "Epoch 395 Validation Accuracy: 0.953\n",
            "[Epoch 396, Batch 100/372] Train Loss: 0.050, Train Accuracy: 0.133\n",
            "[Epoch 396, Batch 200/372] Train Loss: 0.065, Train Accuracy: 1.000\n",
            "[Epoch 396, Batch 300/372] Train Loss: 0.240, Train Accuracy: 0.781\n",
            "Epoch 396 Validation Accuracy: 0.956\n",
            "[Epoch 397, Batch 100/372] Train Loss: 0.059, Train Accuracy: 0.070\n",
            "[Epoch 397, Batch 200/372] Train Loss: 0.054, Train Accuracy: 1.000\n",
            "[Epoch 397, Batch 300/372] Train Loss: 0.112, Train Accuracy: 0.992\n",
            "Epoch 397 Validation Accuracy: 0.953\n",
            "[Epoch 398, Batch 100/372] Train Loss: 0.120, Train Accuracy: 0.148\n",
            "[Epoch 398, Batch 200/372] Train Loss: 0.049, Train Accuracy: 0.094\n",
            "[Epoch 398, Batch 300/372] Train Loss: 0.222, Train Accuracy: 0.914\n",
            "Epoch 398 Validation Accuracy: 0.952\n",
            "[Epoch 399, Batch 100/372] Train Loss: 0.175, Train Accuracy: 0.141\n",
            "[Epoch 399, Batch 200/372] Train Loss: 0.126, Train Accuracy: 0.117\n",
            "[Epoch 399, Batch 300/372] Train Loss: 0.052, Train Accuracy: 0.086\n",
            "Epoch 399 Validation Accuracy: 0.956\n",
            "[Epoch 400, Batch 100/372] Train Loss: 0.047, Train Accuracy: 0.102\n",
            "[Epoch 400, Batch 200/372] Train Loss: 0.087, Train Accuracy: 1.000\n",
            "[Epoch 400, Batch 300/372] Train Loss: 0.282, Train Accuracy: 0.492\n",
            "Epoch 400 Validation Accuracy: 0.958\n",
            "Checkpoint saved at epoch 400: student_BiT_R101x1/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_400.tar\n",
            "Test accuracy:  0.9536\n",
            "Results saved to student_BiT_R101x1/results_student.csv\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 400\n",
        "\n",
        "\n",
        "# Hypothetical setup, please adjust according to actual import paths and methods\n",
        "temperatures = [5]\n",
        "alphas = [1.0]\n",
        "learning_rates = [5e-4]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-4]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "\n",
        "\n",
        "checkpoints_path_student = 'student_BiT_R101x1/'\n",
        "\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {}\n",
        "    hparam['alpha'] = hparam_tuple[0]\n",
        "    hparam['T'] = hparam_tuple[1]\n",
        "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
        "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
        "    hparam['weight_decay'] = hparam_tuple[3]\n",
        "    hparam['lr_decay'] = hparam_tuple[4]\n",
        "    hparam['momentum'] = hparam_tuple[5]\n",
        "    hparam['lr'] = hparam_tuple[6]\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "results_distill = {}\n",
        "pruning_factors = [0]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = checkpoints_path_student + \"results_student.csv\"\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training with hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net = networks.StudentNetwork(teacher_net, q=False, fuse=False, qat=False, dif_arch=True)\n",
        "        student_net.to(fast_device)\n",
        "        hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(student_net)\n",
        "        \n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "        results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparamMixup(\n",
        "                teacher_net, student_net, hparam, num_epochs,\n",
        "                train_loader, val_loader,\n",
        "                print_every=print_every,\n",
        "                fast_device=fast_device, quant=False, checkpoint_save_path= checkpoints_path_student, resume_checkpoint=None\n",
        "            )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Final model save\n",
        "        final_save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
        "        torch.save({\n",
        "            'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            'model_state_dict': student_net.state_dict(),\n",
        "            'epoch': num_epochs\n",
        "        }, final_save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['alpha'], hparam['T'], hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsT=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0 and pruning factor 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\dsc180b\\DSC180B_Q2_Project\\utils.py:350: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(resume_checkpoint)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 11173962 23520842\n",
            "[Epoch 201, Batch 100/372] Train Loss: 1.751, Train Accuracy: 0.102\n",
            "[Epoch 201, Batch 200/372] Train Loss: 2.935, Train Accuracy: 0.383\n",
            "[Epoch 201, Batch 300/372] Train Loss: 1.621, Train Accuracy: 0.984\n",
            "Epoch 201 Validation Accuracy: 0.932\n",
            "[Epoch 202, Batch 100/372] Train Loss: 1.167, Train Accuracy: 0.984\n",
            "[Epoch 202, Batch 200/372] Train Loss: 0.792, Train Accuracy: 0.102\n",
            "[Epoch 202, Batch 300/372] Train Loss: 2.427, Train Accuracy: 0.359\n",
            "Epoch 202 Validation Accuracy: 0.944\n",
            "[Epoch 203, Batch 100/372] Train Loss: 0.768, Train Accuracy: 0.984\n",
            "[Epoch 203, Batch 200/372] Train Loss: 1.635, Train Accuracy: 0.898\n",
            "[Epoch 203, Batch 300/372] Train Loss: 1.053, Train Accuracy: 0.977\n",
            "Epoch 203 Validation Accuracy: 0.938\n",
            "[Epoch 204, Batch 100/372] Train Loss: 2.715, Train Accuracy: 0.547\n",
            "[Epoch 204, Batch 200/372] Train Loss: 1.283, Train Accuracy: 0.969\n",
            "[Epoch 204, Batch 300/372] Train Loss: 0.673, Train Accuracy: 0.094\n",
            "Epoch 204 Validation Accuracy: 0.944\n",
            "[Epoch 205, Batch 100/372] Train Loss: 0.533, Train Accuracy: 0.062\n",
            "[Epoch 205, Batch 200/372] Train Loss: 0.980, Train Accuracy: 0.984\n",
            "[Epoch 205, Batch 300/372] Train Loss: 0.846, Train Accuracy: 0.117\n",
            "Epoch 205 Validation Accuracy: 0.946\n",
            "[Epoch 206, Batch 100/372] Train Loss: 0.935, Train Accuracy: 0.148\n",
            "[Epoch 206, Batch 200/372] Train Loss: 0.777, Train Accuracy: 0.117\n",
            "[Epoch 206, Batch 300/372] Train Loss: 1.384, Train Accuracy: 0.102\n",
            "Epoch 206 Validation Accuracy: 0.947\n",
            "[Epoch 207, Batch 100/372] Train Loss: 0.902, Train Accuracy: 0.109\n",
            "[Epoch 207, Batch 200/372] Train Loss: 0.550, Train Accuracy: 0.078\n",
            "[Epoch 207, Batch 300/372] Train Loss: 2.038, Train Accuracy: 0.797\n",
            "Epoch 207 Validation Accuracy: 0.935\n",
            "[Epoch 208, Batch 100/372] Train Loss: 2.341, Train Accuracy: 0.789\n",
            "[Epoch 208, Batch 200/372] Train Loss: 0.710, Train Accuracy: 0.133\n",
            "[Epoch 208, Batch 300/372] Train Loss: 1.609, Train Accuracy: 0.859\n",
            "Epoch 208 Validation Accuracy: 0.944\n",
            "[Epoch 209, Batch 100/372] Train Loss: 1.693, Train Accuracy: 0.148\n",
            "[Epoch 209, Batch 200/372] Train Loss: 0.756, Train Accuracy: 0.094\n",
            "[Epoch 209, Batch 300/372] Train Loss: 0.794, Train Accuracy: 0.992\n",
            "Epoch 209 Validation Accuracy: 0.950\n",
            "[Epoch 210, Batch 100/372] Train Loss: 0.909, Train Accuracy: 0.133\n",
            "[Epoch 210, Batch 200/372] Train Loss: 2.105, Train Accuracy: 0.172\n",
            "[Epoch 210, Batch 300/372] Train Loss: 1.706, Train Accuracy: 0.227\n",
            "Epoch 210 Validation Accuracy: 0.945\n",
            "[Epoch 211, Batch 100/372] Train Loss: 2.298, Train Accuracy: 0.242\n",
            "[Epoch 211, Batch 200/372] Train Loss: 2.121, Train Accuracy: 0.398\n",
            "[Epoch 211, Batch 300/372] Train Loss: 1.183, Train Accuracy: 0.094\n",
            "Epoch 211 Validation Accuracy: 0.946\n",
            "[Epoch 212, Batch 100/372] Train Loss: 0.951, Train Accuracy: 0.953\n",
            "[Epoch 212, Batch 200/372] Train Loss: 2.196, Train Accuracy: 0.305\n",
            "[Epoch 212, Batch 300/372] Train Loss: 1.117, Train Accuracy: 0.961\n",
            "Epoch 212 Validation Accuracy: 0.946\n",
            "[Epoch 213, Batch 100/372] Train Loss: 2.675, Train Accuracy: 0.555\n",
            "[Epoch 213, Batch 200/372] Train Loss: 1.136, Train Accuracy: 0.961\n",
            "[Epoch 213, Batch 300/372] Train Loss: 2.103, Train Accuracy: 0.289\n",
            "Epoch 213 Validation Accuracy: 0.944\n",
            "[Epoch 214, Batch 100/372] Train Loss: 1.031, Train Accuracy: 0.109\n",
            "[Epoch 214, Batch 200/372] Train Loss: 0.868, Train Accuracy: 0.133\n",
            "[Epoch 214, Batch 300/372] Train Loss: 1.253, Train Accuracy: 0.141\n",
            "Epoch 214 Validation Accuracy: 0.946\n",
            "[Epoch 215, Batch 100/372] Train Loss: 1.196, Train Accuracy: 0.953\n",
            "[Epoch 215, Batch 200/372] Train Loss: 1.801, Train Accuracy: 0.836\n",
            "[Epoch 215, Batch 300/372] Train Loss: 1.032, Train Accuracy: 0.945\n",
            "Epoch 215 Validation Accuracy: 0.942\n",
            "[Epoch 216, Batch 100/372] Train Loss: 2.222, Train Accuracy: 0.555\n",
            "[Epoch 216, Batch 200/372] Train Loss: 0.474, Train Accuracy: 0.086\n",
            "[Epoch 216, Batch 300/372] Train Loss: 0.606, Train Accuracy: 0.977\n",
            "Epoch 216 Validation Accuracy: 0.948\n",
            "[Epoch 217, Batch 100/372] Train Loss: 0.545, Train Accuracy: 1.000\n",
            "[Epoch 217, Batch 200/372] Train Loss: 0.525, Train Accuracy: 0.156\n",
            "[Epoch 217, Batch 300/372] Train Loss: 2.541, Train Accuracy: 0.383\n",
            "Epoch 217 Validation Accuracy: 0.945\n",
            "[Epoch 218, Batch 100/372] Train Loss: 0.531, Train Accuracy: 0.992\n",
            "[Epoch 218, Batch 200/372] Train Loss: 0.815, Train Accuracy: 0.992\n",
            "[Epoch 218, Batch 300/372] Train Loss: 1.619, Train Accuracy: 0.906\n",
            "Epoch 218 Validation Accuracy: 0.941\n",
            "[Epoch 219, Batch 100/372] Train Loss: 0.378, Train Accuracy: 1.000\n",
            "[Epoch 219, Batch 200/372] Train Loss: 0.474, Train Accuracy: 1.000\n",
            "[Epoch 219, Batch 300/372] Train Loss: 2.561, Train Accuracy: 0.344\n",
            "Epoch 219 Validation Accuracy: 0.946\n",
            "[Epoch 220, Batch 100/372] Train Loss: 1.074, Train Accuracy: 0.125\n",
            "[Epoch 220, Batch 200/372] Train Loss: 2.016, Train Accuracy: 0.180\n",
            "[Epoch 220, Batch 300/372] Train Loss: 0.558, Train Accuracy: 0.078\n",
            "Epoch 220 Validation Accuracy: 0.946\n",
            "[Epoch 221, Batch 100/372] Train Loss: 0.738, Train Accuracy: 0.984\n",
            "[Epoch 221, Batch 200/372] Train Loss: 0.457, Train Accuracy: 1.000\n",
            "[Epoch 221, Batch 300/372] Train Loss: 1.991, Train Accuracy: 0.656\n",
            "Epoch 221 Validation Accuracy: 0.947\n",
            "[Epoch 222, Batch 100/372] Train Loss: 2.276, Train Accuracy: 0.695\n",
            "[Epoch 222, Batch 200/372] Train Loss: 2.428, Train Accuracy: 0.602\n",
            "[Epoch 222, Batch 300/372] Train Loss: 2.453, Train Accuracy: 0.422\n",
            "Epoch 222 Validation Accuracy: 0.949\n",
            "[Epoch 223, Batch 100/372] Train Loss: 2.327, Train Accuracy: 0.461\n",
            "[Epoch 223, Batch 200/372] Train Loss: 1.886, Train Accuracy: 0.734\n",
            "[Epoch 223, Batch 300/372] Train Loss: 1.219, Train Accuracy: 0.141\n",
            "Epoch 223 Validation Accuracy: 0.948\n",
            "[Epoch 224, Batch 100/372] Train Loss: 0.956, Train Accuracy: 0.141\n",
            "[Epoch 224, Batch 200/372] Train Loss: 2.356, Train Accuracy: 0.656\n",
            "[Epoch 224, Batch 300/372] Train Loss: 1.525, Train Accuracy: 0.922\n",
            "Epoch 224 Validation Accuracy: 0.947\n",
            "[Epoch 225, Batch 100/372] Train Loss: 0.401, Train Accuracy: 0.164\n",
            "[Epoch 225, Batch 200/372] Train Loss: 0.617, Train Accuracy: 0.992\n",
            "[Epoch 225, Batch 300/372] Train Loss: 0.691, Train Accuracy: 0.992\n",
            "Epoch 225 Validation Accuracy: 0.948\n",
            "Checkpoint saved at epoch 225: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_225.tar\n",
            "[Epoch 226, Batch 100/372] Train Loss: 1.447, Train Accuracy: 0.180\n",
            "[Epoch 226, Batch 200/372] Train Loss: 0.607, Train Accuracy: 0.133\n",
            "[Epoch 226, Batch 300/372] Train Loss: 0.593, Train Accuracy: 0.117\n",
            "Epoch 226 Validation Accuracy: 0.945\n",
            "[Epoch 227, Batch 100/372] Train Loss: 1.708, Train Accuracy: 0.812\n",
            "[Epoch 227, Batch 200/372] Train Loss: 0.633, Train Accuracy: 0.156\n",
            "[Epoch 227, Batch 300/372] Train Loss: 0.683, Train Accuracy: 0.992\n",
            "Epoch 227 Validation Accuracy: 0.946\n",
            "[Epoch 228, Batch 100/372] Train Loss: 1.108, Train Accuracy: 0.961\n",
            "[Epoch 228, Batch 200/372] Train Loss: 2.119, Train Accuracy: 0.773\n",
            "[Epoch 228, Batch 300/372] Train Loss: 0.391, Train Accuracy: 0.148\n",
            "Epoch 228 Validation Accuracy: 0.950\n",
            "[Epoch 229, Batch 100/372] Train Loss: 1.985, Train Accuracy: 0.320\n",
            "[Epoch 229, Batch 200/372] Train Loss: 0.599, Train Accuracy: 0.992\n",
            "[Epoch 229, Batch 300/372] Train Loss: 1.178, Train Accuracy: 0.125\n",
            "Epoch 229 Validation Accuracy: 0.951\n",
            "[Epoch 230, Batch 100/372] Train Loss: 1.191, Train Accuracy: 0.945\n",
            "[Epoch 230, Batch 200/372] Train Loss: 0.621, Train Accuracy: 0.117\n",
            "[Epoch 230, Batch 300/372] Train Loss: 0.877, Train Accuracy: 0.977\n",
            "Epoch 230 Validation Accuracy: 0.947\n",
            "[Epoch 231, Batch 100/372] Train Loss: 0.934, Train Accuracy: 0.133\n",
            "[Epoch 231, Batch 200/372] Train Loss: 1.284, Train Accuracy: 0.930\n",
            "[Epoch 231, Batch 300/372] Train Loss: 0.476, Train Accuracy: 1.000\n",
            "Epoch 231 Validation Accuracy: 0.946\n",
            "[Epoch 232, Batch 100/372] Train Loss: 0.630, Train Accuracy: 0.070\n",
            "[Epoch 232, Batch 200/372] Train Loss: 0.456, Train Accuracy: 1.000\n",
            "[Epoch 232, Batch 300/372] Train Loss: 0.526, Train Accuracy: 1.000\n",
            "Epoch 232 Validation Accuracy: 0.950\n",
            "[Epoch 233, Batch 100/372] Train Loss: 2.493, Train Accuracy: 0.531\n",
            "[Epoch 233, Batch 200/372] Train Loss: 0.719, Train Accuracy: 0.977\n",
            "[Epoch 233, Batch 300/372] Train Loss: 0.568, Train Accuracy: 0.047\n",
            "Epoch 233 Validation Accuracy: 0.948\n",
            "[Epoch 234, Batch 100/372] Train Loss: 0.420, Train Accuracy: 0.164\n",
            "[Epoch 234, Batch 200/372] Train Loss: 1.559, Train Accuracy: 0.141\n",
            "[Epoch 234, Batch 300/372] Train Loss: 1.710, Train Accuracy: 0.859\n",
            "Epoch 234 Validation Accuracy: 0.949\n",
            "[Epoch 235, Batch 100/372] Train Loss: 2.226, Train Accuracy: 0.508\n",
            "[Epoch 235, Batch 200/372] Train Loss: 0.513, Train Accuracy: 1.000\n",
            "[Epoch 235, Batch 300/372] Train Loss: 0.448, Train Accuracy: 0.992\n",
            "Epoch 235 Validation Accuracy: 0.949\n",
            "[Epoch 236, Batch 100/372] Train Loss: 0.446, Train Accuracy: 1.000\n",
            "[Epoch 236, Batch 200/372] Train Loss: 0.414, Train Accuracy: 0.070\n",
            "[Epoch 236, Batch 300/372] Train Loss: 1.461, Train Accuracy: 0.172\n",
            "Epoch 236 Validation Accuracy: 0.946\n",
            "[Epoch 237, Batch 100/372] Train Loss: 0.745, Train Accuracy: 1.000\n",
            "[Epoch 237, Batch 200/372] Train Loss: 1.288, Train Accuracy: 0.125\n",
            "[Epoch 237, Batch 300/372] Train Loss: 0.457, Train Accuracy: 0.992\n",
            "Epoch 237 Validation Accuracy: 0.944\n",
            "[Epoch 238, Batch 100/372] Train Loss: 0.839, Train Accuracy: 0.961\n",
            "[Epoch 238, Batch 200/372] Train Loss: 2.326, Train Accuracy: 0.539\n",
            "[Epoch 238, Batch 300/372] Train Loss: 1.190, Train Accuracy: 0.086\n",
            "Epoch 238 Validation Accuracy: 0.949\n",
            "[Epoch 239, Batch 100/372] Train Loss: 0.491, Train Accuracy: 1.000\n",
            "[Epoch 239, Batch 200/372] Train Loss: 0.444, Train Accuracy: 0.062\n",
            "[Epoch 239, Batch 300/372] Train Loss: 0.542, Train Accuracy: 0.133\n",
            "Epoch 239 Validation Accuracy: 0.948\n",
            "[Epoch 240, Batch 100/372] Train Loss: 0.498, Train Accuracy: 1.000\n",
            "[Epoch 240, Batch 200/372] Train Loss: 1.150, Train Accuracy: 0.094\n",
            "[Epoch 240, Batch 300/372] Train Loss: 0.635, Train Accuracy: 0.094\n",
            "Epoch 240 Validation Accuracy: 0.948\n",
            "[Epoch 241, Batch 100/372] Train Loss: 0.580, Train Accuracy: 0.094\n",
            "[Epoch 241, Batch 200/372] Train Loss: 0.439, Train Accuracy: 1.000\n",
            "[Epoch 241, Batch 300/372] Train Loss: 0.468, Train Accuracy: 0.086\n",
            "Epoch 241 Validation Accuracy: 0.945\n",
            "[Epoch 242, Batch 100/372] Train Loss: 0.471, Train Accuracy: 0.992\n",
            "[Epoch 242, Batch 200/372] Train Loss: 0.541, Train Accuracy: 0.125\n",
            "[Epoch 242, Batch 300/372] Train Loss: 2.140, Train Accuracy: 0.484\n",
            "Epoch 242 Validation Accuracy: 0.950\n",
            "[Epoch 243, Batch 100/372] Train Loss: 0.453, Train Accuracy: 0.094\n",
            "[Epoch 243, Batch 200/372] Train Loss: 2.207, Train Accuracy: 0.539\n",
            "[Epoch 243, Batch 300/372] Train Loss: 1.799, Train Accuracy: 0.766\n",
            "Epoch 243 Validation Accuracy: 0.943\n",
            "[Epoch 244, Batch 100/372] Train Loss: 0.364, Train Accuracy: 0.094\n",
            "[Epoch 244, Batch 200/372] Train Loss: 0.446, Train Accuracy: 0.047\n",
            "[Epoch 244, Batch 300/372] Train Loss: 0.905, Train Accuracy: 0.961\n",
            "Epoch 244 Validation Accuracy: 0.950\n",
            "[Epoch 245, Batch 100/372] Train Loss: 0.620, Train Accuracy: 0.992\n",
            "[Epoch 245, Batch 200/372] Train Loss: 0.624, Train Accuracy: 0.992\n",
            "[Epoch 245, Batch 300/372] Train Loss: 1.005, Train Accuracy: 0.961\n",
            "Epoch 245 Validation Accuracy: 0.946\n",
            "[Epoch 246, Batch 100/372] Train Loss: 1.145, Train Accuracy: 0.133\n",
            "[Epoch 246, Batch 200/372] Train Loss: 0.826, Train Accuracy: 0.977\n",
            "[Epoch 246, Batch 300/372] Train Loss: 1.603, Train Accuracy: 0.141\n",
            "Epoch 246 Validation Accuracy: 0.949\n",
            "[Epoch 247, Batch 100/372] Train Loss: 0.507, Train Accuracy: 1.000\n",
            "[Epoch 247, Batch 200/372] Train Loss: 0.408, Train Accuracy: 0.125\n",
            "[Epoch 247, Batch 300/372] Train Loss: 0.412, Train Accuracy: 1.000\n",
            "Epoch 247 Validation Accuracy: 0.946\n",
            "[Epoch 248, Batch 100/372] Train Loss: 0.351, Train Accuracy: 0.117\n",
            "[Epoch 248, Batch 200/372] Train Loss: 0.360, Train Accuracy: 1.000\n",
            "[Epoch 248, Batch 300/372] Train Loss: 1.278, Train Accuracy: 0.961\n",
            "Epoch 248 Validation Accuracy: 0.948\n",
            "[Epoch 249, Batch 100/372] Train Loss: 1.941, Train Accuracy: 0.406\n",
            "[Epoch 249, Batch 200/372] Train Loss: 1.813, Train Accuracy: 0.211\n",
            "[Epoch 249, Batch 300/372] Train Loss: 1.953, Train Accuracy: 0.758\n",
            "Epoch 249 Validation Accuracy: 0.947\n",
            "[Epoch 250, Batch 100/372] Train Loss: 0.984, Train Accuracy: 0.117\n",
            "[Epoch 250, Batch 200/372] Train Loss: 1.183, Train Accuracy: 0.953\n",
            "[Epoch 250, Batch 300/372] Train Loss: 1.723, Train Accuracy: 0.242\n",
            "Epoch 250 Validation Accuracy: 0.949\n",
            "Checkpoint saved at epoch 250: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_250.tar\n",
            "[Epoch 251, Batch 100/372] Train Loss: 0.372, Train Accuracy: 1.000\n",
            "[Epoch 251, Batch 200/372] Train Loss: 0.381, Train Accuracy: 1.000\n",
            "[Epoch 251, Batch 300/372] Train Loss: 1.683, Train Accuracy: 0.172\n",
            "Epoch 251 Validation Accuracy: 0.948\n",
            "[Epoch 252, Batch 100/372] Train Loss: 1.053, Train Accuracy: 0.953\n",
            "[Epoch 252, Batch 200/372] Train Loss: 1.200, Train Accuracy: 0.953\n",
            "[Epoch 252, Batch 300/372] Train Loss: 0.771, Train Accuracy: 0.117\n",
            "Epoch 252 Validation Accuracy: 0.952\n",
            "[Epoch 253, Batch 100/372] Train Loss: 0.535, Train Accuracy: 0.133\n",
            "[Epoch 253, Batch 200/372] Train Loss: 1.229, Train Accuracy: 0.172\n",
            "[Epoch 253, Batch 300/372] Train Loss: 2.593, Train Accuracy: 0.523\n",
            "Epoch 253 Validation Accuracy: 0.949\n",
            "[Epoch 254, Batch 100/372] Train Loss: 2.079, Train Accuracy: 0.234\n",
            "[Epoch 254, Batch 200/372] Train Loss: 0.697, Train Accuracy: 0.102\n",
            "[Epoch 254, Batch 300/372] Train Loss: 2.631, Train Accuracy: 0.609\n",
            "Epoch 254 Validation Accuracy: 0.946\n",
            "[Epoch 255, Batch 100/372] Train Loss: 0.850, Train Accuracy: 0.086\n",
            "[Epoch 255, Batch 200/372] Train Loss: 2.256, Train Accuracy: 0.711\n",
            "[Epoch 255, Batch 300/372] Train Loss: 0.862, Train Accuracy: 0.078\n",
            "Epoch 255 Validation Accuracy: 0.948\n",
            "[Epoch 256, Batch 100/372] Train Loss: 1.867, Train Accuracy: 0.750\n",
            "[Epoch 256, Batch 200/372] Train Loss: 0.456, Train Accuracy: 1.000\n",
            "[Epoch 256, Batch 300/372] Train Loss: 0.816, Train Accuracy: 1.000\n",
            "Epoch 256 Validation Accuracy: 0.956\n",
            "[Epoch 257, Batch 100/372] Train Loss: 0.383, Train Accuracy: 0.109\n",
            "[Epoch 257, Batch 200/372] Train Loss: 0.367, Train Accuracy: 1.000\n",
            "[Epoch 257, Batch 300/372] Train Loss: 1.307, Train Accuracy: 0.164\n",
            "Epoch 257 Validation Accuracy: 0.946\n",
            "[Epoch 258, Batch 100/372] Train Loss: 1.515, Train Accuracy: 0.914\n",
            "[Epoch 258, Batch 200/372] Train Loss: 0.554, Train Accuracy: 1.000\n",
            "[Epoch 258, Batch 300/372] Train Loss: 1.599, Train Accuracy: 0.891\n",
            "Epoch 258 Validation Accuracy: 0.951\n",
            "[Epoch 259, Batch 100/372] Train Loss: 1.550, Train Accuracy: 0.836\n",
            "[Epoch 259, Batch 200/372] Train Loss: 0.564, Train Accuracy: 0.992\n",
            "[Epoch 259, Batch 300/372] Train Loss: 2.210, Train Accuracy: 0.359\n",
            "Epoch 259 Validation Accuracy: 0.949\n",
            "[Epoch 260, Batch 100/372] Train Loss: 0.427, Train Accuracy: 1.000\n",
            "[Epoch 260, Batch 200/372] Train Loss: 0.641, Train Accuracy: 0.125\n",
            "[Epoch 260, Batch 300/372] Train Loss: 1.659, Train Accuracy: 0.141\n",
            "Epoch 260 Validation Accuracy: 0.941\n",
            "[Epoch 261, Batch 100/372] Train Loss: 0.566, Train Accuracy: 0.086\n",
            "[Epoch 261, Batch 200/372] Train Loss: 1.649, Train Accuracy: 0.180\n",
            "[Epoch 261, Batch 300/372] Train Loss: 1.402, Train Accuracy: 0.188\n",
            "Epoch 261 Validation Accuracy: 0.948\n",
            "[Epoch 262, Batch 100/372] Train Loss: 1.891, Train Accuracy: 0.492\n",
            "[Epoch 262, Batch 200/372] Train Loss: 1.415, Train Accuracy: 0.852\n",
            "[Epoch 262, Batch 300/372] Train Loss: 1.958, Train Accuracy: 0.383\n",
            "Epoch 262 Validation Accuracy: 0.948\n",
            "[Epoch 263, Batch 100/372] Train Loss: 0.435, Train Accuracy: 1.000\n",
            "[Epoch 263, Batch 200/372] Train Loss: 1.158, Train Accuracy: 0.953\n",
            "[Epoch 263, Batch 300/372] Train Loss: 0.684, Train Accuracy: 1.000\n",
            "Epoch 263 Validation Accuracy: 0.952\n",
            "[Epoch 264, Batch 100/372] Train Loss: 0.757, Train Accuracy: 0.086\n",
            "[Epoch 264, Batch 200/372] Train Loss: 0.554, Train Accuracy: 0.070\n",
            "[Epoch 264, Batch 300/372] Train Loss: 2.032, Train Accuracy: 0.477\n",
            "Epoch 264 Validation Accuracy: 0.946\n",
            "[Epoch 265, Batch 100/372] Train Loss: 0.459, Train Accuracy: 0.992\n",
            "[Epoch 265, Batch 200/372] Train Loss: 1.179, Train Accuracy: 0.953\n",
            "[Epoch 265, Batch 300/372] Train Loss: 2.025, Train Accuracy: 0.391\n",
            "Epoch 265 Validation Accuracy: 0.943\n",
            "[Epoch 266, Batch 100/372] Train Loss: 0.370, Train Accuracy: 0.125\n",
            "[Epoch 266, Batch 200/372] Train Loss: 1.807, Train Accuracy: 0.203\n",
            "[Epoch 266, Batch 300/372] Train Loss: 1.986, Train Accuracy: 0.625\n",
            "Epoch 266 Validation Accuracy: 0.950\n",
            "[Epoch 267, Batch 100/372] Train Loss: 1.244, Train Accuracy: 0.125\n",
            "[Epoch 267, Batch 200/372] Train Loss: 0.750, Train Accuracy: 0.984\n",
            "[Epoch 267, Batch 300/372] Train Loss: 0.636, Train Accuracy: 1.000\n",
            "Epoch 267 Validation Accuracy: 0.943\n",
            "[Epoch 268, Batch 100/372] Train Loss: 0.448, Train Accuracy: 0.078\n",
            "[Epoch 268, Batch 200/372] Train Loss: 0.489, Train Accuracy: 0.133\n",
            "[Epoch 268, Batch 300/372] Train Loss: 1.012, Train Accuracy: 0.094\n",
            "Epoch 268 Validation Accuracy: 0.950\n",
            "[Epoch 269, Batch 100/372] Train Loss: 0.343, Train Accuracy: 0.109\n",
            "[Epoch 269, Batch 200/372] Train Loss: 0.941, Train Accuracy: 0.117\n",
            "[Epoch 269, Batch 300/372] Train Loss: 0.391, Train Accuracy: 0.117\n",
            "Epoch 269 Validation Accuracy: 0.951\n",
            "[Epoch 270, Batch 100/372] Train Loss: 0.510, Train Accuracy: 0.062\n",
            "[Epoch 270, Batch 200/372] Train Loss: 1.793, Train Accuracy: 0.195\n",
            "[Epoch 270, Batch 300/372] Train Loss: 2.179, Train Accuracy: 0.508\n",
            "Epoch 270 Validation Accuracy: 0.942\n",
            "[Epoch 271, Batch 100/372] Train Loss: 1.349, Train Accuracy: 0.242\n",
            "[Epoch 271, Batch 200/372] Train Loss: 0.666, Train Accuracy: 0.984\n",
            "[Epoch 271, Batch 300/372] Train Loss: 0.533, Train Accuracy: 0.984\n",
            "Epoch 271 Validation Accuracy: 0.950\n",
            "[Epoch 272, Batch 100/372] Train Loss: 0.326, Train Accuracy: 1.000\n",
            "[Epoch 272, Batch 200/372] Train Loss: 1.341, Train Accuracy: 0.172\n",
            "[Epoch 272, Batch 300/372] Train Loss: 0.466, Train Accuracy: 0.086\n",
            "Epoch 272 Validation Accuracy: 0.946\n",
            "[Epoch 273, Batch 100/372] Train Loss: 0.735, Train Accuracy: 0.133\n",
            "[Epoch 273, Batch 200/372] Train Loss: 1.740, Train Accuracy: 0.836\n",
            "[Epoch 273, Batch 300/372] Train Loss: 0.342, Train Accuracy: 0.086\n",
            "Epoch 273 Validation Accuracy: 0.952\n",
            "[Epoch 274, Batch 100/372] Train Loss: 1.978, Train Accuracy: 0.609\n",
            "[Epoch 274, Batch 200/372] Train Loss: 0.481, Train Accuracy: 0.125\n",
            "[Epoch 274, Batch 300/372] Train Loss: 1.785, Train Accuracy: 0.773\n",
            "Epoch 274 Validation Accuracy: 0.949\n",
            "[Epoch 275, Batch 100/372] Train Loss: 1.304, Train Accuracy: 0.938\n",
            "[Epoch 275, Batch 200/372] Train Loss: 0.348, Train Accuracy: 0.109\n",
            "[Epoch 275, Batch 300/372] Train Loss: 1.716, Train Accuracy: 0.180\n",
            "Epoch 275 Validation Accuracy: 0.950\n",
            "Checkpoint saved at epoch 275: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_275.tar\n",
            "[Epoch 276, Batch 100/372] Train Loss: 1.705, Train Accuracy: 0.172\n",
            "[Epoch 276, Batch 200/372] Train Loss: 1.271, Train Accuracy: 0.938\n",
            "[Epoch 276, Batch 300/372] Train Loss: 1.380, Train Accuracy: 0.172\n",
            "Epoch 276 Validation Accuracy: 0.948\n",
            "[Epoch 277, Batch 100/372] Train Loss: 2.226, Train Accuracy: 0.508\n",
            "[Epoch 277, Batch 200/372] Train Loss: 1.917, Train Accuracy: 0.312\n",
            "[Epoch 277, Batch 300/372] Train Loss: 0.467, Train Accuracy: 0.047\n",
            "Epoch 277 Validation Accuracy: 0.943\n",
            "[Epoch 278, Batch 100/372] Train Loss: 2.010, Train Accuracy: 0.352\n",
            "[Epoch 278, Batch 200/372] Train Loss: 0.379, Train Accuracy: 1.000\n",
            "[Epoch 278, Batch 300/372] Train Loss: 0.360, Train Accuracy: 1.000\n",
            "Epoch 278 Validation Accuracy: 0.947\n",
            "[Epoch 279, Batch 100/372] Train Loss: 0.410, Train Accuracy: 0.148\n",
            "[Epoch 279, Batch 200/372] Train Loss: 0.340, Train Accuracy: 0.078\n",
            "[Epoch 279, Batch 300/372] Train Loss: 0.334, Train Accuracy: 1.000\n",
            "Epoch 279 Validation Accuracy: 0.946\n",
            "[Epoch 280, Batch 100/372] Train Loss: 0.316, Train Accuracy: 0.164\n",
            "[Epoch 280, Batch 200/372] Train Loss: 1.107, Train Accuracy: 0.109\n",
            "[Epoch 280, Batch 300/372] Train Loss: 1.948, Train Accuracy: 0.594\n",
            "Epoch 280 Validation Accuracy: 0.946\n",
            "[Epoch 281, Batch 100/372] Train Loss: 0.965, Train Accuracy: 0.984\n",
            "[Epoch 281, Batch 200/372] Train Loss: 0.439, Train Accuracy: 0.062\n",
            "[Epoch 281, Batch 300/372] Train Loss: 2.297, Train Accuracy: 0.461\n",
            "Epoch 281 Validation Accuracy: 0.951\n",
            "[Epoch 282, Batch 100/372] Train Loss: 2.222, Train Accuracy: 0.391\n",
            "[Epoch 282, Batch 200/372] Train Loss: 0.383, Train Accuracy: 1.000\n",
            "[Epoch 282, Batch 300/372] Train Loss: 1.893, Train Accuracy: 0.758\n",
            "Epoch 282 Validation Accuracy: 0.949\n",
            "[Epoch 283, Batch 100/372] Train Loss: 1.903, Train Accuracy: 0.812\n",
            "[Epoch 283, Batch 200/372] Train Loss: 2.002, Train Accuracy: 0.719\n",
            "[Epoch 283, Batch 300/372] Train Loss: 0.473, Train Accuracy: 0.109\n",
            "Epoch 283 Validation Accuracy: 0.948\n",
            "[Epoch 284, Batch 100/372] Train Loss: 1.587, Train Accuracy: 0.250\n",
            "[Epoch 284, Batch 200/372] Train Loss: 1.078, Train Accuracy: 0.141\n",
            "[Epoch 284, Batch 300/372] Train Loss: 0.287, Train Accuracy: 0.094\n",
            "Epoch 284 Validation Accuracy: 0.953\n",
            "[Epoch 285, Batch 100/372] Train Loss: 0.345, Train Accuracy: 0.062\n",
            "[Epoch 285, Batch 200/372] Train Loss: 1.473, Train Accuracy: 0.164\n",
            "[Epoch 285, Batch 300/372] Train Loss: 1.412, Train Accuracy: 0.898\n",
            "Epoch 285 Validation Accuracy: 0.949\n",
            "[Epoch 286, Batch 100/372] Train Loss: 0.469, Train Accuracy: 0.055\n",
            "[Epoch 286, Batch 200/372] Train Loss: 0.549, Train Accuracy: 1.000\n",
            "[Epoch 286, Batch 300/372] Train Loss: 0.601, Train Accuracy: 1.000\n",
            "Epoch 286 Validation Accuracy: 0.948\n",
            "[Epoch 287, Batch 100/372] Train Loss: 0.411, Train Accuracy: 1.000\n",
            "[Epoch 287, Batch 200/372] Train Loss: 1.253, Train Accuracy: 0.148\n",
            "[Epoch 287, Batch 300/372] Train Loss: 0.310, Train Accuracy: 0.094\n",
            "Epoch 287 Validation Accuracy: 0.950\n",
            "[Epoch 288, Batch 100/372] Train Loss: 0.679, Train Accuracy: 0.984\n",
            "[Epoch 288, Batch 200/372] Train Loss: 0.387, Train Accuracy: 0.117\n",
            "[Epoch 288, Batch 300/372] Train Loss: 0.414, Train Accuracy: 0.102\n",
            "Epoch 288 Validation Accuracy: 0.948\n",
            "[Epoch 289, Batch 100/372] Train Loss: 1.522, Train Accuracy: 0.172\n",
            "[Epoch 289, Batch 200/372] Train Loss: 0.447, Train Accuracy: 0.172\n",
            "[Epoch 289, Batch 300/372] Train Loss: 0.749, Train Accuracy: 0.984\n",
            "Epoch 289 Validation Accuracy: 0.948\n",
            "[Epoch 290, Batch 100/372] Train Loss: 0.284, Train Accuracy: 1.000\n",
            "[Epoch 290, Batch 200/372] Train Loss: 2.092, Train Accuracy: 0.352\n",
            "[Epoch 290, Batch 300/372] Train Loss: 0.395, Train Accuracy: 0.102\n",
            "Epoch 290 Validation Accuracy: 0.947\n",
            "[Epoch 291, Batch 100/372] Train Loss: 2.218, Train Accuracy: 0.633\n",
            "[Epoch 291, Batch 200/372] Train Loss: 0.792, Train Accuracy: 1.000\n",
            "[Epoch 291, Batch 300/372] Train Loss: 0.556, Train Accuracy: 1.000\n",
            "Epoch 291 Validation Accuracy: 0.946\n",
            "[Epoch 292, Batch 100/372] Train Loss: 1.609, Train Accuracy: 0.875\n",
            "[Epoch 292, Batch 200/372] Train Loss: 1.946, Train Accuracy: 0.617\n",
            "[Epoch 292, Batch 300/372] Train Loss: 1.493, Train Accuracy: 0.188\n",
            "Epoch 292 Validation Accuracy: 0.951\n",
            "[Epoch 293, Batch 100/372] Train Loss: 0.337, Train Accuracy: 0.031\n",
            "[Epoch 293, Batch 200/372] Train Loss: 1.594, Train Accuracy: 0.805\n",
            "[Epoch 293, Batch 300/372] Train Loss: 0.322, Train Accuracy: 0.117\n",
            "Epoch 293 Validation Accuracy: 0.948\n",
            "[Epoch 294, Batch 100/372] Train Loss: 0.296, Train Accuracy: 0.992\n",
            "[Epoch 294, Batch 200/372] Train Loss: 1.881, Train Accuracy: 0.406\n",
            "[Epoch 294, Batch 300/372] Train Loss: 0.432, Train Accuracy: 0.102\n",
            "Epoch 294 Validation Accuracy: 0.949\n",
            "[Epoch 295, Batch 100/372] Train Loss: 0.345, Train Accuracy: 1.000\n",
            "[Epoch 295, Batch 200/372] Train Loss: 0.319, Train Accuracy: 0.086\n",
            "[Epoch 295, Batch 300/372] Train Loss: 0.305, Train Accuracy: 1.000\n",
            "Epoch 295 Validation Accuracy: 0.949\n",
            "[Epoch 296, Batch 100/372] Train Loss: 0.737, Train Accuracy: 0.117\n",
            "[Epoch 296, Batch 200/372] Train Loss: 1.256, Train Accuracy: 0.922\n",
            "[Epoch 296, Batch 300/372] Train Loss: 0.575, Train Accuracy: 0.125\n",
            "Epoch 296 Validation Accuracy: 0.949\n",
            "[Epoch 297, Batch 100/372] Train Loss: 0.679, Train Accuracy: 0.984\n",
            "[Epoch 297, Batch 200/372] Train Loss: 0.310, Train Accuracy: 1.000\n",
            "[Epoch 297, Batch 300/372] Train Loss: 1.928, Train Accuracy: 0.234\n",
            "Epoch 297 Validation Accuracy: 0.952\n",
            "[Epoch 298, Batch 100/372] Train Loss: 0.366, Train Accuracy: 1.000\n",
            "[Epoch 298, Batch 200/372] Train Loss: 1.917, Train Accuracy: 0.328\n",
            "[Epoch 298, Batch 300/372] Train Loss: 0.419, Train Accuracy: 1.000\n",
            "Epoch 298 Validation Accuracy: 0.946\n",
            "[Epoch 299, Batch 100/372] Train Loss: 2.094, Train Accuracy: 0.516\n",
            "[Epoch 299, Batch 200/372] Train Loss: 0.642, Train Accuracy: 0.156\n",
            "[Epoch 299, Batch 300/372] Train Loss: 0.693, Train Accuracy: 0.109\n",
            "Epoch 299 Validation Accuracy: 0.949\n",
            "[Epoch 300, Batch 100/372] Train Loss: 0.860, Train Accuracy: 0.984\n",
            "[Epoch 300, Batch 200/372] Train Loss: 0.535, Train Accuracy: 0.125\n",
            "[Epoch 300, Batch 300/372] Train Loss: 0.420, Train Accuracy: 0.031\n",
            "Epoch 300 Validation Accuracy: 0.952\n",
            "Checkpoint saved at epoch 300: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_300.tar\n",
            "[Epoch 301, Batch 100/372] Train Loss: 1.454, Train Accuracy: 0.133\n",
            "[Epoch 301, Batch 200/372] Train Loss: 1.072, Train Accuracy: 0.961\n",
            "[Epoch 301, Batch 300/372] Train Loss: 1.345, Train Accuracy: 0.906\n",
            "Epoch 301 Validation Accuracy: 0.947\n",
            "[Epoch 302, Batch 100/372] Train Loss: 0.853, Train Accuracy: 1.000\n",
            "[Epoch 302, Batch 200/372] Train Loss: 0.364, Train Accuracy: 0.094\n",
            "[Epoch 302, Batch 300/372] Train Loss: 0.364, Train Accuracy: 1.000\n",
            "Epoch 302 Validation Accuracy: 0.949\n",
            "[Epoch 303, Batch 100/372] Train Loss: 0.340, Train Accuracy: 1.000\n",
            "[Epoch 303, Batch 200/372] Train Loss: 0.855, Train Accuracy: 0.125\n",
            "[Epoch 303, Batch 300/372] Train Loss: 0.434, Train Accuracy: 1.000\n",
            "Epoch 303 Validation Accuracy: 0.947\n",
            "[Epoch 304, Batch 100/372] Train Loss: 0.677, Train Accuracy: 0.125\n",
            "[Epoch 304, Batch 200/372] Train Loss: 0.490, Train Accuracy: 0.094\n",
            "[Epoch 304, Batch 300/372] Train Loss: 0.324, Train Accuracy: 0.102\n",
            "Epoch 304 Validation Accuracy: 0.944\n",
            "[Epoch 305, Batch 100/372] Train Loss: 0.635, Train Accuracy: 0.984\n",
            "[Epoch 305, Batch 200/372] Train Loss: 1.866, Train Accuracy: 0.250\n",
            "[Epoch 305, Batch 300/372] Train Loss: 0.293, Train Accuracy: 1.000\n",
            "Epoch 305 Validation Accuracy: 0.946\n",
            "[Epoch 306, Batch 100/372] Train Loss: 1.783, Train Accuracy: 0.656\n",
            "[Epoch 306, Batch 200/372] Train Loss: 1.960, Train Accuracy: 0.633\n",
            "[Epoch 306, Batch 300/372] Train Loss: 0.445, Train Accuracy: 0.102\n",
            "Epoch 306 Validation Accuracy: 0.946\n",
            "[Epoch 307, Batch 100/372] Train Loss: 0.388, Train Accuracy: 1.000\n",
            "[Epoch 307, Batch 200/372] Train Loss: 1.496, Train Accuracy: 0.156\n",
            "[Epoch 307, Batch 300/372] Train Loss: 0.442, Train Accuracy: 0.156\n",
            "Epoch 307 Validation Accuracy: 0.947\n",
            "[Epoch 308, Batch 100/372] Train Loss: 1.588, Train Accuracy: 0.156\n",
            "[Epoch 308, Batch 200/372] Train Loss: 0.365, Train Accuracy: 0.102\n",
            "[Epoch 308, Batch 300/372] Train Loss: 1.580, Train Accuracy: 0.789\n",
            "Epoch 308 Validation Accuracy: 0.947\n",
            "[Epoch 309, Batch 100/372] Train Loss: 0.301, Train Accuracy: 1.000\n",
            "[Epoch 309, Batch 200/372] Train Loss: 0.375, Train Accuracy: 1.000\n",
            "[Epoch 309, Batch 300/372] Train Loss: 1.257, Train Accuracy: 0.180\n",
            "Epoch 309 Validation Accuracy: 0.948\n",
            "[Epoch 310, Batch 100/372] Train Loss: 0.382, Train Accuracy: 1.000\n",
            "[Epoch 310, Batch 200/372] Train Loss: 0.443, Train Accuracy: 0.078\n",
            "[Epoch 310, Batch 300/372] Train Loss: 0.844, Train Accuracy: 0.969\n",
            "Epoch 310 Validation Accuracy: 0.948\n",
            "[Epoch 311, Batch 100/372] Train Loss: 0.827, Train Accuracy: 0.969\n",
            "[Epoch 311, Batch 200/372] Train Loss: 1.712, Train Accuracy: 0.180\n",
            "[Epoch 311, Batch 300/372] Train Loss: 0.379, Train Accuracy: 0.102\n",
            "Epoch 311 Validation Accuracy: 0.951\n",
            "[Epoch 312, Batch 100/372] Train Loss: 1.916, Train Accuracy: 0.453\n",
            "[Epoch 312, Batch 200/372] Train Loss: 0.962, Train Accuracy: 0.094\n",
            "[Epoch 312, Batch 300/372] Train Loss: 0.501, Train Accuracy: 1.000\n",
            "Epoch 312 Validation Accuracy: 0.949\n",
            "[Epoch 313, Batch 100/372] Train Loss: 1.711, Train Accuracy: 0.219\n",
            "[Epoch 313, Batch 200/372] Train Loss: 0.279, Train Accuracy: 0.070\n",
            "[Epoch 313, Batch 300/372] Train Loss: 1.244, Train Accuracy: 0.914\n",
            "Epoch 313 Validation Accuracy: 0.945\n",
            "[Epoch 314, Batch 100/372] Train Loss: 0.341, Train Accuracy: 1.000\n",
            "[Epoch 314, Batch 200/372] Train Loss: 0.902, Train Accuracy: 0.977\n",
            "[Epoch 314, Batch 300/372] Train Loss: 1.795, Train Accuracy: 0.203\n",
            "Epoch 314 Validation Accuracy: 0.947\n",
            "[Epoch 315, Batch 100/372] Train Loss: 1.700, Train Accuracy: 0.305\n",
            "[Epoch 315, Batch 200/372] Train Loss: 1.579, Train Accuracy: 0.891\n",
            "[Epoch 315, Batch 300/372] Train Loss: 0.727, Train Accuracy: 0.992\n",
            "Epoch 315 Validation Accuracy: 0.947\n",
            "[Epoch 316, Batch 100/372] Train Loss: 2.007, Train Accuracy: 0.203\n",
            "[Epoch 316, Batch 200/372] Train Loss: 1.276, Train Accuracy: 0.930\n",
            "[Epoch 316, Batch 300/372] Train Loss: 1.125, Train Accuracy: 0.141\n",
            "Epoch 316 Validation Accuracy: 0.948\n",
            "[Epoch 317, Batch 100/372] Train Loss: 0.367, Train Accuracy: 0.070\n",
            "[Epoch 317, Batch 200/372] Train Loss: 1.526, Train Accuracy: 0.906\n",
            "[Epoch 317, Batch 300/372] Train Loss: 2.093, Train Accuracy: 0.516\n",
            "Epoch 317 Validation Accuracy: 0.951\n",
            "[Epoch 318, Batch 100/372] Train Loss: 0.294, Train Accuracy: 1.000\n",
            "[Epoch 318, Batch 200/372] Train Loss: 1.996, Train Accuracy: 0.539\n",
            "[Epoch 318, Batch 300/372] Train Loss: 1.860, Train Accuracy: 0.398\n",
            "Epoch 318 Validation Accuracy: 0.948\n",
            "[Epoch 319, Batch 100/372] Train Loss: 1.602, Train Accuracy: 0.211\n",
            "[Epoch 319, Batch 200/372] Train Loss: 0.347, Train Accuracy: 0.094\n",
            "[Epoch 319, Batch 300/372] Train Loss: 0.637, Train Accuracy: 0.984\n",
            "Epoch 319 Validation Accuracy: 0.955\n",
            "[Epoch 320, Batch 100/372] Train Loss: 1.665, Train Accuracy: 0.273\n",
            "[Epoch 320, Batch 200/372] Train Loss: 0.790, Train Accuracy: 1.000\n",
            "[Epoch 320, Batch 300/372] Train Loss: 0.335, Train Accuracy: 0.086\n",
            "Epoch 320 Validation Accuracy: 0.948\n",
            "[Epoch 321, Batch 100/372] Train Loss: 0.280, Train Accuracy: 0.055\n",
            "[Epoch 321, Batch 200/372] Train Loss: 0.325, Train Accuracy: 0.992\n",
            "[Epoch 321, Batch 300/372] Train Loss: 1.410, Train Accuracy: 0.156\n",
            "Epoch 321 Validation Accuracy: 0.949\n",
            "[Epoch 322, Batch 100/372] Train Loss: 0.344, Train Accuracy: 0.133\n",
            "[Epoch 322, Batch 200/372] Train Loss: 0.289, Train Accuracy: 0.133\n",
            "[Epoch 322, Batch 300/372] Train Loss: 0.307, Train Accuracy: 0.133\n",
            "Epoch 322 Validation Accuracy: 0.950\n",
            "[Epoch 323, Batch 100/372] Train Loss: 1.247, Train Accuracy: 0.094\n",
            "[Epoch 323, Batch 200/372] Train Loss: 1.295, Train Accuracy: 0.148\n",
            "[Epoch 323, Batch 300/372] Train Loss: 0.848, Train Accuracy: 0.984\n",
            "Epoch 323 Validation Accuracy: 0.946\n",
            "[Epoch 324, Batch 100/372] Train Loss: 0.476, Train Accuracy: 1.000\n",
            "[Epoch 324, Batch 200/372] Train Loss: 0.444, Train Accuracy: 0.102\n",
            "[Epoch 324, Batch 300/372] Train Loss: 0.347, Train Accuracy: 0.062\n",
            "Epoch 324 Validation Accuracy: 0.946\n",
            "[Epoch 325, Batch 100/372] Train Loss: 0.433, Train Accuracy: 0.055\n",
            "[Epoch 325, Batch 200/372] Train Loss: 0.353, Train Accuracy: 0.125\n",
            "[Epoch 325, Batch 300/372] Train Loss: 0.339, Train Accuracy: 1.000\n",
            "Epoch 325 Validation Accuracy: 0.950\n",
            "Checkpoint saved at epoch 325: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_325.tar\n",
            "[Epoch 326, Batch 100/372] Train Loss: 1.900, Train Accuracy: 0.797\n",
            "[Epoch 326, Batch 200/372] Train Loss: 0.486, Train Accuracy: 1.000\n",
            "[Epoch 326, Batch 300/372] Train Loss: 0.370, Train Accuracy: 0.094\n",
            "Epoch 326 Validation Accuracy: 0.948\n",
            "[Epoch 327, Batch 100/372] Train Loss: 0.329, Train Accuracy: 1.000\n",
            "[Epoch 327, Batch 200/372] Train Loss: 1.682, Train Accuracy: 0.742\n",
            "[Epoch 327, Batch 300/372] Train Loss: 0.466, Train Accuracy: 0.992\n",
            "Epoch 327 Validation Accuracy: 0.958\n",
            "[Epoch 328, Batch 100/372] Train Loss: 0.582, Train Accuracy: 0.102\n",
            "[Epoch 328, Batch 200/372] Train Loss: 1.245, Train Accuracy: 0.148\n",
            "[Epoch 328, Batch 300/372] Train Loss: 1.667, Train Accuracy: 0.305\n",
            "Epoch 328 Validation Accuracy: 0.948\n",
            "[Epoch 329, Batch 100/372] Train Loss: 0.799, Train Accuracy: 0.117\n",
            "[Epoch 329, Batch 200/372] Train Loss: 0.357, Train Accuracy: 0.094\n",
            "[Epoch 329, Batch 300/372] Train Loss: 1.966, Train Accuracy: 0.500\n",
            "Epoch 329 Validation Accuracy: 0.953\n",
            "[Epoch 330, Batch 100/372] Train Loss: 0.505, Train Accuracy: 0.070\n",
            "[Epoch 330, Batch 200/372] Train Loss: 1.486, Train Accuracy: 0.250\n",
            "[Epoch 330, Batch 300/372] Train Loss: 1.490, Train Accuracy: 0.203\n",
            "Epoch 330 Validation Accuracy: 0.950\n",
            "[Epoch 331, Batch 100/372] Train Loss: 0.320, Train Accuracy: 0.102\n",
            "[Epoch 331, Batch 200/372] Train Loss: 1.551, Train Accuracy: 0.633\n",
            "[Epoch 331, Batch 300/372] Train Loss: 0.765, Train Accuracy: 0.977\n",
            "Epoch 331 Validation Accuracy: 0.946\n",
            "[Epoch 332, Batch 100/372] Train Loss: 1.332, Train Accuracy: 0.195\n",
            "[Epoch 332, Batch 200/372] Train Loss: 0.466, Train Accuracy: 0.109\n",
            "[Epoch 332, Batch 300/372] Train Loss: 0.261, Train Accuracy: 1.000\n",
            "Epoch 332 Validation Accuracy: 0.950\n",
            "[Epoch 333, Batch 100/372] Train Loss: 0.550, Train Accuracy: 0.102\n",
            "[Epoch 333, Batch 200/372] Train Loss: 1.137, Train Accuracy: 0.961\n",
            "[Epoch 333, Batch 300/372] Train Loss: 0.718, Train Accuracy: 0.125\n",
            "Epoch 333 Validation Accuracy: 0.947\n",
            "[Epoch 334, Batch 100/372] Train Loss: 0.496, Train Accuracy: 1.000\n",
            "[Epoch 334, Batch 200/372] Train Loss: 0.668, Train Accuracy: 0.062\n",
            "[Epoch 334, Batch 300/372] Train Loss: 0.821, Train Accuracy: 0.977\n",
            "Epoch 334 Validation Accuracy: 0.956\n",
            "[Epoch 335, Batch 100/372] Train Loss: 0.339, Train Accuracy: 0.125\n",
            "[Epoch 335, Batch 200/372] Train Loss: 1.957, Train Accuracy: 0.391\n",
            "[Epoch 335, Batch 300/372] Train Loss: 0.484, Train Accuracy: 0.992\n",
            "Epoch 335 Validation Accuracy: 0.953\n",
            "[Epoch 336, Batch 100/372] Train Loss: 0.509, Train Accuracy: 0.055\n",
            "[Epoch 336, Batch 200/372] Train Loss: 0.628, Train Accuracy: 0.992\n",
            "[Epoch 336, Batch 300/372] Train Loss: 0.426, Train Accuracy: 0.992\n",
            "Epoch 336 Validation Accuracy: 0.952\n",
            "[Epoch 337, Batch 100/372] Train Loss: 2.096, Train Accuracy: 0.352\n",
            "[Epoch 337, Batch 200/372] Train Loss: 1.216, Train Accuracy: 0.164\n",
            "[Epoch 337, Batch 300/372] Train Loss: 2.112, Train Accuracy: 0.664\n",
            "Epoch 337 Validation Accuracy: 0.950\n",
            "[Epoch 338, Batch 100/372] Train Loss: 0.429, Train Accuracy: 1.000\n",
            "[Epoch 338, Batch 200/372] Train Loss: 1.746, Train Accuracy: 0.297\n",
            "[Epoch 338, Batch 300/372] Train Loss: 0.516, Train Accuracy: 0.148\n",
            "Epoch 338 Validation Accuracy: 0.955\n",
            "[Epoch 339, Batch 100/372] Train Loss: 0.356, Train Accuracy: 1.000\n",
            "[Epoch 339, Batch 200/372] Train Loss: 1.022, Train Accuracy: 0.977\n",
            "[Epoch 339, Batch 300/372] Train Loss: 0.545, Train Accuracy: 0.102\n",
            "Epoch 339 Validation Accuracy: 0.950\n",
            "[Epoch 340, Batch 100/372] Train Loss: 0.350, Train Accuracy: 1.000\n",
            "[Epoch 340, Batch 200/372] Train Loss: 0.799, Train Accuracy: 0.992\n",
            "[Epoch 340, Batch 300/372] Train Loss: 0.333, Train Accuracy: 0.102\n",
            "Epoch 340 Validation Accuracy: 0.950\n",
            "[Epoch 341, Batch 100/372] Train Loss: 0.628, Train Accuracy: 0.992\n",
            "[Epoch 341, Batch 200/372] Train Loss: 0.748, Train Accuracy: 0.992\n",
            "[Epoch 341, Batch 300/372] Train Loss: 0.347, Train Accuracy: 0.078\n",
            "Epoch 341 Validation Accuracy: 0.952\n",
            "[Epoch 342, Batch 100/372] Train Loss: 0.258, Train Accuracy: 0.070\n",
            "[Epoch 342, Batch 200/372] Train Loss: 0.321, Train Accuracy: 1.000\n",
            "[Epoch 342, Batch 300/372] Train Loss: 0.261, Train Accuracy: 0.094\n",
            "Epoch 342 Validation Accuracy: 0.953\n",
            "[Epoch 343, Batch 100/372] Train Loss: 1.442, Train Accuracy: 0.922\n",
            "[Epoch 343, Batch 200/372] Train Loss: 0.320, Train Accuracy: 1.000\n",
            "[Epoch 343, Batch 300/372] Train Loss: 1.882, Train Accuracy: 0.281\n",
            "Epoch 343 Validation Accuracy: 0.949\n",
            "[Epoch 344, Batch 100/372] Train Loss: 0.705, Train Accuracy: 0.102\n",
            "[Epoch 344, Batch 200/372] Train Loss: 0.361, Train Accuracy: 0.117\n",
            "[Epoch 344, Batch 300/372] Train Loss: 2.170, Train Accuracy: 0.734\n",
            "Epoch 344 Validation Accuracy: 0.952\n",
            "[Epoch 345, Batch 100/372] Train Loss: 1.589, Train Accuracy: 0.234\n",
            "[Epoch 345, Batch 200/372] Train Loss: 1.974, Train Accuracy: 0.438\n",
            "[Epoch 345, Batch 300/372] Train Loss: 1.809, Train Accuracy: 0.812\n",
            "Epoch 345 Validation Accuracy: 0.952\n",
            "[Epoch 346, Batch 100/372] Train Loss: 1.735, Train Accuracy: 0.273\n",
            "[Epoch 346, Batch 200/372] Train Loss: 0.363, Train Accuracy: 0.125\n",
            "[Epoch 346, Batch 300/372] Train Loss: 0.943, Train Accuracy: 0.086\n",
            "Epoch 346 Validation Accuracy: 0.948\n",
            "[Epoch 347, Batch 100/372] Train Loss: 0.366, Train Accuracy: 1.000\n",
            "[Epoch 347, Batch 200/372] Train Loss: 0.368, Train Accuracy: 1.000\n",
            "[Epoch 347, Batch 300/372] Train Loss: 1.854, Train Accuracy: 0.258\n",
            "Epoch 347 Validation Accuracy: 0.951\n",
            "[Epoch 348, Batch 100/372] Train Loss: 0.545, Train Accuracy: 0.195\n",
            "[Epoch 348, Batch 200/372] Train Loss: 1.167, Train Accuracy: 0.156\n",
            "[Epoch 348, Batch 300/372] Train Loss: 1.082, Train Accuracy: 0.164\n",
            "Epoch 348 Validation Accuracy: 0.952\n",
            "[Epoch 349, Batch 100/372] Train Loss: 0.856, Train Accuracy: 0.102\n",
            "[Epoch 349, Batch 200/372] Train Loss: 0.308, Train Accuracy: 0.086\n",
            "[Epoch 349, Batch 300/372] Train Loss: 0.268, Train Accuracy: 1.000\n",
            "Epoch 349 Validation Accuracy: 0.951\n",
            "[Epoch 350, Batch 100/372] Train Loss: 0.488, Train Accuracy: 1.000\n",
            "[Epoch 350, Batch 200/372] Train Loss: 1.072, Train Accuracy: 0.156\n",
            "[Epoch 350, Batch 300/372] Train Loss: 1.441, Train Accuracy: 0.148\n",
            "Epoch 350 Validation Accuracy: 0.948\n",
            "Checkpoint saved at epoch 350: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_350.tar\n",
            "[Epoch 351, Batch 100/372] Train Loss: 1.633, Train Accuracy: 0.805\n",
            "[Epoch 351, Batch 200/372] Train Loss: 0.510, Train Accuracy: 0.992\n",
            "[Epoch 351, Batch 300/372] Train Loss: 0.283, Train Accuracy: 1.000\n",
            "Epoch 351 Validation Accuracy: 0.946\n",
            "[Epoch 352, Batch 100/372] Train Loss: 0.350, Train Accuracy: 1.000\n",
            "[Epoch 352, Batch 200/372] Train Loss: 1.502, Train Accuracy: 0.203\n",
            "[Epoch 352, Batch 300/372] Train Loss: 0.453, Train Accuracy: 0.070\n",
            "Epoch 352 Validation Accuracy: 0.950\n",
            "[Epoch 353, Batch 100/372] Train Loss: 0.949, Train Accuracy: 0.977\n",
            "[Epoch 353, Batch 200/372] Train Loss: 1.322, Train Accuracy: 0.914\n",
            "[Epoch 353, Batch 300/372] Train Loss: 1.879, Train Accuracy: 0.289\n",
            "Epoch 353 Validation Accuracy: 0.946\n",
            "[Epoch 354, Batch 100/372] Train Loss: 0.314, Train Accuracy: 1.000\n",
            "[Epoch 354, Batch 200/372] Train Loss: 1.835, Train Accuracy: 0.305\n",
            "[Epoch 354, Batch 300/372] Train Loss: 1.628, Train Accuracy: 0.234\n",
            "Epoch 354 Validation Accuracy: 0.950\n",
            "[Epoch 355, Batch 100/372] Train Loss: 1.406, Train Accuracy: 0.898\n",
            "[Epoch 355, Batch 200/372] Train Loss: 1.943, Train Accuracy: 0.625\n",
            "[Epoch 355, Batch 300/372] Train Loss: 1.484, Train Accuracy: 0.875\n",
            "Epoch 355 Validation Accuracy: 0.951\n",
            "[Epoch 356, Batch 100/372] Train Loss: 0.262, Train Accuracy: 1.000\n",
            "[Epoch 356, Batch 200/372] Train Loss: 1.507, Train Accuracy: 0.891\n",
            "[Epoch 356, Batch 300/372] Train Loss: 1.742, Train Accuracy: 0.711\n",
            "Epoch 356 Validation Accuracy: 0.954\n",
            "[Epoch 357, Batch 100/372] Train Loss: 0.323, Train Accuracy: 1.000\n",
            "[Epoch 357, Batch 200/372] Train Loss: 0.612, Train Accuracy: 0.086\n",
            "[Epoch 357, Batch 300/372] Train Loss: 1.878, Train Accuracy: 0.367\n",
            "Epoch 357 Validation Accuracy: 0.950\n",
            "[Epoch 358, Batch 100/372] Train Loss: 0.279, Train Accuracy: 0.086\n",
            "[Epoch 358, Batch 200/372] Train Loss: 0.436, Train Accuracy: 0.984\n",
            "[Epoch 358, Batch 300/372] Train Loss: 0.275, Train Accuracy: 0.102\n",
            "Epoch 358 Validation Accuracy: 0.945\n",
            "[Epoch 359, Batch 100/372] Train Loss: 0.990, Train Accuracy: 0.102\n",
            "[Epoch 359, Batch 200/372] Train Loss: 0.302, Train Accuracy: 0.102\n",
            "[Epoch 359, Batch 300/372] Train Loss: 0.275, Train Accuracy: 1.000\n",
            "Epoch 359 Validation Accuracy: 0.952\n",
            "[Epoch 360, Batch 100/372] Train Loss: 1.806, Train Accuracy: 0.258\n",
            "[Epoch 360, Batch 200/372] Train Loss: 1.430, Train Accuracy: 0.156\n",
            "[Epoch 360, Batch 300/372] Train Loss: 0.316, Train Accuracy: 1.000\n",
            "Epoch 360 Validation Accuracy: 0.951\n",
            "[Epoch 361, Batch 100/372] Train Loss: 1.517, Train Accuracy: 0.812\n",
            "[Epoch 361, Batch 200/372] Train Loss: 0.710, Train Accuracy: 0.125\n",
            "[Epoch 361, Batch 300/372] Train Loss: 0.310, Train Accuracy: 1.000\n",
            "Epoch 361 Validation Accuracy: 0.945\n",
            "[Epoch 362, Batch 100/372] Train Loss: 0.563, Train Accuracy: 0.992\n",
            "[Epoch 362, Batch 200/372] Train Loss: 0.514, Train Accuracy: 0.094\n",
            "[Epoch 362, Batch 300/372] Train Loss: 0.513, Train Accuracy: 0.992\n",
            "Epoch 362 Validation Accuracy: 0.952\n",
            "[Epoch 363, Batch 100/372] Train Loss: 1.947, Train Accuracy: 0.266\n",
            "[Epoch 363, Batch 200/372] Train Loss: 0.493, Train Accuracy: 1.000\n",
            "[Epoch 363, Batch 300/372] Train Loss: 0.391, Train Accuracy: 1.000\n",
            "Epoch 363 Validation Accuracy: 0.950\n",
            "[Epoch 364, Batch 100/372] Train Loss: 0.376, Train Accuracy: 0.125\n",
            "[Epoch 364, Batch 200/372] Train Loss: 1.272, Train Accuracy: 0.906\n",
            "[Epoch 364, Batch 300/372] Train Loss: 0.974, Train Accuracy: 0.078\n",
            "Epoch 364 Validation Accuracy: 0.951\n",
            "[Epoch 365, Batch 100/372] Train Loss: 0.943, Train Accuracy: 0.961\n",
            "[Epoch 365, Batch 200/372] Train Loss: 0.863, Train Accuracy: 0.156\n",
            "[Epoch 365, Batch 300/372] Train Loss: 0.645, Train Accuracy: 1.000\n",
            "Epoch 365 Validation Accuracy: 0.949\n",
            "[Epoch 366, Batch 100/372] Train Loss: 0.280, Train Accuracy: 1.000\n",
            "[Epoch 366, Batch 200/372] Train Loss: 1.835, Train Accuracy: 0.461\n",
            "[Epoch 366, Batch 300/372] Train Loss: 1.004, Train Accuracy: 0.125\n",
            "Epoch 366 Validation Accuracy: 0.950\n",
            "[Epoch 367, Batch 100/372] Train Loss: 0.332, Train Accuracy: 0.086\n",
            "[Epoch 367, Batch 200/372] Train Loss: 1.229, Train Accuracy: 0.133\n",
            "[Epoch 367, Batch 300/372] Train Loss: 0.636, Train Accuracy: 0.117\n",
            "Epoch 367 Validation Accuracy: 0.949\n",
            "[Epoch 368, Batch 100/372] Train Loss: 1.571, Train Accuracy: 0.594\n",
            "[Epoch 368, Batch 200/372] Train Loss: 0.385, Train Accuracy: 0.086\n",
            "[Epoch 368, Batch 300/372] Train Loss: 0.739, Train Accuracy: 0.992\n",
            "Epoch 368 Validation Accuracy: 0.948\n",
            "[Epoch 369, Batch 100/372] Train Loss: 0.419, Train Accuracy: 0.094\n",
            "[Epoch 369, Batch 200/372] Train Loss: 0.746, Train Accuracy: 0.992\n",
            "[Epoch 369, Batch 300/372] Train Loss: 2.007, Train Accuracy: 0.367\n",
            "Epoch 369 Validation Accuracy: 0.948\n",
            "[Epoch 370, Batch 100/372] Train Loss: 1.944, Train Accuracy: 0.719\n",
            "[Epoch 370, Batch 200/372] Train Loss: 1.679, Train Accuracy: 0.719\n",
            "[Epoch 370, Batch 300/372] Train Loss: 0.328, Train Accuracy: 0.070\n",
            "Epoch 370 Validation Accuracy: 0.939\n",
            "[Epoch 371, Batch 100/372] Train Loss: 2.198, Train Accuracy: 0.422\n",
            "[Epoch 371, Batch 200/372] Train Loss: 1.957, Train Accuracy: 0.359\n",
            "[Epoch 371, Batch 300/372] Train Loss: 0.351, Train Accuracy: 1.000\n",
            "Epoch 371 Validation Accuracy: 0.953\n",
            "[Epoch 372, Batch 100/372] Train Loss: 1.667, Train Accuracy: 0.188\n",
            "[Epoch 372, Batch 200/372] Train Loss: 0.329, Train Accuracy: 1.000\n",
            "[Epoch 372, Batch 300/372] Train Loss: 1.342, Train Accuracy: 0.891\n",
            "Epoch 372 Validation Accuracy: 0.952\n",
            "[Epoch 373, Batch 100/372] Train Loss: 0.959, Train Accuracy: 0.086\n",
            "[Epoch 373, Batch 200/372] Train Loss: 1.009, Train Accuracy: 0.133\n",
            "[Epoch 373, Batch 300/372] Train Loss: 1.652, Train Accuracy: 0.750\n",
            "Epoch 373 Validation Accuracy: 0.952\n",
            "[Epoch 374, Batch 100/372] Train Loss: 0.975, Train Accuracy: 0.945\n",
            "[Epoch 374, Batch 200/372] Train Loss: 0.801, Train Accuracy: 0.078\n",
            "[Epoch 374, Batch 300/372] Train Loss: 0.331, Train Accuracy: 1.000\n",
            "Epoch 374 Validation Accuracy: 0.952\n",
            "[Epoch 375, Batch 100/372] Train Loss: 1.244, Train Accuracy: 0.133\n",
            "[Epoch 375, Batch 200/372] Train Loss: 0.479, Train Accuracy: 0.141\n",
            "[Epoch 375, Batch 300/372] Train Loss: 1.681, Train Accuracy: 0.289\n",
            "Epoch 375 Validation Accuracy: 0.952\n",
            "Checkpoint saved at epoch 375: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_375.tar\n",
            "[Epoch 376, Batch 100/372] Train Loss: 0.637, Train Accuracy: 0.992\n",
            "[Epoch 376, Batch 200/372] Train Loss: 1.693, Train Accuracy: 0.555\n",
            "[Epoch 376, Batch 300/372] Train Loss: 0.321, Train Accuracy: 0.078\n",
            "Epoch 376 Validation Accuracy: 0.951\n",
            "[Epoch 377, Batch 100/372] Train Loss: 0.620, Train Accuracy: 0.102\n",
            "[Epoch 377, Batch 200/372] Train Loss: 1.837, Train Accuracy: 0.289\n",
            "[Epoch 377, Batch 300/372] Train Loss: 1.054, Train Accuracy: 0.172\n",
            "Epoch 377 Validation Accuracy: 0.954\n",
            "[Epoch 378, Batch 100/372] Train Loss: 1.593, Train Accuracy: 0.227\n",
            "[Epoch 378, Batch 200/372] Train Loss: 0.239, Train Accuracy: 1.000\n",
            "[Epoch 378, Batch 300/372] Train Loss: 0.255, Train Accuracy: 1.000\n",
            "Epoch 378 Validation Accuracy: 0.951\n",
            "[Epoch 379, Batch 100/372] Train Loss: 1.654, Train Accuracy: 0.867\n",
            "[Epoch 379, Batch 200/372] Train Loss: 1.856, Train Accuracy: 0.227\n",
            "[Epoch 379, Batch 300/372] Train Loss: 0.517, Train Accuracy: 0.992\n",
            "Epoch 379 Validation Accuracy: 0.950\n",
            "[Epoch 380, Batch 100/372] Train Loss: 1.350, Train Accuracy: 0.164\n",
            "[Epoch 380, Batch 200/372] Train Loss: 1.842, Train Accuracy: 0.719\n",
            "[Epoch 380, Batch 300/372] Train Loss: 2.136, Train Accuracy: 0.328\n",
            "Epoch 380 Validation Accuracy: 0.953\n",
            "[Epoch 381, Batch 100/372] Train Loss: 0.311, Train Accuracy: 0.117\n",
            "[Epoch 381, Batch 200/372] Train Loss: 0.326, Train Accuracy: 0.148\n",
            "[Epoch 381, Batch 300/372] Train Loss: 0.347, Train Accuracy: 0.094\n",
            "Epoch 381 Validation Accuracy: 0.952\n",
            "[Epoch 382, Batch 100/372] Train Loss: 0.805, Train Accuracy: 0.109\n",
            "[Epoch 382, Batch 200/372] Train Loss: 0.709, Train Accuracy: 0.133\n",
            "[Epoch 382, Batch 300/372] Train Loss: 1.710, Train Accuracy: 0.344\n",
            "Epoch 382 Validation Accuracy: 0.949\n",
            "[Epoch 383, Batch 100/372] Train Loss: 0.287, Train Accuracy: 1.000\n",
            "[Epoch 383, Batch 200/372] Train Loss: 0.409, Train Accuracy: 1.000\n",
            "[Epoch 383, Batch 300/372] Train Loss: 0.257, Train Accuracy: 0.148\n",
            "Epoch 383 Validation Accuracy: 0.954\n",
            "[Epoch 384, Batch 100/372] Train Loss: 1.300, Train Accuracy: 0.945\n",
            "[Epoch 384, Batch 200/372] Train Loss: 0.239, Train Accuracy: 0.109\n",
            "[Epoch 384, Batch 300/372] Train Loss: 0.548, Train Accuracy: 1.000\n",
            "Epoch 384 Validation Accuracy: 0.952\n",
            "[Epoch 385, Batch 100/372] Train Loss: 0.820, Train Accuracy: 0.977\n",
            "[Epoch 385, Batch 200/372] Train Loss: 0.530, Train Accuracy: 0.148\n",
            "[Epoch 385, Batch 300/372] Train Loss: 1.421, Train Accuracy: 0.883\n",
            "Epoch 385 Validation Accuracy: 0.951\n",
            "[Epoch 386, Batch 100/372] Train Loss: 0.828, Train Accuracy: 0.953\n",
            "[Epoch 386, Batch 200/372] Train Loss: 1.616, Train Accuracy: 0.680\n",
            "[Epoch 386, Batch 300/372] Train Loss: 1.051, Train Accuracy: 0.070\n",
            "Epoch 386 Validation Accuracy: 0.950\n",
            "[Epoch 387, Batch 100/372] Train Loss: 1.092, Train Accuracy: 0.125\n",
            "[Epoch 387, Batch 200/372] Train Loss: 0.326, Train Accuracy: 0.133\n",
            "[Epoch 387, Batch 300/372] Train Loss: 1.614, Train Accuracy: 0.250\n",
            "Epoch 387 Validation Accuracy: 0.949\n",
            "[Epoch 388, Batch 100/372] Train Loss: 1.707, Train Accuracy: 0.289\n",
            "[Epoch 388, Batch 200/372] Train Loss: 0.276, Train Accuracy: 0.078\n",
            "[Epoch 388, Batch 300/372] Train Loss: 2.094, Train Accuracy: 0.500\n",
            "Epoch 388 Validation Accuracy: 0.949\n",
            "[Epoch 389, Batch 100/372] Train Loss: 0.445, Train Accuracy: 1.000\n",
            "[Epoch 389, Batch 200/372] Train Loss: 0.278, Train Accuracy: 1.000\n",
            "[Epoch 389, Batch 300/372] Train Loss: 1.022, Train Accuracy: 0.109\n",
            "Epoch 389 Validation Accuracy: 0.953\n",
            "[Epoch 390, Batch 100/372] Train Loss: 0.235, Train Accuracy: 1.000\n",
            "[Epoch 390, Batch 200/372] Train Loss: 1.633, Train Accuracy: 0.695\n",
            "[Epoch 390, Batch 300/372] Train Loss: 0.268, Train Accuracy: 1.000\n",
            "Epoch 390 Validation Accuracy: 0.948\n",
            "[Epoch 391, Batch 100/372] Train Loss: 0.447, Train Accuracy: 0.117\n",
            "[Epoch 391, Batch 200/372] Train Loss: 0.372, Train Accuracy: 0.109\n",
            "[Epoch 391, Batch 300/372] Train Loss: 0.359, Train Accuracy: 1.000\n",
            "Epoch 391 Validation Accuracy: 0.956\n",
            "[Epoch 392, Batch 100/372] Train Loss: 1.874, Train Accuracy: 0.305\n",
            "[Epoch 392, Batch 200/372] Train Loss: 1.839, Train Accuracy: 0.695\n",
            "[Epoch 392, Batch 300/372] Train Loss: 1.119, Train Accuracy: 0.938\n",
            "Epoch 392 Validation Accuracy: 0.951\n",
            "[Epoch 393, Batch 100/372] Train Loss: 0.261, Train Accuracy: 0.102\n",
            "[Epoch 393, Batch 200/372] Train Loss: 1.032, Train Accuracy: 0.133\n",
            "[Epoch 393, Batch 300/372] Train Loss: 0.476, Train Accuracy: 0.992\n",
            "Epoch 393 Validation Accuracy: 0.947\n",
            "[Epoch 394, Batch 100/372] Train Loss: 0.387, Train Accuracy: 1.000\n",
            "[Epoch 394, Batch 200/372] Train Loss: 1.514, Train Accuracy: 0.242\n",
            "[Epoch 394, Batch 300/372] Train Loss: 1.263, Train Accuracy: 0.867\n",
            "Epoch 394 Validation Accuracy: 0.947\n",
            "[Epoch 395, Batch 100/372] Train Loss: 1.708, Train Accuracy: 0.664\n",
            "[Epoch 395, Batch 200/372] Train Loss: 1.158, Train Accuracy: 0.164\n",
            "[Epoch 395, Batch 300/372] Train Loss: 1.690, Train Accuracy: 0.188\n",
            "Epoch 395 Validation Accuracy: 0.954\n",
            "[Epoch 396, Batch 100/372] Train Loss: 0.287, Train Accuracy: 0.133\n",
            "[Epoch 396, Batch 200/372] Train Loss: 1.625, Train Accuracy: 0.195\n",
            "[Epoch 396, Batch 300/372] Train Loss: 0.236, Train Accuracy: 0.109\n",
            "Epoch 396 Validation Accuracy: 0.949\n",
            "[Epoch 397, Batch 100/372] Train Loss: 0.232, Train Accuracy: 0.141\n",
            "[Epoch 397, Batch 200/372] Train Loss: 1.906, Train Accuracy: 0.500\n",
            "[Epoch 397, Batch 300/372] Train Loss: 0.627, Train Accuracy: 0.984\n",
            "Epoch 397 Validation Accuracy: 0.947\n",
            "[Epoch 398, Batch 100/372] Train Loss: 0.228, Train Accuracy: 0.102\n",
            "[Epoch 398, Batch 200/372] Train Loss: 1.282, Train Accuracy: 0.930\n",
            "[Epoch 398, Batch 300/372] Train Loss: 0.194, Train Accuracy: 1.000\n",
            "Epoch 398 Validation Accuracy: 0.950\n",
            "[Epoch 399, Batch 100/372] Train Loss: 0.550, Train Accuracy: 0.117\n",
            "[Epoch 399, Batch 200/372] Train Loss: 0.380, Train Accuracy: 0.992\n",
            "[Epoch 399, Batch 300/372] Train Loss: 1.263, Train Accuracy: 0.906\n",
            "Epoch 399 Validation Accuracy: 0.950\n",
            "[Epoch 400, Batch 100/372] Train Loss: 1.934, Train Accuracy: 0.539\n",
            "[Epoch 400, Batch 200/372] Train Loss: 1.047, Train Accuracy: 0.953\n",
            "[Epoch 400, Batch 300/372] Train Loss: 0.591, Train Accuracy: 1.000\n",
            "Epoch 400 Validation Accuracy: 0.949\n",
            "Checkpoint saved at epoch 400: student_resnet50_cosine_scheduler/T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_400.tar\n",
            "Test accuracy:  0.9463\n",
            "Results saved to student_resnet50_cosine_scheduler/results_student.csv\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 400\n",
        "\n",
        "\n",
        "# Hypothetical setup, please adjust according to actual import paths and methods\n",
        "temperatures = [4]\n",
        "alphas = [1.0]\n",
        "learning_rates = [1e-3]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [0.0]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "\n",
        "\n",
        "checkpoints_path_student = 'student_resnet50_cosine_scheduler/'\n",
        "\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {}\n",
        "    hparam['alpha'] = hparam_tuple[0]\n",
        "    hparam['T'] = hparam_tuple[1]\n",
        "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
        "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
        "    hparam['weight_decay'] = hparam_tuple[3]\n",
        "    hparam['lr_decay'] = hparam_tuple[4]\n",
        "    hparam['momentum'] = hparam_tuple[5]\n",
        "    hparam['lr'] = hparam_tuple[6]\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "results_distill = {}\n",
        "pruning_factors = [0]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = checkpoints_path_student + \"results_student.csv\"\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training with hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
        "        student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
        "        prepared_student = torch.quantization.prepare_qat(student_net)\n",
        "        prepared_student.to(fast_device)\n",
        "        hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(prepared_student)\n",
        "        \n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "        results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparamMixup(\n",
        "                teacher_net, prepared_student, hparam, num_epochs,\n",
        "                train_loader, val_loader,\n",
        "                print_every=print_every,\n",
        "                fast_device=fast_device, quant=True, checkpoint_save_path= checkpoints_path_student, resume_checkpoint='student_resnet50_cosine_scheduler\\T=4, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.001, lr_decay=0.95, momentum=0.9, weight_decay=0.0_checkpoint_epoch_200.tar',\n",
        "                optimizer_choice='sgd'\n",
        "            )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        prepared_student.to('cpu')\n",
        "        prepared_student.eval()\n",
        "        \n",
        "        quantized_model = torch.quantization.convert(prepared_student)\n",
        "\n",
        "        # Final model save\n",
        "        final_save_path = checkpoints_path_student + utils.hparamToString(hparam) + '.tar'\n",
        "        torch.save({\n",
        "            'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            'model_state_dict': quantized_model.state_dict(),\n",
        "            'epoch': num_epochs\n",
        "        }, final_save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, 'cpu')\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['alpha'], hparam['T'], hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46452\\2442132776.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  info = torch.load('checkpoints_student_QAT\\T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_0_final.tar')['results']\n"
          ]
        }
      ],
      "source": [
        "info = torch.load('checkpoints_student_QAT\\T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_0_final.tar')['results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1fe6efb91d0>]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(info['val_acc'])\n",
        "plt.plot(info['train_loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (without training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.05 10623029 11181642\n",
            "Test accuracy:  0.8617\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.1\n",
            "0.1 10064428 11181642\n",
            "Test accuracy:  0.8622\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.15\n",
            "0.15 9505825 11181642\n",
            "Test accuracy:  0.862\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.2\n",
            "0.2 8947224 11181642\n",
            "Test accuracy:  0.8614\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.25\n",
            "0.25 8388631 11181642\n",
            "Test accuracy:  0.8595\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.3\n",
            "0.3 7830025 11181642\n",
            "Test accuracy:  0.8583\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.35\n",
            "0.35 7271420 11181642\n",
            "Test accuracy:  0.8582\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.4\n",
            "0.4 6712818 11181642\n",
            "Test accuracy:  0.8565\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.45\n",
            "0.45 6154222 11181642\n",
            "Test accuracy:  0.8553\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.5\n",
            "0.5 5595620 11181642\n",
            "Test accuracy:  0.8534\n",
            "Results saved to checkpoints_student/results_student_wo\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter ranges\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.90]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "\n",
        "# Prepare the list of hyperparameters\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'dropout_input': hparam_tuple[0][0],\n",
        "        'dropout_hidden': hparam_tuple[0][1],\n",
        "        'weight_decay': hparam_tuple[1],\n",
        "        'lr_decay': hparam_tuple[2],\n",
        "        'momentum': hparam_tuple[3],\n",
        "        'lr': hparam_tuple[4]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Results dictionary\n",
        "results = {}\n",
        "pruning_factors = [i/20 for i in range(1, 20)]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_student/results_student_wo.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Trainable Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training without hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net_wo = networks.StudentNetwork(pruning_factor, teacher_net)\n",
        "        student_net_wo = student_net_wo.to(fast_device)\n",
        "        #hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(student_net_wo)\n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "\n",
        "        # Train the student network\n",
        "        #results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs,\n",
        "                                                                                    #train_val_loader, None,\n",
        "                                                                                    #print_every=print_every,\n",
        "                                                                                    #fast_device=fast_device)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        #save_path = checkpoints_path_student + utils.hparamToString(hparam) + f'_pruning_{pruning_factor}_w/o_final.tar'\n",
        "        #torch.save({\n",
        "            #'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            #'model_state_dict': student_net.state_dict(),\n",
        "            #'epoch': num_epochs\n",
        "        #}, save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_wo, test_loader, fast_device)\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"your_checkpoint.tar\")\n",
        "\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(test_model, test_loader, fast_device)\n",
        "print('Test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqOQ_vv4flkh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result = pd.DataFrame(columns=['Pruning Factor', 'Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iActiWzaIgPY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import networks  # Ensure the correct import of your networks module\n",
        "import utils  # Utilities for hyperparameter string conversion and more\n",
        "\n",
        "\n",
        "# Define your hyperparameters\n",
        "t = [10]\n",
        "alpha = [0.5]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "weight_decays = [1e-5]\n",
        "learning_rate_decays = [0.95]\n",
        "momentums = [0.9]\n",
        "learning_rates = [1e-2]\n",
        "pruning_factors = [i/32 for i in range(1, 33)]\n",
        "#pruning_factors = [0.1, 0.2]  # Example pruning factors\n",
        "\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(t, alpha, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'T': hparam_tuple[0],\n",
        "        'alpha': hparam_tuple[1],\n",
        "        'dropout_input': hparam_tuple[2][0],\n",
        "        'dropout_hidden': hparam_tuple[2][1],\n",
        "        'weight_decay': hparam_tuple[3],\n",
        "        'lr_decay': hparam_tuple[4],\n",
        "        'momentum': hparam_tuple[5],\n",
        "        'lr': hparam_tuple[6]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Define the path to your checkpoints\n",
        "checkpoints_path_student = \"../content/checkpoints_student/\"\n",
        "\n",
        "# Load and set up each student model based on hyperparameters and pruning factor\n",
        "for hparam in hparams_list:\n",
        "    for prune_factor in pruning_factors:\n",
        "        filename = utils.hparamToString(hparam) + f'_pruning_{prune_factor}_final.tar'\n",
        "        load_path = checkpoints_path_student + filename\n",
        "\n",
        "        # Load the student network\n",
        "        student_net = networks.StudentNetwork(prune_amount=prune_factor)\n",
        "        student_net.load_state_dict(torch.load(load_path, map_location=fast_device, weights_only=True)['model_state_dict'])\n",
        "        student_net = student_net.to(fast_device)  # Move to the appropriate device, again adjust as needed\n",
        "\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
        "\n",
        "        # Create a new DataFrame from the data to be added\n",
        "        new_data = pd.DataFrame({'Pruning Factor': [prune_factor], 'Accuracy': [test_accuracy]})\n",
        "        # Use concat to add the new data to the existing DataFrame\n",
        "        result = pd.concat([result, new_data], ignore_index=True)\n",
        "        print('student test accuracy for ' + f'pruning factor = {prune_factor}:', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "dBh4uVqoeAno",
        "outputId": "b721ad18-ccd4-45cb-adc9-21cd1d549150"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d3956de329d1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pruning Factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy vs Pruning Factor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pruning Factor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(result['Pruning Factor'], result['Accuracy'], marker='o')\n",
        "plt.title('Accuracy vs Pruning Factor')\n",
        "plt.xlabel('Pruning Factor')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq2518KJeA5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8QIxar6eBGE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdENhUOdeBSo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VZ89F7hMfnc",
        "outputId": "276e914c-e088-4363-90a2-1db91be81b9f"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "StudentNetwork.__init__() missing 1 required positional argument: 'teacher_net'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming teacher_net and student_net are instances of TeacherNetwork and StudentNetwork, respectively\u001b[39;00m\n\u001b[0;32m      2\u001b[0m teacher_net \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mTeacherNetwork()\n\u001b[1;32m----> 3\u001b[0m student_net \u001b[38;5;241m=\u001b[39m \u001b[43mnetworks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStudentNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate and print the total number of parameters for both models\u001b[39;00m\n\u001b[0;32m      6\u001b[0m teacher_total_params \u001b[38;5;241m=\u001b[39m count_parameters(teacher_net)\n",
            "\u001b[1;31mTypeError\u001b[0m: StudentNetwork.__init__() missing 1 required positional argument: 'teacher_net'"
          ]
        }
      ],
      "source": [
        "# Assuming teacher_net and student_net are instances of TeacherNetwork and StudentNetwork, respectively\n",
        "teacher_net = networks.TeacherNetwork()\n",
        "student_net = networks.StudentNetwork(0.2)\n",
        "\n",
        "# Calculate and print the total number of parameters for both models\n",
        "teacher_total_params = count_parameters(teacher_net)\n",
        "student_total_params = count_parameters(student_net)\n",
        "\n",
        "# Calculate and print the number of zero parameters for both models\n",
        "teacher_zero_params = count_zero_parameters(teacher_net)\n",
        "student_zero_params = count_zero_parameters(student_net)\n",
        "\n",
        "print(f\"Teacher Network: {teacher_total_params} total parameters, {teacher_zero_params} are zero.\")\n",
        "print(f\"Student Network: {student_total_params} total parameters, {student_zero_params} are zero.\")\n",
        "\n",
        "# Optionally, calculate the percentage of zero parameters in each model\n",
        "teacher_zero_percent = 100 * teacher_zero_params / teacher_total_params\n",
        "student_zero_percent = 100 * student_zero_params / student_total_params\n",
        "\n",
        "print(f\"Percentage of zero parameters in Teacher Network: {teacher_zero_percent:.2f}%\")\n",
        "print(f\"Percentage of zero parameters in Student Network: {student_zero_percent:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L97WQ1QFOQkM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Classifier",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
