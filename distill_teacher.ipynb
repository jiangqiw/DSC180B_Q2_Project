{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bREGbUT_Y8F"
      },
      "source": [
        "### Import required packages and limit GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhcqwxR8_Y8I",
        "outputId": "26140e84-9279-479e-cc80-7c9a0b1a8fd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import argparse\n",
        "import time\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "#sys.path.append('/content/KD')\n",
        "# Import the module\n",
        "import networks\n",
        "import utils\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n73sKlgl_Y8K"
      },
      "outputs": [],
      "source": [
        "use_gpu = True    # set use_gpu to True if system has gpu\n",
        "gpu_id = 0        # id of gpu to be used\n",
        "cpu_device = torch.device('cpu')\n",
        "# fast_device is where computation (training, inference) happens\n",
        "fast_device = torch.device('cpu')\n",
        "if use_gpu:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'    # set visible devices depending on system configuration\n",
        "    fast_device = torch.device('cuda:' + str(gpu_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DAfPa7mw_Y8L"
      },
      "outputs": [],
      "source": [
        "def reproducibilitySeed():\n",
        "    \"\"\"\n",
        "    Ensure reproducibility of results; Seeds to 0\n",
        "    \"\"\"\n",
        "    torch_init_seed = 0\n",
        "    torch.manual_seed(torch_init_seed)\n",
        "    numpy_init_seed = 0\n",
        "    np.random.seed(numpy_init_seed)\n",
        "    if use_gpu:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reproducibilitySeed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o0sxuxkJbUEI"
      },
      "outputs": [],
      "source": [
        "checkpoints_path_teacher = 'checkpoints_teacher/'\n",
        "checkpoints_path_student = 'checkpoints_student/'\n",
        "if not os.path.exists(checkpoints_path_teacher):\n",
        "    os.makedirs(checkpoints_path_teacher)\n",
        "if not os.path.exists(checkpoints_path_student):\n",
        "    os.makedirs(checkpoints_path_student)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCz4Pup_Y8M"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO8vAtxV_Y8N",
        "outputId": "797e7d9e-fa75-47c3-977a-22b6ef78b28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set up transformations for CIFAR-10\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),  # Augment training data by padding 4 and random cropping\n",
        "        transforms.RandomHorizontalFlip(),     # Randomly flip images horizontally\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=False,\n",
        "                                            download=True, transform=transform_test)\n",
        "\n",
        "# Split the training dataset into training and validation\n",
        "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
        "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
        "\n",
        "# DataLoader setup\n",
        "batch_size = 128\n",
        "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jywk1DS_Y8O"
      },
      "source": [
        "### Train teacher network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6wDSA64C_Y8R"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "print_every = 100    # Interval size for which to print statistics of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnEnJ1A1_Y8T",
        "outputId": "3d5687b1-f814-41bf-9be6-8bd6d3c86577",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100/  391] train loss: 1.125 train accuracy: 0.648\n",
            "[1,   200/  391] train loss: 0.846 train accuracy: 0.664\n",
            "[1,   300/  391] train loss: 0.937 train accuracy: 0.680\n",
            "[2,   100/  391] train loss: 0.684 train accuracy: 0.734\n",
            "[2,   200/  391] train loss: 0.758 train accuracy: 0.758\n",
            "[2,   300/  391] train loss: 0.851 train accuracy: 0.742\n",
            "[3,   100/  391] train loss: 0.781 train accuracy: 0.703\n",
            "[3,   200/  391] train loss: 0.497 train accuracy: 0.844\n",
            "[3,   300/  391] train loss: 0.683 train accuracy: 0.766\n",
            "[4,   100/  391] train loss: 0.533 train accuracy: 0.820\n",
            "[4,   200/  391] train loss: 0.643 train accuracy: 0.773\n",
            "[4,   300/  391] train loss: 0.632 train accuracy: 0.781\n",
            "[5,   100/  391] train loss: 0.443 train accuracy: 0.844\n",
            "[5,   200/  391] train loss: 0.467 train accuracy: 0.836\n",
            "[5,   300/  391] train loss: 0.693 train accuracy: 0.758\n",
            "[6,   100/  391] train loss: 0.648 train accuracy: 0.773\n",
            "[6,   200/  391] train loss: 0.510 train accuracy: 0.820\n",
            "[6,   300/  391] train loss: 0.733 train accuracy: 0.758\n",
            "[7,   100/  391] train loss: 0.508 train accuracy: 0.828\n",
            "[7,   200/  391] train loss: 0.363 train accuracy: 0.883\n",
            "[7,   300/  391] train loss: 0.542 train accuracy: 0.797\n",
            "[8,   100/  391] train loss: 0.586 train accuracy: 0.789\n",
            "[8,   200/  391] train loss: 0.448 train accuracy: 0.844\n",
            "[8,   300/  391] train loss: 0.533 train accuracy: 0.812\n",
            "[9,   100/  391] train loss: 0.395 train accuracy: 0.875\n",
            "[9,   200/  391] train loss: 0.507 train accuracy: 0.828\n",
            "[9,   300/  391] train loss: 0.455 train accuracy: 0.844\n",
            "[10,   100/  391] train loss: 0.373 train accuracy: 0.891\n",
            "[10,   200/  391] train loss: 0.431 train accuracy: 0.844\n",
            "[10,   300/  391] train loss: 0.562 train accuracy: 0.852\n",
            "[11,   100/  391] train loss: 0.424 train accuracy: 0.875\n",
            "[11,   200/  391] train loss: 0.595 train accuracy: 0.828\n",
            "[11,   300/  391] train loss: 0.483 train accuracy: 0.828\n",
            "[12,   100/  391] train loss: 0.463 train accuracy: 0.820\n",
            "[12,   200/  391] train loss: 0.431 train accuracy: 0.844\n",
            "[12,   300/  391] train loss: 0.422 train accuracy: 0.898\n",
            "[13,   100/  391] train loss: 0.367 train accuracy: 0.859\n",
            "[13,   200/  391] train loss: 0.465 train accuracy: 0.836\n",
            "[13,   300/  391] train loss: 0.385 train accuracy: 0.859\n",
            "[14,   100/  391] train loss: 0.345 train accuracy: 0.859\n",
            "[14,   200/  391] train loss: 0.323 train accuracy: 0.867\n",
            "[14,   300/  391] train loss: 0.336 train accuracy: 0.875\n",
            "[15,   100/  391] train loss: 0.484 train accuracy: 0.805\n",
            "[15,   200/  391] train loss: 0.485 train accuracy: 0.836\n",
            "[15,   300/  391] train loss: 0.352 train accuracy: 0.891\n",
            "[16,   100/  391] train loss: 0.674 train accuracy: 0.766\n",
            "[16,   200/  391] train loss: 0.503 train accuracy: 0.820\n",
            "[16,   300/  391] train loss: 0.508 train accuracy: 0.836\n",
            "[17,   100/  391] train loss: 0.392 train accuracy: 0.859\n",
            "[17,   200/  391] train loss: 0.408 train accuracy: 0.852\n",
            "[17,   300/  391] train loss: 0.331 train accuracy: 0.883\n",
            "[18,   100/  391] train loss: 0.384 train accuracy: 0.859\n",
            "[18,   200/  391] train loss: 0.286 train accuracy: 0.906\n",
            "[18,   300/  391] train loss: 0.325 train accuracy: 0.914\n",
            "[19,   100/  391] train loss: 0.248 train accuracy: 0.914\n",
            "[19,   200/  391] train loss: 0.348 train accuracy: 0.867\n",
            "[19,   300/  391] train loss: 0.386 train accuracy: 0.852\n",
            "[20,   100/  391] train loss: 0.314 train accuracy: 0.875\n",
            "[20,   200/  391] train loss: 0.284 train accuracy: 0.938\n",
            "[20,   300/  391] train loss: 0.488 train accuracy: 0.844\n",
            "[21,   100/  391] train loss: 0.387 train accuracy: 0.875\n",
            "[21,   200/  391] train loss: 0.421 train accuracy: 0.836\n",
            "[21,   300/  391] train loss: 0.475 train accuracy: 0.836\n",
            "[22,   100/  391] train loss: 0.314 train accuracy: 0.859\n",
            "[22,   200/  391] train loss: 0.417 train accuracy: 0.836\n",
            "[22,   300/  391] train loss: 0.395 train accuracy: 0.867\n",
            "[23,   100/  391] train loss: 0.265 train accuracy: 0.898\n",
            "[23,   200/  391] train loss: 0.232 train accuracy: 0.930\n",
            "[23,   300/  391] train loss: 0.436 train accuracy: 0.828\n",
            "[24,   100/  391] train loss: 0.324 train accuracy: 0.906\n",
            "[24,   200/  391] train loss: 0.296 train accuracy: 0.883\n",
            "[24,   300/  391] train loss: 0.348 train accuracy: 0.883\n",
            "[25,   100/  391] train loss: 0.227 train accuracy: 0.922\n",
            "[25,   200/  391] train loss: 0.339 train accuracy: 0.898\n",
            "[25,   300/  391] train loss: 0.281 train accuracy: 0.906\n",
            "[26,   100/  391] train loss: 0.262 train accuracy: 0.898\n",
            "[26,   200/  391] train loss: 0.299 train accuracy: 0.891\n",
            "[26,   300/  391] train loss: 0.469 train accuracy: 0.859\n",
            "[27,   100/  391] train loss: 0.303 train accuracy: 0.922\n",
            "[27,   200/  391] train loss: 0.403 train accuracy: 0.844\n",
            "[27,   300/  391] train loss: 0.296 train accuracy: 0.875\n",
            "[28,   100/  391] train loss: 0.372 train accuracy: 0.836\n",
            "[28,   200/  391] train loss: 0.298 train accuracy: 0.906\n",
            "[28,   300/  391] train loss: 0.425 train accuracy: 0.852\n",
            "[29,   100/  391] train loss: 0.248 train accuracy: 0.898\n",
            "[29,   200/  391] train loss: 0.337 train accuracy: 0.891\n",
            "[29,   300/  391] train loss: 0.449 train accuracy: 0.859\n",
            "[30,   100/  391] train loss: 0.305 train accuracy: 0.852\n",
            "[30,   200/  391] train loss: 0.329 train accuracy: 0.859\n",
            "[30,   300/  391] train loss: 0.350 train accuracy: 0.875\n",
            "[31,   100/  391] train loss: 0.238 train accuracy: 0.891\n",
            "[31,   200/  391] train loss: 0.304 train accuracy: 0.906\n",
            "[31,   300/  391] train loss: 0.415 train accuracy: 0.859\n",
            "[32,   100/  391] train loss: 0.281 train accuracy: 0.883\n",
            "[32,   200/  391] train loss: 0.232 train accuracy: 0.922\n",
            "[32,   300/  391] train loss: 0.229 train accuracy: 0.898\n",
            "[33,   100/  391] train loss: 0.254 train accuracy: 0.922\n",
            "[33,   200/  391] train loss: 0.219 train accuracy: 0.914\n",
            "[33,   300/  391] train loss: 0.361 train accuracy: 0.891\n",
            "[34,   100/  391] train loss: 0.334 train accuracy: 0.867\n",
            "[34,   200/  391] train loss: 0.317 train accuracy: 0.875\n",
            "[34,   300/  391] train loss: 0.235 train accuracy: 0.922\n",
            "[35,   100/  391] train loss: 0.254 train accuracy: 0.898\n",
            "[35,   200/  391] train loss: 0.272 train accuracy: 0.898\n",
            "[35,   300/  391] train loss: 0.164 train accuracy: 0.953\n",
            "[36,   100/  391] train loss: 0.180 train accuracy: 0.945\n",
            "[36,   200/  391] train loss: 0.362 train accuracy: 0.875\n",
            "[36,   300/  391] train loss: 0.307 train accuracy: 0.883\n",
            "[37,   100/  391] train loss: 0.219 train accuracy: 0.938\n",
            "[37,   200/  391] train loss: 0.261 train accuracy: 0.891\n",
            "[37,   300/  391] train loss: 0.248 train accuracy: 0.914\n",
            "[38,   100/  391] train loss: 0.308 train accuracy: 0.906\n",
            "[38,   200/  391] train loss: 0.264 train accuracy: 0.906\n",
            "[38,   300/  391] train loss: 0.179 train accuracy: 0.969\n",
            "[39,   100/  391] train loss: 0.280 train accuracy: 0.875\n",
            "[39,   200/  391] train loss: 0.265 train accuracy: 0.898\n",
            "[39,   300/  391] train loss: 0.278 train accuracy: 0.906\n",
            "[40,   100/  391] train loss: 0.203 train accuracy: 0.906\n",
            "[40,   200/  391] train loss: 0.248 train accuracy: 0.938\n",
            "[40,   300/  391] train loss: 0.342 train accuracy: 0.898\n",
            "[41,   100/  391] train loss: 0.169 train accuracy: 0.953\n",
            "[41,   200/  391] train loss: 0.289 train accuracy: 0.891\n",
            "[41,   300/  391] train loss: 0.300 train accuracy: 0.898\n",
            "[42,   100/  391] train loss: 0.255 train accuracy: 0.914\n",
            "[42,   200/  391] train loss: 0.289 train accuracy: 0.898\n",
            "[42,   300/  391] train loss: 0.330 train accuracy: 0.898\n",
            "[43,   100/  391] train loss: 0.205 train accuracy: 0.945\n",
            "[43,   200/  391] train loss: 0.194 train accuracy: 0.938\n",
            "[43,   300/  391] train loss: 0.284 train accuracy: 0.914\n",
            "[44,   100/  391] train loss: 0.243 train accuracy: 0.922\n",
            "[44,   200/  391] train loss: 0.397 train accuracy: 0.883\n",
            "[44,   300/  391] train loss: 0.276 train accuracy: 0.914\n",
            "[45,   100/  391] train loss: 0.179 train accuracy: 0.922\n",
            "[45,   200/  391] train loss: 0.281 train accuracy: 0.898\n",
            "[45,   300/  391] train loss: 0.248 train accuracy: 0.922\n",
            "[46,   100/  391] train loss: 0.218 train accuracy: 0.922\n",
            "[46,   200/  391] train loss: 0.291 train accuracy: 0.906\n",
            "[46,   300/  391] train loss: 0.335 train accuracy: 0.891\n",
            "[47,   100/  391] train loss: 0.346 train accuracy: 0.859\n",
            "[47,   200/  391] train loss: 0.222 train accuracy: 0.922\n",
            "[47,   300/  391] train loss: 0.213 train accuracy: 0.922\n",
            "[48,   100/  391] train loss: 0.172 train accuracy: 0.938\n",
            "[48,   200/  391] train loss: 0.210 train accuracy: 0.922\n",
            "[48,   300/  391] train loss: 0.204 train accuracy: 0.930\n",
            "[49,   100/  391] train loss: 0.193 train accuracy: 0.930\n",
            "[49,   200/  391] train loss: 0.149 train accuracy: 0.969\n",
            "[49,   300/  391] train loss: 0.185 train accuracy: 0.938\n",
            "[50,   100/  391] train loss: 0.240 train accuracy: 0.930\n",
            "[50,   200/  391] train loss: 0.319 train accuracy: 0.898\n",
            "[50,   300/  391] train loss: 0.282 train accuracy: 0.914\n",
            "[51,   100/  391] train loss: 0.205 train accuracy: 0.922\n",
            "[51,   200/  391] train loss: 0.233 train accuracy: 0.914\n",
            "[51,   300/  391] train loss: 0.229 train accuracy: 0.930\n",
            "[52,   100/  391] train loss: 0.122 train accuracy: 0.961\n",
            "[52,   200/  391] train loss: 0.096 train accuracy: 0.961\n",
            "[52,   300/  391] train loss: 0.227 train accuracy: 0.930\n",
            "[53,   100/  391] train loss: 0.133 train accuracy: 0.961\n",
            "[53,   200/  391] train loss: 0.210 train accuracy: 0.938\n",
            "[53,   300/  391] train loss: 0.163 train accuracy: 0.961\n",
            "[54,   100/  391] train loss: 0.262 train accuracy: 0.922\n",
            "[54,   200/  391] train loss: 0.156 train accuracy: 0.953\n",
            "[54,   300/  391] train loss: 0.186 train accuracy: 0.938\n",
            "[55,   100/  391] train loss: 0.255 train accuracy: 0.922\n",
            "[55,   200/  391] train loss: 0.256 train accuracy: 0.883\n",
            "[55,   300/  391] train loss: 0.210 train accuracy: 0.945\n",
            "[56,   100/  391] train loss: 0.247 train accuracy: 0.906\n",
            "[56,   200/  391] train loss: 0.149 train accuracy: 0.945\n",
            "[56,   300/  391] train loss: 0.191 train accuracy: 0.922\n",
            "[57,   100/  391] train loss: 0.143 train accuracy: 0.961\n",
            "[57,   200/  391] train loss: 0.178 train accuracy: 0.906\n",
            "[57,   300/  391] train loss: 0.108 train accuracy: 0.969\n",
            "[58,   100/  391] train loss: 0.137 train accuracy: 0.961\n",
            "[58,   200/  391] train loss: 0.131 train accuracy: 0.945\n",
            "[58,   300/  391] train loss: 0.177 train accuracy: 0.922\n",
            "[59,   100/  391] train loss: 0.160 train accuracy: 0.922\n",
            "[59,   200/  391] train loss: 0.156 train accuracy: 0.938\n",
            "[59,   300/  391] train loss: 0.068 train accuracy: 0.984\n",
            "[60,   100/  391] train loss: 0.131 train accuracy: 0.953\n",
            "[60,   200/  391] train loss: 0.089 train accuracy: 0.984\n",
            "[60,   300/  391] train loss: 0.049 train accuracy: 0.992\n",
            "[61,   100/  391] train loss: 0.217 train accuracy: 0.930\n",
            "[61,   200/  391] train loss: 0.132 train accuracy: 0.961\n",
            "[61,   300/  391] train loss: 0.145 train accuracy: 0.953\n",
            "[62,   100/  391] train loss: 0.109 train accuracy: 0.953\n",
            "[62,   200/  391] train loss: 0.142 train accuracy: 0.945\n",
            "[62,   300/  391] train loss: 0.086 train accuracy: 0.977\n",
            "[63,   100/  391] train loss: 0.080 train accuracy: 0.969\n",
            "[63,   200/  391] train loss: 0.057 train accuracy: 0.984\n",
            "[63,   300/  391] train loss: 0.105 train accuracy: 0.961\n",
            "[64,   100/  391] train loss: 0.093 train accuracy: 0.961\n",
            "[64,   200/  391] train loss: 0.163 train accuracy: 0.945\n",
            "[64,   300/  391] train loss: 0.117 train accuracy: 0.961\n",
            "[65,   100/  391] train loss: 0.141 train accuracy: 0.938\n",
            "[65,   200/  391] train loss: 0.077 train accuracy: 0.992\n",
            "[65,   300/  391] train loss: 0.066 train accuracy: 0.977\n",
            "[66,   100/  391] train loss: 0.076 train accuracy: 0.977\n",
            "[66,   200/  391] train loss: 0.159 train accuracy: 0.969\n",
            "[66,   300/  391] train loss: 0.071 train accuracy: 0.977\n",
            "[67,   100/  391] train loss: 0.132 train accuracy: 0.969\n",
            "[67,   200/  391] train loss: 0.087 train accuracy: 0.984\n",
            "[67,   300/  391] train loss: 0.110 train accuracy: 0.961\n",
            "[68,   100/  391] train loss: 0.169 train accuracy: 0.953\n",
            "[68,   200/  391] train loss: 0.056 train accuracy: 0.984\n",
            "[68,   300/  391] train loss: 0.064 train accuracy: 0.977\n",
            "[69,   100/  391] train loss: 0.065 train accuracy: 0.984\n",
            "[69,   200/  391] train loss: 0.098 train accuracy: 0.977\n",
            "[69,   300/  391] train loss: 0.040 train accuracy: 0.992\n",
            "[70,   100/  391] train loss: 0.033 train accuracy: 1.000\n",
            "[70,   200/  391] train loss: 0.098 train accuracy: 0.961\n",
            "[70,   300/  391] train loss: 0.032 train accuracy: 1.000\n",
            "[71,   100/  391] train loss: 0.105 train accuracy: 0.953\n",
            "[71,   200/  391] train loss: 0.033 train accuracy: 0.992\n",
            "[71,   300/  391] train loss: 0.108 train accuracy: 0.961\n",
            "[72,   100/  391] train loss: 0.043 train accuracy: 0.984\n",
            "[72,   200/  391] train loss: 0.118 train accuracy: 0.953\n",
            "[72,   300/  391] train loss: 0.049 train accuracy: 0.984\n",
            "[73,   100/  391] train loss: 0.046 train accuracy: 0.977\n",
            "[73,   200/  391] train loss: 0.079 train accuracy: 0.977\n",
            "[73,   300/  391] train loss: 0.061 train accuracy: 0.977\n",
            "[74,   100/  391] train loss: 0.061 train accuracy: 0.977\n",
            "[74,   200/  391] train loss: 0.079 train accuracy: 0.977\n",
            "[74,   300/  391] train loss: 0.043 train accuracy: 0.992\n",
            "[75,   100/  391] train loss: 0.058 train accuracy: 0.977\n",
            "[75,   200/  391] train loss: 0.030 train accuracy: 0.984\n",
            "[75,   300/  391] train loss: 0.059 train accuracy: 0.977\n",
            "[76,   100/  391] train loss: 0.048 train accuracy: 0.984\n",
            "[76,   200/  391] train loss: 0.045 train accuracy: 0.977\n",
            "[76,   300/  391] train loss: 0.090 train accuracy: 0.977\n",
            "[77,   100/  391] train loss: 0.035 train accuracy: 0.992\n",
            "[77,   200/  391] train loss: 0.033 train accuracy: 0.984\n",
            "[77,   300/  391] train loss: 0.045 train accuracy: 0.984\n",
            "[78,   100/  391] train loss: 0.053 train accuracy: 0.977\n",
            "[78,   200/  391] train loss: 0.028 train accuracy: 0.984\n",
            "[78,   300/  391] train loss: 0.039 train accuracy: 0.992\n",
            "[79,   100/  391] train loss: 0.047 train accuracy: 0.977\n",
            "[79,   200/  391] train loss: 0.022 train accuracy: 1.000\n",
            "[79,   300/  391] train loss: 0.067 train accuracy: 0.969\n",
            "[80,   100/  391] train loss: 0.017 train accuracy: 1.000\n",
            "[80,   200/  391] train loss: 0.013 train accuracy: 0.992\n",
            "[80,   300/  391] train loss: 0.035 train accuracy: 0.992\n",
            "[81,   100/  391] train loss: 0.016 train accuracy: 0.992\n",
            "[81,   200/  391] train loss: 0.026 train accuracy: 0.992\n",
            "[81,   300/  391] train loss: 0.030 train accuracy: 0.984\n",
            "[82,   100/  391] train loss: 0.004 train accuracy: 1.000\n",
            "[82,   200/  391] train loss: 0.012 train accuracy: 1.000\n",
            "[82,   300/  391] train loss: 0.028 train accuracy: 1.000\n",
            "[83,   100/  391] train loss: 0.025 train accuracy: 0.992\n",
            "[83,   200/  391] train loss: 0.025 train accuracy: 0.992\n",
            "[83,   300/  391] train loss: 0.028 train accuracy: 0.984\n",
            "[84,   100/  391] train loss: 0.061 train accuracy: 0.984\n",
            "[84,   200/  391] train loss: 0.030 train accuracy: 0.992\n",
            "[84,   300/  391] train loss: 0.019 train accuracy: 0.992\n",
            "[85,   100/  391] train loss: 0.006 train accuracy: 1.000\n",
            "[85,   200/  391] train loss: 0.032 train accuracy: 0.984\n",
            "[85,   300/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[86,   100/  391] train loss: 0.034 train accuracy: 0.984\n",
            "[86,   200/  391] train loss: 0.019 train accuracy: 0.992\n",
            "[86,   300/  391] train loss: 0.006 train accuracy: 1.000\n",
            "[87,   100/  391] train loss: 0.051 train accuracy: 0.977\n",
            "[87,   200/  391] train loss: 0.008 train accuracy: 1.000\n",
            "[87,   300/  391] train loss: 0.015 train accuracy: 1.000\n",
            "[88,   100/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[88,   200/  391] train loss: 0.013 train accuracy: 0.992\n",
            "[88,   300/  391] train loss: 0.016 train accuracy: 0.992\n",
            "[89,   100/  391] train loss: 0.019 train accuracy: 0.992\n",
            "[89,   200/  391] train loss: 0.011 train accuracy: 0.992\n",
            "[89,   300/  391] train loss: 0.054 train accuracy: 0.992\n",
            "[90,   100/  391] train loss: 0.015 train accuracy: 0.992\n",
            "[90,   200/  391] train loss: 0.021 train accuracy: 0.992\n",
            "[90,   300/  391] train loss: 0.008 train accuracy: 1.000\n",
            "[91,   100/  391] train loss: 0.012 train accuracy: 1.000\n",
            "[91,   200/  391] train loss: 0.009 train accuracy: 1.000\n",
            "[91,   300/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[92,   100/  391] train loss: 0.012 train accuracy: 1.000\n",
            "[92,   200/  391] train loss: 0.019 train accuracy: 0.992\n",
            "[92,   300/  391] train loss: 0.012 train accuracy: 1.000\n",
            "[93,   100/  391] train loss: 0.008 train accuracy: 1.000\n",
            "[93,   200/  391] train loss: 0.009 train accuracy: 1.000\n",
            "[93,   300/  391] train loss: 0.005 train accuracy: 1.000\n",
            "[94,   100/  391] train loss: 0.008 train accuracy: 1.000\n",
            "[94,   200/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[94,   300/  391] train loss: 0.008 train accuracy: 1.000\n",
            "[95,   100/  391] train loss: 0.014 train accuracy: 1.000\n",
            "[95,   200/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[95,   300/  391] train loss: 0.030 train accuracy: 0.984\n",
            "[96,   100/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[96,   200/  391] train loss: 0.011 train accuracy: 0.992\n",
            "[96,   300/  391] train loss: 0.004 train accuracy: 1.000\n",
            "[97,   100/  391] train loss: 0.005 train accuracy: 1.000\n",
            "[97,   200/  391] train loss: 0.007 train accuracy: 1.000\n",
            "[97,   300/  391] train loss: 0.005 train accuracy: 1.000\n",
            "[98,   100/  391] train loss: 0.006 train accuracy: 1.000\n",
            "[98,   200/  391] train loss: 0.007 train accuracy: 1.000\n",
            "[98,   300/  391] train loss: 0.028 train accuracy: 0.984\n",
            "[99,   100/  391] train loss: 0.006 train accuracy: 1.000\n",
            "[99,   200/  391] train loss: 0.011 train accuracy: 1.000\n",
            "[99,   300/  391] train loss: 0.005 train accuracy: 1.000\n",
            "[100,   100/  391] train loss: 0.024 train accuracy: 0.992\n",
            "[100,   200/  391] train loss: 0.028 train accuracy: 0.992\n",
            "[100,   300/  391] train loss: 0.029 train accuracy: 0.984\n",
            "Test accuracy:  0.8654\n",
            "Results saved to results.csv\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter ranges\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.90]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "\n",
        "# Prepare the list of hyperparameters\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'dropout_input': hparam_tuple[0][0],\n",
        "        'dropout_hidden': hparam_tuple[0][1],\n",
        "        'weight_decay': hparam_tuple[1],\n",
        "        'lr_decay': hparam_tuple[2],\n",
        "        'momentum': hparam_tuple[3],\n",
        "        'lr': hparam_tuple[4]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Results dictionary\n",
        "results = {}\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_teacher/results_teacher\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for hparam in hparams_list:\n",
        "    print('Training with hparams' + utils.hparamToString(hparam))\n",
        "\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    reproducibilitySeed()\n",
        "    teacher_net = networks.TeacherNetwork()\n",
        "    teacher_net = teacher_net.to(fast_device)\n",
        "\n",
        "    hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "    results[hparam_tuple] = utils.trainTeacherOnHparam(teacher_net, hparam, num_epochs, train_val_loader, None, print_every=print_every, fast_device=fast_device)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Save model\n",
        "    save_path = checkpoints_path_teacher + utils.hparamToString(hparam) + '_final.tar'\n",
        "    torch.save({\n",
        "        'results': results[hparam_tuple],\n",
        "        'model_state_dict': teacher_net.state_dict(),\n",
        "        'epoch': num_epochs\n",
        "    }, save_path)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    _, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "    print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "    # Write results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "            hparam['lr_decay'], hparam['momentum'], hparam['lr'],\n",
        "            test_accuracy, training_time\n",
        "        ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load pre-trained teacher network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\17598\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\17598\\AppData\\Local\\Temp\\ipykernel_4932\\2752411799.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(teacher_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy:  0.8654\n"
          ]
        }
      ],
      "source": [
        "# Path to the saved model\n",
        "teacher_path = \"checkpoints_teacher/dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_final2.tar\"\n",
        "\n",
        "# Initialize the network\n",
        "teacher_net = networks.TeacherNetwork()\n",
        "teacher_net = teacher_net.to(fast_device)\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(teacher_path)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "teacher_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# pre-trained teacher accuracy\n",
        "reproducibilitySeed()\n",
        "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
        "print('test accuracy: ', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rayk6sDh7UXz"
      },
      "source": [
        "### Student Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (with training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hlvtw4Xkxhoh"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "print_every = 100    #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the total number of trainable parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters.\n",
        "    \"\"\"\n",
        "    return sum((p.data != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def count_zero_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the number of trainable parameters that are exactly zero in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model whose zero parameters need to be counted.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters that are exactly zero.\n",
        "    \"\"\"\n",
        "    return sum((p.data == 0).sum().item() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11181642"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_parameters(teacher_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAXvRE-7a9bC",
        "outputId": "86382da5-7836-4b1f-e4e2-a1dd2ec8f8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with hparamsT=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.05\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'count_parameters' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m hparam_tuple \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mhparamDictToTuple(hparam)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Count parameters\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m student_params_num \u001b[38;5;241m=\u001b[39m \u001b[43mcount_parameters\u001b[49m(student_net)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(pruning_factor, student_params_num, count_parameters(teacher_net))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the student network\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'count_parameters' is not defined"
          ]
        }
      ],
      "source": [
        "# Hypothetical setup, please adjust according to actual import paths and methods\n",
        "temperatures = [10]\n",
        "alphas = [0.5]\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.9]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "hparams_list = []\n",
        "\n",
        "for hparam_tuple in itertools.product(alphas, temperatures, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {}\n",
        "    hparam['alpha'] = hparam_tuple[0]\n",
        "    hparam['T'] = hparam_tuple[1]\n",
        "    hparam['dropout_input'] = hparam_tuple[2][0]\n",
        "    hparam['dropout_hidden'] = hparam_tuple[2][1]\n",
        "    hparam['weight_decay'] = hparam_tuple[3]\n",
        "    hparam['lr_decay'] = hparam_tuple[4]\n",
        "    hparam['momentum'] = hparam_tuple[5]\n",
        "    hparam['lr'] = hparam_tuple[6]\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "results_distill = {}\n",
        "pruning_factors = [i/20 for i in range(1, 11)]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_student/results_student.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Alpha\", \"Temperature\", \"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Zero Parameters\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training with hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net = networks.StudentNetwork(pruning_factor, teacher_net)\n",
        "        student_net = student_net.to(fast_device)\n",
        "        hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(student_net)\n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "\n",
        "        # Train the student network\n",
        "        results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs,\n",
        "                                                                                    train_val_loader, None,\n",
        "                                                                                    print_every=print_every,\n",
        "                                                                                    fast_device=fast_device)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        save_path = checkpoints_path_student + utils.hparamToString(hparam) + f'_pruning_{pruning_factor}_final.tar'\n",
        "        torch.save({\n",
        "            'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            'model_state_dict': student_net.state_dict(),\n",
        "            'epoch': num_epochs\n",
        "        }, save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['alpha'], hparam['T'], hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Student Network (without training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.05\n",
            "0.05 10623029 11181642\n",
            "Test accuracy:  0.8655\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.1\n",
            "0.1 10064427 11181642\n",
            "Test accuracy:  0.865\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.15\n",
            "0.15 9505824 11181642\n",
            "Test accuracy:  0.8653\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.2\n",
            "0.2 8947224 11181642\n",
            "Test accuracy:  0.8651\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.25\n",
            "0.25 8388630 11181642\n",
            "Test accuracy:  0.8648\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.3\n",
            "0.3 7830026 11181642\n",
            "Test accuracy:  0.8629\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.35\n",
            "0.35 7271419 11181642\n",
            "Test accuracy:  0.8615\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.4\n",
            "0.4 6712819 11181642\n",
            "Test accuracy:  0.8617\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.45\n",
            "0.45 6154221 11181642\n",
            "Test accuracy:  0.8616\n",
            "Training without hparamsdropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001 and pruning factor 0.5\n",
            "0.5 5595619 11181642\n",
            "Test accuracy:  0.8598\n",
            "Results saved to checkpoints_student/results_student_wo\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter ranges\n",
        "learning_rates = [1e-2]\n",
        "learning_rate_decays = [0.95]\n",
        "weight_decays = [1e-3]\n",
        "momentums = [0.90]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "\n",
        "# Prepare the list of hyperparameters\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'dropout_input': hparam_tuple[0][0],\n",
        "        'dropout_hidden': hparam_tuple[0][1],\n",
        "        'weight_decay': hparam_tuple[1],\n",
        "        'lr_decay': hparam_tuple[2],\n",
        "        'momentum': hparam_tuple[3],\n",
        "        'lr': hparam_tuple[4]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Results dictionary\n",
        "results = {}\n",
        "pruning_factors = [i/20 for i in range(1, 11)]\n",
        "\n",
        "# CSV file setup\n",
        "csv_file = \"checkpoints_student/results_student_wo\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Dropout Input\", \"Dropout Hidden\", \"Weight Decay\", \"LR Decay\", \"Momentum\", \"Learning Rate\", \"Pruning Factor\", \"Test Accuracy\", \"Training Time (s)\"])\n",
        "\n",
        "# Training and logging\n",
        "for pruning_factor in pruning_factors:\n",
        "    for hparam in hparams_list:\n",
        "        print('Training without hparams' + utils.hparamToString(hparam) + f' and pruning factor {pruning_factor}')\n",
        "\n",
        "        # Measure training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        reproducibilitySeed()\n",
        "        student_net_wo = networks.StudentNetwork(pruning_factor, teacher_net)\n",
        "        student_net_wo = student_net_wo.to(fast_device)\n",
        "        #hparam_tuple = utils.hparamDictToTuple(hparam)\n",
        "\n",
        "        # Count parameters\n",
        "        student_params_num = count_parameters(student_net_wo)\n",
        "        print(pruning_factor, student_params_num, count_parameters(teacher_net))\n",
        "\n",
        "        # Train the student network\n",
        "        #results_distill[(hparam_tuple, pruning_factor)] = utils.trainStudentOnHparam(teacher_net, student_net, hparam, num_epochs,\n",
        "                                                                                    #train_val_loader, None,\n",
        "                                                                                    #print_every=print_every,\n",
        "                                                                                    #fast_device=fast_device)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Save model\n",
        "        #save_path = checkpoints_path_student + utils.hparamToString(hparam) + f'_pruning_{pruning_factor}_w/o_final.tar'\n",
        "        #torch.save({\n",
        "            #'results': results_distill[(hparam_tuple, pruning_factor)],\n",
        "            #'model_state_dict': student_net.state_dict(),\n",
        "            #'epoch': num_epochs\n",
        "        #}, save_path)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_wo, test_loader, fast_device)\n",
        "        print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "        # Write results to CSV\n",
        "        with open(csv_file, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                hparam['dropout_input'], hparam['dropout_hidden'], hparam['weight_decay'],\n",
        "                hparam['lr_decay'], hparam['momentum'], hparam['lr'], pruning_factor, student_params_num,\n",
        "                test_accuracy, training_time\n",
        "            ])\n",
        "\n",
        "print(f\"Results saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqOQ_vv4flkh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "result = pd.DataFrame(columns=['Pruning Factor', 'Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iActiWzaIgPY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import networks  # Ensure the correct import of your networks module\n",
        "import utils  # Utilities for hyperparameter string conversion and more\n",
        "\n",
        "\n",
        "# Define your hyperparameters\n",
        "t = [10]\n",
        "alpha = [0.5]\n",
        "dropout_probabilities = [(0.0, 0.0)]\n",
        "weight_decays = [1e-5]\n",
        "learning_rate_decays = [0.95]\n",
        "momentums = [0.9]\n",
        "learning_rates = [1e-2]\n",
        "pruning_factors = [i/32 for i in range(1, 33)]\n",
        "#pruning_factors = [0.1, 0.2]  # Example pruning factors\n",
        "\n",
        "hparams_list = []\n",
        "for hparam_tuple in itertools.product(t, alpha, dropout_probabilities, weight_decays, learning_rate_decays, momentums, learning_rates):\n",
        "    hparam = {\n",
        "        'T': hparam_tuple[0],\n",
        "        'alpha': hparam_tuple[1],\n",
        "        'dropout_input': hparam_tuple[2][0],\n",
        "        'dropout_hidden': hparam_tuple[2][1],\n",
        "        'weight_decay': hparam_tuple[3],\n",
        "        'lr_decay': hparam_tuple[4],\n",
        "        'momentum': hparam_tuple[5],\n",
        "        'lr': hparam_tuple[6]\n",
        "    }\n",
        "    hparams_list.append(hparam)\n",
        "\n",
        "# Define the path to your checkpoints\n",
        "checkpoints_path_student = \"../content/checkpoints_student/\"\n",
        "\n",
        "# Load and set up each student model based on hyperparameters and pruning factor\n",
        "for hparam in hparams_list:\n",
        "    for prune_factor in pruning_factors:\n",
        "        filename = utils.hparamToString(hparam) + f'_pruning_{prune_factor}_final.tar'\n",
        "        load_path = checkpoints_path_student + filename\n",
        "\n",
        "        # Load the student network\n",
        "        student_net = networks.StudentNetwork(prune_amount=prune_factor)\n",
        "        student_net.load_state_dict(torch.load(load_path, map_location=fast_device, weights_only=True)['model_state_dict'])\n",
        "        student_net = student_net.to(fast_device)  # Move to the appropriate device, again adjust as needed\n",
        "\n",
        "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
        "\n",
        "        # Create a new DataFrame from the data to be added\n",
        "        new_data = pd.DataFrame({'Pruning Factor': [prune_factor], 'Accuracy': [test_accuracy]})\n",
        "        # Use concat to add the new data to the existing DataFrame\n",
        "        result = pd.concat([result, new_data], ignore_index=True)\n",
        "        print('student test accuracy for ' + f'pruning factor = {prune_factor}:', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "dBh4uVqoeAno",
        "outputId": "b721ad18-ccd4-45cb-adc9-21cd1d549150"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d3956de329d1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pruning Factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy vs Pruning Factor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pruning Factor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(result['Pruning Factor'], result['Accuracy'], marker='o')\n",
        "plt.title('Accuracy vs Pruning Factor')\n",
        "plt.xlabel('Pruning Factor')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq2518KJeA5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8QIxar6eBGE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdENhUOdeBSo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VZ89F7hMfnc",
        "outputId": "276e914c-e088-4363-90a2-1db91be81b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher Network: 58164298 total parameters, 0 are zero.\n",
            "Student Network: 58164298 total parameters, 11602625 are zero.\n",
            "Percentage of zero parameters in Teacher Network: 0.00%\n",
            "Percentage of zero parameters in Student Network: 19.95%\n"
          ]
        }
      ],
      "source": [
        "# Assuming teacher_net and student_net are instances of TeacherNetwork and StudentNetwork, respectively\n",
        "teacher_net = networks.TeacherNetwork()\n",
        "student_net = networks.StudentNetwork(0.2)\n",
        "\n",
        "# Calculate and print the total number of parameters for both models\n",
        "teacher_total_params = count_parameters(teacher_net)\n",
        "student_total_params = count_parameters(student_net)\n",
        "\n",
        "# Calculate and print the number of zero parameters for both models\n",
        "teacher_zero_params = count_zero_parameters(teacher_net)\n",
        "student_zero_params = count_zero_parameters(student_net)\n",
        "\n",
        "print(f\"Teacher Network: {teacher_total_params} total parameters, {teacher_zero_params} are zero.\")\n",
        "print(f\"Student Network: {student_total_params} total parameters, {student_zero_params} are zero.\")\n",
        "\n",
        "# Optionally, calculate the percentage of zero parameters in each model\n",
        "teacher_zero_percent = 100 * teacher_zero_params / teacher_total_params\n",
        "student_zero_percent = 100 * student_zero_params / student_total_params\n",
        "\n",
        "print(f\"Percentage of zero parameters in Teacher Network: {teacher_zero_percent:.2f}%\")\n",
        "print(f\"Percentage of zero parameters in Student Network: {student_zero_percent:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L97WQ1QFOQkM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
