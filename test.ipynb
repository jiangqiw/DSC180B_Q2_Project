{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'checkpoints_student/T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_0.1_final.tar'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "#sys.path.append('/content/KD')\n",
    "# Import the module\n",
    "import networks\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True    # set use_gpu to True if system has gpu\n",
    "gpu_id = 0        # id of gpu to be used\n",
    "cpu_device = torch.device('cpu')\n",
    "# fast_device is where computation (training, inference) happens\n",
    "fast_device = torch.device('cpu')\n",
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'    # set visible devices depending on system configuration\n",
    "    fast_device = torch.device('cuda:' + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "    if use_gpu:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 4,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up transformations for CIFAR-10\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # Augment training data by padding 4 and random cropping\n",
    "        transforms.RandomHorizontalFlip(),     # Randomly flip images horizontally\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=False,\n",
    "                                            download=True, transform=transform_test)\n",
    "\n",
    "# Split the training dataset into training and validation\n",
    "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
    "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
    "\n",
    "# DataLoader setup\n",
    "batch_size = 128\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 16,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
<<<<<<< HEAD
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_56004\\4278446609.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
=======
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_49368\\4278446609.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
>>>>>>> QAT_experiments
      "  checkpoint = torch.load(teacher_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8616\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved model\n",
    "teacher_path = \"checkpoints_teacher/dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_final.tar\"\n",
    "\n",
    "# Initialize the network\n",
    "teacher_net = networks.TeacherNetwork()\n",
    "teacher_net = teacher_net.to(fast_device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(teacher_path)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "teacher_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# pre-trained teacher accuracy\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from quantize_neural_net import QuantizeNeuralNet\n",
    "pruning_factors = [i/20 for i in range(1, 20)]\n",
    "\n",
    "\n",
    "# Path to the saved model\n",
    "student_path_template = \"checkpoints_student/T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_{scaling_factor}_final.tar\"\n",
    "# CSV file to save results\n",
    "output_csv = \"pruning_quantization_results.csv\"\n",
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_49368\\1596229990.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(student_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8604\n"
     ]
    }
   ],
   "source": [
    "from quantize_neural_net import QuantizeNeuralNet\n",
    "pruning_factors = [i/20 for i in range(1, 11)]\n",
    "\n",
    "\n",
    "# Path to the saved model\n",
    "student_path = \"checkpoints_student/T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_0.4_final.tar\"\n",
    "\n",
    "# Initialize the network\n",
    "student_model = networks.StudentNetwork(.4, teacher_net)\n",
    "student_model = student_model.to(fast_device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(student_path)\n",
    "\n",
    "student_model.load_state_dict(checkpoint['model_state_dict'])\n",
>>>>>>> QAT_experiments
    "\n",
    "# Ensure reproducibility and evaluate pre-trained teacher accuracy\n",
    "reproducibilitySeed()\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Pruning Factor\", \"Pre-Quantized Test Accuracy\", \"Post-Quantized Test Accuracy\"])\n",
    "\n",
    "    # Loop through pruning factors\n",
    "    for pruning_factor in pruning_factors:\n",
    "        print(f\"Processing pruning factor: {pruning_factor}\")\n",
    "\n",
    "        # Generate the student path based on the pruning factor\n",
    "        student_path = student_path_template.format(scaling_factor=pruning_factor)\n",
    "\n",
    "        # Initialize the student network\n",
    "        student_model = networks.StudentNetwork(pruning_factor, teacher_net)\n",
    "        student_model = student_model.to(fast_device)\n",
    "\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(student_path)\n",
    "        student_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Evaluate pre-trained student accuracy\n",
    "        _, pre_quantized_accuracy = utils.getLossAccuracyOnDataset(student_model, test_loader, fast_device)\n",
    "        print(f\"Initial Test Accuracy for pruning factor {pruning_factor}: {pre_quantized_accuracy}\")\n",
    "\n",
    "        # Quantize the network\n",
    "        quantizer = QuantizeNeuralNet(\n",
    "            student_model.model.model,\n",
    "            'resnet18',\n",
    "            batch_size=128,\n",
    "            data_loader=train_loader,\n",
    "            mlp_bits=4,\n",
    "            cnn_bits=4,\n",
    "            ignore_layers=[],\n",
    "            mlp_alphabet_scalar=1.16,\n",
    "            cnn_alphabet_scalar=1.16,\n",
    "            mlp_percentile=1,\n",
    "            cnn_percentile=1,\n",
    "            reg=None,\n",
    "            lamb=0.1,\n",
    "            retain_rate=0.25,\n",
    "            stochastic_quantization=False,\n",
    "            device=fast_device\n",
    "        )\n",
    "\n",
    "        quantized_model = quantizer.quantize_network(verbose=False)\n",
    "\n",
    "        # Evaluate the quantized model's accuracy\n",
    "        _, post_quantized_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, fast_device)\n",
    "        print(f\"Quantized Test Accuracy for pruning factor {pruning_factor}: {post_quantized_accuracy}\")\n",
    "\n",
    "        # Write the result to the CSV file\n",
    "        writer.writerow([pruning_factor, pre_quantized_accuracy, post_quantized_accuracy])\n",
    "\n",
    "print(f\"Results saved to {output_csv}\")"
=======
    "_, test_accuracy = utils.getLossAccuracyOnDataset(student_model, test_loader, fast_device)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "quantizer = QuantizeNeuralNet(student_model.model.model,\n",
    "    'resnet18',  # Default from `-model`\n",
    "    batch_size=128,  # Default from `--batch_size`\n",
    "    data_loader=train_loader,\n",
    "    mlp_bits=4,  # Default from `--bits`\n",
    "    cnn_bits=4,  # Default from `--bits`\n",
    "    ignore_layers=[],  # Default from `--ignore_layer`\n",
    "    mlp_alphabet_scalar=1.16,  # Default from `--scalar`\n",
    "    cnn_alphabet_scalar=1.16,  # Default from `--scalar`\n",
    "    mlp_percentile=1,  # Default from `--percentile`\n",
    "    cnn_percentile=1,  # Default from `--percentile`\n",
    "    reg=None,  # Default from `--regularizer`\n",
    "    lamb=0.1,  # Default from `--lamb`\n",
    "    retain_rate=0.25,  # Default from `--retain_rate`\n",
    "    stochastic_quantization=False,  # Default from `--stochastic_quantization`\n",
    "    device=fast_device\n",
    ")\n",
    "\n",
    "quantized_model = quantizer.quantize_network()\n",
    "\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, fast_device)\n",
    "print('Test accuracy:', test_accuracy)"
>>>>>>> QAT_experiments
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 22,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the total number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose parameters need to be counted.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum((p.data != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_zero_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable parameters that are exactly zero in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose zero parameters need to be counted.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of trainable parameters that are exactly zero.\n",
    "    \"\"\"\n",
    "    return sum((p.data == 0).sum().item() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m count_parameters(\u001b[43mstudent_model\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'student_model' is not defined"
     ]
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6712822"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> QAT_experiments
    }
   ],
   "source": [
    "count_parameters(student_model)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 42,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantize_neural_net import QuantizeNeuralNet\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('pruning_quantization_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the bar graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar_width = 0.4\n",
    "pruning_factors = data[\"Pruning Factor\"]\n",
    "index = range(len(pruning_factors))\n",
    "\n",
    "# Bars for pre-quantized and post-quantized test accuracy\n",
    "ax.bar([i - bar_width / 2 for i in index], data[\"Pre-Quantized Test Accuracy\"], \n",
    "       bar_width, label=\"Pre-Quantized Test Accuracy\")\n",
    "ax.bar([i + bar_width / 2 for i in index], data[\"Post-Quantized Test Accuracy\"], \n",
    "       bar_width, label=\"Post-Quantized Test Accuracy\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel(\"Pruning Factor\", fontsize=12)\n",
    "ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "ax.set_title(\"Comparison of Pre and Post Quantized Test Accuracy\", fontsize=14)\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(pruning_factors)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 1.1191e-02,  7.4928e-02,  5.8481e-02],\n",
       "                        [ 3.2938e-02,  1.4506e-01,  1.2211e-01],\n",
       "                        [ 1.8626e-02,  1.0540e-01,  8.1533e-02]],\n",
       "              \n",
       "                       [[ 1.1426e-02, -1.1192e-01, -6.9946e-02],\n",
       "                        [-1.1474e-01, -2.5705e-01, -1.9169e-01],\n",
       "                        [-4.8331e-02, -1.8667e-01, -1.2167e-01]],\n",
       "              \n",
       "                       [[-1.6111e-02,  4.6198e-02,  1.0609e-02],\n",
       "                        [ 1.5038e-02,  1.0377e-01,  6.2882e-02],\n",
       "                        [-1.8518e-02,  4.0093e-02,  2.3421e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1905e-02, -3.0173e-03, -1.5890e-02],\n",
       "                        [-8.8170e-03,  2.1867e-03, -8.0797e-03],\n",
       "                        [-1.2802e-02,  2.3540e-04, -1.3936e-03]],\n",
       "              \n",
       "                       [[-6.0979e-03,  2.1129e-03, -1.2489e-02],\n",
       "                        [-1.1067e-03,  1.0083e-02, -4.0298e-03],\n",
       "                        [-8.1399e-03,  3.9864e-03, -4.1813e-04]],\n",
       "              \n",
       "                       [[-5.2221e-03,  2.2799e-03, -1.1582e-02],\n",
       "                        [ 1.3158e-03,  9.0331e-03, -3.0306e-03],\n",
       "                        [-7.6475e-03,  1.8812e-03, -1.5350e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2411e-02,  3.0665e-02,  1.7959e-02],\n",
       "                        [ 4.3973e-04, -2.4715e-02,  5.3631e-03],\n",
       "                        [ 1.6869e-02,  7.3507e-03,  1.0416e-02]],\n",
       "              \n",
       "                       [[ 1.8492e-02,  2.6268e-02,  1.2964e-02],\n",
       "                        [ 4.2507e-03, -1.3913e-02,  1.3126e-02],\n",
       "                        [ 3.1519e-02,  3.6533e-02,  3.1146e-02]],\n",
       "              \n",
       "                       [[ 3.4228e-03, -7.4890e-03, -1.3061e-02],\n",
       "                        [-1.6298e-02, -5.0854e-02, -3.0189e-02],\n",
       "                        [ 6.7289e-03, -1.0134e-02, -1.0944e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-9.4614e-03, -3.5172e-03, -5.1002e-03],\n",
       "                        [ 1.0792e-02,  1.7171e-02,  1.2629e-02],\n",
       "                        [ 1.0392e-02,  1.3560e-02,  1.3235e-02]],\n",
       "              \n",
       "                       [[-7.2486e-04,  1.2934e-02,  4.4110e-03],\n",
       "                        [ 2.4657e-02,  4.2396e-02,  2.9031e-02],\n",
       "                        [ 8.7716e-03,  1.9883e-02,  1.6931e-02]],\n",
       "              \n",
       "                       [[ 6.3696e-03,  6.4211e-03,  5.8138e-04],\n",
       "                        [ 8.5513e-03,  1.2403e-03, -1.2745e-03],\n",
       "                        [-2.0306e-02, -3.4306e-02, -2.7646e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2091e-02, -2.3332e-02, -2.7688e-02],\n",
       "                        [-2.3591e-02, -8.2670e-03, -9.9228e-03],\n",
       "                        [-2.5802e-03,  1.4330e-02,  1.8067e-02]],\n",
       "              \n",
       "                       [[-3.3138e-02, -2.5507e-02, -3.0338e-02],\n",
       "                        [-1.2361e-02,  1.2482e-02,  8.1061e-03],\n",
       "                        [-8.6268e-03,  1.7897e-02,  2.6580e-02]],\n",
       "              \n",
       "                       [[ 2.4155e-02,  4.3438e-02,  2.3945e-02],\n",
       "                        [ 4.9986e-02,  7.4620e-02,  6.5783e-02],\n",
       "                        [ 1.6978e-02,  3.6810e-02,  4.5643e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5443e-03,  2.9892e-02,  3.8468e-02],\n",
       "                        [ 3.3571e-02,  1.4507e-01,  5.7450e-02],\n",
       "                        [ 3.6625e-02,  8.0595e-02,  5.4581e-02]],\n",
       "              \n",
       "                       [[-8.7992e-03, -8.7617e-02, -4.5929e-02],\n",
       "                        [-8.1635e-02, -1.1291e-01, -1.4961e-01],\n",
       "                        [-4.9648e-02, -1.4680e-01, -1.0390e-01]],\n",
       "              \n",
       "                       [[ 1.5776e-02,  5.8240e-02,  3.4926e-02],\n",
       "                        [ 6.5241e-02,  1.7920e-01,  8.4552e-02],\n",
       "                        [ 3.8666e-02,  7.9529e-02,  4.8527e-02]]]])),\n",
       "             ('bn1.weight',\n",
       "              tensor([1.1848e-01, 2.1771e-02, 4.5014e-02, 1.5484e-01, 1.2976e-01, 5.1115e-06,\n",
       "                      2.0679e-01, 1.8349e-01, 6.8721e-06, 2.0124e-02, 6.2265e-02, 8.8732e-02,\n",
       "                      2.6850e-03, 3.0027e-02, 7.3188e-03, 1.2835e-01, 1.9702e-01, 1.2440e-01,\n",
       "                      2.6917e-01, 1.5933e-01, 2.4062e-01, 2.1157e-01, 1.5395e-01, 2.2893e-01,\n",
       "                      2.5095e-01, 2.7757e-01, 1.7677e-01, 2.0688e-01, 1.6619e-01, 4.0362e-02,\n",
       "                      3.0732e-01, 1.6511e-01, 1.9407e-02, 2.3582e-01, 2.5586e-01, 1.3994e-01,\n",
       "                      1.5143e-01, 5.8655e-02, 6.9879e-06, 1.9589e-01, 7.9122e-02, 1.3402e-01,\n",
       "                      8.6138e-02, 1.9744e-01, 2.8773e-01, 1.0234e-01, 1.2370e-05, 2.6849e-02,\n",
       "                      1.8433e-01, 6.1179e-02, 1.8231e-01, 1.2828e-01, 9.8935e-02, 4.3494e-02,\n",
       "                      3.3144e-01, 9.3585e-02, 6.5655e-02, 5.3369e-02, 9.4509e-02, 1.2246e-01,\n",
       "                      1.1204e-01, 7.7206e-02, 1.1069e-01, 1.0762e-01])),\n",
       "             ('bn1.bias',\n",
       "              tensor([ 1.2599e-01, -3.4384e-02, -2.9394e-02, -8.9820e-02, -3.9544e-02,\n",
       "                      -2.1529e-05, -4.1791e-02,  2.6632e-01, -1.3641e-05, -1.5420e-02,\n",
       "                      -3.0326e-02, -4.8177e-02, -5.0512e-03, -1.6798e-02, -2.4768e-03,\n",
       "                      -5.8039e-02,  2.6897e-01, -1.1260e-01,  2.1048e-01,  6.7778e-03,\n",
       "                       1.0138e-01,  5.1833e-01,  1.4452e-01,  3.7102e-01,  1.3398e-01,\n",
       "                       2.1135e-01, -8.7743e-02,  4.8599e-01,  1.1161e-01, -5.9097e-03,\n",
       "                       1.8094e-01,  1.2406e-01, -1.6027e-02,  3.1933e-01, -8.6097e-02,\n",
       "                       1.3964e-01,  1.4311e-01, -4.9899e-02, -1.8795e-05,  1.6436e-02,\n",
       "                      -2.2685e-02, -1.6328e-02, -2.4284e-02,  1.9130e-01, -1.9162e-01,\n",
       "                      -6.9634e-02, -5.1065e-05, -9.1152e-03,  1.6377e-01, -3.1207e-03,\n",
       "                       1.3507e-01, -2.2218e-02,  1.5822e-03, -3.1539e-02, -1.7850e-01,\n",
       "                       5.7805e-02, -7.0454e-02, -2.7269e-03, -2.2264e-02, -3.4964e-02,\n",
       "                      -4.4872e-02, -2.9108e-02, -1.4820e-02,  1.8239e-01])),\n",
       "             ('bn1.running_mean',\n",
       "              tensor([ 6.7408e-02,  2.1799e-02, -2.8557e-02,  1.2134e-01, -6.6326e-02,\n",
       "                      -5.7012e-05,  1.3000e-01, -3.9631e-02,  4.9858e-05, -1.6161e-02,\n",
       "                       2.2064e-02,  2.8073e-02,  5.9777e-03,  9.4509e-03, -6.5699e-03,\n",
       "                      -9.2340e-02, -8.7001e-02, -1.0274e-01,  3.7240e-02,  1.4534e-01,\n",
       "                       3.4549e-02,  1.0595e-02,  3.3149e-02, -8.3987e-02,  2.2121e-02,\n",
       "                      -3.0200e-02, -8.6794e-02,  1.1196e-03, -2.9181e-02, -1.5409e-02,\n",
       "                       1.7889e-02,  2.0934e-01,  9.3681e-03, -5.0908e-02,  1.6552e-01,\n",
       "                       7.5347e-02, -6.0333e-02, -3.1034e-02, -3.0461e-05, -1.5785e-01,\n",
       "                      -6.2909e-02, -8.2763e-02, -6.0805e-02, -8.4450e-02,  2.0329e-01,\n",
       "                      -9.0958e-02, -5.9328e-05,  1.4531e-02,  3.2460e-02, -2.5863e-02,\n",
       "                      -1.2725e-02,  7.7212e-02,  3.2420e-02, -5.1934e-02,  2.2011e-01,\n",
       "                      -3.7747e-02, -2.8858e-02,  3.2434e-02, -7.9414e-02,  5.2937e-02,\n",
       "                       2.3406e-02, -4.9405e-02, -7.0184e-02, -7.4817e-02])),\n",
       "             ('bn1.running_var',\n",
       "              tensor([1.6890e-01, 1.0526e-02, 2.6911e-02, 2.6211e-01, 1.5598e-01, 6.8926e-08,\n",
       "                      4.1327e-01, 1.3351e-01, 4.2483e-08, 5.0165e-03, 1.5029e-02, 4.8285e-02,\n",
       "                      7.0537e-04, 4.6403e-03, 9.0711e-04, 1.8753e-01, 2.1379e-01, 1.9976e-01,\n",
       "                      1.5026e+00, 3.6831e-01, 6.5918e-01, 3.1232e-01, 5.3291e-01, 2.8989e-01,\n",
       "                      1.0943e+00, 2.0927e+00, 1.8331e-01, 3.2320e-01, 5.1310e-01, 1.4366e-02,\n",
       "                      2.5476e+00, 7.5441e-01, 2.1565e-03, 2.7488e-01, 6.4984e-01, 4.7246e-01,\n",
       "                      6.5246e-01, 2.9524e-02, 1.7061e-08, 6.6752e-01, 9.3673e-02, 2.2487e-01,\n",
       "                      6.3520e-02, 3.3528e-01, 7.2271e-01, 1.5523e-01, 7.6994e-08, 4.1559e-03,\n",
       "                      6.1012e-01, 4.4118e-02, 2.8819e-01, 1.5814e-01, 1.0621e-01, 5.6499e-02,\n",
       "                      8.9097e-01, 5.6235e-02, 1.9222e-02, 2.8494e-02, 1.4295e-01, 1.0614e-01,\n",
       "                      6.6110e-02, 4.3307e-02, 1.4043e-01, 2.0434e-01])),\n",
       "             ('bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.0.conv1.weight',\n",
       "              tensor([[[[-7.6488e-03]],\n",
       "              \n",
       "                       [[-3.6845e-03]],\n",
       "              \n",
       "                       [[-9.3077e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3784e-03]],\n",
       "              \n",
       "                       [[-3.7135e-03]],\n",
       "              \n",
       "                       [[-1.4087e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5484e-02]],\n",
       "              \n",
       "                       [[ 1.7283e-03]],\n",
       "              \n",
       "                       [[ 7.6010e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2610e-02]],\n",
       "              \n",
       "                       [[-5.4182e-03]],\n",
       "              \n",
       "                       [[-2.0352e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4615e-02]],\n",
       "              \n",
       "                       [[ 5.4330e-03]],\n",
       "              \n",
       "                       [[ 1.6626e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1693e-02]],\n",
       "              \n",
       "                       [[ 7.2877e-03]],\n",
       "              \n",
       "                       [[-3.7906e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.9671e-03]],\n",
       "              \n",
       "                       [[-5.9837e-03]],\n",
       "              \n",
       "                       [[-6.9809e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4437e-03]],\n",
       "              \n",
       "                       [[ 6.8647e-03]],\n",
       "              \n",
       "                       [[ 7.3153e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9286e-02]],\n",
       "              \n",
       "                       [[ 8.7665e-04]],\n",
       "              \n",
       "                       [[ 9.7417e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1342e-02]],\n",
       "              \n",
       "                       [[ 3.6234e-03]],\n",
       "              \n",
       "                       [[ 6.6750e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.4819e-02]],\n",
       "              \n",
       "                       [[-1.0023e-03]],\n",
       "              \n",
       "                       [[-2.4787e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.5759e-03]],\n",
       "              \n",
       "                       [[ 1.1160e-02]],\n",
       "              \n",
       "                       [[ 7.6542e-03]]]])),\n",
       "             ('layer1.0.bn1.weight',\n",
       "              tensor([1.1603e-01, 1.1755e-01, 9.6257e-02, 4.8954e-06, 6.4219e-02, 5.5348e-02,\n",
       "                      1.4168e-01, 4.1396e-02, 1.1656e-01, 1.0240e-01, 1.1545e-01, 1.4750e-01,\n",
       "                      8.2815e-02, 1.1423e-01, 7.1765e-02, 1.5778e-01, 8.0092e-02, 1.0552e-01,\n",
       "                      9.0554e-02, 3.5266e-02, 1.2983e-01, 1.1216e-01, 1.9296e-01, 4.7617e-02,\n",
       "                      6.9639e-02, 1.5466e-01, 1.0784e-01, 1.1677e-01, 1.1981e-01, 1.0488e-01,\n",
       "                      1.1802e-01, 1.3540e-01, 1.3305e-01, 1.1575e-01, 5.3867e-02, 1.2989e-01,\n",
       "                      1.0169e-01, 1.5792e-01, 1.3834e-01, 8.7839e-02, 1.5179e-01, 1.1416e-01,\n",
       "                      1.4892e-01, 9.1571e-02, 9.1706e-02, 5.1749e-02, 1.1228e-01, 1.1293e-01,\n",
       "                      8.5316e-02, 8.8777e-02, 1.1223e-01, 7.4644e-02, 2.4702e-03, 1.3120e-01,\n",
       "                      9.4497e-02, 1.6243e-01, 1.2405e-01, 1.3019e-01, 1.2229e-01, 5.4017e-02,\n",
       "                      5.2694e-02, 1.3641e-01, 1.4147e-01, 1.1673e-01])),\n",
       "             ('layer1.0.bn1.bias',\n",
       "              tensor([ 6.9033e-02,  8.1858e-02,  7.4092e-03, -2.8019e-05, -1.6208e-02,\n",
       "                      -2.0588e-02,  1.5621e-01,  3.2031e-02,  8.5105e-02,  2.0533e-02,\n",
       "                      -4.8364e-02,  4.9272e-02,  5.1143e-02, -6.0232e-02, -4.0387e-02,\n",
       "                       4.0827e-02, -9.3966e-03,  1.3131e-01,  4.3885e-02,  1.5283e-02,\n",
       "                       6.2230e-02,  6.5340e-02,  1.1408e-01,  3.5645e-02,  8.9065e-03,\n",
       "                       6.6917e-02,  1.0192e-01, -7.3213e-02, -4.5865e-02,  5.5485e-04,\n",
       "                      -6.8140e-02,  6.1326e-02,  1.9855e-01,  8.7643e-03, -1.0433e-02,\n",
       "                       4.0231e-02, -6.2376e-02, -7.2662e-02,  2.1407e-01,  4.2063e-02,\n",
       "                       2.7100e-01, -6.8085e-02, -8.1145e-02, -5.8462e-02,  4.3452e-02,\n",
       "                       3.9039e-02, -5.4244e-02,  1.9834e-01, -2.5159e-02,  6.4086e-02,\n",
       "                       1.0920e-01,  3.1865e-02, -1.2377e-02, -4.9532e-02, -2.7496e-02,\n",
       "                       7.4500e-02,  1.8280e-01,  3.8843e-02,  3.0982e-02, -1.0747e-02,\n",
       "                       4.9381e-02,  6.7130e-02,  1.5524e-01,  1.1419e-01])),\n",
       "             ('layer1.0.bn1.running_mean',\n",
       "              tensor([-7.7145e-02,  2.5786e-02,  4.3611e-02,  3.0536e-05,  2.4079e-02,\n",
       "                      -2.4068e-04, -6.7170e-02,  1.4876e-02,  4.2254e-02, -3.2888e-02,\n",
       "                       5.6853e-02, -6.1231e-02, -4.0923e-02,  5.0379e-02,  2.1451e-02,\n",
       "                      -3.8659e-02,  2.6396e-02, -1.4053e-01,  8.4169e-04, -1.4978e-02,\n",
       "                      -4.0367e-02, -2.5713e-02, -7.0037e-02,  2.9947e-02, -7.4944e-02,\n",
       "                      -7.4352e-02, -6.1854e-03,  4.8463e-02,  4.3796e-02,  2.2060e-03,\n",
       "                       8.9071e-03, -6.3022e-02, -7.4993e-02,  2.7206e-02, -2.2160e-04,\n",
       "                      -1.8263e-02, -6.0235e-03,  3.7624e-03, -1.6875e-03, -2.5538e-02,\n",
       "                      -1.1694e-01,  4.0878e-02,  8.4461e-02,  6.9958e-02,  2.9528e-02,\n",
       "                      -2.1454e-02,  3.6620e-02, -3.7765e-02,  4.6710e-02, -5.5042e-02,\n",
       "                      -5.6951e-02, -2.9262e-02, -3.6516e-03, -1.2614e-02,  5.1433e-02,\n",
       "                      -8.1171e-02, -3.5816e-02, -3.5228e-02, -3.2378e-02,  5.6500e-03,\n",
       "                       3.1592e-02, -7.5772e-03, -1.4681e-01, -1.5463e-01])),\n",
       "             ('layer1.0.bn1.running_var',\n",
       "              tensor([4.4041e-03, 4.7180e-03, 2.5762e-03, 1.6853e-10, 5.8001e-04, 1.9806e-04,\n",
       "                      5.3301e-03, 6.6052e-04, 1.6529e-03, 2.5965e-03, 1.4975e-03, 6.3477e-03,\n",
       "                      1.8179e-03, 1.6618e-03, 4.7356e-04, 8.8987e-03, 7.5489e-04, 2.5447e-03,\n",
       "                      2.4492e-03, 5.2454e-04, 7.3689e-03, 2.2035e-03, 1.2646e-02, 4.8475e-04,\n",
       "                      1.2579e-03, 3.9404e-03, 6.3761e-03, 2.9215e-03, 1.1961e-03, 2.2127e-03,\n",
       "                      5.0179e-04, 7.6135e-03, 7.8286e-03, 1.2026e-03, 3.4873e-04, 5.1666e-03,\n",
       "                      6.4454e-04, 2.9801e-03, 8.0734e-03, 2.4285e-03, 8.1091e-03, 7.8711e-04,\n",
       "                      1.6377e-03, 7.3576e-04, 2.8303e-03, 5.4948e-04, 8.5200e-04, 4.0586e-03,\n",
       "                      8.7622e-04, 1.2895e-03, 4.0386e-03, 1.8152e-03, 4.5734e-05, 3.5047e-03,\n",
       "                      8.6017e-04, 7.6445e-03, 4.5029e-03, 3.2131e-03, 3.6580e-03, 9.7444e-04,\n",
       "                      6.1941e-04, 4.2722e-03, 3.6258e-03, 2.7516e-03])),\n",
       "             ('layer1.0.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.0.conv2.weight',\n",
       "              tensor([[[[-0.0274, -0.0001,  0.0274],\n",
       "                        [-0.0063, -0.0267, -0.0048],\n",
       "                        [ 0.0163,  0.0002, -0.0295]],\n",
       "              \n",
       "                       [[-0.0066, -0.0018,  0.0074],\n",
       "                        [-0.0058, -0.0222, -0.0246],\n",
       "                        [-0.0193, -0.0209, -0.0260]],\n",
       "              \n",
       "                       [[ 0.0195,  0.0142,  0.0099],\n",
       "                        [-0.0092, -0.0168, -0.0274],\n",
       "                        [-0.0161, -0.0212, -0.0214]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0137,  0.0267, -0.0052],\n",
       "                        [ 0.0174, -0.0026,  0.0086],\n",
       "                        [-0.0032,  0.0108,  0.0084]],\n",
       "              \n",
       "                       [[-0.0363, -0.0218,  0.0422],\n",
       "                        [-0.0088,  0.0152,  0.0111],\n",
       "                        [ 0.0493,  0.0237, -0.0389]],\n",
       "              \n",
       "                       [[-0.0595, -0.0394,  0.0353],\n",
       "                        [-0.0230, -0.0259, -0.0090],\n",
       "                        [ 0.0586,  0.0168, -0.0391]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0100,  0.0230,  0.0413],\n",
       "                        [-0.0153,  0.0208, -0.0008],\n",
       "                        [ 0.0005, -0.0282, -0.0190]],\n",
       "              \n",
       "                       [[-0.0059, -0.0047,  0.0048],\n",
       "                        [ 0.0042,  0.0145, -0.0149],\n",
       "                        [-0.0013, -0.0196, -0.0143]],\n",
       "              \n",
       "                       [[ 0.0041,  0.0067, -0.0009],\n",
       "                        [-0.0009, -0.0082, -0.0260],\n",
       "                        [-0.0226, -0.0345, -0.0329]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0145,  0.0008,  0.0312],\n",
       "                        [-0.0046,  0.0184,  0.0061],\n",
       "                        [ 0.0094, -0.0031,  0.0272]],\n",
       "              \n",
       "                       [[-0.0301, -0.0108, -0.0197],\n",
       "                        [ 0.0066,  0.0153, -0.0059],\n",
       "                        [ 0.0520,  0.0516,  0.0060]],\n",
       "              \n",
       "                       [[ 0.0048,  0.0117,  0.0162],\n",
       "                        [ 0.0013,  0.0052, -0.0021],\n",
       "                        [ 0.0120, -0.0002, -0.0270]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0014, -0.0081, -0.0063],\n",
       "                        [ 0.0248, -0.0154, -0.0037],\n",
       "                        [-0.0089, -0.0271, -0.0243]],\n",
       "              \n",
       "                       [[-0.0134, -0.0397, -0.0563],\n",
       "                        [ 0.0614, -0.0108, -0.0164],\n",
       "                        [-0.0042,  0.0037, -0.0142]],\n",
       "              \n",
       "                       [[ 0.0303,  0.0056, -0.0244],\n",
       "                        [ 0.0316, -0.0015, -0.0233],\n",
       "                        [-0.0123, -0.0023, -0.0051]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0251, -0.0054,  0.0241],\n",
       "                        [ 0.0210, -0.0222,  0.0123],\n",
       "                        [ 0.0283,  0.0001, -0.0016]],\n",
       "              \n",
       "                       [[-0.0223, -0.0342,  0.0084],\n",
       "                        [-0.0239, -0.0027,  0.0026],\n",
       "                        [ 0.0206,  0.0134, -0.0035]],\n",
       "              \n",
       "                       [[-0.0024, -0.0255, -0.0123],\n",
       "                        [ 0.0057,  0.0108,  0.0145],\n",
       "                        [-0.0072, -0.0040,  0.0094]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0114, -0.0077,  0.0043],\n",
       "                        [ 0.0030,  0.0020,  0.0211],\n",
       "                        [ 0.0100,  0.0003,  0.0097]],\n",
       "              \n",
       "                       [[-0.0154, -0.0134, -0.0126],\n",
       "                        [-0.0157, -0.0132, -0.0180],\n",
       "                        [-0.0137, -0.0112, -0.0289]],\n",
       "              \n",
       "                       [[-0.0148, -0.0040,  0.0003],\n",
       "                        [-0.0105, -0.0093, -0.0054],\n",
       "                        [-0.0069, -0.0128, -0.0153]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0220, -0.0041, -0.0227],\n",
       "                        [ 0.0152, -0.0102, -0.0150],\n",
       "                        [ 0.0028, -0.0082, -0.0048]],\n",
       "              \n",
       "                       [[-0.0297, -0.0075,  0.0353],\n",
       "                        [-0.0202,  0.0131,  0.0430],\n",
       "                        [-0.0068, -0.0013, -0.0047]],\n",
       "              \n",
       "                       [[-0.0299, -0.0103,  0.0202],\n",
       "                        [-0.0176,  0.0062,  0.0274],\n",
       "                        [-0.0038,  0.0075,  0.0113]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0009, -0.0010, -0.0216],\n",
       "                        [ 0.0095,  0.0123, -0.0260],\n",
       "                        [ 0.0015,  0.0067, -0.0346]],\n",
       "              \n",
       "                       [[-0.0038, -0.0059,  0.0098],\n",
       "                        [-0.0161,  0.0047,  0.0291],\n",
       "                        [-0.0264, -0.0058,  0.0148]],\n",
       "              \n",
       "                       [[-0.0134, -0.0089,  0.0027],\n",
       "                        [-0.0121, -0.0035,  0.0176],\n",
       "                        [-0.0145, -0.0133,  0.0076]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0305, -0.0106,  0.0195],\n",
       "                        [-0.0338,  0.0052,  0.0245],\n",
       "                        [-0.0227,  0.0125,  0.0108]],\n",
       "              \n",
       "                       [[ 0.0314,  0.0067, -0.0732],\n",
       "                        [ 0.0678, -0.0006, -0.0809],\n",
       "                        [ 0.0528,  0.0013, -0.0577]],\n",
       "              \n",
       "                       [[ 0.0439,  0.0144, -0.0408],\n",
       "                        [ 0.0526,  0.0006, -0.0541],\n",
       "                        [ 0.0442,  0.0026, -0.0505]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0113, -0.0036, -0.0152],\n",
       "                        [-0.0021,  0.0176, -0.0205],\n",
       "                        [-0.0150,  0.0199,  0.0039]],\n",
       "              \n",
       "                       [[-0.0155, -0.0253, -0.0204],\n",
       "                        [-0.0275,  0.0091,  0.0163],\n",
       "                        [-0.0136,  0.0200,  0.0362]],\n",
       "              \n",
       "                       [[-0.0108, -0.0182, -0.0181],\n",
       "                        [-0.0055,  0.0050, -0.0002],\n",
       "                        [ 0.0018,  0.0138,  0.0134]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0102, -0.0148,  0.0145],\n",
       "                        [-0.0198, -0.0097, -0.0184],\n",
       "                        [-0.0081, -0.0090, -0.0115]],\n",
       "              \n",
       "                       [[ 0.0014,  0.0161, -0.0079],\n",
       "                        [-0.0205, -0.0023,  0.0012],\n",
       "                        [-0.0201,  0.0201,  0.0467]],\n",
       "              \n",
       "                       [[ 0.0052,  0.0110, -0.0207],\n",
       "                        [-0.0150,  0.0162,  0.0044],\n",
       "                        [-0.0534,  0.0091,  0.0199]]]])),\n",
       "             ('layer1.0.bn2.weight',\n",
       "              tensor([0.1070, 0.1362, 0.1526, 0.1238, 0.1055, 0.1381, 0.1879, 0.1266, 0.0861,\n",
       "                      0.1132, 0.0884, 0.1016, 0.1412, 0.2088, 0.1515, 0.1548, 0.1582, 0.1381,\n",
       "                      0.1961, 0.1109, 0.1653, 0.1268, 0.1206, 0.0835, 0.0982, 0.1130, 0.1343,\n",
       "                      0.1675, 0.1163, 0.1155, 0.1090, 0.0852, 0.1140, 0.1837, 0.1755, 0.0774,\n",
       "                      0.1409, 0.1024, 0.1178, 0.1422, 0.1212, 0.1052, 0.1899, 0.1498, 0.1036,\n",
       "                      0.1545, 0.0928, 0.0815, 0.1102, 0.1248, 0.1713, 0.1410, 0.1561, 0.1582,\n",
       "                      0.1628, 0.1079, 0.1455, 0.1442, 0.1115, 0.1285, 0.1289, 0.0930, 0.1368,\n",
       "                      0.1289])),\n",
       "             ('layer1.0.bn2.bias',\n",
       "              tensor([ 0.0961,  0.1212,  0.0734,  0.0284,  0.0574, -0.0070,  0.1787,  0.0438,\n",
       "                       0.0453,  0.0302,  0.0410, -0.0232,  0.0678,  0.2264,  0.0864, -0.0013,\n",
       "                       0.1173, -0.0204, -0.0951,  0.0865, -0.0441,  0.0098,  0.0125,  0.0393,\n",
       "                       0.0685,  0.0882,  0.0901, -0.0309,  0.0866,  0.0071,  0.1197,  0.0353,\n",
       "                      -0.0291, -0.1258, -0.0114,  0.0123,  0.0502,  0.0667,  0.0487,  0.1933,\n",
       "                      -0.0169, -0.0142, -0.0789, -0.0186, -0.0363, -0.0131,  0.0576, -0.0604,\n",
       "                       0.0351,  0.0486,  0.1058,  0.0596,  0.1017,  0.0999,  0.0753,  0.0956,\n",
       "                      -0.0496,  0.1177,  0.0349,  0.0184,  0.0706,  0.0477, -0.0115,  0.0699])),\n",
       "             ('layer1.0.bn2.running_mean',\n",
       "              tensor([-0.0076, -0.0244, -0.1803,  0.0099, -0.1118, -0.0750,  0.1869,  0.0360,\n",
       "                      -0.0038, -0.0389,  0.0250, -0.0127,  0.0072,  0.0175,  0.1060, -0.0612,\n",
       "                      -0.0016, -0.0251, -0.0637, -0.0202,  0.0419, -0.0198, -0.0491, -0.1035,\n",
       "                      -0.1029, -0.1577,  0.0170, -0.1136, -0.0166, -0.0468, -0.0177, -0.0315,\n",
       "                      -0.0900,  0.0430, -0.0196, -0.0697, -0.0360, -0.0192, -0.1012, -0.1212,\n",
       "                      -0.0154, -0.0816, -0.1011, -0.0461, -0.0824, -0.0488,  0.0074, -0.0700,\n",
       "                       0.0168, -0.1353,  0.0162, -0.0488, -0.1159, -0.0745, -0.0835, -0.1099,\n",
       "                      -0.0680, -0.0276, -0.1519, -0.0265, -0.0532, -0.0538, -0.0909, -0.0868])),\n",
       "             ('layer1.0.bn2.running_var',\n",
       "              tensor([0.0044, 0.0118, 0.0131, 0.0112, 0.0055, 0.0051, 0.0227, 0.0045, 0.0042,\n",
       "                      0.0034, 0.0037, 0.0024, 0.0113, 0.0429, 0.0108, 0.0086, 0.0240, 0.0074,\n",
       "                      0.0101, 0.0060, 0.0058, 0.0061, 0.0057, 0.0034, 0.0063, 0.0073, 0.0112,\n",
       "                      0.0151, 0.0074, 0.0033, 0.0103, 0.0049, 0.0055, 0.0109, 0.0200, 0.0029,\n",
       "                      0.0099, 0.0077, 0.0075, 0.0065, 0.0045, 0.0052, 0.0044, 0.0065, 0.0017,\n",
       "                      0.0084, 0.0065, 0.0022, 0.0086, 0.0069, 0.0180, 0.0093, 0.0129, 0.0147,\n",
       "                      0.0133, 0.0098, 0.0061, 0.0117, 0.0048, 0.0100, 0.0057, 0.0036, 0.0121,\n",
       "                      0.0072])),\n",
       "             ('layer1.0.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.0.conv3.weight',\n",
       "              tensor([[[[ 0.0186]],\n",
       "              \n",
       "                       [[ 0.0045]],\n",
       "              \n",
       "                       [[ 0.0131]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0008]],\n",
       "              \n",
       "                       [[ 0.0205]],\n",
       "              \n",
       "                       [[-0.0288]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0120]],\n",
       "              \n",
       "                       [[-0.0201]],\n",
       "              \n",
       "                       [[-0.0009]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0065]],\n",
       "              \n",
       "                       [[ 0.0198]],\n",
       "              \n",
       "                       [[-0.0041]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0027]],\n",
       "              \n",
       "                       [[-0.0108]],\n",
       "              \n",
       "                       [[ 0.0040]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0007]],\n",
       "              \n",
       "                       [[ 0.0067]],\n",
       "              \n",
       "                       [[-0.0048]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0029]],\n",
       "              \n",
       "                       [[ 0.0073]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0008]],\n",
       "              \n",
       "                       [[ 0.0029]],\n",
       "              \n",
       "                       [[ 0.0041]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0093]],\n",
       "              \n",
       "                       [[ 0.0061]],\n",
       "              \n",
       "                       [[ 0.0098]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0170]],\n",
       "              \n",
       "                       [[ 0.0552]],\n",
       "              \n",
       "                       [[-0.0241]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0065]],\n",
       "              \n",
       "                       [[ 0.0987]],\n",
       "              \n",
       "                       [[ 0.0191]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0060]],\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[-0.0287]]]])),\n",
       "             ('layer1.0.bn3.weight',\n",
       "              tensor([-5.8404e-02, -5.7065e-02,  3.8760e-02, -7.5011e-02,  2.6422e-02,\n",
       "                       4.3887e-02,  5.5625e-02, -2.7266e-03, -8.2128e-02, -1.9605e-01,\n",
       "                      -1.4063e-01,  1.3177e-02, -9.4249e-02,  2.3682e-02,  3.9732e-05,\n",
       "                      -9.1530e-02, -1.6076e-01,  1.0234e-01,  6.9117e-04, -4.4617e-02,\n",
       "                      -1.0047e-01,  1.5773e-02, -5.2013e-02,  1.2906e-02, -7.8954e-02,\n",
       "                       7.3331e-02,  2.0256e-01,  3.3015e-02, -9.9979e-02,  6.4218e-02,\n",
       "                       3.9171e-02, -1.1448e-01,  1.3529e-01,  4.3314e-02, -2.3398e-02,\n",
       "                       2.6425e-03, -1.4571e-01,  4.0503e-02, -2.2701e-01,  1.0614e-03,\n",
       "                      -4.0871e-02,  1.9890e-02,  7.0742e-02,  8.6898e-02,  6.9686e-02,\n",
       "                      -3.3815e-06, -1.4239e-01, -4.2154e-03, -1.4603e-01,  2.3178e-02,\n",
       "                       1.1978e-01, -3.7230e-02, -2.2490e-01, -6.9413e-02, -5.4937e-02,\n",
       "                       9.7315e-05,  1.0937e-01, -6.5262e-02, -1.3605e-02,  8.7919e-02,\n",
       "                      -1.0693e-01, -2.1597e-02, -1.6012e-02, -1.7658e-01, -1.3751e-01,\n",
       "                       1.3437e-02,  7.4181e-07,  1.1803e-01,  3.0482e-02,  2.5932e-02,\n",
       "                       4.5887e-02,  2.2557e-01,  1.1029e-01,  8.1408e-02,  1.0805e-02,\n",
       "                       6.4192e-05,  5.4663e-02,  2.0887e-01,  3.0124e-04,  4.5757e-02,\n",
       "                       2.4084e-02,  4.9909e-02, -5.4311e-02,  2.0868e-02, -1.9785e-03,\n",
       "                      -2.9666e-02, -8.1898e-06,  1.0138e-01,  2.3461e-01, -1.8606e-01,\n",
       "                      -2.7804e-03,  9.7245e-03,  3.1874e-02, -6.7843e-02,  8.0774e-02,\n",
       "                       6.3987e-03,  6.2325e-06,  3.6985e-04, -1.2890e-01,  2.2442e-01,\n",
       "                       1.2696e-02, -1.2111e-01,  4.1136e-03,  6.4129e-03, -6.3829e-03,\n",
       "                      -7.1491e-02, -4.1323e-02, -1.3870e-01,  1.0619e-01, -2.6971e-02,\n",
       "                       2.4056e-01, -8.7918e-03,  6.8772e-06,  2.3483e-01,  6.0074e-03,\n",
       "                      -1.2410e-01,  8.8088e-02,  7.2578e-04, -3.0452e-02,  4.7788e-02,\n",
       "                      -6.1904e-02, -2.4365e-02, -2.8248e-02, -1.7392e-01, -2.2673e-02,\n",
       "                       8.3044e-03, -3.1359e-02,  7.9746e-02, -4.7955e-02,  1.5893e-01,\n",
       "                      -2.3606e-06,  8.7713e-03, -3.0999e-02,  2.5546e-02, -1.9736e-05,\n",
       "                      -7.0470e-03, -1.6688e-02,  1.5186e-01, -1.1492e-01,  6.8753e-03,\n",
       "                      -1.6453e-01,  1.6603e-04, -3.1437e-02, -1.9531e-02, -9.5692e-02,\n",
       "                      -2.1043e-02,  1.2539e-02,  4.6705e-02, -3.0497e-03,  1.8854e-01,\n",
       "                      -6.5813e-03, -6.5106e-03, -1.9754e-01,  1.4761e-02,  8.6006e-02,\n",
       "                       1.3552e-01,  1.8235e-01, -3.7154e-02,  2.5327e-01,  1.0760e-02,\n",
       "                      -2.0447e-02,  2.7723e-02, -1.0300e-01,  6.6069e-02, -4.7566e-02,\n",
       "                      -2.1666e-01,  1.1730e-01,  8.4296e-02, -1.3521e-02, -7.5670e-02,\n",
       "                       2.0360e-02, -5.1561e-02, -5.9280e-02, -1.3385e-01,  4.9136e-02,\n",
       "                      -1.7135e-03, -1.9208e-06, -3.9276e-02,  1.1247e-02, -5.8728e-02,\n",
       "                       6.9152e-03, -6.4741e-02,  6.5574e-03,  1.4945e-02, -9.2175e-03,\n",
       "                       1.2570e-01, -3.1899e-02, -6.1097e-02,  8.9754e-02,  4.8790e-02,\n",
       "                       7.2716e-02, -1.1129e-02,  6.5344e-02, -5.1578e-07, -7.9603e-06,\n",
       "                      -2.0709e-03, -7.7866e-02, -1.5077e-01, -9.3676e-02, -4.4736e-04,\n",
       "                       7.5985e-02,  1.4009e-01,  3.5683e-06, -3.8952e-02, -2.9876e-02,\n",
       "                      -1.7943e-02, -6.5093e-03,  4.1271e-05, -1.5378e-01, -9.4763e-02,\n",
       "                      -1.5400e-01, -1.5840e-01,  3.2073e-02,  2.5306e-02, -1.5726e-01,\n",
       "                       1.0324e-01,  5.4487e-02,  9.1352e-04,  4.5025e-02,  9.8761e-02,\n",
       "                       1.6405e-01, -3.3077e-02,  1.5777e-02,  5.1967e-03, -9.1830e-02,\n",
       "                      -2.5518e-01,  3.8816e-02,  9.4380e-02,  1.2099e-02,  1.9612e-01,\n",
       "                      -1.5695e-01, -3.6075e-02,  4.1072e-02,  1.2774e-01,  9.0178e-02,\n",
       "                       1.6993e-01, -2.0458e-02,  3.7512e-03, -9.2107e-02, -1.3865e-01,\n",
       "                       5.6757e-03,  7.3004e-02,  8.3462e-02, -9.7163e-02, -9.0110e-02,\n",
       "                      -8.6330e-02, -1.1940e-02,  9.4880e-03,  2.8864e-03,  3.4121e-02,\n",
       "                       3.6932e-03,  3.9218e-03,  1.0168e-01, -9.0315e-03, -1.0276e-01,\n",
       "                       1.6637e-01])),\n",
       "             ('layer1.0.bn3.bias',\n",
       "              tensor([-2.3051e-02, -9.5719e-02, -1.3844e-02,  1.5066e-02, -5.0398e-03,\n",
       "                       6.9904e-03, -2.3656e-02, -2.0590e-02, -5.8271e-02,  2.9182e-02,\n",
       "                       3.6566e-02, -2.3249e-03, -1.2257e-02, -1.5299e-02, -6.8108e-04,\n",
       "                      -6.6012e-02, -3.7298e-02, -3.3706e-02, -2.4770e-03, -1.6475e-02,\n",
       "                       2.5438e-02, -5.3337e-02, -4.0282e-03,  8.6555e-03, -1.9792e-02,\n",
       "                      -6.8987e-03,  2.9427e-02, -1.9580e-02,  3.3350e-02, -1.4784e-02,\n",
       "                       1.3781e-02,  1.2469e-03,  4.5462e-02, -2.5770e-02, -1.5104e-02,\n",
       "                      -2.6888e-03, -2.1170e-02, -1.3617e-02, -2.4528e-02, -1.3360e-02,\n",
       "                      -1.5834e-02, -4.4539e-02, -2.4174e-02,  1.8653e-02,  4.7715e-02,\n",
       "                      -2.1097e-04,  6.0061e-02,  8.9762e-03, -3.8476e-02, -9.4800e-03,\n",
       "                      -1.8028e-03, -1.2327e-03, -2.4583e-02,  8.0375e-03, -4.3834e-02,\n",
       "                      -5.3517e-04, -3.9074e-02, -8.6143e-03,  7.3812e-03, -1.8551e-02,\n",
       "                      -2.0318e-03,  2.3548e-02, -4.2510e-04,  8.6504e-02, -1.8252e-03,\n",
       "                      -6.4115e-03, -1.5893e-05,  2.2768e-03,  7.7272e-03, -2.5992e-02,\n",
       "                      -3.9924e-02,  4.6156e-02, -7.3450e-02, -3.6614e-02, -1.5002e-02,\n",
       "                      -2.5075e-04, -4.0031e-02, -9.4379e-03, -1.2530e-03, -1.4467e-02,\n",
       "                      -3.9372e-03, -1.0250e-02, -6.3989e-02, -7.7277e-03, -2.8071e-02,\n",
       "                       6.5599e-02, -2.2330e-05,  6.1125e-02,  8.6229e-03,  6.9189e-02,\n",
       "                      -2.1858e-02, -4.8446e-04,  1.7955e-02, -3.8697e-02, -2.8182e-02,\n",
       "                       5.1017e-02, -9.4663e-05,  1.0232e-02, -4.7256e-03,  8.1535e-02,\n",
       "                      -4.8616e-03,  4.7227e-03,  4.3557e-03, -2.4933e-02,  5.8877e-02,\n",
       "                      -1.9825e-03, -1.3589e-02,  4.0877e-03, -1.4776e-02,  8.3459e-03,\n",
       "                      -2.7658e-02, -4.0239e-02, -6.2266e-05,  8.4492e-02,  1.4020e-02,\n",
       "                      -7.1044e-02,  1.1110e-03, -3.5831e-03, -1.7228e-02, -5.5705e-03,\n",
       "                       2.5990e-02, -1.3481e-03,  9.7803e-03,  2.2863e-02, -2.2828e-02,\n",
       "                      -4.5230e-02,  3.8303e-04, -6.8478e-02, -3.4535e-02,  4.8050e-02,\n",
       "                      -1.5036e-05,  1.3440e-02,  8.6376e-04, -4.6264e-02, -5.9221e-05,\n",
       "                      -3.3771e-02,  1.0690e-03, -1.9157e-02,  3.9741e-02, -1.1691e-02,\n",
       "                      -4.3358e-03,  1.2692e-02,  1.1228e-03, -3.2769e-02, -6.6045e-03,\n",
       "                      -3.4364e-03, -1.1342e-03, -2.4327e-02,  2.3318e-02, -4.1069e-02,\n",
       "                      -4.9431e-02,  1.9855e-02, -2.0047e-02, -5.2761e-02,  7.1702e-04,\n",
       "                       5.4181e-02,  4.7716e-02, -2.5320e-02, -4.3896e-02,  3.9570e-02,\n",
       "                      -2.7652e-02, -4.6689e-03, -3.0182e-02, -2.0274e-02, -1.1429e-02,\n",
       "                      -4.4490e-02,  9.5074e-04, -1.7013e-02, -8.1900e-03, -1.0156e-02,\n",
       "                      -6.6896e-02,  2.5457e-02,  1.3033e-02, -6.8200e-02, -4.6965e-02,\n",
       "                      -6.7310e-03, -3.5070e-05, -8.0533e-03,  1.7293e-03,  1.1585e-02,\n",
       "                      -2.7404e-03,  1.2040e-02, -1.0064e-02,  1.2221e-02,  2.2122e-02,\n",
       "                      -2.2188e-02, -4.0385e-02, -3.1924e-02,  4.1779e-02, -3.8297e-02,\n",
       "                      -1.9010e-02,  1.1414e-02, -3.0499e-02, -3.6992e-04, -1.0376e-04,\n",
       "                       3.9713e-03, -3.4281e-02, -2.3059e-02, -1.3002e-02, -4.1969e-02,\n",
       "                      -1.1039e-02, -2.0640e-02, -2.3020e-05, -2.1411e-02, -5.1864e-02,\n",
       "                      -1.4971e-02, -1.8191e-03, -2.1666e-04,  1.5380e-03,  4.3293e-03,\n",
       "                      -5.8153e-02, -5.3945e-02, -4.7438e-02, -2.5043e-02,  4.0241e-02,\n",
       "                      -2.0291e-02, -2.0917e-02, -2.8644e-03, -6.6017e-04,  7.8595e-03,\n",
       "                       1.8251e-01,  2.4806e-03, -2.0836e-02,  1.6122e-02, -6.7950e-02,\n",
       "                       7.5475e-02, -1.2367e-02,  3.4617e-02, -4.7985e-02,  8.5569e-02,\n",
       "                      -1.6104e-02, -2.2691e-02, -3.2360e-02, -2.8293e-02, -3.1415e-02,\n",
       "                       1.1314e-02, -8.6504e-03, -6.9060e-02, -1.3160e-02,  8.4479e-03,\n",
       "                      -5.1634e-03, -1.7617e-02,  1.7861e-02,  1.3608e-02,  4.1787e-03,\n",
       "                      -2.3763e-03,  9.3135e-03,  1.0990e-02,  5.0125e-02, -3.6603e-02,\n",
       "                      -4.0398e-02, -1.5477e-02,  3.9810e-02, -2.8943e-02,  4.2699e-02,\n",
       "                      -3.1104e-02])),\n",
       "             ('layer1.0.bn3.running_mean',\n",
       "              tensor([-4.3101e-04, -2.7039e-03, -7.3329e-03,  7.2885e-03,  2.7407e-03,\n",
       "                       5.4813e-03,  8.1934e-03,  3.2773e-03, -1.9600e-02,  1.3418e-02,\n",
       "                       2.1411e-02,  3.1272e-03,  1.7527e-02,  3.3146e-03,  3.8370e-05,\n",
       "                      -1.5362e-02,  9.6017e-03,  1.1853e-02,  2.4220e-05, -1.5447e-02,\n",
       "                       3.2018e-02,  1.0816e-02,  1.8616e-02,  1.4279e-02, -9.1558e-04,\n",
       "                       1.4154e-02,  2.1533e-02,  2.4986e-02,  3.6340e-02,  1.4927e-02,\n",
       "                       8.3114e-03,  1.8821e-02, -1.4990e-02,  2.3847e-02,  2.9670e-03,\n",
       "                      -5.7976e-03,  2.5849e-03, -4.9126e-03,  5.3348e-02,  4.6328e-03,\n",
       "                       1.9917e-02,  1.7533e-02, -1.1490e-02,  1.4313e-02, -2.4515e-03,\n",
       "                       5.8423e-05, -1.3447e-02,  1.3889e-02, -1.2107e-02,  2.0372e-03,\n",
       "                      -1.4560e-02, -1.2421e-02,  7.6996e-04,  3.3996e-02,  1.3546e-03,\n",
       "                       1.6954e-04, -1.4029e-02,  2.1824e-02,  3.8412e-03,  7.3381e-03,\n",
       "                       1.5622e-02, -1.0561e-02,  9.5614e-03,  4.7104e-02, -2.1176e-02,\n",
       "                      -3.1271e-04, -1.0356e-06,  1.9245e-02,  3.4146e-03, -7.0051e-03,\n",
       "                       1.1945e-02, -5.5758e-02,  5.3321e-02, -9.1208e-04, -4.9218e-04,\n",
       "                      -6.3466e-05,  4.9220e-03,  6.2882e-03, -9.3028e-05,  1.1164e-02,\n",
       "                       1.7886e-02, -3.1554e-05, -1.4854e-02, -2.2915e-03,  3.6316e-03,\n",
       "                       2.6938e-03,  1.2811e-07,  2.7776e-02,  1.0628e-02,  4.1651e-03,\n",
       "                      -3.1970e-03,  1.1836e-02, -1.4467e-02, -7.7508e-03,  6.2305e-03,\n",
       "                       1.2629e-03,  7.7544e-06,  5.6679e-03, -6.8330e-03, -5.9695e-03,\n",
       "                      -2.6893e-03, -4.8454e-03, -1.5778e-03,  5.0046e-03,  8.7760e-03,\n",
       "                       1.1574e-02,  2.5604e-02, -2.6410e-02,  5.0140e-02,  1.9261e-03,\n",
       "                      -3.2662e-02, -4.9172e-03,  7.8565e-06,  4.3249e-02, -1.1119e-02,\n",
       "                      -6.5547e-03, -3.6723e-02, -1.8685e-03, -4.3161e-03, -1.1607e-02,\n",
       "                      -1.4025e-03,  6.7556e-03,  2.2690e-02,  2.8103e-02,  5.1514e-03,\n",
       "                      -5.9495e-03,  1.6252e-02,  4.4295e-03, -1.6510e-02, -2.8407e-02,\n",
       "                       5.8629e-06,  1.0020e-02,  4.7088e-03,  5.2698e-03, -2.3850e-05,\n",
       "                      -5.2704e-03, -2.3035e-03,  2.1068e-02,  4.8393e-03, -3.9719e-03,\n",
       "                       1.3507e-02,  5.4804e-03, -2.2559e-02,  1.5164e-03, -1.3572e-02,\n",
       "                      -9.0347e-03, -8.7753e-03,  1.9737e-03,  3.1081e-03,  1.9796e-02,\n",
       "                      -3.9040e-03, -3.7172e-03,  2.5373e-02,  4.7506e-03,  1.0875e-02,\n",
       "                       2.7812e-02, -4.8597e-02, -2.4505e-03, -2.1137e-02,  8.6535e-03,\n",
       "                      -1.8611e-03,  1.7335e-02, -1.2973e-02, -1.9998e-02,  3.4715e-03,\n",
       "                       2.8268e-02,  9.8750e-03, -5.3590e-03, -3.4330e-03,  1.2141e-02,\n",
       "                      -1.6543e-02,  1.6739e-02,  3.6508e-03, -1.9901e-02,  2.2405e-04,\n",
       "                      -2.7875e-04, -5.4039e-06, -1.5665e-02,  3.7747e-03,  3.2966e-02,\n",
       "                      -8.5829e-03,  4.7234e-02, -1.8965e-04,  1.0582e-02,  1.0633e-02,\n",
       "                      -6.5044e-03,  2.3931e-03, -4.4354e-03,  2.5061e-02,  1.2144e-02,\n",
       "                       3.3494e-02, -2.8394e-03, -1.9433e-02, -6.5047e-06, -2.3759e-05,\n",
       "                       2.0821e-03,  1.6991e-02,  1.4003e-02,  1.2693e-02, -3.1440e-03,\n",
       "                      -2.1034e-02,  1.1167e-02,  7.3017e-06, -1.2602e-02, -4.4641e-03,\n",
       "                       7.6709e-04,  5.4566e-03, -5.0474e-05, -5.1310e-03, -1.3863e-02,\n",
       "                      -1.0110e-02,  2.9824e-02,  1.2052e-02, -9.8069e-03,  2.1332e-02,\n",
       "                      -4.0178e-02, -4.0387e-03,  4.9026e-04, -1.2123e-02,  1.3565e-02,\n",
       "                       3.7184e-02,  8.5872e-03,  7.1470e-03,  7.5371e-03, -2.4070e-02,\n",
       "                       6.5968e-02, -9.1350e-03, -1.7501e-03, -9.8984e-03, -2.4166e-02,\n",
       "                       2.6454e-02,  1.5677e-03, -3.8325e-03, -2.8967e-02,  1.2180e-02,\n",
       "                      -3.7859e-02,  4.2962e-06,  7.9949e-03, -9.4512e-03,  1.6004e-02,\n",
       "                      -1.8882e-03, -1.7463e-02, -2.1829e-02, -9.5958e-03,  1.4699e-02,\n",
       "                       2.1642e-03,  1.9402e-03, -2.0144e-03,  6.9498e-03,  2.0901e-02,\n",
       "                      -1.5095e-03, -9.7438e-03,  9.2532e-03,  5.3562e-04,  2.9558e-02,\n",
       "                       1.1571e-03])),\n",
       "             ('layer1.0.bn3.running_var',\n",
       "              tensor([1.2879e-04, 3.4439e-04, 1.3728e-04, 5.6472e-04, 9.4214e-05, 3.4304e-04,\n",
       "                      3.6904e-04, 1.2809e-05, 8.4659e-04, 2.0121e-03, 8.4123e-04, 1.3836e-04,\n",
       "                      5.2645e-04, 4.2052e-05, 5.1224e-08, 3.4181e-04, 2.1920e-03, 6.1263e-04,\n",
       "                      1.8417e-06, 8.7207e-05, 6.6987e-04, 2.9055e-04, 1.7253e-04, 2.1910e-04,\n",
       "                      3.1945e-04, 6.6974e-04, 6.3583e-04, 8.9354e-05, 1.4407e-03, 2.9533e-04,\n",
       "                      4.7938e-04, 6.7103e-04, 1.2403e-03, 1.9049e-04, 1.4460e-04, 6.7037e-05,\n",
       "                      1.5272e-03, 1.8950e-04, 3.3358e-03, 2.7621e-05, 1.9505e-04, 3.6076e-04,\n",
       "                      5.0736e-04, 1.0457e-03, 1.4516e-03, 1.9691e-09, 4.0270e-03, 2.5520e-04,\n",
       "                      9.4376e-04, 5.7591e-05, 2.3024e-03, 5.4554e-04, 1.7715e-03, 9.3696e-04,\n",
       "                      1.8709e-04, 1.5812e-08, 5.6656e-04, 4.2569e-04, 9.7763e-05, 5.8840e-04,\n",
       "                      1.0415e-03, 3.8762e-04, 7.0103e-05, 5.1130e-03, 1.5287e-03, 1.8596e-04,\n",
       "                      1.2443e-11, 8.9332e-04, 3.5582e-04, 2.6756e-04, 1.9251e-04, 4.4578e-03,\n",
       "                      7.5575e-04, 5.1179e-04, 2.5515e-05, 8.1510e-09, 1.7116e-04, 1.7744e-03,\n",
       "                      2.5779e-07, 8.0122e-05, 4.4316e-04, 1.7301e-04, 3.0755e-04, 9.4462e-05,\n",
       "                      9.3909e-05, 4.4077e-04, 7.1652e-11, 1.0827e-03, 2.9890e-03, 4.1813e-03,\n",
       "                      6.4543e-06, 4.6981e-05, 5.5600e-04, 3.3580e-04, 4.8312e-04, 2.4578e-04,\n",
       "                      3.8397e-10, 6.0892e-05, 1.1136e-03, 4.1564e-03, 8.5492e-06, 6.1972e-04,\n",
       "                      3.8993e-05, 1.5563e-05, 7.5872e-04, 4.4314e-04, 1.9936e-04, 1.6620e-03,\n",
       "                      1.6928e-03, 9.5257e-05, 2.5853e-03, 1.0521e-04, 5.1205e-10, 8.1235e-03,\n",
       "                      2.5187e-04, 1.0178e-03, 6.9120e-04, 6.2756e-06, 8.1104e-05, 1.7469e-04,\n",
       "                      6.6271e-04, 2.6140e-04, 1.0344e-04, 1.6129e-03, 5.6522e-04, 1.2187e-04,\n",
       "                      1.9213e-04, 6.6430e-04, 1.2599e-04, 1.7615e-03, 2.3566e-11, 6.8799e-04,\n",
       "                      2.4633e-04, 8.4057e-05, 4.1486e-10, 4.6852e-05, 1.4004e-04, 1.9618e-03,\n",
       "                      7.9723e-04, 1.5924e-04, 1.7682e-03, 1.0066e-04, 5.4690e-04, 1.1447e-04,\n",
       "                      3.3024e-04, 8.1352e-05, 1.4689e-04, 1.9765e-04, 4.2026e-05, 9.9950e-04,\n",
       "                      1.8115e-04, 4.0600e-05, 2.0505e-03, 5.4187e-05, 4.4968e-04, 1.7246e-03,\n",
       "                      1.5651e-03, 2.2930e-04, 3.8508e-03, 4.4353e-04, 1.6288e-04, 2.6584e-04,\n",
       "                      3.7579e-04, 8.7461e-04, 1.4640e-04, 1.8782e-03, 7.4552e-04, 7.8667e-04,\n",
       "                      3.8745e-05, 4.7865e-04, 2.3505e-04, 2.5345e-04, 3.0835e-04, 9.3378e-04,\n",
       "                      3.7801e-04, 2.6262e-06, 2.5830e-11, 8.4690e-05, 1.2383e-04, 2.3516e-04,\n",
       "                      3.1565e-05, 9.1488e-04, 2.7237e-05, 1.9489e-04, 1.6409e-04, 5.8875e-04,\n",
       "                      1.8078e-04, 2.2786e-04, 1.8754e-03, 4.6999e-04, 2.9245e-04, 5.8181e-04,\n",
       "                      4.3988e-04, 6.6066e-10, 4.3807e-10, 8.4091e-06, 4.0709e-04, 1.5740e-03,\n",
       "                      5.2084e-04, 1.3757e-04, 2.3733e-04, 1.0193e-03, 5.3612e-11, 1.4104e-04,\n",
       "                      8.8536e-05, 3.6642e-05, 5.7299e-05, 2.2027e-09, 1.7149e-03, 7.3170e-04,\n",
       "                      1.0392e-03, 1.5891e-03, 1.7900e-04, 1.6367e-04, 3.2895e-03, 6.4036e-04,\n",
       "                      5.9593e-04, 1.0302e-06, 1.1874e-04, 1.7654e-03, 5.2044e-03, 1.8561e-04,\n",
       "                      3.0155e-05, 1.8387e-04, 6.9470e-04, 8.3208e-03, 1.1363e-04, 1.0892e-03,\n",
       "                      1.1474e-04, 5.2454e-03, 2.0242e-03, 1.9799e-04, 2.1412e-04, 1.6028e-03,\n",
       "                      9.3319e-04, 1.5750e-03, 2.2009e-05, 6.3672e-05, 6.8171e-04, 1.2469e-03,\n",
       "                      5.1846e-05, 1.3847e-04, 4.6863e-04, 1.0822e-03, 1.6228e-03, 3.0489e-04,\n",
       "                      2.7237e-04, 7.7686e-05, 4.6600e-04, 1.5661e-04, 1.2694e-04, 5.7025e-05,\n",
       "                      1.8637e-03, 2.0313e-05, 6.3143e-04, 1.9573e-03])),\n",
       "             ('layer1.0.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.0.downsample.0.weight',\n",
       "              tensor([[[[-2.9767e-03]],\n",
       "              \n",
       "                       [[ 4.5039e-03]],\n",
       "              \n",
       "                       [[-1.7235e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0759e-02]],\n",
       "              \n",
       "                       [[ 1.7261e-02]],\n",
       "              \n",
       "                       [[ 7.8693e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6271e-02]],\n",
       "              \n",
       "                       [[-1.2204e-03]],\n",
       "              \n",
       "                       [[ 5.3919e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.5437e-03]],\n",
       "              \n",
       "                       [[-3.0438e-03]],\n",
       "              \n",
       "                       [[ 4.0516e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2602e-02]],\n",
       "              \n",
       "                       [[-4.3740e-04]],\n",
       "              \n",
       "                       [[ 3.7147e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1603e-03]],\n",
       "              \n",
       "                       [[ 2.9030e-03]],\n",
       "              \n",
       "                       [[ 9.7316e-05]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4855e-04]],\n",
       "              \n",
       "                       [[ 3.8718e-04]],\n",
       "              \n",
       "                       [[ 2.6393e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.5939e-05]],\n",
       "              \n",
       "                       [[-2.4350e-03]],\n",
       "              \n",
       "                       [[-2.9459e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.1972e-04]],\n",
       "              \n",
       "                       [[-9.5596e-04]],\n",
       "              \n",
       "                       [[-2.7204e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.8644e-03]],\n",
       "              \n",
       "                       [[ 9.8368e-03]],\n",
       "              \n",
       "                       [[ 1.0361e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7492e-02]],\n",
       "              \n",
       "                       [[-7.3588e-04]],\n",
       "              \n",
       "                       [[ 5.5660e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.5596e-04]],\n",
       "              \n",
       "                       [[ 3.3272e-03]],\n",
       "              \n",
       "                       [[ 3.4611e-03]]]])),\n",
       "             ('layer1.0.downsample.1.weight',\n",
       "              tensor([ 6.0596e-02,  1.9921e-01,  5.1067e-02,  2.3041e-02,  9.5375e-02,\n",
       "                       1.1349e-01,  1.4261e-01,  5.3789e-02,  1.9741e-01, -1.8973e-02,\n",
       "                       4.3051e-02,  8.9947e-02,  7.1037e-02,  7.0295e-02,  1.2180e-04,\n",
       "                       1.3432e-01,  4.6356e-03,  1.3136e-02,  5.7793e-04,  3.2897e-02,\n",
       "                       4.0678e-02,  2.5378e-01,  3.9592e-02,  1.1969e-01,  7.3694e-02,\n",
       "                       1.6735e-01, -9.5021e-03,  8.2740e-02,  2.9697e-02,  7.7478e-02,\n",
       "                       1.1213e-01,  2.9498e-02,  2.8815e-02,  1.0642e-01,  9.6801e-02,\n",
       "                       1.4124e-01,  1.5557e-01,  3.1368e-02,  4.1427e-02,  7.4433e-02,\n",
       "                       5.3110e-02,  2.0504e-01,  1.3654e-01,  1.0048e-01,  1.8169e-01,\n",
       "                       5.2461e-05,  1.6942e-02,  1.6406e-01,  9.0392e-02,  6.1913e-02,\n",
       "                       1.2081e-02,  3.2108e-01,  4.6478e-03,  1.9428e-01,  1.1133e-01,\n",
       "                       2.0349e-04,  5.1960e-02,  1.1136e-01,  1.1289e-01,  5.5547e-02,\n",
       "                       3.1756e-03,  2.0368e-01,  6.0179e-02,  5.7233e-02, -2.2068e-03,\n",
       "                       1.3197e-01,  8.8112e-06,  8.4157e-02,  1.6479e-01,  2.1411e-01,\n",
       "                       1.4611e-01,  2.6733e-02,  1.0971e-01,  8.6846e-02,  6.6904e-02,\n",
       "                       9.2008e-05,  1.3470e-01, -2.9516e-04, -1.1134e-04,  6.5842e-02,\n",
       "                       1.5930e-01,  4.9384e-02,  6.7292e-02,  1.0855e-01,  1.5518e-01,\n",
       "                       1.9470e-01,  1.4356e-06,  5.1671e-03, -1.6328e-04,  7.4630e-02,\n",
       "                       9.3438e-02,  1.2082e-01,  8.2757e-02,  1.1620e-01,  1.1906e-01,\n",
       "                       1.3305e-01,  3.4169e-05,  9.1346e-02,  4.8593e-02, -1.1341e-03,\n",
       "                       3.7446e-02,  8.5756e-03,  6.1705e-02,  7.0366e-02,  1.6872e-01,\n",
       "                       1.0736e-01,  3.9100e-02,  1.0319e-01,  2.0637e-01,  5.1439e-02,\n",
       "                      -8.8115e-03,  1.8090e-01,  2.0669e-05,  1.1077e-01,  2.2370e-01,\n",
       "                       1.4245e-01,  4.3036e-02,  7.0509e-04,  9.4650e-02,  8.5482e-02,\n",
       "                       1.3266e-01,  1.5302e-01,  7.5711e-02, -3.8499e-04,  2.5635e-01,\n",
       "                       1.8928e-01,  1.1719e-01,  2.0056e-01,  1.1652e-01,  7.9003e-03,\n",
       "                       3.6618e-06,  2.3904e-01,  1.6265e-01,  1.4962e-01,  8.3203e-06,\n",
       "                       1.2105e-01,  1.0752e-01,  6.6105e-02,  1.9151e-02,  1.2609e-01,\n",
       "                       5.0860e-02,  8.1552e-02,  1.6010e-01,  7.0974e-02,  6.3345e-02,\n",
       "                       6.6818e-02,  1.7187e-01,  1.1676e-01,  7.2592e-02,  1.2566e-02,\n",
       "                       2.4466e-01,  4.6746e-02,  1.1771e-01,  1.5471e-01,  1.2308e-01,\n",
       "                       5.3486e-02,  9.7001e-03,  1.2899e-01,  1.6002e-02,  1.5907e-01,\n",
       "                       1.7872e-01,  1.6676e-01,  5.2713e-02,  1.2219e-01,  5.6831e-02,\n",
       "                       6.1465e-02,  3.4646e-02,  1.9907e-01,  9.7096e-02,  8.7672e-02,\n",
       "                       2.7381e-01,  3.0776e-02,  2.0254e-02,  1.1153e-01,  2.0645e-01,\n",
       "                       1.1689e-03,  1.0392e-05,  3.0214e-02,  1.1634e-01,  8.4156e-02,\n",
       "                       1.0752e-01,  1.9566e-01,  9.2547e-02,  1.1726e-01,  1.6303e-01,\n",
       "                       8.5252e-02,  1.3624e-01,  8.6534e-02,  3.6674e-02,  1.4053e-01,\n",
       "                       7.6209e-03,  1.7311e-01,  1.6994e-01,  1.0740e-04,  1.9291e-05,\n",
       "                       5.6737e-02,  4.5353e-02, -3.9432e-03,  9.4644e-02,  1.6965e-01,\n",
       "                       3.9531e-02, -1.7491e-02, -2.6016e-06,  3.6954e-03,  1.0721e-01,\n",
       "                       8.1913e-02,  1.0393e-01,  6.0548e-05,  3.8561e-02,  7.2121e-02,\n",
       "                       1.3093e-01,  1.3630e-01,  1.6584e-01,  1.4453e-01,  1.9818e-01,\n",
       "                       6.7505e-02,  2.0947e-01, -1.6181e-05,  4.1080e-02,  1.7598e-01,\n",
       "                       1.4283e-01,  1.0489e-01,  8.6963e-02,  2.0796e-01,  1.8626e-01,\n",
       "                       9.4964e-02,  7.8413e-02,  4.4724e-02,  7.8607e-02,  9.4586e-02,\n",
       "                       1.7205e-01,  9.5966e-02,  1.2212e-01,  1.4492e-01,  1.8639e-01,\n",
       "                      -4.5346e-03,  4.0288e-02,  1.9261e-01,  1.3037e-01,  5.4253e-02,\n",
       "                       9.7157e-02,  3.7072e-02,  3.7510e-03,  6.7714e-02,  1.3503e-01,\n",
       "                       1.5258e-02,  1.8148e-01,  1.0374e-01,  1.5722e-01,  1.2689e-01,\n",
       "                       2.2016e-01,  3.9250e-03,  1.1650e-01,  3.0969e-02,  4.5495e-02,\n",
       "                       8.7996e-02])),\n",
       "             ('layer1.0.downsample.1.bias',\n",
       "              tensor([-2.3051e-02, -9.5719e-02, -1.3844e-02,  1.5066e-02, -5.0398e-03,\n",
       "                       6.9904e-03, -2.3656e-02, -2.0590e-02, -5.8271e-02,  2.9182e-02,\n",
       "                       3.6566e-02, -2.3249e-03, -1.2257e-02, -1.5299e-02, -6.8108e-04,\n",
       "                      -6.6012e-02, -3.7298e-02, -3.3706e-02, -2.4770e-03, -1.6475e-02,\n",
       "                       2.5438e-02, -5.3337e-02, -4.0282e-03,  8.6555e-03, -1.9792e-02,\n",
       "                      -6.8987e-03,  2.9427e-02, -1.9580e-02,  3.3350e-02, -1.4784e-02,\n",
       "                       1.3781e-02,  1.2469e-03,  4.5462e-02, -2.5770e-02, -1.5104e-02,\n",
       "                      -2.6888e-03, -2.1170e-02, -1.3617e-02, -2.4528e-02, -1.3360e-02,\n",
       "                      -1.5834e-02, -4.4539e-02, -2.4174e-02,  1.8653e-02,  4.7715e-02,\n",
       "                      -2.1097e-04,  6.0061e-02,  8.9762e-03, -3.8476e-02, -9.4800e-03,\n",
       "                      -1.8028e-03, -1.2327e-03, -2.4583e-02,  8.0375e-03, -4.3834e-02,\n",
       "                      -5.3517e-04, -3.9074e-02, -8.6143e-03,  7.3812e-03, -1.8551e-02,\n",
       "                      -2.0318e-03,  2.3548e-02, -4.2510e-04,  8.6504e-02, -1.8252e-03,\n",
       "                      -6.4115e-03, -1.5893e-05,  2.2768e-03,  7.7272e-03, -2.5992e-02,\n",
       "                      -3.9924e-02,  4.6156e-02, -7.3450e-02, -3.6614e-02, -1.5002e-02,\n",
       "                      -2.5075e-04, -4.0031e-02, -9.4379e-03, -1.2530e-03, -1.4467e-02,\n",
       "                      -3.9372e-03, -1.0250e-02, -6.3989e-02, -7.7277e-03, -2.8071e-02,\n",
       "                       6.5599e-02, -2.2330e-05,  6.1125e-02,  8.6229e-03,  6.9189e-02,\n",
       "                      -2.1858e-02, -4.8446e-04,  1.7955e-02, -3.8697e-02, -2.8182e-02,\n",
       "                       5.1017e-02, -9.4663e-05,  1.0232e-02, -4.7256e-03,  8.1535e-02,\n",
       "                      -4.8616e-03,  4.7227e-03,  4.3557e-03, -2.4933e-02,  5.8877e-02,\n",
       "                      -1.9825e-03, -1.3589e-02,  4.0877e-03, -1.4776e-02,  8.3459e-03,\n",
       "                      -2.7658e-02, -4.0239e-02, -6.2266e-05,  8.4492e-02,  1.4020e-02,\n",
       "                      -7.1044e-02,  1.1110e-03, -3.5831e-03, -1.7228e-02, -5.5705e-03,\n",
       "                       2.5990e-02, -1.3481e-03,  9.7803e-03,  2.2863e-02, -2.2828e-02,\n",
       "                      -4.5230e-02,  3.8303e-04, -6.8478e-02, -3.4535e-02,  4.8050e-02,\n",
       "                      -1.5036e-05,  1.3440e-02,  8.6376e-04, -4.6264e-02, -5.9221e-05,\n",
       "                      -3.3771e-02,  1.0690e-03, -1.9157e-02,  3.9741e-02, -1.1691e-02,\n",
       "                      -4.3358e-03,  1.2692e-02,  1.1228e-03, -3.2769e-02, -6.6045e-03,\n",
       "                      -3.4364e-03, -1.1342e-03, -2.4327e-02,  2.3318e-02, -4.1069e-02,\n",
       "                      -4.9431e-02,  1.9855e-02, -2.0047e-02, -5.2761e-02,  7.1702e-04,\n",
       "                       5.4181e-02,  4.7716e-02, -2.5320e-02, -4.3896e-02,  3.9570e-02,\n",
       "                      -2.7652e-02, -4.6689e-03, -3.0182e-02, -2.0274e-02, -1.1429e-02,\n",
       "                      -4.4490e-02,  9.5074e-04, -1.7013e-02, -8.1900e-03, -1.0156e-02,\n",
       "                      -6.6896e-02,  2.5457e-02,  1.3033e-02, -6.8200e-02, -4.6965e-02,\n",
       "                      -6.7310e-03, -3.5070e-05, -8.0533e-03,  1.7293e-03,  1.1585e-02,\n",
       "                      -2.7404e-03,  1.2040e-02, -1.0064e-02,  1.2221e-02,  2.2122e-02,\n",
       "                      -2.2188e-02, -4.0385e-02, -3.1924e-02,  4.1779e-02, -3.8297e-02,\n",
       "                      -1.9010e-02,  1.1414e-02, -3.0499e-02, -3.6992e-04, -1.0376e-04,\n",
       "                       3.9713e-03, -3.4281e-02, -2.3059e-02, -1.3002e-02, -4.1969e-02,\n",
       "                      -1.1039e-02, -2.0640e-02, -2.3020e-05, -2.1411e-02, -5.1864e-02,\n",
       "                      -1.4971e-02, -1.8191e-03, -2.1666e-04,  1.5380e-03,  4.3293e-03,\n",
       "                      -5.8153e-02, -5.3945e-02, -4.7438e-02, -2.5043e-02,  4.0241e-02,\n",
       "                      -2.0291e-02, -2.0917e-02, -2.8644e-03, -6.6017e-04,  7.8595e-03,\n",
       "                       1.8251e-01,  2.4806e-03, -2.0836e-02,  1.6122e-02, -6.7950e-02,\n",
       "                       7.5475e-02, -1.2367e-02,  3.4617e-02, -4.7985e-02,  8.5569e-02,\n",
       "                      -1.6104e-02, -2.2691e-02, -3.2360e-02, -2.8293e-02, -3.1415e-02,\n",
       "                       1.1314e-02, -8.6504e-03, -6.9060e-02, -1.3160e-02,  8.4479e-03,\n",
       "                      -5.1634e-03, -1.7617e-02,  1.7861e-02,  1.3608e-02,  4.1787e-03,\n",
       "                      -2.3763e-03,  9.3135e-03,  1.0990e-02,  5.0125e-02, -3.6603e-02,\n",
       "                      -4.0398e-02, -1.5477e-02,  3.9810e-02, -2.8943e-02,  4.2699e-02,\n",
       "                      -3.1104e-02])),\n",
       "             ('layer1.0.downsample.1.running_mean',\n",
       "              tensor([ 3.9998e-03,  7.9344e-02,  2.0021e-03, -1.4489e-02, -8.0650e-03,\n",
       "                      -1.5046e-02, -3.2447e-03,  3.9103e-02, -1.7207e-02,  1.7228e-03,\n",
       "                      -6.4947e-03,  1.6165e-02, -3.7389e-02,  1.3395e-02,  6.0877e-05,\n",
       "                       6.0774e-02, -9.8016e-03, -1.8383e-02,  5.9402e-04, -9.9124e-03,\n",
       "                      -1.1426e-04, -6.1043e-02, -1.4853e-02, -2.8546e-02,  1.2398e-02,\n",
       "                      -1.3582e-02, -2.8583e-02,  2.9137e-02, -6.8922e-03, -9.5276e-03,\n",
       "                       2.8310e-02,  8.9127e-03, -3.4393e-03,  1.9475e-02, -6.6358e-02,\n",
       "                      -8.4706e-03, -5.5523e-02,  1.0122e-02, -1.6474e-02,  1.3184e-02,\n",
       "                       2.2050e-02, -3.0403e-02, -3.2962e-03,  2.6286e-02, -5.7857e-02,\n",
       "                       7.8420e-05, -3.5100e-03, -9.9271e-02, -1.3028e-02,  8.0033e-03,\n",
       "                       1.8373e-02,  4.9459e-02, -1.3178e-02, -5.4073e-02,  4.5757e-02,\n",
       "                       1.6368e-04, -7.2490e-03, -2.8866e-02,  7.1832e-02, -1.2795e-02,\n",
       "                      -6.7482e-03, -4.5699e-02,  5.9232e-02, -7.3281e-03, -2.3481e-02,\n",
       "                      -4.3226e-02, -7.2230e-06,  4.9212e-02, -1.8166e-02, -3.8214e-02,\n",
       "                       4.3339e-02, -1.8768e-02, -8.8023e-03, -4.6357e-02, -2.1946e-02,\n",
       "                       2.2264e-06,  3.4894e-02, -4.9414e-03,  1.8651e-04,  4.8663e-02,\n",
       "                      -5.9326e-02, -2.5790e-03, -3.4587e-03, -7.1797e-03, -3.2538e-02,\n",
       "                      -8.5596e-02, -1.6761e-05, -2.3562e-03, -1.6150e-02,  2.1610e-02,\n",
       "                       3.0903e-02,  4.0399e-02,  1.1651e-02,  4.9899e-02, -2.7034e-02,\n",
       "                      -5.8465e-02, -3.1759e-05, -5.1970e-02, -2.3322e-02,  1.1464e-02,\n",
       "                       1.2701e-02, -1.3182e-02,  2.8584e-02,  1.8356e-02, -1.0165e-01,\n",
       "                      -4.7416e-02, -6.6264e-04, -2.9960e-02,  4.9461e-02, -1.7606e-02,\n",
       "                      -1.1151e-03, -6.8234e-02, -2.2073e-06,  3.5997e-02,  1.8829e-02,\n",
       "                       4.5632e-02,  1.3084e-02,  1.4542e-03,  3.4939e-02, -6.9913e-03,\n",
       "                      -1.4584e-02,  2.3247e-02, -6.6399e-03, -1.7258e-02, -5.9390e-02,\n",
       "                      -3.8926e-02, -8.7146e-03, -1.4016e-01,  3.5671e-02, -1.0283e-02,\n",
       "                      -2.6089e-05, -1.0789e-01, -7.4841e-03,  4.2880e-02, -4.1620e-05,\n",
       "                       3.2533e-02, -1.8186e-02, -7.1340e-02, -9.4495e-03, -1.0710e-02,\n",
       "                       8.3490e-03, -5.9652e-02,  9.5944e-03,  8.9689e-03,  1.6595e-02,\n",
       "                       2.8736e-02, -2.9423e-02,  1.6125e-02,  5.6576e-02, -3.3777e-02,\n",
       "                      -1.3933e-01, -5.5953e-02, -3.6196e-02,  4.1952e-02, -3.2466e-02,\n",
       "                       3.0466e-02, -1.1948e-02, -1.5362e-04, -1.9512e-02, -1.2320e-01,\n",
       "                      -5.1505e-02, -2.0407e-02, -8.8936e-03, -3.5960e-02,  2.7379e-02,\n",
       "                      -2.9146e-02,  1.4002e-02, -5.7567e-02, -1.3076e-02, -5.3153e-02,\n",
       "                       8.2851e-02, -1.9952e-02,  1.8898e-03, -2.9828e-02, -6.7220e-02,\n",
       "                      -1.1706e-03,  3.5042e-05,  3.9488e-02,  3.2067e-02,  6.2750e-03,\n",
       "                      -4.2826e-03, -1.3471e-02,  1.5438e-02, -2.0675e-03,  2.4095e-02,\n",
       "                       2.3497e-02, -7.8069e-02, -6.1740e-03,  6.5369e-03,  4.4993e-02,\n",
       "                      -1.5218e-02, -6.5701e-02, -2.2805e-02, -9.2083e-05,  7.6034e-05,\n",
       "                      -1.3616e-02, -1.9468e-02,  1.5720e-03, -2.4467e-02,  3.6907e-02,\n",
       "                      -1.3096e-02, -2.5202e-03, -3.3276e-05, -1.1754e-02,  2.4256e-02,\n",
       "                       3.7992e-02,  9.1803e-04, -1.7512e-06, -4.5980e-02,  5.7037e-02,\n",
       "                       1.0456e-02, -7.4006e-02,  6.6817e-02,  3.8281e-02,  3.3894e-02,\n",
       "                      -4.9250e-02,  3.8292e-03, -1.0196e-03, -6.4088e-03, -3.2065e-02,\n",
       "                       5.6772e-03, -8.3058e-02,  3.6715e-03,  5.5431e-02,  2.5220e-02,\n",
       "                      -4.2577e-03, -3.1866e-02,  2.5915e-02, -1.5874e-02, -1.4315e-02,\n",
       "                      -1.0362e-01,  2.9899e-02,  5.2642e-02, -4.7035e-02, -1.0149e-02,\n",
       "                      -1.1771e-02,  1.9448e-02,  6.6441e-02, -2.9366e-02, -1.2002e-02,\n",
       "                      -1.5717e-02, -8.8434e-05,  1.2010e-03,  4.4389e-02, -1.4563e-01,\n",
       "                      -1.4471e-03, -5.2739e-02,  2.3823e-02, -6.2487e-02,  3.9423e-02,\n",
       "                       4.1823e-02, -6.7054e-03,  3.4064e-02,  1.2434e-02, -3.6011e-02,\n",
       "                      -1.9461e-02])),\n",
       "             ('layer1.0.downsample.1.running_var',\n",
       "              tensor([4.0309e-04, 1.8599e-03, 3.7894e-04, 3.5679e-04, 7.5794e-04, 2.5376e-03,\n",
       "                      1.2038e-03, 2.5427e-04, 3.3389e-03, 2.4626e-04, 5.5927e-04, 1.5915e-03,\n",
       "                      1.1246e-03, 1.3483e-04, 3.8762e-08, 1.0375e-03, 3.8446e-04, 1.7932e-04,\n",
       "                      8.2728e-07, 8.2887e-05, 5.5301e-04, 4.8893e-03, 3.5622e-04, 1.2742e-03,\n",
       "                      7.3935e-04, 4.1078e-03, 3.6755e-04, 6.0442e-04, 6.6322e-04, 1.0390e-03,\n",
       "                      2.1088e-03, 3.1947e-04, 7.2716e-04, 8.6164e-04, 8.1620e-04, 2.7029e-03,\n",
       "                      5.2741e-03, 4.5552e-04, 1.1695e-03, 4.6297e-04, 3.3977e-04, 4.3138e-03,\n",
       "                      2.1555e-03, 3.0746e-03, 7.6970e-03, 1.8472e-08, 1.0348e-03, 3.0078e-03,\n",
       "                      7.5132e-04, 2.3482e-04, 8.8232e-04, 1.7964e-02, 2.4755e-04, 6.9945e-03,\n",
       "                      6.9796e-04, 4.1806e-08, 8.1773e-04, 1.9388e-03, 9.0846e-04, 8.6735e-04,\n",
       "                      2.8302e-04, 4.7039e-03, 2.4677e-04, 2.1065e-03, 1.8121e-04, 2.0311e-03,\n",
       "                      1.5394e-10, 1.6935e-03, 2.5782e-03, 3.8144e-03, 8.9014e-04, 7.4828e-04,\n",
       "                      9.5233e-04, 9.8313e-04, 2.8836e-04, 1.5884e-08, 6.4932e-04, 2.3997e-04,\n",
       "                      1.0683e-07, 1.9095e-04, 2.3119e-03, 3.0677e-04, 5.9161e-04, 1.0755e-03,\n",
       "                      2.2873e-03, 9.6216e-03, 1.2010e-10, 2.0556e-04, 5.2950e-04, 2.6508e-03,\n",
       "                      4.5331e-04, 9.2488e-04, 2.0939e-03, 1.4147e-03, 2.1274e-03, 3.5469e-03,\n",
       "                      1.0835e-09, 6.4233e-04, 8.6098e-04, 4.7344e-04, 7.9037e-05, 2.2042e-04,\n",
       "                      4.4964e-04, 2.4175e-04, 5.3778e-03, 1.9082e-03, 4.2002e-04, 3.9580e-03,\n",
       "                      6.9055e-03, 3.6697e-04, 3.7672e-04, 1.6624e-03, 5.6448e-10, 5.5868e-03,\n",
       "                      7.0879e-03, 2.1781e-03, 7.8719e-04, 1.9996e-05, 6.7519e-04, 6.4996e-04,\n",
       "                      3.2672e-03, 3.0854e-03, 1.0379e-03, 1.8688e-04, 8.4115e-03, 1.5917e-03,\n",
       "                      2.2613e-03, 4.5331e-03, 5.2553e-04, 3.1891e-04, 5.2127e-11, 8.8169e-03,\n",
       "                      2.9620e-03, 1.0961e-03, 4.0536e-10, 6.4073e-04, 2.0326e-03, 1.4525e-03,\n",
       "                      2.9648e-04, 2.2785e-03, 8.0468e-04, 1.1481e-03, 2.9008e-03, 4.7534e-04,\n",
       "                      3.8756e-04, 6.1600e-04, 4.0997e-03, 9.3071e-04, 6.9201e-04, 3.2380e-04,\n",
       "                      2.7534e-03, 4.9245e-04, 5.5555e-03, 1.0920e-03, 1.4041e-03, 1.4932e-03,\n",
       "                      3.6153e-04, 1.2754e-03, 4.5200e-04, 4.8940e-03, 2.1601e-03, 3.2632e-03,\n",
       "                      7.2005e-04, 3.6675e-03, 3.5463e-04, 1.6925e-03, 4.3107e-04, 4.0041e-03,\n",
       "                      4.0662e-04, 1.2291e-03, 4.1159e-03, 3.3744e-04, 2.2676e-04, 1.6425e-03,\n",
       "                      3.0879e-03, 1.8113e-06, 1.5408e-10, 8.8472e-05, 1.8182e-03, 1.1522e-03,\n",
       "                      1.5905e-03, 5.0662e-03, 4.6620e-04, 1.1500e-03, 2.3705e-03, 8.7395e-04,\n",
       "                      1.0517e-03, 7.9028e-04, 1.0353e-03, 1.6224e-03, 2.6942e-04, 4.9766e-03,\n",
       "                      3.5475e-03, 1.9910e-08, 1.1695e-09, 5.5646e-04, 9.6042e-04, 4.2563e-04,\n",
       "                      1.8879e-03, 1.5693e-03, 3.4953e-04, 2.3005e-04, 8.4470e-11, 2.1652e-04,\n",
       "                      5.2895e-04, 4.0915e-04, 5.8885e-04, 3.4311e-08, 1.2094e-03, 9.6590e-04,\n",
       "                      2.6145e-03, 2.9564e-03, 1.2726e-03, 2.4314e-03, 9.5427e-03, 1.5912e-03,\n",
       "                      6.1735e-03, 4.0539e-07, 3.4819e-04, 2.7582e-03, 5.9246e-03, 9.4620e-04,\n",
       "                      3.3749e-04, 4.1215e-03, 2.4133e-03, 4.1689e-03, 8.4224e-04, 1.0094e-03,\n",
       "                      7.5830e-04, 3.4551e-03, 6.1275e-03, 1.0874e-03, 1.4710e-03, 5.0583e-03,\n",
       "                      5.4662e-03, 2.2992e-04, 1.7324e-04, 1.3206e-03, 3.7110e-03, 1.0106e-03,\n",
       "                      8.7766e-04, 1.6949e-04, 9.2193e-05, 1.0006e-03, 4.1756e-03, 1.9555e-04,\n",
       "                      4.1712e-03, 2.0271e-03, 4.5445e-03, 1.1714e-03, 3.6006e-03, 9.2083e-05,\n",
       "                      4.6830e-03, 1.9136e-04, 8.3497e-04, 1.7221e-03])),\n",
       "             ('layer1.0.downsample.1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.1.conv1.weight',\n",
       "              tensor([[[[ 1.8691e-03]],\n",
       "              \n",
       "                       [[ 6.1978e-03]],\n",
       "              \n",
       "                       [[ 2.5450e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.5836e-03]],\n",
       "              \n",
       "                       [[-2.9188e-04]],\n",
       "              \n",
       "                       [[ 7.2238e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3808e-03]],\n",
       "              \n",
       "                       [[ 2.0683e-02]],\n",
       "              \n",
       "                       [[-2.6429e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.5007e-04]],\n",
       "              \n",
       "                       [[ 2.0487e-03]],\n",
       "              \n",
       "                       [[ 1.1316e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.2934e-03]],\n",
       "              \n",
       "                       [[ 1.9781e-02]],\n",
       "              \n",
       "                       [[-4.5798e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8596e-03]],\n",
       "              \n",
       "                       [[-5.6665e-03]],\n",
       "              \n",
       "                       [[-5.8056e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3095e-05]],\n",
       "              \n",
       "                       [[ 1.7418e-02]],\n",
       "              \n",
       "                       [[ 5.0983e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.3324e-04]],\n",
       "              \n",
       "                       [[ 9.2260e-03]],\n",
       "              \n",
       "                       [[ 1.2361e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7749e-03]],\n",
       "              \n",
       "                       [[ 1.7647e-02]],\n",
       "              \n",
       "                       [[-9.3048e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2151e-03]],\n",
       "              \n",
       "                       [[ 8.4608e-03]],\n",
       "              \n",
       "                       [[-2.0424e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8508e-03]],\n",
       "              \n",
       "                       [[ 1.0489e-02]],\n",
       "              \n",
       "                       [[-7.0200e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.5393e-03]],\n",
       "              \n",
       "                       [[ 1.2509e-02]],\n",
       "              \n",
       "                       [[ 4.5488e-03]]]])),\n",
       "             ('layer1.1.bn1.weight',\n",
       "              tensor([0.0421, 0.1129, 0.0746, 0.0793, 0.1164, 0.0776, 0.0776, 0.1177, 0.0148,\n",
       "                      0.1245, 0.0411, 0.0851, 0.0912, 0.0753, 0.0875, 0.0744, 0.1456, 0.0675,\n",
       "                      0.0931, 0.1384, 0.1281, 0.1035, 0.0799, 0.1048, 0.0831, 0.0254, 0.0821,\n",
       "                      0.1200, 0.1143, 0.0631, 0.1098, 0.0443, 0.0020, 0.0975, 0.1019, 0.0389,\n",
       "                      0.0985, 0.1225, 0.0951, 0.0715, 0.0441, 0.0959, 0.0692, 0.1038, 0.0845,\n",
       "                      0.0834, 0.0735, 0.1055, 0.1317, 0.0344, 0.0302, 0.0811, 0.0553, 0.1292,\n",
       "                      0.1255, 0.1040, 0.1693, 0.0704, 0.0760, 0.1348, 0.0910, 0.1593, 0.1191,\n",
       "                      0.1008])),\n",
       "             ('layer1.1.bn1.bias',\n",
       "              tensor([-0.0475, -0.0017, -0.0176,  0.0383, -0.0039,  0.0120, -0.0168, -0.0325,\n",
       "                      -0.0036,  0.0525, -0.0142,  0.0112,  0.0946, -0.0385, -0.0746,  0.0028,\n",
       "                       0.0526,  0.0016,  0.0597,  0.0269,  0.0186,  0.0300, -0.0443, -0.1084,\n",
       "                       0.0063,  0.0028, -0.0200,  0.1358,  0.0229,  0.0009,  0.0473,  0.0291,\n",
       "                      -0.0122,  0.0657,  0.0376,  0.0749,  0.0032,  0.0594,  0.0561,  0.0655,\n",
       "                      -0.0242,  0.0267, -0.0035, -0.0543, -0.0387,  0.0173, -0.0299, -0.1127,\n",
       "                      -0.1187, -0.0082,  0.0002,  0.0067, -0.0081,  0.0667,  0.0685,  0.0101,\n",
       "                       0.0263,  0.0277, -0.0795,  0.0515,  0.0645,  0.0627,  0.0427, -0.0124])),\n",
       "             ('layer1.1.bn1.running_mean',\n",
       "              tensor([ 0.0164, -0.0527, -0.0279, -0.0664, -0.0535,  0.0081, -0.0387,  0.0354,\n",
       "                       0.0075, -0.0411,  0.0017, -0.0205,  0.0101,  0.0125,  0.0371, -0.0305,\n",
       "                       0.0202, -0.0322, -0.0073, -0.0955, -0.0808,  0.0507,  0.0111,  0.0380,\n",
       "                      -0.0477, -0.0093,  0.0122,  0.0049,  0.0089, -0.0355,  0.0269,  0.0071,\n",
       "                       0.0106,  0.0118, -0.0078,  0.0048, -0.0374, -0.0646, -0.0185,  0.0208,\n",
       "                       0.0152,  0.0042, -0.0270,  0.0200,  0.0013, -0.0141,  0.0013,  0.0433,\n",
       "                       0.0594, -0.0043, -0.0159, -0.0629, -0.0517,  0.0355, -0.0170, -0.0660,\n",
       "                       0.0214, -0.0429,  0.0309, -0.0575,  0.0073, -0.0562,  0.0028, -0.0669])),\n",
       "             ('layer1.1.bn1.running_var',\n",
       "              tensor([3.4703e-04, 3.8552e-03, 2.0437e-03, 4.0439e-03, 2.8684e-03, 3.2595e-03,\n",
       "                      2.1031e-03, 2.6875e-03, 6.7416e-05, 1.0894e-02, 2.0200e-04, 4.8558e-03,\n",
       "                      1.0022e-02, 1.4267e-03, 1.4952e-03, 2.2800e-03, 9.4339e-03, 1.3096e-03,\n",
       "                      5.1685e-03, 8.5741e-03, 7.6534e-03, 6.3201e-03, 1.2533e-03, 1.9721e-03,\n",
       "                      2.7579e-03, 2.1172e-04, 1.2398e-03, 1.1942e-02, 5.7769e-03, 2.4430e-03,\n",
       "                      5.5289e-03, 1.1214e-03, 4.6054e-05, 6.7821e-03, 3.9525e-03, 3.1423e-03,\n",
       "                      5.1671e-03, 1.2389e-02, 7.2639e-03, 5.2322e-03, 3.2204e-04, 4.8954e-03,\n",
       "                      2.1917e-03, 1.8674e-03, 1.3521e-03, 3.5477e-03, 1.1422e-03, 2.2352e-03,\n",
       "                      3.4193e-03, 1.8947e-04, 3.2344e-04, 4.5650e-03, 1.1663e-03, 9.9863e-03,\n",
       "                      9.2435e-03, 6.2202e-03, 1.4227e-02, 2.8070e-03, 1.1876e-03, 1.4147e-02,\n",
       "                      4.6749e-03, 1.3253e-02, 7.8723e-03, 2.9846e-03])),\n",
       "             ('layer1.1.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.1.conv2.weight',\n",
       "              tensor([[[[ 4.1968e-04,  1.0589e-04,  3.1190e-04],\n",
       "                        [-1.0279e-03, -1.0115e-03, -1.0923e-03],\n",
       "                        [-2.6645e-03, -1.6866e-03, -3.7068e-03]],\n",
       "              \n",
       "                       [[ 1.8882e-02,  8.6208e-03, -1.1268e-02],\n",
       "                        [-2.2003e-02,  8.4080e-03,  9.7687e-03],\n",
       "                        [-1.2304e-02, -2.2203e-02,  1.9184e-03]],\n",
       "              \n",
       "                       [[-4.4706e-03,  2.1329e-03, -1.9792e-02],\n",
       "                        [-1.0644e-02,  1.2742e-02,  4.5459e-03],\n",
       "                        [-4.5146e-03, -8.5440e-03,  2.5070e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9552e-02,  3.1552e-02, -6.0597e-02],\n",
       "                        [-3.2902e-02,  1.9774e-02,  2.4971e-02],\n",
       "                        [-1.6223e-02, -3.8526e-02,  8.5800e-03]],\n",
       "              \n",
       "                       [[ 7.6145e-03, -1.0041e-02, -1.7608e-02],\n",
       "                        [ 1.4645e-02, -7.3493e-05,  3.4989e-03],\n",
       "                        [-1.1162e-02,  4.2623e-03,  3.5197e-02]],\n",
       "              \n",
       "                       [[-1.3508e-02, -5.2385e-04,  1.2706e-02],\n",
       "                        [-4.1426e-05, -1.4996e-03,  5.6235e-03],\n",
       "                        [ 1.1083e-02, -1.2817e-02, -2.0787e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9757e-03,  3.3167e-03,  3.3922e-03],\n",
       "                        [-1.6976e-03, -1.2799e-03, -2.0632e-03],\n",
       "                        [ 4.9431e-03,  3.9028e-03,  3.7058e-03]],\n",
       "              \n",
       "                       [[ 9.8608e-03, -2.9264e-04, -8.1706e-03],\n",
       "                        [ 1.0479e-02,  9.5850e-03,  5.4191e-03],\n",
       "                        [ 2.4920e-03,  1.0643e-02,  5.0974e-03]],\n",
       "              \n",
       "                       [[ 1.2968e-03,  5.4862e-03,  1.4411e-03],\n",
       "                        [-4.6452e-03, -2.2938e-03, -4.1027e-03],\n",
       "                        [-2.8519e-03, -1.3749e-02, -1.3528e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.8248e-03, -8.3704e-03,  1.5201e-03],\n",
       "                        [ 4.6472e-03,  1.0414e-02,  1.0048e-02],\n",
       "                        [-2.2835e-02, -2.8688e-02, -2.4864e-02]],\n",
       "              \n",
       "                       [[ 6.5471e-03,  5.6830e-03,  2.4685e-03],\n",
       "                        [-3.5861e-03,  2.4015e-03,  4.3410e-04],\n",
       "                        [ 1.4681e-02, -1.0559e-02,  7.8873e-04]],\n",
       "              \n",
       "                       [[ 1.1771e-02,  2.8905e-03,  7.0179e-04],\n",
       "                        [ 5.6674e-04, -1.2867e-03, -1.0364e-02],\n",
       "                        [ 2.1845e-02,  1.2135e-02,  1.1705e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9210e-04, -4.0449e-05, -6.8156e-04],\n",
       "                        [-4.2972e-04, -1.3028e-03,  2.2054e-05],\n",
       "                        [ 9.5618e-04,  1.1751e-03,  1.4208e-03]],\n",
       "              \n",
       "                       [[ 3.7223e-03,  3.5108e-03,  2.9440e-03],\n",
       "                        [ 8.3145e-04,  3.9735e-03, -6.2618e-03],\n",
       "                        [-3.8705e-03,  3.6359e-03, -2.5203e-03]],\n",
       "              \n",
       "                       [[ 3.5584e-03,  4.7780e-05,  3.0018e-03],\n",
       "                        [-6.4758e-04, -2.5605e-03,  2.6880e-03],\n",
       "                        [ 5.4714e-03,  6.2782e-04,  2.4060e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4646e-03,  2.6330e-03, -1.0001e-02],\n",
       "                        [-1.2958e-02,  5.5315e-03, -2.9353e-02],\n",
       "                        [-9.3824e-03,  5.3185e-03, -4.9559e-03]],\n",
       "              \n",
       "                       [[-8.6671e-03,  5.5397e-03,  9.2940e-03],\n",
       "                        [-6.2005e-03,  1.1660e-03,  3.2639e-03],\n",
       "                        [ 7.9671e-03,  5.3813e-04,  5.5283e-03]],\n",
       "              \n",
       "                       [[ 5.7566e-03,  3.4412e-03, -4.6789e-03],\n",
       "                        [-1.3690e-03,  1.8028e-03, -3.6435e-03],\n",
       "                        [-3.1105e-03,  4.2035e-03, -3.9122e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.6796e-03, -1.0532e-03, -5.6143e-04],\n",
       "                        [-1.8738e-03, -1.1459e-03, -1.7937e-03],\n",
       "                        [-5.7335e-04, -1.4318e-04, -2.5174e-04]],\n",
       "              \n",
       "                       [[-6.0681e-03, -7.8531e-03, -1.1215e-02],\n",
       "                        [-8.6755e-04, -6.9476e-03, -6.0474e-03],\n",
       "                        [ 4.0839e-03, -5.5941e-03, -1.1735e-02]],\n",
       "              \n",
       "                       [[ 1.3810e-03,  8.6032e-03,  2.5372e-03],\n",
       "                        [ 5.2224e-03,  6.6301e-03,  4.2526e-03],\n",
       "                        [-5.8143e-03,  6.9890e-04, -1.6932e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.8126e-03, -9.4750e-03, -9.6089e-03],\n",
       "                        [ 2.1144e-02, -1.1306e-02,  1.5407e-02],\n",
       "                        [ 1.9603e-02, -8.9267e-03, -6.6210e-03]],\n",
       "              \n",
       "                       [[ 1.1837e-02, -7.6209e-03, -6.0220e-03],\n",
       "                        [ 6.8837e-03,  9.3519e-03, -1.2285e-02],\n",
       "                        [-1.0549e-02, -8.7568e-03, -1.0658e-02]],\n",
       "              \n",
       "                       [[ 1.1062e-03, -6.4932e-03,  2.6567e-03],\n",
       "                        [ 5.7301e-03,  1.1661e-04, -3.2481e-03],\n",
       "                        [-3.1955e-03, -5.1661e-03, -1.0338e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6310e-03, -1.0132e-03, -1.1655e-03],\n",
       "                        [ 4.9492e-03,  6.2732e-03,  6.7826e-03],\n",
       "                        [ 1.8495e-03,  1.3056e-03, -4.1697e-04]],\n",
       "              \n",
       "                       [[-6.1532e-03,  1.0778e-02, -3.8538e-04],\n",
       "                        [ 8.7024e-03, -2.3572e-02,  1.3784e-02],\n",
       "                        [-4.4410e-03,  3.0714e-02, -1.3016e-02]],\n",
       "              \n",
       "                       [[ 2.0111e-03, -3.0474e-03, -1.1570e-03],\n",
       "                        [-1.5009e-02, -1.2378e-02,  1.2277e-02],\n",
       "                        [-1.4814e-02, -1.1172e-02, -5.6539e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.7455e-02,  8.9786e-03, -3.0581e-02],\n",
       "                        [ 3.9654e-02, -4.6878e-02,  2.8529e-02],\n",
       "                        [-2.7613e-02,  3.6277e-02, -5.7942e-02]],\n",
       "              \n",
       "                       [[ 1.3201e-02, -2.4606e-02,  1.3198e-02],\n",
       "                        [-4.5257e-02,  1.8028e-02, -1.9737e-02],\n",
       "                        [-2.5047e-03, -2.0766e-02, -3.7556e-03]],\n",
       "              \n",
       "                       [[ 6.9927e-03,  6.7732e-03, -3.0609e-03],\n",
       "                        [-1.2925e-04, -5.0626e-03,  3.8717e-03],\n",
       "                        [-1.0624e-02,  4.8058e-03, -6.0221e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9979e-04,  7.2520e-04,  7.0709e-04],\n",
       "                        [-7.5212e-04,  1.0142e-03,  1.0731e-03],\n",
       "                        [-4.2508e-04, -5.6517e-05,  5.8333e-04]],\n",
       "              \n",
       "                       [[-5.2558e-03, -6.1926e-03,  1.2763e-02],\n",
       "                        [ 8.6412e-03, -1.7033e-02, -2.7232e-03],\n",
       "                        [ 9.0315e-03,  6.6848e-03, -4.7003e-03]],\n",
       "              \n",
       "                       [[-5.3065e-03, -2.4421e-03,  2.4741e-03],\n",
       "                        [-2.0276e-03, -1.8208e-04,  1.0216e-02],\n",
       "                        [-5.3184e-03,  1.5321e-03, -6.2944e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.3601e-03, -1.9544e-02,  2.0071e-02],\n",
       "                        [ 2.9024e-02, -1.7228e-02, -1.3582e-02],\n",
       "                        [-3.9233e-03,  1.6022e-02, -1.6014e-02]],\n",
       "              \n",
       "                       [[ 4.7065e-03,  4.6268e-03, -7.9133e-03],\n",
       "                        [-2.6972e-03,  5.6819e-03,  2.0439e-02],\n",
       "                        [ 3.5985e-03, -4.8999e-04,  9.9790e-03]],\n",
       "              \n",
       "                       [[ 5.5066e-03,  1.0274e-03,  1.0305e-02],\n",
       "                        [-5.2584e-04,  3.0113e-03,  3.2150e-03],\n",
       "                        [-1.1750e-04,  1.9686e-03,  3.3771e-03]]]])),\n",
       "             ('layer1.1.bn2.weight',\n",
       "              tensor([0.1414, 0.1375, 0.0808, 0.0924, 0.0912, 0.1228, 0.1041, 0.1322, 0.0560,\n",
       "                      0.0636, 0.2020, 0.1802, 0.1540, 0.0867, 0.1285, 0.1542, 0.1812, 0.1354,\n",
       "                      0.0021, 0.1822, 0.0651, 0.1870, 0.1996, 0.1237, 0.1082, 0.1536, 0.0605,\n",
       "                      0.2104, 0.2151, 0.1723, 0.1350, 0.1075, 0.1749, 0.0988, 0.0668, 0.2605,\n",
       "                      0.1955, 0.1668, 0.1629, 0.1401, 0.1541, 0.1087, 0.1581, 0.1306, 0.2322,\n",
       "                      0.1192, 0.1239, 0.0602, 0.0872, 0.0994, 0.1211, 0.0955, 0.1691, 0.1509,\n",
       "                      0.1427, 0.2116, 0.1841, 0.0856, 0.1615, 0.1896, 0.0368, 0.0697, 0.1777,\n",
       "                      0.0928])),\n",
       "             ('layer1.1.bn2.bias',\n",
       "              tensor([-0.0331, -0.1125,  0.0604, -0.0122,  0.1473, -0.0662, -0.0171, -0.0577,\n",
       "                      -0.0008,  0.0485, -0.0331, -0.0494, -0.0303, -0.0439, -0.0497, -0.0309,\n",
       "                      -0.0612, -0.0323, -0.0090, -0.0539,  0.0189, -0.0488, -0.0861, -0.0352,\n",
       "                      -0.0092, -0.0975, -0.0212, -0.0474, -0.0695, -0.0756, -0.0506, -0.0250,\n",
       "                      -0.0771, -0.0455, -0.0220, -0.1872, -0.0429, -0.0495, -0.0679, -0.0495,\n",
       "                      -0.0487,  0.0021, -0.0711, -0.0362, -0.1414,  0.0024, -0.0107,  0.0523,\n",
       "                      -0.0136, -0.0427, -0.0389, -0.0662, -0.0744,  0.1150, -0.0298, -0.0480,\n",
       "                      -0.0483, -0.0342, -0.0360, -0.0564,  0.0126,  0.0275, -0.0857, -0.0225])),\n",
       "             ('layer1.1.bn2.running_mean',\n",
       "              tensor([ 0.0102, -0.0690,  0.0350,  0.0036,  0.0063,  0.0106,  0.0243, -0.0134,\n",
       "                      -0.0309, -0.0345, -0.0316, -0.0664, -0.0597, -0.0349, -0.0539, -0.0014,\n",
       "                      -0.0247, -0.0049, -0.0015,  0.0067, -0.0417, -0.0345,  0.0074, -0.0020,\n",
       "                       0.0188, -0.0408, -0.0164, -0.0187,  0.0386, -0.0653, -0.0084, -0.0037,\n",
       "                      -0.0006,  0.0101,  0.0003, -0.0356, -0.0020, -0.0232, -0.0557, -0.0260,\n",
       "                      -0.0431, -0.0705, -0.0235, -0.0417, -0.0183, -0.0199, -0.0106, -0.0438,\n",
       "                      -0.0172, -0.0047,  0.0112, -0.0461, -0.0619, -0.0356, -0.0081, -0.0207,\n",
       "                      -0.0738, -0.0216, -0.0108, -0.0367,  0.0073, -0.0527, -0.0846, -0.0079])),\n",
       "             ('layer1.1.bn2.running_var',\n",
       "              tensor([8.4947e-03, 2.0522e-03, 2.1724e-03, 3.8903e-03, 6.6164e-03, 1.3162e-03,\n",
       "                      5.4760e-03, 3.8259e-03, 9.7777e-04, 2.5753e-03, 1.7288e-02, 1.0973e-02,\n",
       "                      1.1724e-02, 1.3184e-03, 3.9237e-03, 9.4750e-03, 1.3341e-02, 6.7923e-03,\n",
       "                      2.5117e-05, 1.1588e-02, 9.9126e-04, 1.2047e-02, 7.7026e-03, 4.6075e-03,\n",
       "                      5.0804e-03, 3.6411e-03, 8.1102e-04, 1.3800e-02, 1.8267e-02, 6.7677e-03,\n",
       "                      5.0376e-03, 2.8809e-03, 5.5590e-03, 9.7040e-04, 5.6913e-04, 2.7518e-03,\n",
       "                      1.3829e-02, 7.7127e-03, 7.9918e-03, 8.2215e-03, 7.0501e-03, 3.8308e-03,\n",
       "                      7.5306e-03, 3.5501e-03, 3.1542e-03, 6.3555e-03, 4.7167e-03, 1.1950e-03,\n",
       "                      2.1001e-03, 1.4178e-03, 5.8382e-03, 1.4094e-03, 9.9817e-03, 1.1654e-02,\n",
       "                      8.6086e-03, 1.8269e-02, 6.9271e-03, 1.4275e-03, 7.7686e-03, 1.1708e-02,\n",
       "                      2.4081e-04, 1.2718e-03, 6.1858e-03, 1.9871e-03])),\n",
       "             ('layer1.1.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.1.conv3.weight',\n",
       "              tensor([[[[-3.9441e-03]],\n",
       "              \n",
       "                       [[-2.3059e-03]],\n",
       "              \n",
       "                       [[-1.9473e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.7814e-03]],\n",
       "              \n",
       "                       [[-5.8718e-03]],\n",
       "              \n",
       "                       [[-7.4300e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.8352e-03]],\n",
       "              \n",
       "                       [[-4.0590e-03]],\n",
       "              \n",
       "                       [[-1.0463e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.0295e-04]],\n",
       "              \n",
       "                       [[-4.4775e-03]],\n",
       "              \n",
       "                       [[-3.0499e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1894e-03]],\n",
       "              \n",
       "                       [[ 3.1595e-03]],\n",
       "              \n",
       "                       [[ 1.2772e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7708e-03]],\n",
       "              \n",
       "                       [[ 1.0236e-02]],\n",
       "              \n",
       "                       [[ 3.3488e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.4599e-02]],\n",
       "              \n",
       "                       [[ 8.9698e-03]],\n",
       "              \n",
       "                       [[-8.3906e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2028e-02]],\n",
       "              \n",
       "                       [[-2.7143e-04]],\n",
       "              \n",
       "                       [[-2.0839e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.4236e-03]],\n",
       "              \n",
       "                       [[ 5.2815e-03]],\n",
       "              \n",
       "                       [[-1.5847e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0301e-03]],\n",
       "              \n",
       "                       [[-7.0921e-03]],\n",
       "              \n",
       "                       [[ 1.1001e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3164e-03]],\n",
       "              \n",
       "                       [[-6.8109e-03]],\n",
       "              \n",
       "                       [[ 1.3322e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.9192e-05]],\n",
       "              \n",
       "                       [[-5.4764e-03]],\n",
       "              \n",
       "                       [[ 4.4187e-03]]]])),\n",
       "             ('layer1.1.bn3.weight',\n",
       "              tensor([-1.3953e-02,  4.7727e-03, -3.8013e-03,  1.9026e-02, -5.1548e-03,\n",
       "                       1.4332e-02,  1.9028e-03,  5.8218e-02,  6.3033e-05,  4.6972e-02,\n",
       "                      -9.7782e-02,  3.5975e-02,  1.9933e-02, -7.0196e-03, -9.7592e-02,\n",
       "                      -1.0144e-02, -2.0102e-01, -2.4290e-01, -1.3963e-01, -2.5992e-02,\n",
       "                       3.0739e-02, -1.0602e-03, -3.8775e-02,  4.1123e-03, -7.9176e-02,\n",
       "                      -2.5367e-03,  6.4280e-03,  1.7524e-01, -1.0348e-01, -3.7385e-02,\n",
       "                       2.5607e-03, -4.1717e-02,  6.8569e-03, -3.8958e-03,  1.6737e-02,\n",
       "                      -1.9731e-03,  2.1188e-03, -5.9789e-02,  1.1622e-01,  1.1083e-03,\n",
       "                       1.5001e-01,  9.3316e-03, -1.0084e-02,  7.3661e-03,  1.7520e-03,\n",
       "                      -1.1988e-01,  1.9261e-01,  3.4368e-03, -2.7231e-02,  2.4854e-02,\n",
       "                       1.2138e-01, -3.7719e-02, -3.1286e-02, -2.7515e-03,  1.0622e-02,\n",
       "                       1.0400e-01,  2.1713e-02,  4.0561e-02,  8.9377e-03,  7.9409e-02,\n",
       "                       8.8903e-02, -5.3673e-03, -1.4194e-03,  6.6988e-02,  2.3137e-02,\n",
       "                       5.8584e-04, -7.1922e-07,  1.8160e-02,  5.5278e-03, -1.2493e-02,\n",
       "                      -2.3046e-02, -2.9161e-02, -4.9482e-02, -3.5051e-03, -6.5875e-04,\n",
       "                       1.6241e-01, -2.3093e-03,  5.3336e-03, -1.4666e-01, -3.3556e-02,\n",
       "                       5.4613e-03, -5.5904e-03, -7.3801e-02,  9.8129e-04, -7.1053e-03,\n",
       "                       2.5517e-02,  2.1920e-01,  8.5913e-02, -6.5990e-04, -2.7297e-02,\n",
       "                       3.5684e-02, -6.2072e-03, -1.0501e-01,  2.0531e-02,  1.1089e-02,\n",
       "                       6.8637e-03,  1.2442e-01,  5.8249e-04,  7.4627e-03, -3.5698e-03,\n",
       "                      -8.5268e-04, -4.6962e-02, -5.5643e-03,  2.2872e-02,  2.4273e-03,\n",
       "                       1.2027e-02,  5.8187e-02,  9.0849e-02,  8.0404e-03, -3.1247e-03,\n",
       "                       6.0422e-03, -5.6706e-04,  3.2139e-01, -1.8081e-02,  5.1591e-03,\n",
       "                      -2.1520e-03,  1.1459e-01,  1.3503e-01, -1.5084e-02, -5.1163e-03,\n",
       "                      -1.2746e-03,  8.0846e-03, -6.6392e-02, -8.4126e-02, -1.3962e-02,\n",
       "                       2.7361e-03, -5.3836e-03,  3.0389e-03, -1.1245e-02,  2.1554e-02,\n",
       "                       2.4910e-01, -3.8139e-03,  5.3456e-03,  1.5937e-02,  1.5521e-01,\n",
       "                       8.0546e-02, -1.8407e-02, -1.2377e-02,  5.7438e-03, -3.2318e-02,\n",
       "                       6.2327e-02, -3.4597e-03,  4.4047e-03,  7.6840e-02,  1.6459e-02,\n",
       "                      -3.6571e-03,  3.1081e-02,  4.8624e-04, -8.9808e-04,  3.8173e-02,\n",
       "                       1.3487e-02, -4.0561e-03,  1.2813e-01,  7.8012e-02,  6.4941e-03,\n",
       "                       4.4646e-02, -6.6159e-05,  6.5950e-03, -1.0869e-02, -3.0442e-03,\n",
       "                       6.9329e-03, -2.4326e-03,  2.2806e-02, -7.3252e-02, -5.7282e-03,\n",
       "                       5.3812e-02,  2.4172e-02,  1.4563e-02, -2.5306e-02, -9.1172e-02,\n",
       "                      -5.4052e-03,  1.5603e-02, -2.4031e-03, -3.1074e-02, -7.4985e-03,\n",
       "                      -1.5728e-01, -1.0622e-01, -9.3046e-02,  1.1979e-02, -6.8736e-03,\n",
       "                      -7.0189e-03, -7.4443e-02, -4.2100e-03, -2.3646e-05,  2.2194e-03,\n",
       "                       1.7070e-02, -2.9816e-03,  3.2667e-02,  6.9989e-02,  8.8683e-03,\n",
       "                       9.7118e-02, -5.3023e-03,  3.3766e-02, -1.3280e-01, -1.6019e-04,\n",
       "                       1.2891e-02,  2.2017e-01,  1.7616e-01,  4.3546e-02, -9.3908e-03,\n",
       "                      -5.3699e-02, -9.5560e-02, -1.5681e-02, -8.9023e-02,  1.8191e-02,\n",
       "                      -4.2625e-03, -2.9845e-03,  2.4064e-01, -7.5666e-02, -7.7354e-02,\n",
       "                      -2.9472e-02, -4.3267e-03, -1.7952e-02, -2.0754e-02, -4.6451e-02,\n",
       "                      -5.3040e-02, -1.7010e-02, -1.2338e-01, -1.4551e-03, -7.1791e-03,\n",
       "                      -1.4072e-01, -7.3148e-03,  9.8403e-03, -1.9049e-04, -6.2698e-02,\n",
       "                       2.8451e-02, -7.8296e-03,  3.4427e-02,  9.2509e-03, -1.3626e-01,\n",
       "                       8.1770e-03, -3.9766e-03, -2.3721e-03, -1.4534e-02,  4.3208e-02,\n",
       "                      -1.1617e-01, -3.0159e-02, -2.1287e-02,  7.0230e-02, -5.8158e-02,\n",
       "                      -7.9501e-03,  4.8663e-02, -4.3942e-02, -7.7969e-02, -3.4210e-03,\n",
       "                      -6.4594e-02,  5.4093e-03,  1.8430e-02,  5.6695e-03,  1.7786e-01,\n",
       "                       2.6587e-03,  1.3856e-01, -1.0760e-01,  1.1492e-01, -2.6358e-03,\n",
       "                       3.8702e-03])),\n",
       "             ('layer1.1.bn3.bias',\n",
       "              tensor([-1.9806e-02, -6.6240e-02, -2.4250e-02, -7.2695e-02, -4.0746e-03,\n",
       "                      -2.2825e-02,  1.3741e-03,  1.0482e-02,  1.0627e-02, -5.8268e-02,\n",
       "                       4.3404e-03,  5.5326e-03, -7.0924e-04, -1.0406e-02,  2.6026e-02,\n",
       "                      -3.9438e-02, -1.7814e-03, -2.3547e-02, -1.1937e-02,  2.9754e-04,\n",
       "                      -4.0448e-02,  7.5921e-03,  1.5023e-02,  1.1911e-02,  5.7250e-02,\n",
       "                      -1.4154e-02,  5.6424e-03, -1.5590e-02, -8.1841e-02, -2.2069e-02,\n",
       "                       9.4347e-03, -1.1942e-02, -5.9865e-02,  1.2526e-02,  1.6711e-02,\n",
       "                       6.7811e-03, -2.3285e-02,  2.9068e-03,  8.0818e-02,  7.2716e-03,\n",
       "                       3.0346e-02,  1.2551e-02,  2.0013e-02,  1.1093e-03, -7.5211e-03,\n",
       "                       5.9528e-02, -1.0133e-01,  6.2541e-03,  1.8337e-02,  6.9928e-03,\n",
       "                      -6.1967e-02,  6.4705e-03,  2.4272e-02,  1.4424e-02, -1.9033e-02,\n",
       "                       3.6094e-02,  2.1112e-02, -2.0202e-02,  1.7957e-04,  1.0862e-03,\n",
       "                      -1.2116e-02,  8.6860e-03,  1.9217e-03, -5.7888e-02,  9.7800e-03,\n",
       "                       9.3129e-03, -2.7551e-05, -4.2847e-02,  1.3974e-02,  6.7684e-03,\n",
       "                       9.2102e-04, -8.5575e-02, -1.2082e-02,  6.3425e-03,  1.3845e-03,\n",
       "                       4.2014e-02, -6.3714e-03,  1.2765e-02,  4.8790e-03, -1.8186e-02,\n",
       "                       9.6803e-03, -9.1332e-03, -2.9771e-02,  8.5280e-03, -2.8613e-02,\n",
       "                       4.8672e-04,  4.4051e-02, -3.9825e-02,  5.2660e-03, -4.9191e-02,\n",
       "                      -3.3102e-02,  5.9824e-03, -1.4073e-02,  4.1819e-03,  1.1172e-02,\n",
       "                       1.6723e-02,  8.5086e-02,  1.5495e-04,  9.4648e-03, -9.6250e-02,\n",
       "                       4.6032e-04, -5.1973e-03,  1.1569e-03, -6.6988e-03,  1.2910e-02,\n",
       "                      -3.0984e-02, -1.1282e-02,  3.1304e-02,  1.4933e-02,  2.4163e-03,\n",
       "                       1.0397e-02,  6.8851e-03, -3.2060e-02, -1.4014e-01,  3.2318e-03,\n",
       "                      -2.8751e-02, -9.0639e-03,  5.4253e-02, -3.8767e-03,  8.7915e-03,\n",
       "                      -4.4914e-03,  1.0883e-02,  1.9946e-02,  6.4011e-03,  1.9236e-02,\n",
       "                      -2.3965e-02,  9.8111e-03,  7.9853e-03, -8.7853e-03, -2.0867e-02,\n",
       "                       7.3223e-02,  1.7143e-02,  9.4074e-03, -2.5921e-02,  7.6661e-02,\n",
       "                      -2.3697e-02,  1.3350e-02,  1.8321e-02, -5.1617e-02,  2.4853e-02,\n",
       "                      -8.1478e-02,  3.5208e-03,  1.6157e-02, -2.7611e-02, -8.7126e-03,\n",
       "                       3.3275e-05, -1.7046e-02, -1.0448e-02,  6.7970e-03,  9.2191e-03,\n",
       "                      -2.0303e-02,  2.4409e-03,  2.0696e-02, -4.1213e-02,  6.6157e-03,\n",
       "                      -3.4297e-02,  1.1992e-02,  8.6535e-03,  1.6089e-02,  5.8564e-03,\n",
       "                       1.0299e-02,  9.2318e-03,  2.9707e-02,  5.4909e-02, -1.1628e-02,\n",
       "                       1.6990e-02,  1.2337e-02,  8.1866e-03,  1.9527e-02,  3.5972e-02,\n",
       "                       1.6759e-02, -2.5482e-03,  1.9757e-03,  3.5874e-02,  8.7840e-03,\n",
       "                       9.0697e-02,  6.0647e-02, -2.2201e-02, -1.9881e-03,  5.4265e-03,\n",
       "                       2.1325e-03, -2.3918e-02, -1.0887e-02,  3.9637e-03,  1.3848e-02,\n",
       "                      -2.2123e-02,  1.0907e-02,  2.2188e-02, -5.6873e-02,  9.4852e-03,\n",
       "                      -1.7066e-04,  1.8724e-02,  2.7090e-02,  6.3027e-02, -9.1199e-04,\n",
       "                      -3.8623e-03,  2.8179e-02, -4.8257e-02,  1.2264e-02,  1.0893e-02,\n",
       "                      -9.1983e-03,  5.9256e-02,  4.4387e-02, -8.1018e-03, -3.4738e-02,\n",
       "                      -3.8991e-03,  8.5866e-03,  2.4296e-02,  7.3699e-02, -3.3806e-02,\n",
       "                       5.1702e-02,  2.4357e-02,  6.3903e-03, -1.1790e-02, -1.0221e-01,\n",
       "                       5.4201e-02,  2.2545e-02, -3.8356e-02,  3.5345e-03,  1.3304e-02,\n",
       "                       4.5750e-02,  1.5329e-02,  4.7007e-03,  8.7053e-03,  2.2311e-02,\n",
       "                      -1.1323e-01,  2.7248e-02,  3.0284e-03,  3.9108e-02, -6.4164e-02,\n",
       "                      -6.2471e-03, -2.0630e-02,  2.2041e-02,  1.8789e-02,  2.6846e-02,\n",
       "                      -8.4164e-02, -8.2293e-03, -2.9405e-02,  2.8614e-02, -3.6054e-02,\n",
       "                      -3.7920e-03, -2.3722e-02, -2.9348e-02, -2.2093e-02,  2.5955e-02,\n",
       "                       1.7400e-02,  1.2531e-02,  6.6607e-04,  5.5180e-03, -2.5723e-02,\n",
       "                       1.8980e-02,  5.6036e-02, -4.7618e-02,  1.4167e-02,  7.9466e-03,\n",
       "                       2.2993e-02])),\n",
       "             ('layer1.1.bn3.running_mean',\n",
       "              tensor([-2.9361e-03, -2.6621e-03, -6.2138e-04, -1.1497e-03, -5.7649e-05,\n",
       "                      -2.3893e-03, -7.2623e-04, -6.4383e-03, -5.9624e-04, -1.0178e-02,\n",
       "                       1.4921e-02, -2.4761e-03,  1.5659e-05, -1.6290e-03,  2.5140e-02,\n",
       "                       9.8826e-03,  1.6988e-02,  4.8524e-02,  2.7649e-02, -1.4945e-02,\n",
       "                      -1.0488e-03, -3.8194e-03,  6.4187e-03, -1.0240e-03,  1.1187e-02,\n",
       "                      -2.3516e-03, -3.6324e-04, -3.1664e-03,  1.3785e-02, -1.0180e-02,\n",
       "                      -3.5130e-03,  3.0335e-03,  5.3257e-04,  4.2212e-03,  4.7790e-03,\n",
       "                      -2.1761e-03,  2.3157e-03,  1.2448e-02,  3.6648e-02, -3.5794e-04,\n",
       "                      -1.3857e-02, -4.6026e-03,  5.0978e-03, -1.0508e-03, -2.4759e-03,\n",
       "                       2.4867e-02, -5.0601e-03, -3.4616e-03, -6.7078e-03,  1.6267e-03,\n",
       "                      -2.0441e-02,  1.0383e-02, -2.9047e-03,  1.7392e-03, -3.8626e-03,\n",
       "                      -1.7488e-02,  8.7380e-03, -1.1372e-02, -9.2561e-04, -2.5176e-02,\n",
       "                      -5.7376e-03,  3.4982e-03, -5.5701e-04,  4.9129e-03, -2.9304e-03,\n",
       "                       2.3028e-03,  3.7957e-06, -5.2486e-03, -2.2269e-03,  3.0870e-03,\n",
       "                      -7.2287e-03,  3.2671e-03, -1.3836e-02,  2.2897e-03, -9.6576e-04,\n",
       "                      -2.3236e-02, -1.4662e-03, -3.2963e-03,  9.6377e-04, -4.6864e-03,\n",
       "                      -1.9639e-03,  3.1324e-03, -1.8228e-02,  1.2148e-04,  3.4431e-03,\n",
       "                      -5.1313e-03, -4.6792e-02,  1.3924e-02, -1.3331e-03,  1.4258e-03,\n",
       "                       1.6530e-03, -1.2753e-04,  1.7667e-02,  3.7180e-03,  3.6940e-03,\n",
       "                      -4.0345e-03, -1.0494e-02, -7.2214e-04, -2.7050e-03,  5.7770e-03,\n",
       "                       2.0154e-04, -8.2755e-03,  1.6317e-03,  1.2503e-03,  5.9386e-03,\n",
       "                      -1.0229e-03, -9.7627e-03, -2.7748e-02,  9.5698e-04,  7.0960e-04,\n",
       "                       2.0163e-04, -2.0590e-03, -7.7160e-02,  6.9493e-03, -1.8800e-03,\n",
       "                      -3.3594e-03, -1.4774e-02, -2.9242e-02, -1.6805e-03, -1.5173e-03,\n",
       "                       3.5548e-03,  5.6698e-04,  8.0609e-03, -7.9677e-04, -1.6109e-03,\n",
       "                       3.1013e-03,  1.1252e-03, -5.4872e-03, -1.2533e-03, -1.0493e-03,\n",
       "                      -4.0849e-02, -3.4945e-03,  1.4160e-04,  5.5616e-03, -3.8147e-02,\n",
       "                       2.9456e-03, -2.5884e-03,  1.4474e-03,  3.7723e-03, -3.5603e-03,\n",
       "                      -2.0525e-03,  3.3073e-03, -2.2682e-03,  7.2444e-03, -4.2180e-03,\n",
       "                       1.3968e-03, -9.8230e-03,  5.2903e-05,  2.5639e-03, -1.7756e-03,\n",
       "                       5.6664e-03,  1.7028e-03, -3.9694e-02,  2.2524e-03, -1.0131e-03,\n",
       "                       8.4611e-03,  1.7668e-03,  1.2916e-03,  1.3149e-02,  7.7185e-04,\n",
       "                      -1.1059e-03, -2.0704e-03, -2.0994e-03, -2.7286e-02, -1.2753e-04,\n",
       "                      -7.5244e-03, -3.6064e-03, -4.0215e-03,  8.0408e-03,  1.4346e-02,\n",
       "                      -3.9134e-03, -4.0122e-03,  2.2367e-04, -4.3367e-03,  2.1079e-03,\n",
       "                      -4.4170e-04,  2.4760e-02, -4.0756e-03, -4.3714e-03,  9.0038e-04,\n",
       "                       2.7031e-03,  1.5712e-02,  1.2267e-03,  3.6061e-03, -2.2509e-03,\n",
       "                      -2.8236e-05, -1.4013e-03, -1.3578e-03, -9.6120e-04,  4.8848e-03,\n",
       "                       4.2064e-02,  5.1273e-03, -8.4651e-03, -5.2860e-03,  1.3595e-04,\n",
       "                      -2.7581e-03, -2.5265e-02, -6.7655e-03, -5.3021e-03,  4.5257e-03,\n",
       "                       1.3272e-02, -2.2860e-02,  2.5522e-03, -3.5989e-02, -6.1862e-04,\n",
       "                       2.9232e-05, -2.8152e-03, -4.3341e-02, -2.9317e-02,  1.4149e-02,\n",
       "                      -3.7849e-03,  1.2192e-03, -6.2610e-03,  3.8681e-03,  1.3212e-02,\n",
       "                      -1.8895e-02, -8.8108e-04, -1.2038e-02,  3.0013e-04,  6.5448e-03,\n",
       "                      -2.5335e-02,  3.3982e-03, -2.1676e-03, -3.2817e-03,  3.4690e-02,\n",
       "                      -9.2186e-04,  7.6254e-03,  6.0784e-03, -3.8929e-03,  1.4682e-02,\n",
       "                       5.1211e-03, -1.8987e-03,  1.4604e-03,  3.8751e-03, -1.0101e-02,\n",
       "                       1.5578e-03,  7.7107e-03, -1.8240e-03,  9.7006e-03,  7.8780e-03,\n",
       "                       2.6198e-03,  4.6424e-05,  1.0518e-03,  8.4009e-03,  9.5303e-04,\n",
       "                       2.0223e-02,  5.5037e-03, -3.2022e-03, -6.0889e-03,  7.1385e-03,\n",
       "                      -4.3725e-03, -1.8613e-02,  1.5191e-02, -1.3931e-02,  1.8277e-03,\n",
       "                      -2.1094e-03])),\n",
       "             ('layer1.1.bn3.running_var',\n",
       "              tensor([2.9843e-05, 3.1927e-05, 4.2738e-05, 1.1764e-04, 3.1990e-05, 1.1281e-04,\n",
       "                      7.0193e-05, 1.2890e-04, 2.6771e-04, 1.1772e-04, 2.8510e-04, 2.0312e-04,\n",
       "                      1.7087e-04, 1.0929e-05, 2.8162e-04, 7.5299e-05, 1.6775e-03, 1.7741e-03,\n",
       "                      4.1308e-04, 9.5118e-05, 1.7703e-04, 1.4596e-04, 7.2107e-05, 1.0158e-04,\n",
       "                      3.2324e-04, 1.1628e-04, 7.6114e-05, 6.9240e-04, 3.5589e-04, 2.0982e-04,\n",
       "                      1.4234e-04, 1.3337e-04, 1.9467e-05, 4.2583e-05, 1.2779e-04, 6.9841e-05,\n",
       "                      1.2418e-04, 1.9325e-04, 1.7354e-03, 1.3586e-05, 6.3502e-04, 2.0036e-04,\n",
       "                      1.1193e-04, 8.2098e-05, 1.6641e-04, 4.2859e-04, 1.9252e-03, 2.1691e-04,\n",
       "                      3.4709e-04, 3.8277e-05, 1.0836e-03, 7.2126e-04, 4.6542e-04, 1.4864e-04,\n",
       "                      1.5220e-05, 5.7258e-04, 1.4536e-04, 1.6666e-04, 2.6152e-05, 4.0066e-04,\n",
       "                      5.7731e-04, 2.4162e-04, 1.1892e-05, 4.0217e-04, 2.1084e-04, 1.3137e-04,\n",
       "                      1.9952e-11, 4.5269e-05, 1.1595e-04, 2.1850e-04, 4.5011e-05, 1.8395e-04,\n",
       "                      3.5272e-04, 4.6863e-05, 1.1230e-05, 7.1001e-04, 1.0781e-05, 1.1871e-04,\n",
       "                      6.7140e-04, 4.4061e-05, 1.6183e-04, 1.9942e-05, 3.5673e-04, 5.1215e-05,\n",
       "                      1.1038e-04, 4.3504e-04, 1.5713e-03, 2.8449e-04, 1.9298e-04, 4.2234e-04,\n",
       "                      5.3147e-05, 5.7139e-05, 4.4682e-04, 1.0901e-04, 9.4247e-05, 2.6558e-04,\n",
       "                      5.7297e-04, 4.4519e-05, 1.3055e-04, 1.0568e-04, 1.2231e-06, 2.0580e-04,\n",
       "                      1.6037e-05, 2.0925e-05, 4.0847e-04, 9.8740e-05, 1.2170e-04, 7.1105e-04,\n",
       "                      2.6536e-04, 2.6119e-05, 2.2941e-04, 7.6617e-05, 2.8141e-03, 1.8674e-04,\n",
       "                      2.1074e-04, 1.1746e-04, 4.7014e-04, 6.0162e-04, 4.4777e-05, 4.3436e-05,\n",
       "                      8.3857e-05, 1.3092e-04, 1.8486e-04, 4.6030e-04, 2.1649e-04, 6.1043e-05,\n",
       "                      5.0563e-05, 2.5440e-04, 1.1733e-05, 1.5078e-04, 2.2535e-03, 4.0904e-04,\n",
       "                      1.7144e-04, 3.6805e-05, 1.0225e-03, 1.4220e-04, 6.3703e-05, 1.3039e-04,\n",
       "                      4.1054e-05, 1.6201e-04, 2.9810e-04, 6.1558e-05, 1.2549e-04, 2.0593e-04,\n",
       "                      4.3903e-05, 1.5744e-05, 1.6542e-04, 8.4077e-06, 2.8909e-05, 2.1689e-04,\n",
       "                      2.0688e-04, 3.3637e-05, 1.3534e-03, 1.2875e-04, 1.2262e-04, 3.9909e-04,\n",
       "                      1.5573e-04, 2.8367e-05, 3.3567e-04, 2.2566e-04, 8.0608e-05, 1.2208e-04,\n",
       "                      1.5664e-04, 7.0952e-04, 2.2329e-05, 6.0239e-04, 1.1610e-04, 1.4904e-04,\n",
       "                      1.0048e-04, 2.9258e-04, 1.9321e-04, 5.8679e-05, 8.9115e-06, 3.2282e-04,\n",
       "                      1.4581e-04, 6.8847e-04, 4.6247e-04, 1.8894e-04, 7.2929e-05, 2.5943e-05,\n",
       "                      5.3461e-05, 4.4871e-04, 2.9634e-05, 4.7606e-05, 1.2231e-04, 4.6474e-05,\n",
       "                      5.8049e-05, 1.4585e-04, 3.1201e-04, 6.8322e-05, 6.7981e-04, 3.8190e-04,\n",
       "                      3.6023e-04, 5.3356e-04, 1.7001e-07, 3.6018e-05, 1.4420e-03, 1.0449e-03,\n",
       "                      2.2788e-04, 1.1987e-04, 1.0546e-04, 9.9772e-04, 2.9572e-04, 5.1962e-04,\n",
       "                      2.0571e-05, 1.9884e-05, 1.7559e-05, 1.8297e-03, 1.0375e-03, 1.8207e-04,\n",
       "                      3.4011e-04, 1.9122e-04, 4.5221e-05, 1.2032e-04, 5.0837e-04, 4.3126e-04,\n",
       "                      4.3555e-04, 3.6687e-04, 1.6154e-05, 2.4037e-04, 2.0364e-03, 7.4243e-05,\n",
       "                      1.8137e-05, 1.7027e-04, 5.0076e-04, 2.4689e-04, 6.8106e-05, 1.0340e-04,\n",
       "                      1.6130e-04, 1.9068e-03, 2.3913e-04, 4.0696e-05, 5.9720e-05, 2.1741e-04,\n",
       "                      5.9412e-04, 6.2778e-04, 2.2928e-05, 7.3832e-05, 5.2367e-04, 2.4639e-04,\n",
       "                      2.7286e-05, 4.9779e-05, 4.0498e-05, 1.2420e-04, 2.6209e-04, 2.1214e-04,\n",
       "                      1.9872e-04, 1.4944e-04, 3.3978e-04, 6.0489e-04, 1.0529e-04, 5.5494e-04,\n",
       "                      1.0443e-03, 3.5717e-04, 8.5821e-05, 1.0474e-04])),\n",
       "             ('layer1.1.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.2.conv1.weight',\n",
       "              tensor([[[[-0.0103]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       [[-0.0016]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0113]],\n",
       "              \n",
       "                       [[-0.0057]],\n",
       "              \n",
       "                       [[ 0.0072]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0016]],\n",
       "              \n",
       "                       [[-0.0059]],\n",
       "              \n",
       "                       [[ 0.0010]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0103]],\n",
       "              \n",
       "                       [[ 0.0066]],\n",
       "              \n",
       "                       [[-0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0067]],\n",
       "              \n",
       "                       [[ 0.0042]],\n",
       "              \n",
       "                       [[ 0.0004]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0063]],\n",
       "              \n",
       "                       [[ 0.0076]],\n",
       "              \n",
       "                       [[-0.0003]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0018]],\n",
       "              \n",
       "                       [[ 0.0031]],\n",
       "              \n",
       "                       [[-0.0036]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0007]],\n",
       "              \n",
       "                       [[ 0.0147]],\n",
       "              \n",
       "                       [[-0.0098]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0004]],\n",
       "              \n",
       "                       [[ 0.0055]],\n",
       "              \n",
       "                       [[-0.0029]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0111]],\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       [[ 0.0032]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0001]],\n",
       "              \n",
       "                       [[ 0.0047]],\n",
       "              \n",
       "                       [[ 0.0014]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0124]],\n",
       "              \n",
       "                       [[-0.0282]],\n",
       "              \n",
       "                       [[ 0.0005]]]])),\n",
       "             ('layer1.2.bn1.weight',\n",
       "              tensor([7.7192e-02, 5.1663e-02, 4.5441e-02, 1.0562e-01, 2.6634e-02, 1.3134e-01,\n",
       "                      6.8678e-02, 1.2836e-01, 8.8034e-02, 1.4523e-01, 8.1503e-02, 1.1258e-01,\n",
       "                      1.6611e-01, 9.2463e-02, 3.8446e-02, 5.6668e-02, 8.9584e-02, 1.0393e-01,\n",
       "                      9.7935e-02, 1.0410e-02, 5.2782e-02, 1.2814e-01, 6.8586e-02, 8.6610e-02,\n",
       "                      3.5081e-02, 6.9598e-02, 5.7344e-02, 4.1121e-02, 3.2654e-02, 6.1724e-02,\n",
       "                      1.1870e-01, 1.2165e-01, 7.0147e-02, 1.0003e-01, 9.8955e-02, 4.9179e-02,\n",
       "                      1.3992e-04, 5.1776e-02, 3.9381e-02, 1.8487e-02, 3.0308e-02, 7.7979e-02,\n",
       "                      7.7158e-02, 9.3039e-02, 4.8514e-02, 3.2039e-04, 1.0694e-01, 9.3818e-02,\n",
       "                      2.8044e-02, 9.3051e-02, 3.3241e-02, 7.9940e-02, 5.6791e-02, 4.6530e-02,\n",
       "                      3.6647e-02, 8.2822e-02, 1.1005e-01, 1.1281e-01, 7.6101e-02, 8.7502e-02,\n",
       "                      5.5243e-02, 3.8794e-02, 7.3535e-02, 8.1255e-02])),\n",
       "             ('layer1.2.bn1.bias',\n",
       "              tensor([ 0.1061,  0.0377, -0.0371,  0.0284,  0.0120, -0.0022,  0.0228,  0.0005,\n",
       "                       0.0110,  0.0399, -0.0380, -0.0807,  0.0991,  0.0379,  0.0077,  0.0537,\n",
       "                       0.0382, -0.0768, -0.0386, -0.0056, -0.0072, -0.0738,  0.0291, -0.0542,\n",
       "                      -0.0206,  0.0762,  0.0151,  0.0091,  0.0153, -0.0276, -0.1286,  0.0114,\n",
       "                      -0.0188,  0.0376,  0.0578,  0.0095, -0.0004, -0.0351, -0.0128,  0.0117,\n",
       "                      -0.0019,  0.0133,  0.0007,  0.0297,  0.0213, -0.0023, -0.0787,  0.1615,\n",
       "                      -0.0055,  0.0239, -0.0236,  0.0555, -0.0034, -0.0073,  0.0043,  0.0599,\n",
       "                       0.0225,  0.0130, -0.0904, -0.0354, -0.0559,  0.0402,  0.0166,  0.0317])),\n",
       "             ('layer1.2.bn1.running_mean',\n",
       "              tensor([ 5.9817e-03,  3.5510e-02,  1.0258e-02,  3.1906e-02, -8.5856e-03,\n",
       "                      -4.3159e-02, -3.7692e-02,  7.9173e-02, -5.2529e-02, -1.0910e-02,\n",
       "                      -1.4034e-02,  8.3410e-02, -2.9442e-02, -4.6106e-02, -1.1906e-02,\n",
       "                      -2.0592e-02,  2.2043e-02,  8.5614e-02, -3.7864e-02,  3.2142e-03,\n",
       "                       3.0310e-03,  7.0063e-03, -4.2130e-02,  4.6088e-02,  1.4249e-02,\n",
       "                       6.7997e-04, -5.7250e-03, -1.4784e-02,  5.5008e-03,  4.8987e-02,\n",
       "                       7.8737e-02,  4.6363e-02, -2.6166e-02, -8.5359e-03, -6.3423e-02,\n",
       "                      -8.0072e-04, -3.8750e-05,  2.5310e-02, -1.0030e-02, -9.0353e-05,\n",
       "                      -2.0775e-02,  9.4694e-03,  3.0178e-02, -7.0126e-02, -2.0495e-02,\n",
       "                      -1.3644e-05,  9.7370e-02,  2.6291e-03,  4.9503e-03,  9.3736e-03,\n",
       "                       1.9445e-02,  2.0810e-02, -4.2776e-03, -2.4975e-02, -6.1855e-02,\n",
       "                      -2.2552e-02,  2.2647e-03, -2.0398e-02,  4.0204e-02, -5.4551e-02,\n",
       "                       6.4923e-02, -2.0440e-02,  9.8329e-03, -1.9171e-02])),\n",
       "             ('layer1.2.bn1.running_var',\n",
       "              tensor([5.2332e-03, 2.2178e-03, 9.6015e-04, 6.2137e-03, 3.1190e-04, 9.4402e-03,\n",
       "                      3.0642e-03, 6.1377e-03, 1.7949e-03, 2.2096e-02, 2.1350e-03, 4.5740e-03,\n",
       "                      1.8859e-02, 5.3859e-03, 5.3227e-04, 1.8818e-03, 3.0097e-03, 2.7658e-03,\n",
       "                      1.8591e-03, 3.1866e-05, 8.4774e-04, 3.4888e-03, 2.3882e-03, 1.5316e-03,\n",
       "                      1.5715e-04, 6.3087e-03, 1.1933e-03, 5.8693e-04, 5.2206e-04, 8.3596e-04,\n",
       "                      6.7489e-03, 7.4872e-03, 1.6674e-03, 3.3479e-03, 4.2936e-03, 1.2566e-03,\n",
       "                      1.2370e-07, 5.1263e-04, 5.3792e-04, 1.5570e-04, 8.1356e-04, 2.5348e-03,\n",
       "                      2.5466e-03, 5.1314e-03, 1.7735e-03, 1.6712e-06, 3.1521e-03, 9.5372e-03,\n",
       "                      3.3969e-04, 5.1603e-03, 1.8079e-04, 3.1545e-03, 7.6349e-04, 6.5824e-04,\n",
       "                      1.5269e-03, 5.8566e-03, 4.9678e-03, 4.9602e-03, 1.2163e-03, 2.6122e-03,\n",
       "                      9.9462e-04, 2.3566e-03, 1.4457e-03, 3.7936e-03])),\n",
       "             ('layer1.2.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.2.conv2.weight',\n",
       "              tensor([[[[ 4.2594e-03, -1.4390e-03, -6.2478e-03],\n",
       "                        [-5.3543e-03, -9.6090e-03, -3.6061e-03],\n",
       "                        [ 1.5650e-03,  8.5544e-03,  1.2521e-02]],\n",
       "              \n",
       "                       [[ 3.3251e-03,  3.2828e-03, -3.4757e-03],\n",
       "                        [-1.2382e-03, -5.9389e-03, -4.3720e-03],\n",
       "                        [-3.4050e-03,  4.4025e-04,  3.9605e-03]],\n",
       "              \n",
       "                       [[-6.2810e-03,  6.0232e-04,  1.1003e-05],\n",
       "                        [-2.0185e-03,  3.6389e-03,  4.8989e-03],\n",
       "                        [-1.5084e-03,  3.3615e-03,  1.6591e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.1651e-03, -2.8170e-03,  5.6597e-04],\n",
       "                        [ 3.1164e-03,  4.6858e-03,  7.4352e-03],\n",
       "                        [ 4.0521e-03, -1.2078e-04, -2.0604e-03]],\n",
       "              \n",
       "                       [[-2.8629e-03, -2.7663e-03, -4.0775e-03],\n",
       "                        [-2.2914e-04, -4.3333e-03, -2.0618e-03],\n",
       "                        [-2.5081e-03, -1.6714e-04,  1.4486e-03]],\n",
       "              \n",
       "                       [[ 7.9840e-03,  2.1678e-03, -1.5426e-03],\n",
       "                        [-5.5143e-03, -1.0147e-02, -5.0557e-03],\n",
       "                        [-5.6771e-03,  1.1836e-03,  3.1229e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6378e-02,  1.8031e-02,  8.3881e-03],\n",
       "                        [-3.5149e-03, -1.8821e-02,  1.5029e-02],\n",
       "                        [ 2.0330e-02,  1.8228e-02, -9.4644e-03]],\n",
       "              \n",
       "                       [[ 3.5391e-03,  1.2355e-02,  2.3070e-03],\n",
       "                        [-1.2801e-02,  3.3657e-03, -9.1217e-03],\n",
       "                        [-1.2249e-02,  5.6946e-04, -1.6004e-02]],\n",
       "              \n",
       "                       [[-2.2948e-04, -6.5037e-03, -1.3101e-03],\n",
       "                        [ 8.4230e-03,  5.1369e-03, -5.5104e-04],\n",
       "                        [ 7.5699e-03,  5.2547e-03,  5.2908e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.2392e-03,  2.1685e-03,  3.3162e-04],\n",
       "                        [ 1.4664e-02,  8.0389e-03, -5.1428e-03],\n",
       "                        [ 8.6378e-03,  2.8397e-03,  1.0883e-02]],\n",
       "              \n",
       "                       [[-2.1845e-02, -2.1524e-03,  4.0098e-03],\n",
       "                        [ 1.1325e-02, -1.0426e-02,  1.8473e-03],\n",
       "                        [ 2.4003e-02,  7.6488e-03, -4.2123e-03]],\n",
       "              \n",
       "                       [[-2.8167e-02, -3.5836e-03,  6.6944e-03],\n",
       "                        [ 9.6202e-03, -2.4267e-02,  1.9179e-02],\n",
       "                        [ 2.9797e-03, -1.2356e-03, -1.3519e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.5613e-03,  2.2704e-02, -2.6681e-02],\n",
       "                        [ 1.9961e-03, -2.4275e-02,  6.0333e-03],\n",
       "                        [-2.5307e-02,  3.3367e-02,  2.9109e-03]],\n",
       "              \n",
       "                       [[-5.9781e-03, -1.1376e-03,  6.4963e-04],\n",
       "                        [-1.4401e-04, -6.9976e-03,  9.2614e-03],\n",
       "                        [-1.5398e-02,  8.1051e-03,  1.3574e-03]],\n",
       "              \n",
       "                       [[-5.9844e-03, -1.7658e-03,  3.1173e-03],\n",
       "                        [-7.1971e-03,  8.7010e-06,  9.6783e-03],\n",
       "                        [-6.5437e-04,  2.3203e-03, -2.8859e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.6113e-03, -6.3358e-04,  5.2060e-04],\n",
       "                        [ 3.3652e-03,  3.7454e-03,  7.5496e-04],\n",
       "                        [ 6.4116e-03, -7.3237e-03,  3.3839e-04]],\n",
       "              \n",
       "                       [[ 4.4224e-03,  7.6644e-03, -1.4435e-02],\n",
       "                        [ 4.4348e-03, -1.6545e-02,  1.0013e-03],\n",
       "                        [-3.5773e-03,  1.2099e-02,  2.8949e-03]],\n",
       "              \n",
       "                       [[ 7.0719e-05,  1.2652e-02, -1.5102e-02],\n",
       "                        [-3.2092e-03, -1.7059e-02,  2.7249e-03],\n",
       "                        [-2.5732e-03,  1.2973e-02,  1.1443e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-7.5354e-03,  1.6481e-03, -1.4859e-02],\n",
       "                        [-6.3882e-03,  8.7650e-03, -8.8593e-03],\n",
       "                        [-4.2392e-03,  6.0814e-03, -9.1545e-03]],\n",
       "              \n",
       "                       [[-5.1801e-03, -2.1711e-03, -7.5737e-03],\n",
       "                        [-1.1958e-04, -3.8650e-03, -1.8044e-03],\n",
       "                        [ 8.6672e-04, -6.0316e-03,  4.1415e-03]],\n",
       "              \n",
       "                       [[-1.2653e-03, -5.5778e-05, -3.6896e-03],\n",
       "                        [-6.2664e-04,  2.6166e-03, -3.2538e-03],\n",
       "                        [-4.3746e-03,  1.3280e-04, -4.2426e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7224e-03,  4.8781e-03, -1.2534e-03],\n",
       "                        [ 4.4848e-04,  2.3351e-03, -2.5584e-03],\n",
       "                        [-9.9019e-04, -7.5015e-04, -2.7833e-03]],\n",
       "              \n",
       "                       [[-5.1755e-03,  2.3807e-03, -4.1359e-03],\n",
       "                        [-4.1914e-03,  9.9922e-03, -5.6120e-03],\n",
       "                        [-3.1698e-04,  1.1087e-02, -1.5115e-03]],\n",
       "              \n",
       "                       [[ 1.7927e-03, -6.7809e-03, -7.5349e-03],\n",
       "                        [-5.6149e-03,  5.6423e-03, -1.0170e-02],\n",
       "                        [ 1.6794e-03,  9.5215e-03, -8.4830e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.5863e-03, -5.9681e-03,  4.6110e-03],\n",
       "                        [ 2.4714e-02, -5.7099e-03, -5.1229e-03],\n",
       "                        [-1.4665e-02,  3.6224e-02, -2.6537e-03]],\n",
       "              \n",
       "                       [[-7.0073e-03,  1.0449e-03,  8.5916e-03],\n",
       "                        [-1.3894e-02, -1.3049e-02,  1.1813e-02],\n",
       "                        [ 1.0476e-02, -2.2225e-03,  1.6280e-04]],\n",
       "              \n",
       "                       [[ 1.1509e-02,  3.1159e-03, -5.7931e-03],\n",
       "                        [-7.4160e-03,  1.5941e-02,  4.8749e-03],\n",
       "                        [-3.0179e-03, -1.6039e-03, -2.5597e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4639e-03, -2.7422e-03, -3.5235e-03],\n",
       "                        [-9.9288e-04,  9.7663e-03,  2.5324e-03],\n",
       "                        [-9.5563e-03, -8.8548e-04,  4.5111e-03]],\n",
       "              \n",
       "                       [[-1.2705e-03, -1.6997e-02, -1.2660e-02],\n",
       "                        [ 3.1189e-02,  1.2010e-02, -1.4788e-02],\n",
       "                        [-2.7347e-02,  2.1408e-02, -5.6522e-03]],\n",
       "              \n",
       "                       [[ 2.4667e-02, -9.2070e-03, -7.3312e-04],\n",
       "                        [ 3.2848e-02,  8.0676e-04, -4.0286e-02],\n",
       "                        [-2.8364e-02,  2.7848e-02, -1.8816e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1035e-02, -1.1093e-02,  1.0960e-03],\n",
       "                        [-1.0686e-02, -8.7928e-03,  2.9368e-03],\n",
       "                        [-8.0992e-03,  8.0328e-03, -3.6836e-04]],\n",
       "              \n",
       "                       [[-2.7665e-04,  2.3892e-03, -4.7622e-03],\n",
       "                        [-3.9461e-03, -4.8122e-03, -6.5323e-03],\n",
       "                        [-1.2618e-02, -1.4420e-02, -9.0237e-03]],\n",
       "              \n",
       "                       [[-4.3802e-03, -9.8658e-04, -1.8084e-03],\n",
       "                        [-6.5991e-03, -2.4903e-03, -6.4172e-03],\n",
       "                        [-1.0491e-03,  5.6414e-03,  4.4412e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.6415e-04,  2.7918e-03,  7.3648e-05],\n",
       "                        [ 7.2283e-03,  1.0943e-02,  5.2849e-03],\n",
       "                        [ 1.0490e-02,  1.5081e-02,  1.1923e-02]],\n",
       "              \n",
       "                       [[-7.3927e-03, -4.0568e-03, -6.2524e-03],\n",
       "                        [-2.4187e-03,  2.5705e-03,  7.0531e-04],\n",
       "                        [ 6.0257e-04,  1.8297e-02,  7.3894e-04]],\n",
       "              \n",
       "                       [[-4.5202e-03, -7.9500e-04,  1.7152e-03],\n",
       "                        [-1.2943e-02, -9.2897e-03,  2.1254e-03],\n",
       "                        [-1.5981e-02,  1.3064e-03, -1.5346e-02]]]])),\n",
       "             ('layer1.2.bn2.weight',\n",
       "              tensor([0.0538, 0.1307, 0.1118, 0.1037, 0.1615, 0.1361, 0.1411, 0.0813, 0.1269,\n",
       "                      0.1582, 0.0849, 0.1150, 0.1935, 0.1287, 0.1012, 0.1853, 0.1604, 0.0758,\n",
       "                      0.1777, 0.1700, 0.0307, 0.1680, 0.2056, 0.0273, 0.1187, 0.1215, 0.0563,\n",
       "                      0.0084, 0.1646, 0.0254, 0.1137, 0.1213, 0.0531, 0.0503, 0.0822, 0.1485,\n",
       "                      0.1248, 0.0549, 0.1476, 0.0852, 0.1422, 0.0333, 0.0928, 0.0958, 0.1090,\n",
       "                      0.1121, 0.1378, 0.1243, 0.1227, 0.1085, 0.0875, 0.1350, 0.0405, 0.0187,\n",
       "                      0.1055, 0.0936, 0.0687, 0.0343, 0.0810, 0.0822, 0.2055, 0.1766, 0.1633,\n",
       "                      0.0880])),\n",
       "             ('layer1.2.bn2.bias',\n",
       "              tensor([-0.0141, -0.0810, -0.0396, -0.0279, -0.0553, -0.0426, -0.0575, -0.0359,\n",
       "                      -0.0822, -0.0593, -0.0483, -0.0402, -0.0292, -0.0104, -0.0407, -0.0539,\n",
       "                      -0.0206, -0.0381, -0.0784, -0.0622, -0.0104, -0.0911, -0.1036,  0.0056,\n",
       "                      -0.0293, -0.0492, -0.0210, -0.0039, -0.0580, -0.0142, -0.0422, -0.0431,\n",
       "                      -0.0275, -0.0212, -0.0399, -0.0518, -0.0344,  0.0025, -0.0929, -0.0536,\n",
       "                      -0.0777, -0.0103, -0.0132, -0.0401, -0.0172, -0.0478, -0.0452, -0.0195,\n",
       "                      -0.0403, -0.0421, -0.0342, -0.0651, -0.0112, -0.0078, -0.0462, -0.0349,\n",
       "                      -0.0394, -0.0189, -0.0271, -0.0394, -0.0500, -0.0916, -0.0684, -0.0414])),\n",
       "             ('layer1.2.bn2.running_mean',\n",
       "              tensor([ 0.0017, -0.0417, -0.0158, -0.0211,  0.0024, -0.0358, -0.0213, -0.0197,\n",
       "                      -0.0314, -0.0296,  0.0176, -0.0442,  0.0227, -0.0067, -0.0383,  0.0114,\n",
       "                       0.0084, -0.0037, -0.0070, -0.0162, -0.0016, -0.0361, -0.0158, -0.0135,\n",
       "                      -0.0261, -0.0569, -0.0189, -0.0024, -0.0466, -0.0045,  0.0052, -0.0270,\n",
       "                      -0.0066, -0.0099, -0.0060, -0.0419, -0.0535, -0.0321,  0.0040, -0.0188,\n",
       "                      -0.0228, -0.0050, -0.0063, -0.0306, -0.0187, -0.0061, -0.0196, -0.0348,\n",
       "                       0.0106, -0.0165, -0.0119, -0.0105, -0.0075, -0.0003, -0.0103, -0.0048,\n",
       "                      -0.0230,  0.0048, -0.0360, -0.0104, -0.0350, -0.0152, -0.0245, -0.0437])),\n",
       "             ('layer1.2.bn2.running_var',\n",
       "              tensor([7.6277e-04, 3.1996e-03, 2.2891e-03, 2.5475e-03, 7.0652e-03, 3.0696e-03,\n",
       "                      5.3842e-03, 1.7642e-03, 1.0961e-03, 5.0183e-03, 5.9130e-04, 3.0781e-03,\n",
       "                      1.6251e-02, 7.3371e-03, 3.1310e-03, 1.1143e-02, 4.9899e-03, 5.0866e-04,\n",
       "                      8.4530e-03, 7.1512e-03, 2.6198e-04, 6.5775e-03, 3.3286e-03, 3.5651e-04,\n",
       "                      3.7192e-03, 2.2156e-03, 4.2231e-04, 1.4175e-05, 7.9873e-03, 1.2549e-04,\n",
       "                      3.7341e-03, 4.0974e-03, 1.6949e-04, 5.7177e-04, 1.8309e-03, 5.2476e-03,\n",
       "                      3.6266e-03, 6.9315e-04, 1.7192e-03, 1.3898e-03, 8.5338e-03, 2.1499e-04,\n",
       "                      3.4568e-03, 6.5839e-04, 2.5242e-03, 3.2021e-03, 7.0318e-03, 5.9140e-03,\n",
       "                      1.0136e-03, 1.6004e-03, 1.9577e-03, 3.5310e-03, 3.8534e-04, 1.2555e-04,\n",
       "                      1.9540e-03, 2.9833e-03, 6.2011e-04, 9.5636e-05, 1.8159e-03, 6.4056e-04,\n",
       "                      1.7203e-02, 2.4275e-03, 3.7445e-03, 1.5998e-03])),\n",
       "             ('layer1.2.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer1.2.conv3.weight',\n",
       "              tensor([[[[-1.0725e-04]],\n",
       "              \n",
       "                       [[ 2.0041e-05]],\n",
       "              \n",
       "                       [[-1.4311e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3760e-03]],\n",
       "              \n",
       "                       [[-1.4120e-03]],\n",
       "              \n",
       "                       [[ 7.3110e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.8773e-04]],\n",
       "              \n",
       "                       [[ 2.7492e-03]],\n",
       "              \n",
       "                       [[-1.2128e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.3785e-03]],\n",
       "              \n",
       "                       [[ 1.6977e-03]],\n",
       "              \n",
       "                       [[-2.2510e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.6219e-03]],\n",
       "              \n",
       "                       [[-6.1931e-03]],\n",
       "              \n",
       "                       [[ 1.7963e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1283e-03]],\n",
       "              \n",
       "                       [[ 1.6391e-02]],\n",
       "              \n",
       "                       [[-2.6992e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.8833e-04]],\n",
       "              \n",
       "                       [[ 7.4731e-03]],\n",
       "              \n",
       "                       [[ 2.5384e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2519e-02]],\n",
       "              \n",
       "                       [[ 1.5877e-02]],\n",
       "              \n",
       "                       [[ 1.5746e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.6056e-04]],\n",
       "              \n",
       "                       [[-1.1684e-03]],\n",
       "              \n",
       "                       [[-1.1114e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4854e-02]],\n",
       "              \n",
       "                       [[ 3.0946e-03]],\n",
       "              \n",
       "                       [[ 1.2172e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4610e-03]],\n",
       "              \n",
       "                       [[ 2.1399e-02]],\n",
       "              \n",
       "                       [[-1.9268e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3172e-03]],\n",
       "              \n",
       "                       [[ 1.1330e-02]],\n",
       "              \n",
       "                       [[ 4.5628e-03]]]])),\n",
       "             ('layer1.2.bn3.weight',\n",
       "              tensor([-5.6602e-05,  6.2128e-03, -9.0309e-02, -8.8959e-02,  3.4100e-03,\n",
       "                      -4.4489e-02,  1.0413e-02,  2.2673e-02, -1.2147e-02,  1.5658e-02,\n",
       "                       8.2988e-03,  1.8284e-02, -2.6021e-03, -1.7678e-03, -1.0871e-03,\n",
       "                       5.2955e-03,  9.7255e-02,  1.2701e-01,  3.7143e-03,  2.5493e-03,\n",
       "                       5.7748e-02,  2.9982e-03,  4.6578e-02, -2.0088e-03,  6.2894e-03,\n",
       "                      -8.8489e-03,  9.2718e-04,  9.0317e-02, -8.7421e-02, -4.4853e-02,\n",
       "                       4.2581e-03,  9.0715e-02,  1.6322e-02,  1.9042e-03,  4.9676e-03,\n",
       "                       3.2938e-02,  5.3201e-02,  5.6470e-02, -8.0003e-03, -8.7918e-03,\n",
       "                      -4.6005e-02,  1.3979e-03, -5.0206e-02,  6.4943e-02,  8.2791e-02,\n",
       "                       4.5583e-02, -2.0492e-01, -9.7661e-03,  2.3167e-02, -1.8763e-02,\n",
       "                      -1.5401e-01,  6.5442e-02, -5.4470e-02,  6.7223e-02,  1.1697e-02,\n",
       "                      -1.8006e-01, -8.5478e-04,  3.7028e-02, -4.9641e-03, -1.0026e-01,\n",
       "                      -4.9455e-03, -2.3818e-03,  3.9814e-03, -4.4016e-02, -1.5030e-02,\n",
       "                       1.1336e-03, -2.4211e-01,  1.7932e-02, -1.2518e-02,  1.7449e-03,\n",
       "                       1.2943e-02,  7.5248e-02,  7.3718e-02,  8.2545e-03,  1.4910e-03,\n",
       "                       1.9901e-03,  3.7935e-03,  2.1071e-02,  1.0559e-01,  4.9451e-02,\n",
       "                       4.9734e-04,  3.1029e-02, -9.0010e-02, -2.7104e-02,  4.6843e-02,\n",
       "                       8.2661e-03,  6.8356e-02,  3.0075e-03, -6.7154e-02,  4.5005e-03,\n",
       "                      -1.2294e-01,  2.2255e-03,  1.3253e-01,  3.9154e-03,  6.1246e-02,\n",
       "                       7.7521e-03,  7.7106e-02,  4.1804e-03, -7.8486e-02,  5.8662e-03,\n",
       "                       7.9841e-05,  4.6071e-02, -4.4799e-06, -1.7771e-02,  3.7567e-03,\n",
       "                       3.8958e-02,  4.9580e-02, -4.3012e-02,  2.8107e-02,  1.0121e-03,\n",
       "                       2.8465e-03,  5.1375e-03, -1.5061e-02,  2.7403e-02, -4.8608e-03,\n",
       "                      -2.1909e-03,  8.2380e-02,  8.6583e-04, -6.6035e-04,  4.5904e-03,\n",
       "                       1.1298e-01,  5.3408e-03,  3.7041e-03, -5.4600e-02,  2.4839e-02,\n",
       "                      -6.8324e-03, -2.6531e-02,  3.7670e-02,  2.4957e-03, -1.6336e-02,\n",
       "                      -4.8559e-02, -1.0348e-03,  2.0409e-02, -1.2428e-03, -1.5379e-02,\n",
       "                       6.8556e-02,  1.1978e-02, -2.3129e-02, -3.7190e-03,  2.3562e-02,\n",
       "                       6.4490e-02, -5.2572e-03, -1.4129e-02,  8.5935e-02,  3.6473e-02,\n",
       "                       9.1700e-04,  2.6729e-03, -2.5837e-02, -4.9340e-04, -6.4761e-02,\n",
       "                       1.0910e-02,  6.2620e-03,  5.8471e-02, -3.4682e-02, -7.3944e-03,\n",
       "                       5.6494e-02,  6.1880e-03, -1.1743e-02, -2.9294e-03,  4.2531e-03,\n",
       "                       5.5835e-03, -5.4309e-03, -3.9073e-02,  1.6646e-02, -2.7506e-03,\n",
       "                      -6.1708e-02, -3.5114e-02, -1.2273e-03, -1.1787e-03,  8.6873e-02,\n",
       "                      -1.2253e-02, -3.4340e-02, -1.3378e-02,  1.8999e-03,  8.5247e-03,\n",
       "                       4.3251e-03,  4.5597e-02, -4.8682e-02,  4.2319e-04,  2.5809e-02,\n",
       "                      -1.1779e-03, -4.4958e-02,  8.0429e-03, -2.4392e-03,  8.0036e-03,\n",
       "                       2.6701e-02, -7.6388e-03,  7.9204e-03,  2.2130e-02,  5.1313e-03,\n",
       "                       9.6349e-05,  1.2434e-03,  3.4114e-02, -2.8128e-02,  1.8817e-01,\n",
       "                      -1.1535e-03,  9.4023e-02, -1.6056e-01,  5.6569e-02,  2.0074e-03,\n",
       "                      -3.9740e-02, -1.3828e-02, -2.2825e-01, -6.4838e-03,  2.5851e-02,\n",
       "                      -5.9649e-03,  1.2426e-03, -3.6768e-02, -4.5191e-02, -8.7133e-02,\n",
       "                      -9.7454e-03,  3.5049e-02,  5.7460e-03, -3.9343e-03,  7.2047e-02,\n",
       "                      -4.3949e-03,  2.6714e-02, -1.7759e-01, -5.8870e-03,  1.1159e-02,\n",
       "                      -2.0802e-01,  6.5045e-04, -1.7971e-03,  1.0716e-03, -4.5905e-03,\n",
       "                      -8.1713e-02, -3.1853e-02,  4.8392e-03, -1.7860e-01,  8.4029e-03,\n",
       "                      -4.7371e-02,  2.2629e-03, -4.2811e-02, -5.7667e-02, -4.9510e-02,\n",
       "                      -1.5384e-02,  2.1912e-02,  1.2758e-03,  2.7685e-02, -8.7408e-02,\n",
       "                      -3.2132e-03, -2.3919e-02, -2.2765e-02,  9.3236e-02, -1.8425e-02,\n",
       "                       2.4771e-02,  3.4239e-03,  5.4466e-03, -5.0252e-03, -4.9155e-02,\n",
       "                       2.6588e-02,  3.0990e-02,  7.6077e-02, -5.1798e-02, -1.0304e-02,\n",
       "                       7.4635e-02])),\n",
       "             ('layer1.2.bn3.bias',\n",
       "              tensor([-1.2001e-02, -4.8653e-02, -2.9921e-02,  5.6301e-02,  8.2854e-04,\n",
       "                      -3.9329e-03,  1.8911e-02, -9.2343e-03,  1.3883e-02, -8.4982e-02,\n",
       "                      -7.3583e-02,  7.2301e-03,  3.2333e-03,  3.6726e-03,  4.6493e-03,\n",
       "                      -1.7371e-02,  2.7380e-02,  3.0852e-03,  1.0009e-02,  1.1994e-03,\n",
       "                      -3.9646e-02,  1.7332e-03,  1.0901e-03,  3.6035e-03,  3.8846e-03,\n",
       "                      -1.9880e-02,  5.4275e-03, -2.3951e-02,  1.4786e-02, -4.2598e-02,\n",
       "                       7.3226e-03, -1.5734e-02, -7.4266e-02,  4.8440e-03,  1.6972e-03,\n",
       "                       1.4565e-02,  2.7983e-02, -6.8384e-03,  5.4785e-03,  8.0982e-03,\n",
       "                      -8.9161e-04,  7.2427e-03,  1.5546e-02, -1.3117e-02,  5.4764e-03,\n",
       "                       1.8851e-02, -2.5230e-02,  7.6444e-03, -8.2093e-03, -2.1602e-03,\n",
       "                      -7.4514e-02,  1.1384e-02,  2.9062e-02,  3.6703e-02, -1.5705e-02,\n",
       "                       5.4777e-02,  3.3430e-03, -3.8175e-04,  6.5882e-04,  1.8076e-02,\n",
       "                       2.7671e-03,  3.9514e-03,  4.0272e-03, -2.0018e-01, -7.4516e-04,\n",
       "                      -2.7441e-03,  2.8185e-02, -7.0126e-02,  3.7661e-03,  1.8998e-03,\n",
       "                       1.0899e-02, -1.0624e-01, -2.3516e-02,  5.8359e-03,  2.7852e-03,\n",
       "                       3.9672e-03,  4.5936e-03,  1.4208e-02,  5.2288e-02, -4.0737e-03,\n",
       "                       5.2820e-03,  1.3608e-02, -7.7952e-02,  1.2814e-02,  2.5166e-02,\n",
       "                       4.3664e-03, -6.4182e-04, -8.9027e-02,  6.9527e-03, -1.7279e-01,\n",
       "                      -1.3108e-02,  9.5939e-03, -1.1289e-02,  6.6471e-03, -1.5975e-02,\n",
       "                       5.8852e-03, -8.4171e-03, -2.2001e-03, -1.8662e-02, -1.2268e-01,\n",
       "                       8.0478e-04, -2.8276e-02,  1.2088e-03, -3.0145e-04, -3.3379e-04,\n",
       "                      -2.3611e-02, -2.5741e-02,  3.8483e-03,  1.3546e-02,  2.7310e-03,\n",
       "                       1.7349e-02,  5.0779e-03,  1.9881e-02, -1.5553e-01,  3.0821e-03,\n",
       "                       5.7842e-03, -2.2764e-03, -6.9013e-03,  3.6565e-03,  2.3631e-03,\n",
       "                      -4.3790e-02,  6.2657e-03, -9.2666e-03, -2.5029e-02,  4.8566e-03,\n",
       "                       9.5687e-03,  1.0773e-02,  1.8033e-02, -4.5605e-03, -8.9986e-02,\n",
       "                       1.6289e-03,  1.7220e-02,  1.2349e-02, -1.3807e-02,  7.3684e-04,\n",
       "                      -5.0560e-03,  2.9965e-03,  2.6997e-03, -6.7283e-02,  2.7763e-03,\n",
       "                       5.8624e-02,  2.7474e-03, -3.0418e-03, -5.8966e-02,  1.9715e-02,\n",
       "                       1.7321e-03,  3.1376e-03, -3.5946e-03,  5.5369e-04,  2.4152e-02,\n",
       "                       7.2738e-03,  4.1574e-05, -3.1780e-02, -1.0463e-02,  8.3301e-03,\n",
       "                      -1.1959e-01,  8.1282e-04,  5.4181e-03,  2.2598e-03,  7.5604e-03,\n",
       "                       2.2618e-04, -1.1668e-03,  8.2148e-03,  3.6575e-03,  1.1362e-03,\n",
       "                       2.1268e-02, -2.6304e-02,  2.4191e-03,  5.2516e-03, -1.7387e-02,\n",
       "                       1.9010e-02, -1.5066e-02, -1.9664e-02,  2.7246e-03,  7.8057e-03,\n",
       "                       1.8679e-03,  1.5046e-02,  1.6956e-03,  3.4722e-03,  1.8951e-03,\n",
       "                       1.5786e-03,  1.9725e-03, -7.8134e-03,  5.7380e-03,  7.9078e-03,\n",
       "                       1.0445e-02,  5.8452e-03,  1.9046e-03, -8.9990e-02,  1.2809e-02,\n",
       "                       1.9603e-03,  1.2538e-02,  2.5152e-02,  1.4152e-02,  3.1748e-02,\n",
       "                       5.2551e-04, -1.1730e-03, -4.2104e-02,  5.8455e-03,  6.4803e-03,\n",
       "                       1.2989e-03,  8.6738e-03,  3.8986e-02,  2.2305e-03, -9.2389e-03,\n",
       "                      -1.0426e-02,  2.7844e-03,  9.3643e-03, -1.4547e-02, -4.0325e-02,\n",
       "                       9.3806e-03,  8.1562e-03, -8.8643e-03,  4.1979e-03,  5.2873e-02,\n",
       "                       3.4598e-03,  3.9576e-03, -5.3416e-02,  3.0639e-03,  1.2566e-02,\n",
       "                      -2.2301e-03,  2.4120e-03,  4.3517e-03,  6.6963e-03,  1.8700e-02,\n",
       "                      -1.5196e-01,  1.3207e-02, -1.0011e-01,  4.3237e-02, -2.3486e-01,\n",
       "                       4.1816e-03,  6.5148e-03,  2.3190e-02,  3.7432e-02,  2.2916e-02,\n",
       "                      -2.7031e-02, -5.0587e-03, -9.3510e-03,  5.2170e-03, -2.8338e-02,\n",
       "                       4.7981e-03, -2.0958e-02, -2.5108e-02, -5.3906e-02,  1.2612e-02,\n",
       "                       1.0618e-02,  8.9091e-04,  5.1651e-03,  9.2638e-03, -1.5129e-02,\n",
       "                       2.5185e-02, -6.1231e-03, -8.1716e-02,  8.7712e-03,  2.3224e-03,\n",
       "                       2.4040e-02])),\n",
       "             ('layer1.2.bn3.running_mean',\n",
       "              tensor([-3.3852e-04, -1.1851e-03, -7.4296e-03, -2.4080e-02, -1.3717e-03,\n",
       "                       4.3552e-03, -3.3574e-03, -1.2931e-03,  2.8898e-03, -2.0887e-03,\n",
       "                      -3.4311e-03,  2.0709e-03,  1.6441e-03, -5.0050e-04, -1.4849e-03,\n",
       "                      -8.0636e-04, -2.3556e-02,  7.6976e-03,  3.1291e-03, -4.6688e-04,\n",
       "                       4.7286e-03, -1.2620e-03, -2.7357e-03,  2.9367e-03, -2.6654e-03,\n",
       "                       3.0672e-03,  1.3967e-03, -1.2507e-02, -9.4554e-04, -7.3176e-03,\n",
       "                       2.2555e-03, -1.1176e-02, -1.8815e-03,  1.0454e-03, -1.4494e-03,\n",
       "                       3.9277e-03, -3.8382e-03,  1.0557e-02,  5.4331e-03, -5.3926e-03,\n",
       "                       5.6420e-03,  2.8477e-03, -5.0951e-03,  3.6619e-03,  2.7179e-03,\n",
       "                      -3.8559e-03,  7.1308e-03,  2.6471e-03, -3.2561e-03, -2.0012e-03,\n",
       "                      -1.8402e-02, -3.0952e-04,  3.6234e-03,  1.3937e-02,  4.1386e-05,\n",
       "                      -9.4380e-03,  1.7678e-03, -1.8548e-03,  2.4112e-04, -3.8162e-03,\n",
       "                       3.4103e-03, -4.2225e-03, -1.0575e-03,  7.6129e-03,  4.1293e-03,\n",
       "                       2.3165e-03,  5.1555e-02, -2.7418e-03,  8.2082e-04, -1.8301e-03,\n",
       "                      -1.1380e-03, -9.3531e-03,  1.6828e-02,  2.5550e-03, -1.5902e-03,\n",
       "                      -2.0855e-04,  1.6199e-04,  1.2064e-02, -1.8441e-02,  1.9453e-03,\n",
       "                      -3.7424e-03, -5.0988e-04, -1.7102e-02, -5.2983e-03,  4.2367e-03,\n",
       "                      -1.7369e-03, -8.8499e-03, -1.2590e-03,  6.0286e-03, -7.7982e-04,\n",
       "                      -3.0620e-03,  1.5302e-03, -1.9454e-02, -1.4674e-03,  1.1005e-02,\n",
       "                       7.4325e-04, -1.1837e-02, -1.5026e-03, -1.7621e-03, -5.0293e-05,\n",
       "                      -1.0326e-04,  4.0756e-03, -3.9030e-04, -1.1200e-03,  5.2751e-03,\n",
       "                      -4.3970e-03, -3.9590e-03,  1.1568e-02, -1.0293e-02, -1.3272e-03,\n",
       "                      -4.3548e-03, -3.4743e-03, -6.4105e-03, -1.0135e-03,  4.0862e-03,\n",
       "                      -1.1512e-03, -9.1422e-03,  1.5413e-03, -1.0068e-03, -1.4687e-03,\n",
       "                      -1.0018e-02, -2.2696e-03,  8.0018e-04,  9.5545e-03, -5.7544e-03,\n",
       "                       2.3579e-03, -3.3985e-03,  5.7088e-04,  2.3628e-04,  1.0630e-03,\n",
       "                       1.4576e-02, -6.5123e-03, -3.2187e-03,  2.8657e-04,  4.6076e-03,\n",
       "                       1.4161e-03, -2.7191e-03, -2.2969e-03,  2.6237e-05, -2.2186e-03,\n",
       "                       1.7962e-02,  1.3981e-03,  3.8064e-03,  1.3261e-02,  1.0498e-02,\n",
       "                      -1.3333e-03, -2.7460e-03,  3.5139e-03,  1.0980e-03,  4.1776e-03,\n",
       "                      -3.7446e-03, -9.1311e-04, -1.2486e-02, -1.5187e-03, -1.7247e-03,\n",
       "                       1.2859e-02,  4.4467e-03,  1.4892e-03, -3.9349e-03, -4.4539e-03,\n",
       "                      -1.6502e-03,  2.0098e-03,  6.4360e-03, -2.7773e-03,  1.0949e-03,\n",
       "                       1.6811e-02, -2.8067e-03,  1.6345e-03, -9.1730e-04, -4.6470e-03,\n",
       "                       6.9457e-03,  1.0676e-03,  4.7004e-04, -2.4918e-03,  1.8977e-03,\n",
       "                      -2.0568e-03, -1.2460e-02,  1.1706e-02,  4.3278e-03, -2.4470e-03,\n",
       "                      -1.4092e-03,  1.6504e-03,  1.4955e-03,  9.1827e-04, -4.8972e-03,\n",
       "                       3.8243e-03,  2.4099e-03, -2.8963e-03, -7.3876e-03, -3.7822e-03,\n",
       "                      -1.6758e-03, -5.1656e-03,  2.3180e-04,  1.9394e-03, -4.2615e-02,\n",
       "                       3.5467e-04, -2.0923e-02,  2.7146e-02, -6.9014e-03,  1.1535e-03,\n",
       "                      -5.3713e-05,  3.1443e-03,  4.3365e-02,  6.0123e-04,  2.3993e-04,\n",
       "                       4.4068e-04, -8.7717e-04,  2.8960e-03,  8.9883e-03,  1.2162e-02,\n",
       "                       3.9728e-03, -2.4664e-03, -2.6418e-03, -2.4132e-03, -7.6056e-03,\n",
       "                      -1.9114e-03, -4.8737e-03,  3.1729e-02, -7.3435e-04, -2.6184e-03,\n",
       "                       1.0908e-02,  1.7911e-03, -6.8998e-04,  2.1500e-03,  8.0382e-04,\n",
       "                       1.1947e-02,  2.6226e-03,  3.2021e-04,  2.7038e-02,  4.5933e-03,\n",
       "                       9.2260e-03, -1.5299e-03, -1.0019e-02,  9.2367e-04,  7.1583e-03,\n",
       "                       8.8162e-03,  2.0719e-03,  9.4417e-04, -4.7359e-03,  7.3873e-03,\n",
       "                       2.8699e-04,  4.2328e-03,  2.1003e-03, -1.3485e-02, -6.4938e-04,\n",
       "                      -3.4906e-03, -3.5003e-03,  1.5368e-03, -3.7555e-03,  3.7246e-03,\n",
       "                      -9.2284e-03, -3.7491e-03,  3.6435e-03, -3.4836e-03, -1.2399e-03,\n",
       "                       2.4629e-02])),\n",
       "             ('layer1.2.bn3.running_var',\n",
       "              tensor([2.6738e-06, 1.9604e-05, 2.6132e-04, 4.1717e-04, 1.8107e-05, 3.1585e-04,\n",
       "                      1.3746e-04, 1.5127e-05, 1.9484e-04, 2.6148e-05, 6.5608e-05, 7.0440e-05,\n",
       "                      3.5545e-05, 5.5875e-06, 2.2051e-05, 4.3964e-05, 5.6507e-04, 8.0081e-04,\n",
       "                      6.0815e-05, 8.5133e-06, 1.6438e-04, 1.3787e-04, 7.0818e-05, 5.2569e-05,\n",
       "                      2.8334e-05, 1.1255e-04, 3.1191e-05, 1.3889e-04, 2.4165e-04, 1.1441e-04,\n",
       "                      8.0258e-05, 1.0870e-04, 3.3468e-05, 2.4730e-05, 2.4218e-05, 1.0059e-04,\n",
       "                      4.6325e-04, 8.6100e-05, 3.2564e-04, 4.6882e-05, 9.4571e-05, 1.0977e-04,\n",
       "                      1.7894e-04, 2.6021e-04, 3.9243e-04, 1.6844e-04, 1.7766e-03, 9.6421e-05,\n",
       "                      1.9109e-04, 2.8081e-05, 6.0754e-04, 5.2087e-04, 3.2263e-04, 5.2185e-04,\n",
       "                      1.7471e-05, 5.2789e-04, 4.5087e-05, 7.3944e-05, 2.0672e-05, 2.4879e-04,\n",
       "                      7.5886e-05, 1.9197e-04, 7.3425e-06, 1.5130e-04, 8.9813e-05, 5.1978e-05,\n",
       "                      1.1261e-03, 4.3238e-05, 7.8703e-05, 6.7253e-05, 3.2359e-05, 2.5505e-04,\n",
       "                      2.4344e-04, 4.2750e-05, 1.2202e-05, 2.2606e-05, 1.0221e-05, 2.0307e-04,\n",
       "                      5.3901e-04, 6.7376e-05, 8.4478e-05, 1.0460e-04, 1.8953e-04, 9.7460e-05,\n",
       "                      1.8330e-04, 1.0109e-04, 2.7875e-04, 2.6184e-05, 5.4928e-04, 8.0125e-05,\n",
       "                      2.9140e-04, 3.7233e-05, 3.6422e-04, 4.0271e-05, 1.4554e-04, 9.1487e-05,\n",
       "                      2.5231e-04, 2.4482e-05, 3.6572e-04, 3.0596e-05, 1.3523e-07, 7.7599e-05,\n",
       "                      4.2782e-06, 1.6744e-05, 2.2691e-04, 2.0496e-04, 5.7504e-05, 6.2337e-04,\n",
       "                      6.3723e-04, 1.0300e-05, 1.6142e-04, 7.2152e-05, 3.5219e-04, 1.5618e-04,\n",
       "                      1.0602e-04, 7.6766e-05, 1.3654e-04, 4.1549e-05, 8.1767e-06, 3.0183e-05,\n",
       "                      2.6744e-04, 3.8624e-05, 2.0685e-05, 9.9016e-05, 3.9461e-04, 7.3370e-05,\n",
       "                      8.5433e-05, 2.9809e-04, 7.9471e-06, 2.1761e-05, 3.6082e-04, 2.7193e-04,\n",
       "                      1.8598e-04, 1.5318e-05, 2.0700e-04, 1.6319e-04, 8.0896e-05, 1.2309e-04,\n",
       "                      3.3403e-06, 1.4552e-04, 3.3133e-04, 2.1087e-05, 1.5356e-04, 1.4340e-04,\n",
       "                      1.1052e-04, 8.1359e-06, 5.9018e-05, 4.0609e-05, 1.0242e-05, 1.9255e-04,\n",
       "                      9.5114e-05, 1.3783e-05, 6.4674e-04, 5.9344e-05, 4.2661e-05, 2.4156e-04,\n",
       "                      1.3678e-04, 4.1513e-05, 1.5333e-04, 1.5699e-04, 4.2904e-05, 1.1692e-04,\n",
       "                      8.1949e-05, 2.3223e-04, 1.1104e-05, 3.2606e-04, 7.2068e-05, 9.0452e-05,\n",
       "                      1.3289e-05, 1.1035e-04, 2.8959e-04, 3.5273e-05, 2.6977e-05, 7.4971e-05,\n",
       "                      1.3209e-04, 1.0975e-04, 1.2141e-04, 6.9335e-05, 4.4738e-05, 5.1055e-05,\n",
       "                      1.5971e-05, 1.3228e-04, 7.2582e-06, 5.2074e-05, 1.4101e-04, 9.1612e-05,\n",
       "                      4.9161e-05, 5.1847e-05, 6.8693e-05, 7.2219e-05, 5.1751e-05, 1.9921e-04,\n",
       "                      1.9845e-04, 2.1874e-04, 7.2522e-04, 2.4956e-06, 3.8108e-04, 6.1068e-04,\n",
       "                      1.8718e-04, 4.6154e-05, 1.0894e-04, 3.2781e-04, 1.0510e-03, 4.6501e-05,\n",
       "                      6.2952e-05, 6.9881e-06, 1.2010e-05, 4.6896e-04, 5.8518e-04, 1.1707e-04,\n",
       "                      1.2176e-04, 2.2207e-04, 2.6091e-05, 3.1624e-05, 9.7941e-04, 6.4503e-05,\n",
       "                      3.4587e-04, 4.8709e-04, 1.4554e-05, 1.7890e-04, 7.0825e-04, 2.2067e-05,\n",
       "                      5.5742e-06, 1.0750e-04, 1.4024e-04, 3.7172e-04, 5.6455e-05, 7.9866e-06,\n",
       "                      5.9567e-04, 9.6956e-05, 5.6745e-04, 2.5391e-05, 2.3339e-04, 4.0781e-04,\n",
       "                      4.2386e-04, 1.1499e-04, 3.7498e-05, 3.2500e-05, 3.6435e-04, 1.5793e-04,\n",
       "                      1.2149e-05, 2.3338e-05, 1.1951e-05, 1.1491e-04, 1.5362e-04, 5.2316e-05,\n",
       "                      1.5098e-04, 3.2662e-05, 1.5197e-04, 1.1783e-04, 3.8181e-04, 1.6644e-04,\n",
       "                      3.2860e-04, 1.7005e-04, 5.2203e-05, 4.6716e-04])),\n",
       "             ('layer1.2.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.0.conv1.weight',\n",
       "              tensor([[[[-0.0030]],\n",
       "              \n",
       "                       [[-0.0160]],\n",
       "              \n",
       "                       [[-0.0306]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0230]],\n",
       "              \n",
       "                       [[ 0.0114]],\n",
       "              \n",
       "                       [[-0.0274]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0043]],\n",
       "              \n",
       "                       [[-0.0070]],\n",
       "              \n",
       "                       [[ 0.0027]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0065]],\n",
       "              \n",
       "                       [[ 0.0034]],\n",
       "              \n",
       "                       [[ 0.0043]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0027]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       [[-0.0041]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0410]],\n",
       "              \n",
       "                       [[-0.0017]],\n",
       "              \n",
       "                       [[-0.0128]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0014]],\n",
       "              \n",
       "                       [[-0.0009]],\n",
       "              \n",
       "                       [[ 0.0069]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0183]],\n",
       "              \n",
       "                       [[-0.0128]],\n",
       "              \n",
       "                       [[ 0.0118]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0023]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       [[ 0.0068]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0012]],\n",
       "              \n",
       "                       [[-0.0098]],\n",
       "              \n",
       "                       [[ 0.0140]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0063]],\n",
       "              \n",
       "                       [[-0.0030]],\n",
       "              \n",
       "                       [[ 0.0032]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0026]],\n",
       "              \n",
       "                       [[ 0.0030]],\n",
       "              \n",
       "                       [[ 0.0082]]]])),\n",
       "             ('layer2.0.bn1.weight',\n",
       "              tensor([0.0965, 0.1139, 0.0911, 0.1152, 0.1307, 0.1126, 0.1366, 0.0811, 0.0744,\n",
       "                      0.0768, 0.1333, 0.1578, 0.0303, 0.0489, 0.0493, 0.0793, 0.0892, 0.0837,\n",
       "                      0.0565, 0.0837, 0.0758, 0.0946, 0.0739, 0.0606, 0.0596, 0.1099, 0.0823,\n",
       "                      0.1585, 0.0716, 0.0910, 0.0796, 0.1255, 0.1597, 0.0556, 0.0825, 0.1242,\n",
       "                      0.1557, 0.1009, 0.0928, 0.0846, 0.1266, 0.0898, 0.1167, 0.0515, 0.1078,\n",
       "                      0.0698, 0.0903, 0.0938, 0.0558, 0.1521, 0.1823, 0.0707, 0.1031, 0.0835,\n",
       "                      0.1483, 0.0453, 0.0681, 0.1350, 0.1007, 0.0879, 0.0632, 0.1111, 0.0772,\n",
       "                      0.0618, 0.1141, 0.0810, 0.0837, 0.0933, 0.1306, 0.0657, 0.1394, 0.0483,\n",
       "                      0.0406, 0.0983, 0.1540, 0.0884, 0.0590, 0.0889, 0.0697, 0.1654, 0.0574,\n",
       "                      0.1141, 0.0855, 0.0935, 0.0715, 0.0639, 0.1205, 0.0780, 0.1182, 0.0879,\n",
       "                      0.0826, 0.0511, 0.1019, 0.1284, 0.0601, 0.0555, 0.1074, 0.1082, 0.0807,\n",
       "                      0.0770, 0.0668, 0.1367, 0.0445, 0.1335, 0.1106, 0.0774, 0.1289, 0.0486,\n",
       "                      0.1081, 0.0766, 0.0654, 0.0801, 0.0832, 0.0990, 0.0889, 0.1166, 0.0498,\n",
       "                      0.0999, 0.0664, 0.1173, 0.1104, 0.0875, 0.0776, 0.1123, 0.0830, 0.0699,\n",
       "                      0.0688, 0.0588])),\n",
       "             ('layer2.0.bn1.bias',\n",
       "              tensor([ 0.0631, -0.0267,  0.0484,  0.1003, -0.0277, -0.0027, -0.0026,  0.0543,\n",
       "                       0.0193,  0.0480, -0.0167, -0.0252,  0.0409, -0.0093,  0.0039, -0.0234,\n",
       "                      -0.0073,  0.0007, -0.0017,  0.0026,  0.0234,  0.0476,  0.0776,  0.0833,\n",
       "                       0.0417, -0.0090,  0.0458,  0.0446,  0.0153, -0.0374,  0.0203, -0.0420,\n",
       "                       0.0096,  0.0517, -0.0042, -0.0652, -0.0862,  0.0555, -0.0089,  0.1093,\n",
       "                       0.0381,  0.0238,  0.0624, -0.0057, -0.0007, -0.0128,  0.0643,  0.0166,\n",
       "                       0.0157, -0.0284,  0.0360, -0.0122, -0.0253, -0.0433, -0.0285,  0.0224,\n",
       "                      -0.0164, -0.1110,  0.0494,  0.0169,  0.0908, -0.0102,  0.0772, -0.0562,\n",
       "                       0.1222,  0.0420, -0.0182,  0.0180, -0.0634,  0.0164, -0.0149,  0.0161,\n",
       "                       0.0195,  0.0627, -0.0404, -0.0258,  0.0317,  0.0464,  0.0535, -0.0614,\n",
       "                      -0.0066, -0.0290,  0.0169, -0.0432, -0.0161, -0.0325,  0.0021, -0.0084,\n",
       "                       0.0048, -0.0151,  0.0512, -0.0234, -0.0203,  0.0413, -0.0264,  0.0034,\n",
       "                      -0.0850, -0.0135, -0.0293, -0.0028,  0.0326, -0.0213, -0.0107, -0.0324,\n",
       "                      -0.0174,  0.0694,  0.0090,  0.0141, -0.0373,  0.0223, -0.0268, -0.0667,\n",
       "                       0.0270, -0.0958, -0.0315, -0.0624, -0.0241, -0.0244, -0.0286, -0.0562,\n",
       "                       0.0208, -0.0934,  0.1244, -0.0264, -0.0148,  0.0094, -0.0317, -0.0125])),\n",
       "             ('layer2.0.bn1.running_mean',\n",
       "              tensor([ 1.9815e-02, -1.7980e-03, -3.0401e-03, -3.4666e-02, -2.6005e-02,\n",
       "                       7.7142e-02,  1.0078e-02, -1.2915e-02,  3.5596e-02, -8.7946e-04,\n",
       "                       4.8565e-02,  1.0225e-01, -3.1309e-03,  2.3954e-02,  3.2481e-02,\n",
       "                       4.3672e-02,  3.1981e-02,  5.2043e-02,  3.4184e-02, -4.6926e-02,\n",
       "                      -4.0140e-02,  3.4520e-03, -5.1066e-02, -1.5078e-02, -4.9282e-02,\n",
       "                      -8.1850e-02, -4.6132e-02, -1.1450e-01,  5.7387e-02, -2.4585e-03,\n",
       "                      -5.2703e-02,  8.9709e-02, -6.6216e-02, -1.0076e-02,  2.2380e-02,\n",
       "                      -3.1187e-02,  7.7312e-02, -7.4993e-02, -7.9338e-02, -3.5066e-02,\n",
       "                       3.2115e-02,  3.2480e-02,  1.1949e-02,  2.9912e-03, -1.0918e-01,\n",
       "                      -1.9401e-02,  2.7031e-02,  3.3157e-02,  9.3753e-03,  7.5499e-02,\n",
       "                      -2.1002e-01, -2.1875e-02,  1.2728e-02,  2.3565e-02, -1.4145e-02,\n",
       "                      -4.3916e-02, -1.7638e-02, -2.1939e-02, -6.5608e-02, -2.4497e-02,\n",
       "                      -5.4991e-03, -8.4022e-02, -7.9724e-03, -1.7035e-02, -7.4127e-02,\n",
       "                      -5.0968e-03, -1.8492e-02, -7.9513e-02, -4.9741e-02, -3.8394e-02,\n",
       "                       4.4647e-02, -2.1711e-03, -6.0743e-03, -4.2105e-02,  3.8875e-02,\n",
       "                       3.2185e-02,  1.6425e-02,  3.6267e-02, -5.3660e-02, -2.2399e-02,\n",
       "                       5.2315e-02,  5.8337e-02,  4.3013e-02,  1.3950e-03,  3.1609e-02,\n",
       "                       2.4889e-03,  3.4912e-02,  4.4455e-02, -8.8724e-02,  3.7507e-02,\n",
       "                       2.9205e-02,  1.0813e-02, -9.5657e-02, -2.2258e-02,  5.2627e-03,\n",
       "                      -1.5792e-02, -5.0514e-02, -2.9242e-02, -2.4693e-02, -3.8318e-03,\n",
       "                       1.6598e-02, -1.3316e-02, -1.5517e-02,  3.7689e-02, -8.2278e-02,\n",
       "                      -7.0725e-03,  3.4379e-02,  2.1814e-02, -8.0715e-03, -2.7412e-02,\n",
       "                       2.9359e-02, -1.5735e-02, -2.1276e-02,  1.3040e-02,  1.0231e-02,\n",
       "                      -3.4986e-02, -4.6428e-03,  1.4391e-02,  2.8265e-02,  2.0664e-02,\n",
       "                       8.1535e-02, -1.3640e-02, -2.2290e-02,  2.0565e-04,  1.1048e-02,\n",
       "                       5.0921e-02, -4.3243e-03,  8.2498e-03])),\n",
       "             ('layer2.0.bn1.running_var',\n",
       "              tensor([0.0056, 0.0023, 0.0092, 0.0093, 0.0070, 0.0058, 0.0067, 0.0054, 0.0027,\n",
       "                      0.0046, 0.0044, 0.0099, 0.0011, 0.0015, 0.0017, 0.0015, 0.0053, 0.0015,\n",
       "                      0.0009, 0.0030, 0.0019, 0.0093, 0.0056, 0.0049, 0.0017, 0.0028, 0.0036,\n",
       "                      0.0094, 0.0028, 0.0012, 0.0015, 0.0047, 0.0080, 0.0042, 0.0043, 0.0023,\n",
       "                      0.0060, 0.0041, 0.0021, 0.0106, 0.0111, 0.0048, 0.0148, 0.0015, 0.0035,\n",
       "                      0.0017, 0.0047, 0.0053, 0.0026, 0.0127, 0.0104, 0.0020, 0.0018, 0.0019,\n",
       "                      0.0068, 0.0014, 0.0014, 0.0025, 0.0045, 0.0054, 0.0036, 0.0026, 0.0055,\n",
       "                      0.0007, 0.0075, 0.0032, 0.0014, 0.0025, 0.0037, 0.0019, 0.0119, 0.0023,\n",
       "                      0.0010, 0.0076, 0.0053, 0.0016, 0.0037, 0.0057, 0.0032, 0.0062, 0.0016,\n",
       "                      0.0088, 0.0016, 0.0033, 0.0017, 0.0007, 0.0050, 0.0036, 0.0040, 0.0038,\n",
       "                      0.0058, 0.0007, 0.0023, 0.0093, 0.0003, 0.0012, 0.0022, 0.0036, 0.0013,\n",
       "                      0.0014, 0.0022, 0.0048, 0.0006, 0.0062, 0.0029, 0.0050, 0.0069, 0.0013,\n",
       "                      0.0024, 0.0027, 0.0014, 0.0016, 0.0028, 0.0015, 0.0020, 0.0029, 0.0003,\n",
       "                      0.0031, 0.0023, 0.0033, 0.0056, 0.0011, 0.0082, 0.0047, 0.0032, 0.0033,\n",
       "                      0.0008, 0.0006])),\n",
       "             ('layer2.0.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.0.conv2.weight',\n",
       "              tensor([[[[ 2.2674e-02,  2.0722e-02,  3.5462e-04],\n",
       "                        [ 1.4882e-02,  6.7019e-03, -1.3628e-02],\n",
       "                        [ 6.6641e-03, -7.6494e-03, -1.1168e-02]],\n",
       "              \n",
       "                       [[-1.1254e-02, -2.4030e-02, -1.4535e-02],\n",
       "                        [-9.5155e-03, -6.3372e-03, -7.4388e-04],\n",
       "                        [-1.4141e-02, -1.3217e-02,  1.3271e-02]],\n",
       "              \n",
       "                       [[-1.8447e-02, -3.9775e-02, -1.3257e-02],\n",
       "                        [-1.3207e-02, -2.7502e-03,  8.4943e-04],\n",
       "                        [ 9.1544e-03,  7.6065e-03,  3.4730e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.7905e-03, -4.4537e-03, -8.6829e-03],\n",
       "                        [-9.8021e-03, -2.2688e-04, -1.0038e-03],\n",
       "                        [ 3.7238e-03,  1.8656e-02,  2.3266e-04]],\n",
       "              \n",
       "                       [[-9.6917e-03, -8.2462e-03, -9.7804e-03],\n",
       "                        [-5.5137e-03,  1.2458e-03, -3.9792e-04],\n",
       "                        [-1.4175e-03,  5.2816e-03, -2.6796e-03]],\n",
       "              \n",
       "                       [[-5.8687e-03, -1.5195e-03, -2.4552e-03],\n",
       "                        [-4.4129e-03, -8.7648e-03, -4.7797e-03],\n",
       "                        [-9.8456e-04,  1.8348e-04,  1.9003e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.2591e-03,  2.4057e-03,  3.2275e-03],\n",
       "                        [ 6.5197e-03,  7.7794e-03,  4.5114e-03],\n",
       "                        [ 1.2035e-03,  8.0776e-03,  4.0318e-03]],\n",
       "              \n",
       "                       [[ 7.3801e-03,  1.0393e-02,  9.1538e-03],\n",
       "                        [-1.0579e-04,  8.8425e-03,  1.7024e-03],\n",
       "                        [-6.1680e-03, -3.4738e-03, -5.1845e-03]],\n",
       "              \n",
       "                       [[ 5.1452e-04,  9.0312e-03,  4.3718e-03],\n",
       "                        [-7.0392e-03,  7.7936e-04, -8.9869e-03],\n",
       "                        [-7.4616e-03, -9.1623e-03, -1.3138e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.8102e-04,  1.3688e-03,  1.0628e-03],\n",
       "                        [ 2.0450e-03,  2.3710e-04,  2.2098e-03],\n",
       "                        [ 2.4335e-03,  4.4909e-03,  6.8955e-03]],\n",
       "              \n",
       "                       [[ 9.6539e-04,  1.2425e-03,  4.1412e-03],\n",
       "                        [ 2.2837e-03,  1.6333e-03,  2.4712e-03],\n",
       "                        [-2.8632e-04,  2.3222e-04,  3.0984e-03]],\n",
       "              \n",
       "                       [[ 9.7358e-04, -1.0818e-03, -5.2567e-04],\n",
       "                        [-1.3836e-03, -9.1066e-04, -1.8901e-03],\n",
       "                        [-5.4202e-04, -1.3294e-03, -1.1136e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5595e-03,  8.8294e-04, -2.0723e-03],\n",
       "                        [-6.7338e-03, -6.7788e-04, -5.0885e-04],\n",
       "                        [-2.8479e-03,  1.5488e-03, -7.7549e-04]],\n",
       "              \n",
       "                       [[ 4.4969e-03, -2.6913e-03, -6.0979e-03],\n",
       "                        [-3.3747e-03, -7.7901e-03, -6.7635e-04],\n",
       "                        [-4.8573e-03, -2.8275e-03,  6.0129e-03]],\n",
       "              \n",
       "                       [[-1.7215e-04, -1.1593e-02, -7.8738e-03],\n",
       "                        [ 2.0224e-03, -4.4560e-03,  8.8805e-05],\n",
       "                        [-2.6632e-04,  9.5602e-04,  1.4616e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.4531e-03,  1.3304e-03, -5.1065e-04],\n",
       "                        [-3.0282e-03,  7.1148e-04, -5.5090e-03],\n",
       "                        [ 4.3765e-03,  2.9027e-03, -6.1135e-03]],\n",
       "              \n",
       "                       [[ 2.7852e-05,  8.3698e-04,  2.8507e-03],\n",
       "                        [ 6.6595e-04, -1.4852e-04,  1.5877e-03],\n",
       "                        [ 2.4033e-03,  2.0390e-03,  1.8168e-03]],\n",
       "              \n",
       "                       [[ 1.5792e-04, -3.4387e-04,  4.1194e-03],\n",
       "                        [ 2.8976e-03, -1.9548e-03,  2.6244e-03],\n",
       "                        [ 2.5972e-03,  3.2484e-03,  1.8983e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-6.5972e-03,  1.1621e-03,  1.1303e-02],\n",
       "                        [-1.0475e-02,  1.4756e-03,  7.9742e-03],\n",
       "                        [-1.3838e-02, -8.3423e-03,  6.4868e-03]],\n",
       "              \n",
       "                       [[-2.2625e-03,  1.7025e-02,  1.2789e-02],\n",
       "                        [-1.1988e-02, -6.7685e-03,  2.0162e-02],\n",
       "                        [-1.1651e-02,  8.0782e-03,  2.4401e-03]],\n",
       "              \n",
       "                       [[-1.5881e-02,  4.9923e-03,  4.1078e-03],\n",
       "                        [-2.0846e-02, -5.1294e-03,  2.0896e-02],\n",
       "                        [-1.6897e-02, -4.9016e-03,  6.6211e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0011e-03, -7.8355e-03,  5.6219e-04],\n",
       "                        [-8.9746e-03,  6.2084e-03, -3.3195e-03],\n",
       "                        [-8.7216e-03,  3.5190e-03,  3.7790e-03]],\n",
       "              \n",
       "                       [[ 1.8548e-03, -3.8807e-03, -3.9245e-03],\n",
       "                        [ 1.6728e-03,  4.3364e-03, -5.2842e-03],\n",
       "                        [ 9.4136e-03,  8.8662e-03, -6.4178e-03]],\n",
       "              \n",
       "                       [[ 2.7964e-03, -3.8587e-03, -2.6619e-03],\n",
       "                        [-7.0061e-04, -3.2108e-03, -3.6502e-03],\n",
       "                        [-1.1880e-03, -4.5405e-03, -1.5954e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2973e-04, -1.2853e-03,  3.8528e-03],\n",
       "                        [-1.2964e-02, -1.6895e-03,  6.0050e-03],\n",
       "                        [-9.1498e-03, -4.3338e-03, -3.0894e-03]],\n",
       "              \n",
       "                       [[-4.3233e-03,  1.0444e-02, -1.6001e-02],\n",
       "                        [-1.1878e-02, -7.6802e-03, -1.3685e-02],\n",
       "                        [-9.2159e-03, -4.4234e-03, -1.0497e-02]],\n",
       "              \n",
       "                       [[-1.4660e-02,  1.7604e-02,  1.9102e-02],\n",
       "                        [-9.3338e-03,  4.5115e-03, -2.0930e-03],\n",
       "                        [ 6.1458e-03,  1.6888e-02,  8.8150e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.8019e-03, -1.2325e-02, -2.0400e-02],\n",
       "                        [-1.0847e-02,  5.4479e-03, -2.6128e-02],\n",
       "                        [-9.8709e-04, -5.1844e-03, -2.1557e-02]],\n",
       "              \n",
       "                       [[-4.1359e-03, -3.6453e-03, -9.3322e-03],\n",
       "                        [-3.5761e-04,  1.4184e-03, -2.5956e-03],\n",
       "                        [-2.1949e-03, -6.6760e-04, -2.7948e-03]],\n",
       "              \n",
       "                       [[-2.3243e-03, -8.8488e-04, -3.7182e-03],\n",
       "                        [-1.6062e-03, -8.8130e-04, -7.3212e-03],\n",
       "                        [-2.2363e-03, -2.3182e-04, -5.9636e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1945e-03, -1.4698e-03,  7.8110e-03],\n",
       "                        [ 1.0387e-02, -9.5075e-03,  1.5902e-02],\n",
       "                        [ 1.6352e-02,  1.3990e-02,  1.9580e-02]],\n",
       "              \n",
       "                       [[ 2.1011e-04,  3.5811e-03,  5.2471e-04],\n",
       "                        [-6.3895e-03,  2.4496e-03,  1.5072e-04],\n",
       "                        [-3.8738e-03,  1.2072e-03,  2.7479e-03]],\n",
       "              \n",
       "                       [[-5.6397e-03,  1.4625e-03,  5.5961e-03],\n",
       "                        [ 3.1410e-03,  1.1406e-03,  2.9044e-03],\n",
       "                        [ 7.7865e-03,  3.3230e-04,  8.3418e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2839e-03, -3.9885e-03, -1.2474e-03],\n",
       "                        [-5.0966e-03, -5.7393e-03, -8.4751e-03],\n",
       "                        [-3.9720e-03, -4.6789e-03, -1.2420e-03]],\n",
       "              \n",
       "                       [[ 2.1488e-04, -2.1775e-03, -1.9231e-03],\n",
       "                        [-2.3912e-03,  7.7972e-04, -4.1729e-03],\n",
       "                        [-2.8453e-03, -1.8288e-03, -2.8534e-03]],\n",
       "              \n",
       "                       [[-5.3583e-04, -1.1952e-03, -2.3087e-03],\n",
       "                        [ 3.0048e-03,  8.9721e-04,  4.8088e-04],\n",
       "                        [ 4.2014e-03,  2.9079e-03,  3.4175e-03]]]])),\n",
       "             ('layer2.0.bn2.weight',\n",
       "              tensor([0.1108, 0.0609, 0.0526, 0.1020, 0.0757, 0.1564, 0.0952, 0.0993, 0.1113,\n",
       "                      0.0590, 0.1345, 0.0828, 0.1006, 0.1098, 0.1143, 0.0952, 0.1030, 0.1352,\n",
       "                      0.1247, 0.1383, 0.1344, 0.1157, 0.1120, 0.1013, 0.1009, 0.0539, 0.1134,\n",
       "                      0.1704, 0.1716, 0.0656, 0.0631, 0.0672, 0.0693, 0.0959, 0.0703, 0.1246,\n",
       "                      0.1113, 0.0604, 0.0835, 0.0927, 0.1131, 0.0834, 0.0668, 0.1817, 0.0972,\n",
       "                      0.1440, 0.0761, 0.1478, 0.1366, 0.0723, 0.1208, 0.1411, 0.1258, 0.0962,\n",
       "                      0.1081, 0.0931, 0.1137, 0.0704, 0.0838, 0.0874, 0.1288, 0.1537, 0.1228,\n",
       "                      0.0889, 0.0874, 0.1450, 0.0888, 0.1226, 0.1115, 0.0449, 0.0481, 0.1112,\n",
       "                      0.1050, 0.0966, 0.1467, 0.1164, 0.1103, 0.0964, 0.0719, 0.1045, 0.0512,\n",
       "                      0.1077, 0.0857, 0.0760, 0.0866, 0.0456, 0.0644, 0.1599, 0.1331, 0.1187,\n",
       "                      0.1079, 0.0863, 0.1008, 0.0858, 0.0881, 0.0860, 0.0868, 0.0927, 0.0916,\n",
       "                      0.0916, 0.1051, 0.1163, 0.0751, 0.1264, 0.0928, 0.1305, 0.1107, 0.1368,\n",
       "                      0.1519, 0.0421, 0.1282, 0.0778, 0.0843, 0.1271, 0.0599, 0.0731, 0.1214,\n",
       "                      0.1053, 0.1799, 0.0439, 0.0810, 0.1397, 0.2831, 0.0483, 0.0790, 0.1355,\n",
       "                      0.0946, 0.1018])),\n",
       "             ('layer2.0.bn2.bias',\n",
       "              tensor([ 0.0624,  0.0160, -0.0215,  0.0232, -0.0041, -0.0073,  0.0175,  0.0472,\n",
       "                       0.0008,  0.0514, -0.0116,  0.0097, -0.0102, -0.0091, -0.0605,  0.0441,\n",
       "                       0.0815, -0.0080,  0.0320,  0.0183, -0.0194, -0.0005, -0.0332, -0.0099,\n",
       "                       0.0077,  0.0341, -0.0405, -0.0372, -0.0282,  0.0182,  0.0736,  0.0635,\n",
       "                       0.0037,  0.0074,  0.0083,  0.0287, -0.0047, -0.0092,  0.0111,  0.0307,\n",
       "                      -0.0310, -0.0538,  0.0157, -0.0231, -0.0276, -0.0010,  0.0068, -0.0159,\n",
       "                      -0.0308,  0.0235, -0.0206,  0.0076, -0.0117,  0.0321,  0.0102,  0.0637,\n",
       "                       0.0268,  0.0415,  0.0292, -0.0042, -0.0458, -0.0389, -0.0162,  0.0040,\n",
       "                      -0.0097, -0.0816,  0.0429, -0.0094, -0.0308,  0.0143,  0.0690,  0.0322,\n",
       "                       0.0085, -0.0106,  0.0258,  0.0149,  0.0201, -0.0106,  0.0221,  0.0160,\n",
       "                       0.0679,  0.0395,  0.0299,  0.0015,  0.0922,  0.0185,  0.0006, -0.0388,\n",
       "                       0.0505,  0.0164,  0.0584,  0.0009, -0.0140,  0.0141, -0.0296,  0.0174,\n",
       "                       0.0235,  0.0328, -0.0035,  0.0051, -0.0147,  0.0014,  0.0824, -0.0422,\n",
       "                       0.0253, -0.0626,  0.0419, -0.0291, -0.0344,  0.0006, -0.0223, -0.0277,\n",
       "                      -0.0054,  0.0284,  0.0333,  0.0219,  0.0121,  0.0128, -0.0107,  0.0270,\n",
       "                       0.0316, -0.0553, -0.0176,  0.0308,  0.0375, -0.0254,  0.0597, -0.0481])),\n",
       "             ('layer2.0.bn2.running_mean',\n",
       "              tensor([-0.0167, -0.0214, -0.0040, -0.0210, -0.0169,  0.0229, -0.0207,  0.0245,\n",
       "                       0.0984, -0.0025,  0.0232, -0.0650, -0.0038, -0.0472,  0.0569, -0.0142,\n",
       "                      -0.0350,  0.0169, -0.0024, -0.1120, -0.0486, -0.0045, -0.0900,  0.0690,\n",
       "                      -0.0373,  0.0104, -0.1010, -0.0495, -0.0051, -0.0301, -0.0357,  0.0270,\n",
       "                      -0.0321, -0.0822, -0.0426, -0.0827, -0.0085,  0.0099,  0.0655,  0.0233,\n",
       "                       0.0171, -0.0824, -0.0161, -0.0450,  0.0520,  0.0144, -0.0826,  0.0525,\n",
       "                      -0.0246, -0.0380, -0.0569, -0.0394, -0.0373, -0.0146,  0.0451, -0.0765,\n",
       "                      -0.0369, -0.0064,  0.0150, -0.0052, -0.0031, -0.0574, -0.0380, -0.0042,\n",
       "                       0.0263, -0.0303, -0.0588,  0.0695,  0.0199, -0.0452,  0.0094, -0.0568,\n",
       "                       0.0180, -0.0349,  0.0730, -0.0233, -0.0199, -0.0508, -0.0713, -0.0592,\n",
       "                       0.0015,  0.0018, -0.0108, -0.0481, -0.0227, -0.0214, -0.0138, -0.0787,\n",
       "                      -0.0180, -0.0631, -0.0247, -0.0181, -0.0622, -0.0719, -0.0092, -0.0414,\n",
       "                       0.0752, -0.0286, -0.0138, -0.0325, -0.0210, -0.0353,  0.0196,  0.0793,\n",
       "                      -0.0625, -0.0613, -0.0660, -0.0188, -0.0310, -0.0048, -0.0770,  0.0043,\n",
       "                      -0.0095, -0.0313, -0.0366, -0.0265, -0.0279, -0.0614, -0.0507, -0.0222,\n",
       "                      -0.0153, -0.0088, -0.0879, -0.0248, -0.0282,  0.0098, -0.0263,  0.0041])),\n",
       "             ('layer2.0.bn2.running_var',\n",
       "              tensor([0.0075, 0.0025, 0.0010, 0.0081, 0.0030, 0.0114, 0.0068, 0.0116, 0.0081,\n",
       "                      0.0039, 0.0060, 0.0038, 0.0060, 0.0150, 0.0068, 0.0062, 0.0187, 0.0087,\n",
       "                      0.0105, 0.0120, 0.0053, 0.0075, 0.0055, 0.0043, 0.0032, 0.0044, 0.0056,\n",
       "                      0.0126, 0.0147, 0.0027, 0.0106, 0.0070, 0.0056, 0.0052, 0.0038, 0.0080,\n",
       "                      0.0038, 0.0016, 0.0043, 0.0047, 0.0049, 0.0045, 0.0062, 0.0158, 0.0029,\n",
       "                      0.0102, 0.0042, 0.0109, 0.0120, 0.0062, 0.0045, 0.0083, 0.0062, 0.0055,\n",
       "                      0.0051, 0.0042, 0.0050, 0.0092, 0.0046, 0.0031, 0.0062, 0.0110, 0.0040,\n",
       "                      0.0065, 0.0021, 0.0073, 0.0086, 0.0080, 0.0043, 0.0025, 0.0052, 0.0180,\n",
       "                      0.0073, 0.0051, 0.0099, 0.0062, 0.0186, 0.0033, 0.0028, 0.0045, 0.0061,\n",
       "                      0.0080, 0.0071, 0.0037, 0.0122, 0.0026, 0.0015, 0.0086, 0.0099, 0.0133,\n",
       "                      0.0110, 0.0028, 0.0033, 0.0031, 0.0077, 0.0050, 0.0049, 0.0103, 0.0029,\n",
       "                      0.0041, 0.0042, 0.0074, 0.0145, 0.0067, 0.0084, 0.0070, 0.0084, 0.0076,\n",
       "                      0.0082, 0.0021, 0.0075, 0.0023, 0.0026, 0.0067, 0.0027, 0.0020, 0.0062,\n",
       "                      0.0082, 0.0080, 0.0025, 0.0058, 0.0064, 0.0167, 0.0022, 0.0030, 0.0073,\n",
       "                      0.0071, 0.0066])),\n",
       "             ('layer2.0.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.0.conv3.weight',\n",
       "              tensor([[[[ 0.0002]],\n",
       "              \n",
       "                       [[-0.0138]],\n",
       "              \n",
       "                       [[ 0.0032]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0174]],\n",
       "              \n",
       "                       [[ 0.0109]],\n",
       "              \n",
       "                       [[ 0.0047]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0221]],\n",
       "              \n",
       "                       [[ 0.0028]],\n",
       "              \n",
       "                       [[ 0.0048]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0002]],\n",
       "              \n",
       "                       [[ 0.0029]],\n",
       "              \n",
       "                       [[-0.0112]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0456]],\n",
       "              \n",
       "                       [[ 0.0044]],\n",
       "              \n",
       "                       [[-0.0093]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0408]],\n",
       "              \n",
       "                       [[-0.0071]],\n",
       "              \n",
       "                       [[ 0.0064]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0276]],\n",
       "              \n",
       "                       [[ 0.0044]],\n",
       "              \n",
       "                       [[ 0.0131]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0238]],\n",
       "              \n",
       "                       [[ 0.0273]],\n",
       "              \n",
       "                       [[ 0.0029]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0087]],\n",
       "              \n",
       "                       [[-0.0014]],\n",
       "              \n",
       "                       [[-0.0019]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0024]],\n",
       "              \n",
       "                       [[-0.0086]],\n",
       "              \n",
       "                       [[-0.0021]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0175]],\n",
       "              \n",
       "                       [[-0.0046]],\n",
       "              \n",
       "                       [[-0.0014]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0083]],\n",
       "              \n",
       "                       [[-0.0169]],\n",
       "              \n",
       "                       [[-0.0133]]]])),\n",
       "             ('layer2.0.bn3.weight',\n",
       "              tensor([-5.2197e-02,  5.1248e-02, -1.6202e-01, -8.9730e-02,  1.0097e-01,\n",
       "                      -2.4682e-02, -5.9527e-02,  1.2121e-01, -2.5640e-02,  2.4773e-02,\n",
       "                       6.9645e-02,  9.9280e-02, -5.4058e-02, -1.3079e-03, -7.0399e-02,\n",
       "                      -2.3195e-02, -1.4912e-02, -1.6791e-02, -3.6002e-04,  8.9134e-02,\n",
       "                       8.6620e-02, -1.1341e-01,  5.5849e-02, -7.5246e-03,  2.0989e-02,\n",
       "                      -7.5753e-03, -2.1454e-03, -4.3312e-02, -2.7855e-02,  1.2832e-02,\n",
       "                       3.1296e-02,  3.4498e-02, -4.3115e-02, -4.0526e-02, -3.3153e-03,\n",
       "                       2.4391e-03, -5.4451e-02, -3.1529e-02, -1.2094e-02, -2.8724e-02,\n",
       "                      -2.1319e-02, -7.6896e-03,  1.2431e-02,  7.4280e-02,  7.5570e-02,\n",
       "                      -8.0070e-02,  9.4405e-02,  6.4985e-03,  6.3453e-02,  1.0532e-01,\n",
       "                      -4.7266e-02,  3.9390e-02, -1.7513e-02,  3.2782e-02,  1.6329e-02,\n",
       "                       4.5585e-03, -4.4827e-02, -1.8404e-05, -1.6859e-02, -2.7807e-02,\n",
       "                       5.3903e-02, -2.4116e-03,  1.3225e-01, -7.4900e-02,  9.4079e-02,\n",
       "                       5.5378e-02,  4.6059e-02,  5.1690e-02,  1.1593e-01,  9.3989e-02,\n",
       "                      -2.8153e-03,  3.0083e-03, -2.5762e-03,  3.2765e-02,  1.1488e-01,\n",
       "                       8.3104e-02, -1.0668e-01, -6.7324e-02,  5.0648e-03,  2.9821e-02,\n",
       "                       6.8025e-02, -3.8315e-05,  1.5515e-02,  9.7986e-02,  5.4847e-02,\n",
       "                      -5.2583e-02,  1.2184e-01,  1.9622e-02,  6.9588e-02, -2.9551e-04,\n",
       "                      -1.0687e-01,  4.8311e-02, -7.5895e-02,  5.6741e-02, -3.2625e-02,\n",
       "                       9.0650e-02, -1.2954e-02,  2.4492e-02, -4.8236e-02, -1.5140e-02,\n",
       "                       4.7810e-02, -5.5441e-02, -8.2206e-02, -2.1852e-02,  8.7638e-02,\n",
       "                      -5.1779e-02, -7.6340e-02, -1.1836e-01, -4.6504e-02, -7.9795e-02,\n",
       "                       1.3747e-01,  1.2414e-02,  1.4554e-02,  1.5898e-02, -6.6915e-02,\n",
       "                      -3.4859e-02, -3.5197e-02, -3.8070e-02,  5.0421e-02,  2.8935e-02,\n",
       "                      -5.5282e-02,  8.9908e-02, -1.5863e-01,  2.7359e-02, -4.7019e-02,\n",
       "                       5.0248e-02, -1.0468e-01,  1.8663e-02, -5.6507e-02, -2.4804e-02,\n",
       "                      -1.3959e-02,  1.8036e-02,  3.2869e-02, -5.2282e-02,  5.8837e-03,\n",
       "                      -1.4651e-02,  1.7226e-01,  2.4647e-04, -3.4920e-03, -7.3276e-02,\n",
       "                       9.3390e-02, -2.2842e-03,  4.4974e-02,  3.6112e-02,  5.3521e-02,\n",
       "                      -4.3313e-02,  1.7580e-02,  8.0048e-02, -7.6056e-02,  2.1563e-04,\n",
       "                      -1.7860e-02, -1.1669e-02, -9.8165e-02,  8.0049e-02,  2.9250e-02,\n",
       "                       1.8608e-01,  1.1068e-02, -4.5626e-02, -1.0949e-02,  3.8347e-03,\n",
       "                      -7.7893e-02,  5.4714e-02,  5.3262e-02,  1.8239e-01, -3.0374e-02,\n",
       "                      -8.5719e-02,  4.2295e-02, -1.5602e-02, -1.8649e-02,  2.8085e-02,\n",
       "                       8.3847e-03, -3.7786e-03, -5.0152e-02,  3.1220e-02, -1.1031e-01,\n",
       "                       6.0296e-02,  2.8683e-02, -3.4576e-02,  4.8397e-02,  3.0095e-02,\n",
       "                      -4.4164e-02, -7.4786e-02,  5.9440e-02, -3.1255e-02, -9.8115e-03,\n",
       "                      -8.0920e-02,  9.7951e-02, -4.6449e-02, -6.9166e-03,  8.4199e-03,\n",
       "                       7.6309e-02,  4.9664e-02,  5.9692e-02, -5.7284e-02, -4.0393e-02,\n",
       "                      -3.9729e-02,  3.0887e-02, -5.9073e-02,  1.3607e-01,  4.7364e-02,\n",
       "                      -7.4426e-02, -3.3556e-02,  1.4278e-02,  1.8926e-01,  3.2924e-02,\n",
       "                      -9.3773e-04, -3.1770e-02, -2.5556e-03, -1.2325e-01, -3.7271e-03,\n",
       "                      -4.2474e-02, -5.1701e-02,  3.4333e-03, -1.5144e-02, -1.4008e-05,\n",
       "                       9.1252e-03,  5.7566e-02, -9.7814e-02,  4.1122e-04,  2.1329e-03,\n",
       "                      -4.5321e-02,  4.9707e-03, -7.1357e-03, -1.6756e-01,  5.7607e-02,\n",
       "                      -7.5327e-02, -1.1791e-02, -1.5932e-02,  4.8923e-03, -3.9078e-03,\n",
       "                      -1.7400e-02,  4.0881e-02, -1.0631e-01,  5.4161e-02,  4.4645e-02,\n",
       "                       5.8523e-02,  4.9963e-02,  1.2463e-02, -8.9826e-04, -6.2057e-02,\n",
       "                       1.2836e-01, -2.0152e-02, -4.7984e-02, -9.5736e-02, -8.0144e-02,\n",
       "                      -5.3498e-02,  4.7640e-02,  3.2454e-02, -3.2610e-02,  3.6292e-02,\n",
       "                      -5.0485e-02,  2.0604e-04, -1.8112e-02, -1.2564e-01, -4.6492e-02,\n",
       "                      -4.7755e-02,  4.6017e-02, -2.9209e-02, -7.2828e-02, -1.1635e-02,\n",
       "                       5.3015e-02, -6.6533e-03, -1.6225e-01, -5.0005e-02, -1.1588e-01,\n",
       "                       1.0236e-01,  4.7411e-02, -2.8289e-02,  9.5788e-02, -9.9632e-02,\n",
       "                       4.2800e-02,  4.1430e-02,  3.6894e-02,  1.6500e-02, -9.0984e-02,\n",
       "                      -1.5333e-01, -1.4118e-01,  9.5700e-02,  5.9358e-03, -1.9729e-02,\n",
       "                       5.0152e-02,  3.5322e-02,  2.6046e-02, -3.8832e-02, -5.9793e-02,\n",
       "                       1.0255e-02, -1.2975e-01, -8.8027e-02, -6.2638e-02,  1.4171e-01,\n",
       "                       6.3468e-02, -9.9224e-03, -6.5661e-02, -1.1195e-01,  1.0023e-02,\n",
       "                       9.8337e-02,  2.6156e-02,  2.8275e-02, -3.9168e-03, -2.6562e-02,\n",
       "                       1.5496e-02, -1.4865e-03, -7.8947e-02,  1.2587e-01, -4.3285e-02,\n",
       "                      -8.6869e-02,  5.7780e-02,  5.3465e-02,  1.2807e-01,  7.2566e-02,\n",
       "                       3.2664e-02, -1.7414e-02, -1.0547e-04,  7.7235e-04, -7.9538e-02,\n",
       "                      -4.3943e-02, -7.2722e-03, -6.1020e-02, -1.5541e-02,  7.1819e-02,\n",
       "                       1.6208e-02,  3.1078e-02,  5.8658e-03,  2.3301e-02, -1.1649e-05,\n",
       "                      -7.1009e-02, -2.5451e-02, -5.9006e-03,  8.6081e-02, -7.5074e-02,\n",
       "                      -9.0072e-04, -1.3535e-02, -1.8820e-02, -1.5649e-02,  6.4092e-03,\n",
       "                      -5.4066e-02,  1.0981e-01, -7.7021e-02, -7.1906e-02,  1.9792e-02,\n",
       "                      -1.5559e-01,  5.1853e-02,  5.2664e-03, -9.5266e-03, -9.3357e-02,\n",
       "                      -2.6026e-02, -5.3403e-03, -2.3648e-03, -3.4435e-02, -3.0068e-02,\n",
       "                      -3.4141e-02, -1.6533e-02, -8.3459e-02,  2.3160e-02,  3.1176e-02,\n",
       "                      -7.8223e-02,  7.3820e-03, -2.9292e-02, -2.5741e-02, -2.4549e-04,\n",
       "                      -2.3093e-02,  6.7949e-03, -3.4729e-02, -1.2629e-01,  8.0970e-02,\n",
       "                      -7.7860e-02, -5.0813e-02, -4.3587e-02, -5.4877e-02,  1.0187e-01,\n",
       "                      -1.7796e-01, -8.3355e-02, -7.3231e-02,  2.8165e-02,  1.1561e-02,\n",
       "                      -5.2938e-02, -1.5789e-02, -3.1947e-02,  5.1780e-02, -1.9616e-01,\n",
       "                      -1.0844e-01,  1.0505e-03,  5.5306e-02,  2.7422e-02,  1.4831e-01,\n",
       "                       1.9211e-02,  6.4806e-02, -4.5538e-02, -8.8272e-02, -7.0496e-02,\n",
       "                      -3.3928e-02,  2.6630e-02,  5.0240e-02, -3.3511e-02,  1.8955e-02,\n",
       "                       9.7684e-02, -3.2264e-02, -6.1431e-02, -1.2634e-01,  3.4749e-02,\n",
       "                       6.7699e-02,  1.6049e-02, -8.0910e-02,  6.2732e-03, -3.3968e-02,\n",
       "                       1.7636e-01,  8.8430e-02,  5.1892e-02, -2.8204e-02, -1.9889e-02,\n",
       "                       5.0609e-02, -1.2852e-01,  1.3325e-01, -3.9230e-02, -1.1610e-01,\n",
       "                       2.8859e-02, -6.0532e-02,  1.2633e-02,  2.7450e-02,  1.9074e-03,\n",
       "                      -1.8903e-02, -5.2434e-02, -1.0866e-01,  5.2834e-02, -5.7655e-02,\n",
       "                      -2.8231e-02,  2.9714e-03, -7.9482e-02,  9.4530e-02, -2.0644e-02,\n",
       "                       5.2696e-02, -8.4198e-06,  8.6490e-02,  2.8264e-02,  2.0796e-02,\n",
       "                      -3.9221e-02,  9.5691e-04,  5.6229e-02,  3.3977e-05,  1.0066e-02,\n",
       "                      -1.4365e-03,  1.6321e-01, -2.3833e-05, -8.2538e-03,  3.4051e-02,\n",
       "                      -1.1416e-01, -7.6610e-02, -1.2204e-02,  1.1828e-01, -1.4432e-02,\n",
       "                       6.7399e-02, -3.2843e-02,  1.8197e-02,  1.2549e-02,  4.9842e-02,\n",
       "                      -1.2462e-01, -3.1114e-02, -7.0389e-02, -1.7995e-02, -1.2423e-01,\n",
       "                      -1.7953e-01, -5.5853e-03,  3.4073e-02,  3.3568e-03, -1.2975e-01,\n",
       "                       7.2382e-03,  5.7643e-04, -3.3500e-03,  6.4194e-02,  1.7570e-01,\n",
       "                       1.2022e-02, -5.7131e-05, -9.7675e-02, -8.8758e-02, -4.1028e-03,\n",
       "                       4.2635e-02, -6.7340e-02,  3.5825e-02, -1.1136e-02,  1.8179e-03,\n",
       "                       3.3186e-03, -7.6013e-02,  5.1885e-02,  5.0101e-02,  4.6884e-02,\n",
       "                       6.1635e-02, -8.3510e-03, -3.9610e-02, -1.0348e-02,  2.9811e-02,\n",
       "                       5.2171e-02,  3.5110e-02,  7.7161e-02,  4.0564e-03,  5.6863e-02,\n",
       "                      -8.7559e-03, -9.1185e-02, -2.3585e-03,  1.4002e-02,  2.6706e-05,\n",
       "                      -2.3972e-02, -9.4760e-03, -6.1618e-03,  1.0983e-02, -5.0499e-02,\n",
       "                       2.8980e-02, -7.6049e-02,  2.5645e-03, -9.9701e-02, -9.8390e-02,\n",
       "                      -1.2490e-02,  1.3489e-01])),\n",
       "             ('layer2.0.bn3.bias',\n",
       "              tensor([ 3.3824e-02, -2.6047e-03, -1.6732e-02,  5.9837e-04, -1.5353e-02,\n",
       "                       3.9819e-03, -7.1864e-04, -4.5769e-02,  4.0193e-03,  6.7994e-03,\n",
       "                       9.7809e-03, -1.7096e-02,  3.1275e-02,  1.8799e-02, -6.0214e-03,\n",
       "                       1.7165e-03, -8.9159e-03,  3.2090e-02,  3.2421e-02, -5.3906e-03,\n",
       "                       1.5627e-02, -1.0766e-02,  7.0678e-03, -4.5300e-03,  1.4072e-02,\n",
       "                       4.9520e-03, -5.4931e-03, -9.6218e-03, -2.4590e-03,  3.2873e-03,\n",
       "                       7.2235e-03,  7.9440e-03, -1.9372e-02,  1.1088e-02, -6.3783e-03,\n",
       "                      -1.9668e-02,  2.0794e-03, -1.7835e-03, -4.5557e-04,  1.1219e-02,\n",
       "                      -3.1214e-03, -3.8131e-03,  6.9265e-03,  5.8402e-03, -2.4209e-02,\n",
       "                       1.6232e-02, -4.8928e-03,  2.4059e-02,  9.5472e-03,  6.7785e-04,\n",
       "                       1.9784e-02,  3.2664e-02, -1.0632e-03,  6.3059e-04, -9.5212e-04,\n",
       "                      -4.7543e-03, -9.0653e-03, -1.9038e-04, -2.0962e-02,  7.2396e-03,\n",
       "                      -6.7345e-03,  1.2864e-03,  2.1478e-03,  2.5562e-02, -3.0452e-02,\n",
       "                       3.3611e-02,  1.6851e-03, -2.2040e-02, -3.5913e-02, -4.1229e-03,\n",
       "                       8.4935e-03, -7.2436e-03, -3.3623e-02,  3.2561e-02,  1.5456e-02,\n",
       "                      -1.4539e-02, -6.3838e-03, -5.8704e-03,  8.1845e-03,  9.7343e-03,\n",
       "                      -1.5602e-02, -1.1606e-04,  3.2095e-03, -4.7290e-03, -2.0737e-02,\n",
       "                      -4.8020e-03, -3.6224e-02, -7.5584e-03, -3.9103e-03, -9.1736e-04,\n",
       "                       6.0094e-03,  3.3372e-03, -1.2310e-02,  1.6109e-02, -1.2063e-02,\n",
       "                      -3.3791e-03, -5.3916e-04, -1.5179e-03,  1.7888e-04,  1.5313e-03,\n",
       "                       4.5531e-03,  9.4051e-03,  3.6411e-02, -1.2525e-03,  6.0353e-03,\n",
       "                      -4.9961e-03,  6.9729e-03, -2.5784e-02, -2.1228e-02, -4.5962e-03,\n",
       "                      -1.5203e-02, -1.0874e-02,  2.3093e-03,  7.4508e-03,  2.8742e-03,\n",
       "                       3.4460e-03,  2.9879e-03, -3.9550e-03,  1.3364e-02,  5.7887e-04,\n",
       "                       3.1321e-03, -3.3281e-02, -1.3214e-02, -1.6437e-02, -2.7990e-02,\n",
       "                       4.7132e-03, -3.6653e-02, -7.9457e-03, -5.7680e-03, -7.4998e-03,\n",
       "                       5.5318e-03, -4.9935e-04, -6.2480e-03, -9.8127e-04,  1.1565e-02,\n",
       "                      -1.6484e-03, -2.1851e-02, -6.1814e-04,  8.1516e-03, -2.3882e-02,\n",
       "                      -2.0623e-02,  6.9460e-03, -1.6842e-02,  1.5465e-02,  1.8917e-02,\n",
       "                       3.5664e-02,  7.2671e-03,  7.6214e-04, -8.7513e-03, -6.2562e-04,\n",
       "                      -1.6026e-02, -2.9168e-03,  1.9663e-02, -1.2904e-03,  7.5160e-03,\n",
       "                      -9.7239e-05,  3.3482e-03,  1.2439e-02,  2.4790e-02, -1.8102e-02,\n",
       "                      -3.7170e-06,  2.4783e-04, -8.8856e-03, -1.4613e-02,  2.1967e-02,\n",
       "                      -2.2253e-02,  2.1581e-02, -1.2285e-02,  1.3829e-02,  2.4286e-02,\n",
       "                       7.3525e-03, -8.1414e-03, -4.3058e-02,  1.0515e-02, -2.2556e-02,\n",
       "                      -1.4690e-02, -2.2190e-02,  7.8173e-03,  8.9160e-03, -1.7626e-03,\n",
       "                       4.0264e-03,  5.6762e-03, -1.0073e-03,  1.6727e-03, -2.9576e-02,\n",
       "                      -5.1109e-03, -1.0507e-02, -5.4845e-03,  9.1518e-03,  6.1695e-03,\n",
       "                       6.6858e-03, -7.5155e-03,  7.5894e-03, -9.6678e-03,  6.4868e-05,\n",
       "                       4.8990e-04, -1.6590e-03, -1.0613e-02,  9.3086e-03,  1.0908e-02,\n",
       "                      -8.9552e-04, -5.1680e-04,  7.1046e-03, -1.2152e-02, -4.4859e-03,\n",
       "                      -6.7060e-02,  2.7033e-02, -6.8964e-03,  4.4629e-02,  3.7824e-02,\n",
       "                      -1.0136e-03,  2.3820e-02, -1.9351e-03,  9.4677e-03, -7.8759e-05,\n",
       "                      -1.4999e-02,  3.1733e-02, -1.8734e-02, -1.0327e-02,  1.1754e-02,\n",
       "                       4.7553e-03, -2.4779e-03,  1.2824e-02, -3.0894e-02, -5.0283e-03,\n",
       "                       2.3716e-02, -8.4647e-03,  1.3639e-02, -2.3090e-03,  2.8945e-03,\n",
       "                       2.6616e-03, -1.0193e-02,  2.3884e-03, -7.0685e-03, -1.2962e-02,\n",
       "                      -3.4365e-04,  1.0568e-02,  3.8828e-03, -1.6666e-03,  2.6652e-03,\n",
       "                       5.0333e-02,  9.1508e-03, -6.8018e-03, -1.2557e-02, -3.0316e-02,\n",
       "                      -7.6796e-03, -1.5013e-02,  5.6976e-03, -3.7744e-02,  4.6106e-02,\n",
       "                       2.1530e-03,  3.6769e-04,  2.2205e-02, -2.7443e-02, -1.3026e-02,\n",
       "                       8.6303e-03, -2.2596e-03,  1.8931e-03,  2.7702e-03,  2.6900e-02,\n",
       "                       1.0671e-03,  1.6107e-02,  1.9970e-02, -4.4597e-03, -3.9116e-03,\n",
       "                       4.9573e-03,  4.0603e-03, -4.8045e-03, -9.1606e-03,  3.2684e-03,\n",
       "                       9.3202e-03, -1.2765e-02,  1.6661e-03,  3.7924e-02,  5.1221e-02,\n",
       "                      -1.3153e-02, -1.7259e-02, -8.5915e-04, -3.5864e-02, -4.4496e-02,\n",
       "                      -1.0373e-02,  2.4461e-02, -1.4089e-04, -8.1029e-03, -8.3374e-03,\n",
       "                       1.5562e-02,  3.0119e-02, -2.7318e-03,  7.1437e-03, -8.2767e-03,\n",
       "                      -1.6938e-03, -6.8909e-03,  5.5183e-03, -2.0665e-03,  1.8811e-02,\n",
       "                      -3.0584e-03,  5.7128e-03, -1.0369e-02,  7.7737e-03,  1.9930e-02,\n",
       "                      -4.7280e-03, -3.4843e-02, -1.6369e-02, -4.4654e-02, -4.8631e-03,\n",
       "                       1.0984e-02, -1.5505e-02, -5.2024e-03,  1.0867e-02,  5.5555e-03,\n",
       "                       2.8317e-03, -3.8683e-03, -3.8877e-04,  2.5451e-03, -4.5148e-03,\n",
       "                       9.2290e-03,  8.0716e-03, -5.6292e-03, -2.0006e-03, -1.2698e-02,\n",
       "                      -1.0742e-02, -1.7009e-02, -3.8558e-04, -1.3002e-02, -7.1345e-05,\n",
       "                       2.1222e-02, -5.8535e-03, -5.3322e-03, -1.5611e-02,  2.4683e-02,\n",
       "                       4.0145e-02,  2.2666e-04, -3.7282e-03, -2.4271e-02, -6.6814e-03,\n",
       "                       1.1668e-02,  3.4512e-02,  1.5859e-03,  1.2236e-02,  7.6477e-05,\n",
       "                       1.6798e-02,  9.9264e-03, -1.1853e-02, -5.7524e-03,  1.3634e-02,\n",
       "                      -1.6159e-02,  3.1174e-03,  6.2337e-03, -8.1357e-03,  1.5050e-02,\n",
       "                       2.4448e-03,  2.4860e-02,  1.1793e-02, -2.5192e-03,  1.1124e-02,\n",
       "                       6.9928e-03, -1.6517e-02, -6.2937e-03, -2.3632e-02, -4.2067e-03,\n",
       "                       7.9993e-03, -3.0144e-03, -1.9788e-03,  7.1632e-02,  4.2660e-02,\n",
       "                       1.3317e-02,  4.3381e-02, -1.1057e-03,  7.0495e-03,  7.0446e-03,\n",
       "                       6.4138e-03,  7.6731e-03, -4.5546e-03, -9.3834e-03,  3.4237e-02,\n",
       "                       2.2232e-02,  6.0292e-03, -4.8586e-03,  2.0352e-03, -6.5137e-03,\n",
       "                       1.1235e-02,  1.2232e-02,  9.3775e-03, -8.7847e-03, -2.3955e-02,\n",
       "                      -3.5733e-03,  1.3138e-02, -1.3171e-02, -8.9317e-03, -1.7957e-02,\n",
       "                      -8.1642e-03,  4.5631e-03, -1.6995e-03,  2.6420e-02,  3.4412e-02,\n",
       "                       8.3558e-04, -1.5352e-02,  2.8168e-03, -3.3684e-02,  6.6818e-03,\n",
       "                      -7.2084e-04, -1.1951e-02,  1.7742e-02, -1.4083e-02, -7.8707e-03,\n",
       "                      -1.5631e-02, -1.7903e-02,  1.0908e-02, -8.9580e-03, -4.6454e-04,\n",
       "                      -1.3548e-02, -1.4645e-02, -3.9785e-03, -1.4989e-02, -2.3067e-02,\n",
       "                       4.2784e-03, -1.1986e-02, -1.7303e-02, -3.4726e-02,  1.4300e-03,\n",
       "                       8.4295e-03, -2.7121e-02,  8.6106e-03, -1.9675e-02,  1.2190e-02,\n",
       "                      -7.4277e-03,  5.0227e-03, -2.4382e-02, -2.2719e-02, -3.1571e-02,\n",
       "                       1.4546e-02, -3.9153e-05, -8.0821e-03,  2.5303e-03, -1.2269e-03,\n",
       "                       2.0729e-02,  4.2484e-03, -1.3004e-02, -8.9005e-05,  9.7262e-03,\n",
       "                       1.1967e-02, -5.7223e-03, -4.1763e-04,  2.9492e-02, -1.9246e-02,\n",
       "                       7.7236e-03, -8.7909e-03,  5.6738e-03, -9.4157e-03, -6.5971e-03,\n",
       "                      -1.5731e-02,  1.3793e-02,  1.5450e-02,  3.2173e-03, -2.2713e-02,\n",
       "                      -3.3185e-03,  5.0734e-03, -3.1155e-02,  1.9183e-03,  4.9972e-02,\n",
       "                       5.2967e-02,  4.5080e-02, -6.5600e-03,  1.1936e-03,  6.9124e-03,\n",
       "                      -2.9263e-03,  7.1675e-03,  1.2202e-02, -6.2074e-03,  5.3216e-02,\n",
       "                      -2.0861e-03, -2.6448e-04,  6.0077e-03, -1.5086e-02, -6.3085e-03,\n",
       "                       1.5466e-03, -2.8052e-02,  5.7890e-03, -2.0732e-02,  3.2765e-02,\n",
       "                       3.2801e-03, -2.9662e-03, -1.0770e-02,  1.5262e-02,  7.3545e-03,\n",
       "                       1.2867e-03,  1.2210e-02,  3.3709e-03, -3.3705e-03, -2.6569e-02,\n",
       "                      -9.6980e-03,  4.5029e-03, -1.8446e-02,  6.7877e-03,  3.3936e-03,\n",
       "                      -3.8178e-03, -6.2146e-03, -6.3080e-03, -3.7384e-03, -5.4363e-05,\n",
       "                       4.3445e-03,  1.0617e-02, -2.0194e-02, -7.6743e-02,  1.6831e-03,\n",
       "                      -2.3786e-02,  7.5207e-02,  1.7562e-02, -7.2932e-03,  1.6118e-02,\n",
       "                      -4.9214e-02, -1.4645e-02])),\n",
       "             ('layer2.0.bn3.running_mean',\n",
       "              tensor([-1.1578e-02, -8.9948e-03, -6.8786e-03,  7.4982e-03, -1.7495e-02,\n",
       "                       8.1208e-03, -3.8031e-03, -2.6240e-02, -2.0630e-03, -1.3440e-02,\n",
       "                      -3.0023e-02, -2.6266e-02,  8.5773e-03, -2.5957e-03, -2.2566e-02,\n",
       "                      -2.6453e-03,  6.2090e-03, -9.0412e-03,  7.9789e-03,  1.9954e-02,\n",
       "                       1.6880e-02,  2.9571e-03, -4.7703e-03,  1.8483e-03, -1.0544e-02,\n",
       "                       7.8652e-03, -2.0005e-03, -4.7569e-03, -3.2538e-03,  4.2300e-04,\n",
       "                      -1.9496e-03,  2.1013e-02, -7.8387e-03,  4.8440e-03,  3.4004e-03,\n",
       "                      -7.7284e-03,  3.0031e-03,  1.2112e-02, -1.7638e-03,  5.8449e-04,\n",
       "                       1.1105e-02,  1.2404e-03,  1.2668e-04,  3.4055e-02, -6.1076e-02,\n",
       "                       2.5806e-02, -1.4370e-02, -9.5896e-03, -2.7575e-04,  1.1762e-02,\n",
       "                       1.4608e-02, -3.7145e-02, -4.6809e-03,  1.3113e-02, -2.8783e-04,\n",
       "                       2.4006e-03, -5.1969e-03, -1.3596e-05, -8.6913e-03,  2.7486e-02,\n",
       "                      -1.9678e-02,  4.5532e-03,  1.0500e-02,  1.8218e-02, -6.1636e-04,\n",
       "                      -6.0763e-03, -2.8534e-02,  4.6274e-04,  1.2609e-02, -1.7712e-03,\n",
       "                      -1.0528e-02,  1.4419e-03,  4.4155e-03, -2.2503e-02, -2.2924e-02,\n",
       "                       8.0162e-03, -5.3457e-03, -6.2049e-03, -4.0788e-04,  2.3897e-03,\n",
       "                       3.3597e-03, -4.0093e-05,  1.6367e-03, -2.0915e-02, -7.9019e-03,\n",
       "                      -2.4073e-03, -5.6442e-02, -9.8327e-03,  2.1506e-02, -1.4972e-04,\n",
       "                       1.1325e-02, -2.2785e-03,  1.1542e-02,  1.2297e-02,  1.7249e-02,\n",
       "                      -4.2417e-04, -2.4977e-03,  3.6149e-03,  2.0796e-03,  6.1635e-03,\n",
       "                       1.7888e-02,  8.1594e-03,  1.3392e-02, -1.0212e-03,  1.2827e-02,\n",
       "                       1.7422e-03,  1.1800e-03,  2.9421e-02, -1.9748e-02,  9.5544e-03,\n",
       "                       7.6326e-03,  4.6289e-03, -7.1425e-04, -5.6371e-04,  3.6872e-02,\n",
       "                       8.7585e-03, -4.4476e-03,  6.7783e-03,  8.9801e-03,  6.4003e-03,\n",
       "                       2.2899e-02,  7.8760e-03,  5.1417e-02, -4.7171e-03,  8.5438e-03,\n",
       "                      -2.8387e-03, -1.8410e-02, -4.1702e-04,  8.5693e-03,  1.9753e-03,\n",
       "                      -2.3116e-03,  1.9193e-03, -2.5203e-03,  1.1243e-02,  2.2384e-03,\n",
       "                       1.0715e-03,  3.4930e-03, -2.9190e-05, -1.6897e-03, -1.5354e-02,\n",
       "                       6.0074e-03,  3.5006e-03,  1.0462e-03, -2.8488e-03, -2.4672e-02,\n",
       "                       1.0751e-02,  7.2698e-03, -1.9005e-02, -4.0843e-03, -7.5040e-05,\n",
       "                       5.1553e-03,  1.6805e-03,  2.3250e-02, -3.4752e-03,  1.1588e-02,\n",
       "                      -2.6482e-02, -1.0574e-02,  2.5389e-02, -6.8505e-03, -1.2324e-02,\n",
       "                      -2.6075e-03,  1.2679e-02, -7.9492e-03, -8.5673e-03,  1.5495e-02,\n",
       "                       3.6526e-02, -2.1078e-02,  1.0663e-02,  1.5055e-03, -1.4482e-02,\n",
       "                       1.4629e-03,  1.2230e-04,  2.7446e-02, -1.1939e-02,  3.3328e-04,\n",
       "                      -5.5750e-03,  5.5735e-03, -6.1556e-04, -1.6697e-02,  3.2647e-03,\n",
       "                      -1.1967e-02,  1.9198e-02,  4.7434e-04, -7.1974e-03, -2.1986e-03,\n",
       "                       9.6700e-04,  2.1103e-02,  2.2529e-02,  3.3051e-03,  1.7996e-03,\n",
       "                      -5.1368e-03, -1.7321e-02,  1.3406e-02,  1.8963e-02,  2.0715e-02,\n",
       "                      -1.2843e-02,  6.0248e-03, -7.4887e-04, -3.0629e-03,  4.4904e-03,\n",
       "                      -2.4508e-02, -3.1819e-03, -4.8379e-03,  2.3933e-02, -5.1196e-04,\n",
       "                      -3.7100e-03,  2.0475e-02, -5.9924e-03,  5.2665e-04,  1.0819e-02,\n",
       "                      -7.2507e-03,  1.4942e-03, -1.5211e-03, -3.4773e-03,  7.2303e-06,\n",
       "                       2.9637e-03,  4.4068e-02, -2.9262e-02,  1.8227e-03,  2.2934e-03,\n",
       "                      -8.7029e-03, -3.2207e-03, -1.0511e-03,  7.1034e-02, -1.2557e-02,\n",
       "                       2.8056e-02, -3.5691e-03,  1.3092e-02,  7.5415e-04, -9.4897e-04,\n",
       "                      -6.0357e-03,  1.8696e-03,  3.8496e-02,  2.5783e-03,  2.0770e-03,\n",
       "                       1.0381e-02,  6.4931e-03,  6.7426e-03,  3.7281e-05,  1.7201e-02,\n",
       "                      -6.8025e-02,  5.3546e-03,  3.6512e-02,  3.0232e-02,  2.8913e-02,\n",
       "                       2.7664e-03, -1.8498e-02,  8.8369e-03, -2.2176e-03, -1.8849e-02,\n",
       "                      -2.3206e-03,  5.1839e-05, -5.7947e-03, -2.4363e-02,  2.9672e-03,\n",
       "                       6.9154e-03,  2.4877e-03,  7.3827e-03,  1.3352e-02,  4.9624e-03,\n",
       "                       9.5262e-03, -1.3982e-03,  5.5290e-03,  1.4360e-02,  4.2023e-03,\n",
       "                      -1.6054e-02,  2.1419e-02,  7.2304e-03,  1.9060e-02,  1.2980e-02,\n",
       "                       1.5760e-02, -1.1355e-02,  1.8063e-02, -1.8356e-02,  1.0953e-02,\n",
       "                       5.5784e-03,  3.8080e-03, -1.4141e-02, -6.7269e-03, -1.0631e-02,\n",
       "                       1.1664e-03,  3.1179e-03, -1.1363e-02, -6.3961e-03, -7.0596e-03,\n",
       "                      -4.5354e-04,  2.4519e-02, -1.3357e-02,  3.4989e-02, -3.6162e-02,\n",
       "                      -1.3848e-03, -2.5949e-03,  1.7471e-02, -2.6525e-02, -7.5439e-03,\n",
       "                      -7.2955e-03,  1.0139e-03, -6.8091e-03, -4.8268e-03,  5.9340e-03,\n",
       "                       3.8540e-03,  8.3171e-03,  6.6091e-03,  2.0713e-02, -4.9681e-03,\n",
       "                       1.8071e-03, -2.8025e-02,  8.8621e-03, -1.4184e-02, -7.2677e-03,\n",
       "                      -2.3865e-03,  1.9746e-02, -1.2494e-04, -2.1206e-03,  3.1589e-02,\n",
       "                      -1.0136e-02,  5.7805e-03,  1.0010e-02,  1.7055e-04, -9.5917e-03,\n",
       "                       1.2298e-02, -4.0169e-03, -9.7087e-04, -3.3987e-03, -2.1543e-05,\n",
       "                       1.7323e-02, -3.5607e-03,  1.9822e-03,  1.1878e-02,  1.6012e-02,\n",
       "                      -5.6017e-03,  1.4767e-03,  4.7214e-03,  9.2539e-03, -6.6955e-03,\n",
       "                      -1.5225e-02,  1.0339e-02,  5.4312e-02, -6.5818e-04, -1.7618e-03,\n",
       "                      -1.2214e-04,  2.8025e-02, -6.8775e-03, -1.4953e-03, -2.7101e-02,\n",
       "                       5.4523e-03, -2.0409e-03,  4.9810e-03,  5.9865e-03,  1.6026e-02,\n",
       "                       5.8188e-03,  1.7759e-02, -6.8604e-03,  4.7013e-03, -9.2393e-03,\n",
       "                       6.2683e-02, -5.3028e-03,  5.0690e-03, -8.5703e-03,  7.8787e-05,\n",
       "                       7.9172e-04, -6.9371e-03, -1.6836e-02,  2.3541e-02, -1.0282e-02,\n",
       "                       2.1972e-02, -5.7383e-03, -6.0014e-04,  1.5494e-02, -3.3724e-02,\n",
       "                       8.9980e-03, -2.8378e-02,  3.5326e-03, -1.1403e-02, -1.8224e-02,\n",
       "                       9.4694e-03, -2.8210e-03, -5.8754e-03, -9.1761e-04,  2.9853e-02,\n",
       "                       2.7826e-02,  1.0864e-03, -9.1411e-03,  3.3817e-03, -9.4135e-03,\n",
       "                       3.4514e-03, -1.2298e-02,  2.3602e-02,  8.6478e-04,  1.8992e-02,\n",
       "                      -4.3356e-03,  1.4743e-02,  2.4389e-02,  3.7446e-03, -2.8220e-03,\n",
       "                      -4.0327e-03, -3.4453e-03,  2.5107e-02, -2.5651e-03,  1.1513e-02,\n",
       "                       2.9521e-02,  1.3784e-03, -1.1963e-02,  2.4358e-03,  2.0799e-02,\n",
       "                      -1.1483e-02,  5.5195e-03, -8.7535e-04, -1.0476e-02, -5.4577e-03,\n",
       "                       5.5341e-03, -1.3032e-02, -4.1009e-03,  8.5072e-03,  3.1047e-03,\n",
       "                      -2.1074e-02, -1.0119e-02, -4.7479e-03, -4.2859e-02,  3.2270e-03,\n",
       "                       2.9494e-03,  6.0530e-03,  4.8414e-02, -2.5554e-02, -7.2330e-03,\n",
       "                      -1.4011e-02, -3.0952e-03, -1.7197e-02, -2.1241e-03,  5.8331e-03,\n",
       "                      -3.7670e-03,  1.2509e-05,  1.5939e-02,  7.1252e-03,  9.6216e-04,\n",
       "                       3.3798e-02,  8.2442e-03,  2.8524e-03,  4.9676e-06,  4.3471e-03,\n",
       "                       1.0187e-03, -2.6407e-02, -1.7333e-05,  9.3317e-03,  2.0060e-03,\n",
       "                       3.5125e-02, -5.3892e-03,  1.5768e-02, -1.7551e-02,  1.2272e-03,\n",
       "                       1.3597e-02, -5.7334e-03, -1.4907e-03,  1.9369e-03,  2.7000e-03,\n",
       "                       1.2840e-02, -8.5366e-03, -2.0529e-02, -1.6679e-03,  4.5696e-02,\n",
       "                       1.3223e-02, -4.4486e-03, -1.0630e-02, -4.3643e-03,  4.5547e-02,\n",
       "                      -6.7704e-04, -4.8483e-04, -3.5776e-03, -1.7889e-02, -5.9251e-02,\n",
       "                       3.0516e-03, -5.9316e-05, -1.7940e-02,  2.2037e-02, -4.7078e-03,\n",
       "                       7.6052e-03,  1.4594e-02,  1.2524e-02,  6.2666e-03,  1.2442e-02,\n",
       "                       3.6313e-03, -1.1716e-02,  3.0147e-03, -4.1902e-03,  8.7118e-03,\n",
       "                      -8.1381e-03,  1.1042e-03, -5.4071e-03, -1.1934e-03, -9.6476e-03,\n",
       "                       2.3050e-03,  1.0982e-02, -4.6808e-03,  7.8829e-04, -5.7117e-04,\n",
       "                      -5.3719e-03, -1.7618e-02, -6.3649e-05, -1.2882e-02, -5.1349e-06,\n",
       "                       2.0684e-02,  6.3458e-03,  2.2417e-03, -2.7584e-03, -1.0242e-02,\n",
       "                      -6.7414e-03,  2.4469e-02, -1.8698e-03,  4.6977e-02,  5.2180e-02,\n",
       "                      -8.9050e-03, -1.3120e-02])),\n",
       "             ('layer2.0.bn3.running_var',\n",
       "              tensor([4.0585e-04, 2.2981e-04, 1.0383e-03, 2.4877e-04, 6.9764e-04, 1.4150e-04,\n",
       "                      4.0964e-04, 6.7241e-04, 4.4232e-05, 8.1797e-05, 2.4461e-04, 3.9249e-04,\n",
       "                      4.0095e-04, 2.7654e-05, 4.9889e-04, 5.7895e-05, 3.3906e-05, 2.6891e-04,\n",
       "                      7.0963e-05, 2.9618e-04, 7.3736e-04, 5.7884e-04, 3.7196e-04, 6.2935e-06,\n",
       "                      7.3007e-05, 1.2193e-04, 1.4611e-05, 2.3129e-04, 1.4895e-04, 1.7032e-04,\n",
       "                      1.6200e-04, 6.5169e-05, 1.4482e-04, 1.6068e-04, 1.6085e-05, 9.0768e-05,\n",
       "                      3.8944e-04, 4.2685e-05, 4.5463e-05, 7.4555e-05, 1.0391e-04, 1.0061e-05,\n",
       "                      2.3398e-05, 2.4542e-04, 4.4421e-04, 4.5624e-04, 5.8076e-04, 1.3104e-04,\n",
       "                      2.8432e-04, 3.6544e-04, 3.1554e-04, 6.0060e-04, 2.6381e-05, 8.7404e-05,\n",
       "                      6.9816e-05, 1.7404e-05, 5.2763e-05, 2.8486e-09, 8.3822e-05, 2.3634e-04,\n",
       "                      9.6617e-05, 2.7710e-05, 2.4030e-03, 8.6545e-04, 4.8898e-04, 4.6294e-04,\n",
       "                      4.8266e-04, 1.2937e-04, 5.7187e-04, 1.3103e-03, 1.6057e-04, 2.3128e-06,\n",
       "                      1.4317e-05, 3.9445e-04, 9.5859e-04, 5.7399e-04, 1.0049e-03, 3.6854e-04,\n",
       "                      5.4348e-05, 1.3784e-04, 1.5836e-04, 3.5034e-09, 2.7603e-05, 5.2393e-04,\n",
       "                      4.8684e-04, 1.6599e-04, 9.1032e-04, 6.2647e-05, 1.8803e-04, 1.0159e-07,\n",
       "                      6.6302e-04, 2.2691e-04, 1.8282e-04, 2.9260e-04, 2.0887e-04, 4.1144e-04,\n",
       "                      1.7900e-05, 5.8272e-05, 2.2882e-04, 5.6703e-05, 1.9782e-04, 4.0866e-04,\n",
       "                      1.0371e-03, 5.1985e-05, 8.9906e-04, 2.0081e-04, 7.1549e-04, 8.7313e-04,\n",
       "                      2.7343e-04, 3.3422e-04, 5.3939e-04, 4.1615e-05, 1.8027e-04, 5.1138e-05,\n",
       "                      4.3956e-04, 1.9145e-04, 6.4228e-05, 7.7957e-05, 1.5153e-04, 4.3816e-05,\n",
       "                      3.0785e-04, 3.4478e-04, 9.6747e-04, 1.2808e-04, 1.7104e-04, 2.6733e-04,\n",
       "                      3.6695e-04, 2.4221e-04, 3.0215e-04, 1.1297e-04, 4.4420e-05, 4.7517e-05,\n",
       "                      1.8066e-04, 2.3030e-04, 4.9083e-05, 3.3084e-05, 8.4095e-04, 2.1388e-08,\n",
       "                      6.6839e-06, 5.8752e-04, 9.7793e-04, 4.5735e-05, 1.1078e-04, 1.7785e-04,\n",
       "                      2.9177e-04, 3.3821e-04, 3.2265e-05, 6.5175e-04, 4.6946e-04, 6.3413e-09,\n",
       "                      5.5148e-05, 7.2987e-05, 9.1256e-04, 5.3014e-04, 8.5196e-05, 1.3984e-03,\n",
       "                      5.3537e-05, 2.0610e-04, 1.3386e-04, 1.0101e-04, 1.8132e-04, 1.6247e-04,\n",
       "                      2.8787e-04, 2.3232e-03, 4.0114e-04, 3.0357e-04, 2.3239e-04, 1.1217e-04,\n",
       "                      4.9219e-05, 1.1741e-04, 1.4489e-04, 9.8730e-06, 4.5999e-04, 1.3579e-04,\n",
       "                      4.8243e-04, 2.0899e-04, 1.4064e-04, 1.2203e-04, 1.9191e-04, 9.6130e-05,\n",
       "                      1.3278e-04, 4.9532e-04, 3.2759e-04, 5.0111e-05, 9.5576e-05, 2.2566e-04,\n",
       "                      4.8224e-04, 2.8887e-04, 1.6626e-05, 4.7661e-05, 6.8311e-04, 1.3969e-04,\n",
       "                      2.8588e-04, 3.8801e-04, 9.4774e-05, 2.2658e-04, 7.6049e-05, 2.1091e-04,\n",
       "                      8.8098e-04, 3.3888e-04, 3.9923e-04, 1.6220e-04, 6.9556e-05, 1.6477e-03,\n",
       "                      1.1550e-04, 9.6466e-05, 2.1884e-04, 6.9253e-05, 2.0649e-03, 1.9527e-04,\n",
       "                      6.4934e-05, 5.1763e-04, 2.7282e-05, 4.7600e-05, 2.1351e-10, 7.4099e-05,\n",
       "                      2.8163e-04, 4.4576e-04, 3.5777e-05, 9.3478e-06, 9.6208e-05, 3.7437e-05,\n",
       "                      1.4664e-05, 7.7533e-04, 3.8673e-04, 6.5058e-04, 4.2456e-05, 1.6202e-04,\n",
       "                      8.0888e-05, 1.7073e-05, 2.8668e-05, 1.5170e-04, 3.8124e-04, 1.3404e-04,\n",
       "                      2.9683e-04, 1.7622e-04, 1.1963e-04, 2.1075e-05, 6.1192e-07, 2.4209e-04,\n",
       "                      3.1159e-03, 1.1183e-04, 7.7930e-04, 5.7655e-04, 5.3920e-04, 2.7279e-04,\n",
       "                      2.4695e-04, 4.9407e-05, 1.0308e-04, 5.7653e-04, 9.5699e-05, 1.4806e-09,\n",
       "                      1.3572e-04, 5.6979e-04, 2.2330e-04, 3.3057e-04, 8.2512e-05, 1.8241e-04,\n",
       "                      4.6291e-04, 1.4997e-04, 2.2638e-04, 6.4429e-05, 1.6180e-03, 2.9108e-04,\n",
       "                      6.7763e-04, 6.8010e-04, 2.2496e-04, 9.2315e-05, 5.2126e-04, 7.0450e-04,\n",
       "                      3.8256e-04, 1.1725e-04, 1.9441e-04, 2.6594e-04, 1.5323e-03, 8.2634e-04,\n",
       "                      7.8107e-04, 3.4826e-04, 3.6198e-04, 1.3680e-04, 1.1971e-04, 1.2392e-04,\n",
       "                      1.5206e-04, 8.1761e-05, 2.6215e-04, 9.4366e-05, 9.7987e-04, 3.1036e-04,\n",
       "                      4.5222e-04, 8.0235e-04, 2.2769e-04, 3.6368e-05, 3.8763e-04, 4.6785e-04,\n",
       "                      1.0601e-04, 4.6692e-04, 8.5005e-05, 7.0547e-05, 2.7515e-05, 1.7190e-04,\n",
       "                      9.7167e-05, 8.8901e-05, 3.0130e-04, 7.3509e-04, 9.6091e-05, 5.4582e-04,\n",
       "                      2.5273e-04, 2.7104e-04, 1.0755e-03, 5.4203e-04, 1.7093e-04, 2.2336e-04,\n",
       "                      2.4178e-08, 1.5694e-05, 4.5562e-04, 1.3715e-04, 5.7496e-05, 4.5478e-04,\n",
       "                      2.3565e-05, 2.8411e-04, 9.4829e-05, 2.2645e-04, 2.0658e-05, 1.5288e-04,\n",
       "                      3.0054e-10, 4.5641e-04, 6.1831e-05, 1.9335e-05, 6.3198e-04, 6.9047e-04,\n",
       "                      7.6582e-05, 2.2580e-05, 6.8899e-05, 9.5904e-05, 5.6603e-05, 2.4720e-04,\n",
       "                      1.8436e-03, 9.2391e-04, 3.2303e-04, 1.2746e-04, 1.4605e-03, 2.1471e-04,\n",
       "                      5.6087e-05, 6.2652e-05, 4.7084e-04, 1.0207e-04, 9.7649e-06, 3.5789e-05,\n",
       "                      9.7190e-05, 1.1342e-04, 1.8513e-04, 1.2900e-04, 2.9512e-04, 5.8015e-05,\n",
       "                      1.1514e-04, 7.7194e-04, 3.0667e-05, 1.2137e-04, 4.6221e-05, 5.7809e-07,\n",
       "                      5.7096e-05, 7.8199e-06, 4.7955e-05, 3.4215e-03, 8.9855e-04, 3.5033e-04,\n",
       "                      1.0783e-03, 1.6552e-04, 1.9692e-04, 9.8760e-04, 1.6374e-03, 3.1083e-04,\n",
       "                      4.5650e-04, 8.6964e-05, 1.7759e-04, 3.8863e-04, 3.3361e-05, 5.3769e-05,\n",
       "                      2.1918e-04, 1.5559e-03, 6.0841e-04, 8.0780e-06, 4.8849e-04, 7.2790e-05,\n",
       "                      8.3573e-04, 1.1958e-04, 2.6318e-04, 2.6357e-04, 2.9059e-04, 2.9233e-04,\n",
       "                      4.7410e-05, 8.0418e-05, 3.1178e-04, 2.8806e-04, 2.1851e-04, 6.4626e-04,\n",
       "                      1.3012e-04, 3.2374e-04, 7.9479e-04, 5.5917e-05, 3.9068e-04, 1.0772e-04,\n",
       "                      1.4103e-03, 1.9296e-05, 1.2812e-04, 9.1930e-04, 3.4537e-04, 4.6154e-04,\n",
       "                      6.5952e-05, 5.3429e-05, 1.6886e-04, 3.5637e-04, 2.9187e-03, 1.2963e-04,\n",
       "                      7.5452e-04, 2.9446e-04, 1.4452e-04, 4.5187e-05, 2.5771e-04, 2.7969e-05,\n",
       "                      6.9631e-05, 5.7325e-04, 2.9616e-03, 2.2755e-04, 2.3479e-04, 6.8666e-05,\n",
       "                      1.4091e-05, 3.9420e-04, 4.0838e-04, 9.3498e-05, 1.5933e-04, 1.7052e-10,\n",
       "                      4.1983e-04, 6.5178e-05, 2.8758e-05, 3.5350e-04, 8.0288e-05, 2.3810e-04,\n",
       "                      2.7996e-10, 5.1213e-05, 6.4096e-06, 1.2664e-03, 1.0686e-08, 7.7240e-05,\n",
       "                      6.0636e-05, 5.5683e-04, 2.1422e-04, 1.7206e-04, 6.2364e-04, 1.8600e-05,\n",
       "                      2.7960e-04, 1.3922e-04, 4.8946e-05, 3.7136e-05, 1.1431e-04, 1.0469e-03,\n",
       "                      5.3770e-05, 2.0756e-04, 1.2619e-05, 2.8350e-03, 2.2905e-03, 2.1146e-04,\n",
       "                      1.7157e-04, 1.1505e-05, 1.6407e-03, 1.9032e-05, 6.7624e-07, 3.6675e-05,\n",
       "                      2.1308e-04, 3.3325e-03, 5.3640e-05, 2.8753e-09, 4.0714e-04, 5.8510e-04,\n",
       "                      5.3086e-05, 9.0004e-05, 2.6648e-04, 1.3837e-04, 6.8555e-05, 8.0035e-05,\n",
       "                      1.3404e-05, 2.4205e-04, 1.3562e-04, 3.1912e-04, 3.3946e-04, 2.2914e-04,\n",
       "                      3.4075e-05, 1.4027e-04, 1.9116e-05, 1.8187e-04, 1.0273e-04, 8.8540e-05,\n",
       "                      2.7546e-04, 1.3156e-05, 1.1230e-04, 2.5222e-05, 4.6339e-04, 2.0302e-06,\n",
       "                      9.6402e-05, 6.4496e-11, 2.8940e-04, 4.0931e-05, 5.2778e-05, 7.4911e-05,\n",
       "                      9.9109e-05, 1.1310e-04, 2.0832e-03, 2.8115e-05, 6.8936e-04, 7.1945e-04,\n",
       "                      7.8926e-05, 5.6995e-04])),\n",
       "             ('layer2.0.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.0.downsample.0.weight',\n",
       "              tensor([[[[-0.0041]],\n",
       "              \n",
       "                       [[-0.0090]],\n",
       "              \n",
       "                       [[ 0.0065]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0054]],\n",
       "              \n",
       "                       [[ 0.0092]],\n",
       "              \n",
       "                       [[ 0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0028]],\n",
       "              \n",
       "                       [[ 0.0160]],\n",
       "              \n",
       "                       [[ 0.0050]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0017]],\n",
       "              \n",
       "                       [[-0.0023]],\n",
       "              \n",
       "                       [[-0.0105]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0047]],\n",
       "              \n",
       "                       [[ 0.0049]],\n",
       "              \n",
       "                       [[ 0.0013]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0076]],\n",
       "              \n",
       "                       [[-0.0025]],\n",
       "              \n",
       "                       [[ 0.0130]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0097]],\n",
       "              \n",
       "                       [[-0.0105]],\n",
       "              \n",
       "                       [[-0.0016]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0118]],\n",
       "              \n",
       "                       [[-0.0106]],\n",
       "              \n",
       "                       [[-0.0101]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0084]],\n",
       "              \n",
       "                       [[-0.0893]],\n",
       "              \n",
       "                       [[-0.0139]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0090]],\n",
       "              \n",
       "                       [[ 0.0176]],\n",
       "              \n",
       "                       [[ 0.0229]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0013]],\n",
       "              \n",
       "                       [[-0.0016]],\n",
       "              \n",
       "                       [[-0.0048]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0020]],\n",
       "              \n",
       "                       [[-0.0044]]]])),\n",
       "             ('layer2.0.downsample.1.weight',\n",
       "              tensor([ 7.5022e-02,  1.2258e-01,  2.6509e-02,  8.2822e-02,  1.2395e-02,\n",
       "                       1.0150e-01,  1.3676e-01,  1.7207e-01,  4.0666e-02,  5.9553e-02,\n",
       "                       6.7287e-02,  1.0795e-01,  8.0415e-02,  7.8306e-02,  9.3954e-02,\n",
       "                       3.7738e-02,  1.2652e-01,  2.7729e-01,  1.4041e-01,  8.0215e-02,\n",
       "                       6.7396e-02,  2.6713e-02,  1.3157e-01,  2.1932e-02,  4.4293e-02,\n",
       "                       1.4308e-01,  5.9281e-02,  8.0221e-02,  1.0005e-01,  2.0777e-01,\n",
       "                       6.6178e-02,  3.7083e-02,  4.5793e-02,  1.3128e-01,  1.0060e-02,\n",
       "                       1.9151e-01,  6.8538e-02,  2.0442e-02,  9.1059e-02,  4.4418e-02,\n",
       "                       9.7168e-02,  3.6683e-02,  4.1319e-02,  8.0808e-02,  1.1049e-01,\n",
       "                       6.3092e-02,  4.7228e-02,  1.3403e-01,  3.1220e-02,  1.1453e-02,\n",
       "                       1.3032e-01,  1.1982e-01,  2.8131e-02,  4.1194e-02,  1.2735e-01,\n",
       "                       9.0701e-02,  5.4712e-02,  5.7095e-05,  1.1834e-01,  4.6464e-02,\n",
       "                       5.8090e-02,  9.5441e-02,  5.6537e-02, -4.1532e-03,  8.6169e-02,\n",
       "                       1.6620e-01,  1.6093e-01,  7.8943e-02,  5.5838e-02,  6.0112e-02,\n",
       "                       1.8477e-01,  5.5267e-02,  1.0907e-01,  1.5194e-01,  4.5388e-02,\n",
       "                       1.1779e-01,  7.6297e-02,  3.8158e-02,  1.1108e-01,  8.6880e-02,\n",
       "                       9.2887e-02,  3.1483e-05,  4.0241e-02,  6.5465e-02,  5.0994e-02,\n",
       "                       4.6588e-02,  1.5223e-01,  9.3460e-02,  7.1378e-02, -1.0333e-04,\n",
       "                       2.2904e-02,  5.1886e-03,  8.4987e-02,  1.1162e-01,  1.3337e-01,\n",
       "                       2.6074e-02,  1.3045e-02,  4.1835e-02,  1.5780e-01,  5.5603e-02,\n",
       "                       5.6886e-02,  9.1894e-02,  9.0626e-02,  6.1380e-02,  8.8933e-02,\n",
       "                       1.0351e-01,  7.5693e-02,  1.3838e-01,  1.6518e-01,  8.9805e-02,\n",
       "                      -1.8578e-02,  6.1657e-02,  1.8953e-01,  3.2580e-02,  1.5845e-01,\n",
       "                       4.7262e-02,  2.8897e-02,  6.7022e-02,  3.2796e-02,  3.6867e-02,\n",
       "                       1.0481e-01,  8.2812e-02,  1.4755e-02,  2.1544e-01,  3.7192e-02,\n",
       "                       8.1340e-02,  1.5070e-01,  1.7961e-01,  4.0850e-02,  7.7850e-02,\n",
       "                       4.4447e-02,  5.5745e-02,  7.6342e-02,  3.8902e-02,  8.6730e-02,\n",
       "                       4.7125e-02,  1.5936e-02,  4.0791e-04,  3.8853e-02,  7.4601e-02,\n",
       "                       1.6201e-01,  5.7950e-02,  6.4034e-02,  2.5368e-02,  7.9865e-02,\n",
       "                       1.4963e-01,  6.0670e-02,  4.5866e-02,  2.7457e-02,  2.8320e-04,\n",
       "                       6.3389e-02,  5.3697e-02,  7.9190e-02,  8.2739e-02,  4.2587e-02,\n",
       "                       7.1609e-02,  8.3843e-02,  8.7339e-02,  1.3619e-01,  1.8704e-01,\n",
       "                       1.8581e-02,  8.5250e-02,  5.1443e-02, -3.4915e-03,  8.1583e-02,\n",
       "                       9.3241e-02,  1.3999e-01,  9.3453e-02,  6.0050e-02,  1.1839e-01,\n",
       "                       2.1412e-01, -2.2086e-03,  1.8224e-01,  4.0202e-02,  6.7990e-02,\n",
       "                       1.1469e-01,  7.7117e-02,  1.2303e-01,  3.0385e-02,  4.2364e-02,\n",
       "                       5.1939e-02,  6.1299e-02,  7.7931e-02,  2.7075e-02,  1.4776e-01,\n",
       "                       2.9237e-02,  6.0321e-02,  1.1006e-01,  6.7478e-02,  1.3797e-01,\n",
       "                       7.1101e-02,  7.2989e-02,  5.5449e-02,  2.3620e-01,  4.9086e-02,\n",
       "                       9.3166e-02,  4.0373e-02,  2.9313e-02,  4.7558e-02,  8.2075e-02,\n",
       "                       1.0589e-01,  8.2998e-02,  8.4370e-02, -7.9562e-03,  4.9346e-02,\n",
       "                       3.1256e-01,  5.9551e-02,  1.3366e-01,  9.8319e-03,  2.2011e-01,\n",
       "                       4.5961e-02,  1.1935e-01,  9.7063e-02,  9.7004e-02,  1.7705e-05,\n",
       "                       4.1380e-02,  5.6415e-02,  7.5074e-02,  6.3180e-02,  4.3199e-02,\n",
       "                       3.0318e-02,  4.0981e-02,  4.3049e-02,  9.5452e-02,  1.4331e-01,\n",
       "                       9.9264e-02,  5.6248e-02,  9.8492e-02,  1.4269e-01,  6.3658e-02,\n",
       "                       1.2687e-02,  7.4158e-02,  3.9349e-02,  4.7913e-02,  1.3493e-01,\n",
       "                       6.7063e-02,  4.2642e-02,  4.1180e-02, -5.9401e-04,  5.6271e-02,\n",
       "                       6.0186e-02,  6.5202e-02,  1.0934e-01,  1.7101e-01,  1.4402e-01,\n",
       "                       1.4400e-02,  1.2647e-01,  3.1390e-02,  9.1152e-02,  9.6460e-02,\n",
       "                       3.9000e-02,  8.5199e-03,  5.2690e-02,  7.1162e-02,  6.9164e-02,\n",
       "                       1.0882e-01,  4.7871e-02,  6.1755e-02,  2.8456e-02,  1.6843e-01,\n",
       "                       5.2085e-02,  8.9743e-02,  4.6674e-02,  9.5654e-02,  5.5153e-02,\n",
       "                       7.0545e-02,  1.1844e-01,  8.1143e-02,  5.0128e-02,  2.8209e-02,\n",
       "                       9.2029e-02,  1.5835e-01,  9.4591e-02,  1.4962e-01,  1.1824e-01,\n",
       "                       3.7734e-02,  4.9246e-02,  5.4457e-02,  2.8150e-01,  2.3374e-01,\n",
       "                      -4.9352e-03,  1.5089e-01,  1.2392e-01,  4.5857e-02,  4.7362e-02,\n",
       "                       1.2076e-01,  6.3893e-02,  1.1112e-01,  1.5115e-01,  6.3062e-02,\n",
       "                       5.8732e-02,  8.5126e-02,  1.0654e-01,  4.8576e-02,  1.3088e-01,\n",
       "                       8.5103e-02,  7.5886e-02,  8.4170e-02,  5.1931e-02,  9.8218e-02,\n",
       "                       1.5768e-01,  2.0622e-01,  8.0663e-02,  1.1810e-01,  3.8329e-02,\n",
       "                       5.0667e-02,  8.1780e-02,  6.6784e-02,  6.5103e-02,  1.1912e-01,\n",
       "                       1.3242e-01,  9.4030e-02,  2.0929e-04,  8.1394e-02,  8.9403e-02,\n",
       "                       8.9671e-02,  2.4494e-02,  8.9698e-02,  3.7865e-02,  4.5971e-02,\n",
       "                       1.4680e-01,  1.0803e-01,  1.0932e-01,  1.4655e-01,  1.8384e-05,\n",
       "                       6.5250e-02,  5.7782e-02,  1.4493e-02,  1.1392e-01,  7.1893e-02,\n",
       "                       1.4497e-01,  2.4733e-02,  1.4593e-01,  1.8794e-01,  1.1122e-01,\n",
       "                       5.3728e-02,  6.1442e-02,  6.4185e-02,  7.7007e-02,  7.1991e-02,\n",
       "                      -2.9393e-03,  7.5524e-02,  1.3864e-01,  1.5901e-01,  5.4197e-02,\n",
       "                       1.3032e-01,  3.7874e-02,  6.5642e-02,  5.2605e-02,  1.1720e-01,\n",
       "                       6.0349e-02,  6.8773e-02,  1.9696e-03,  8.5673e-02,  2.4673e-02,\n",
       "                       1.0606e-01,  8.1101e-02,  6.8672e-02,  9.3374e-02,  1.2854e-03,\n",
       "                       7.3198e-02,  5.0262e-02,  4.8914e-02,  5.3774e-02,  9.7494e-02,\n",
       "                       6.1141e-02,  8.2898e-02,  1.0164e-01,  3.7476e-02,  5.3935e-02,\n",
       "                       1.1801e-02,  1.0407e-01,  6.2726e-02,  1.1777e-01,  1.0369e-01,\n",
       "                       6.9208e-02,  7.8691e-02,  3.3255e-02,  5.1409e-02, -6.2005e-04,\n",
       "                       5.6427e-02,  7.1668e-02,  6.2833e-02,  6.8509e-02,  5.2885e-02,\n",
       "                       1.2838e-01,  7.3204e-02,  8.5625e-02,  2.2473e-02,  1.0812e-01,\n",
       "                       6.3294e-02,  5.9293e-02,  1.1292e-01,  1.7890e-01,  1.3150e-01,\n",
       "                       8.0066e-02,  8.8940e-02,  8.6787e-02,  1.2428e-01,  2.3537e-02,\n",
       "                       1.4112e-01,  1.4807e-01,  7.4182e-02,  1.0075e-01,  5.6811e-02,\n",
       "                       4.4670e-02,  7.4517e-02,  1.2483e-01,  8.1040e-02,  6.4846e-02,\n",
       "                       7.7351e-02,  1.2086e-01,  6.6201e-02,  5.3008e-02,  3.2764e-02,\n",
       "                       9.6546e-02,  4.9123e-02,  5.2825e-02,  2.2344e-01,  7.3078e-02,\n",
       "                       1.4621e-01,  1.2557e-01,  9.5715e-02,  1.3438e-01,  2.6487e-02,\n",
       "                       1.1141e-01,  6.4463e-02,  8.6185e-02,  6.6329e-02,  1.3626e-01,\n",
       "                       8.0652e-02,  6.4895e-06,  1.1663e-01,  4.4694e-02,  3.5563e-02,\n",
       "                       1.7994e-01,  1.0666e-01,  1.1009e-01,  2.5139e-05,  1.3564e-01,\n",
       "                       6.1243e-02,  1.1203e-01,  1.7928e-04,  9.4665e-02,  9.3179e-02,\n",
       "                       6.2660e-02,  3.5199e-02,  1.1612e-01,  3.1350e-02,  4.7167e-02,\n",
       "                       1.1865e-01,  3.7188e-02,  3.6041e-02,  6.7989e-02,  4.6318e-02,\n",
       "                       4.0876e-02,  2.6158e-02,  1.5629e-01,  1.5871e-02,  4.3556e-02,\n",
       "                      -3.7622e-04,  2.0683e-01,  1.4745e-01,  5.7704e-02,  7.6108e-02,\n",
       "                       6.5376e-02,  3.2064e-02,  9.6837e-02,  2.7335e-02,  5.6983e-02,\n",
       "                       4.0582e-02,  1.8955e-04,  6.7276e-02,  9.6493e-02,  1.0581e-01,\n",
       "                       4.4731e-02,  5.1867e-02,  8.5751e-02,  1.4855e-01,  1.3320e-01,\n",
       "                       4.4589e-02,  8.5964e-02,  6.7030e-02,  9.9294e-02,  1.4107e-01,\n",
       "                       6.3863e-02,  8.5312e-02,  5.3569e-02,  6.5329e-02,  2.1399e-01,\n",
       "                       7.2227e-02,  4.2853e-02,  9.9685e-02,  3.7156e-02,  5.8505e-02,\n",
       "                       8.8363e-02,  1.3121e-01,  5.6970e-02,  9.8364e-02,  3.9492e-05,\n",
       "                       1.1114e-01,  6.1595e-02,  9.3807e-02,  2.1904e-01,  4.5540e-02,\n",
       "                       8.4967e-02,  5.5056e-02,  9.8746e-02,  1.0890e-01,  6.4992e-02,\n",
       "                       2.0848e-01,  3.1609e-02])),\n",
       "             ('layer2.0.downsample.1.bias',\n",
       "              tensor([ 3.3824e-02, -2.6047e-03, -1.6732e-02,  5.9837e-04, -1.5353e-02,\n",
       "                       3.9819e-03, -7.1864e-04, -4.5769e-02,  4.0193e-03,  6.7994e-03,\n",
       "                       9.7809e-03, -1.7096e-02,  3.1275e-02,  1.8799e-02, -6.0214e-03,\n",
       "                       1.7165e-03, -8.9159e-03,  3.2090e-02,  3.2421e-02, -5.3906e-03,\n",
       "                       1.5627e-02, -1.0766e-02,  7.0678e-03, -4.5300e-03,  1.4072e-02,\n",
       "                       4.9520e-03, -5.4931e-03, -9.6218e-03, -2.4590e-03,  3.2873e-03,\n",
       "                       7.2235e-03,  7.9440e-03, -1.9372e-02,  1.1088e-02, -6.3783e-03,\n",
       "                      -1.9668e-02,  2.0794e-03, -1.7835e-03, -4.5557e-04,  1.1219e-02,\n",
       "                      -3.1214e-03, -3.8131e-03,  6.9265e-03,  5.8402e-03, -2.4209e-02,\n",
       "                       1.6232e-02, -4.8928e-03,  2.4059e-02,  9.5472e-03,  6.7785e-04,\n",
       "                       1.9784e-02,  3.2664e-02, -1.0632e-03,  6.3059e-04, -9.5212e-04,\n",
       "                      -4.7543e-03, -9.0653e-03, -1.9038e-04, -2.0962e-02,  7.2396e-03,\n",
       "                      -6.7345e-03,  1.2864e-03,  2.1478e-03,  2.5562e-02, -3.0452e-02,\n",
       "                       3.3611e-02,  1.6851e-03, -2.2040e-02, -3.5913e-02, -4.1229e-03,\n",
       "                       8.4935e-03, -7.2436e-03, -3.3623e-02,  3.2561e-02,  1.5456e-02,\n",
       "                      -1.4539e-02, -6.3838e-03, -5.8704e-03,  8.1845e-03,  9.7343e-03,\n",
       "                      -1.5602e-02, -1.1606e-04,  3.2095e-03, -4.7290e-03, -2.0737e-02,\n",
       "                      -4.8020e-03, -3.6224e-02, -7.5584e-03, -3.9103e-03, -9.1736e-04,\n",
       "                       6.0094e-03,  3.3372e-03, -1.2310e-02,  1.6109e-02, -1.2063e-02,\n",
       "                      -3.3791e-03, -5.3916e-04, -1.5179e-03,  1.7888e-04,  1.5313e-03,\n",
       "                       4.5531e-03,  9.4051e-03,  3.6411e-02, -1.2525e-03,  6.0353e-03,\n",
       "                      -4.9961e-03,  6.9729e-03, -2.5784e-02, -2.1228e-02, -4.5962e-03,\n",
       "                      -1.5203e-02, -1.0874e-02,  2.3093e-03,  7.4508e-03,  2.8742e-03,\n",
       "                       3.4460e-03,  2.9879e-03, -3.9550e-03,  1.3364e-02,  5.7887e-04,\n",
       "                       3.1321e-03, -3.3281e-02, -1.3214e-02, -1.6437e-02, -2.7990e-02,\n",
       "                       4.7132e-03, -3.6653e-02, -7.9457e-03, -5.7680e-03, -7.4998e-03,\n",
       "                       5.5318e-03, -4.9935e-04, -6.2480e-03, -9.8127e-04,  1.1565e-02,\n",
       "                      -1.6484e-03, -2.1851e-02, -6.1814e-04,  8.1516e-03, -2.3882e-02,\n",
       "                      -2.0623e-02,  6.9460e-03, -1.6842e-02,  1.5465e-02,  1.8917e-02,\n",
       "                       3.5664e-02,  7.2671e-03,  7.6214e-04, -8.7513e-03, -6.2562e-04,\n",
       "                      -1.6026e-02, -2.9168e-03,  1.9663e-02, -1.2904e-03,  7.5160e-03,\n",
       "                      -9.7239e-05,  3.3482e-03,  1.2439e-02,  2.4790e-02, -1.8102e-02,\n",
       "                      -3.7170e-06,  2.4783e-04, -8.8856e-03, -1.4613e-02,  2.1967e-02,\n",
       "                      -2.2253e-02,  2.1581e-02, -1.2285e-02,  1.3829e-02,  2.4286e-02,\n",
       "                       7.3525e-03, -8.1414e-03, -4.3058e-02,  1.0515e-02, -2.2556e-02,\n",
       "                      -1.4690e-02, -2.2190e-02,  7.8173e-03,  8.9160e-03, -1.7626e-03,\n",
       "                       4.0264e-03,  5.6762e-03, -1.0073e-03,  1.6727e-03, -2.9576e-02,\n",
       "                      -5.1109e-03, -1.0507e-02, -5.4845e-03,  9.1518e-03,  6.1695e-03,\n",
       "                       6.6858e-03, -7.5155e-03,  7.5894e-03, -9.6678e-03,  6.4868e-05,\n",
       "                       4.8990e-04, -1.6590e-03, -1.0613e-02,  9.3086e-03,  1.0908e-02,\n",
       "                      -8.9552e-04, -5.1680e-04,  7.1046e-03, -1.2152e-02, -4.4859e-03,\n",
       "                      -6.7060e-02,  2.7033e-02, -6.8964e-03,  4.4629e-02,  3.7824e-02,\n",
       "                      -1.0136e-03,  2.3820e-02, -1.9351e-03,  9.4677e-03, -7.8759e-05,\n",
       "                      -1.4999e-02,  3.1733e-02, -1.8734e-02, -1.0327e-02,  1.1754e-02,\n",
       "                       4.7553e-03, -2.4779e-03,  1.2824e-02, -3.0894e-02, -5.0283e-03,\n",
       "                       2.3716e-02, -8.4647e-03,  1.3639e-02, -2.3090e-03,  2.8945e-03,\n",
       "                       2.6616e-03, -1.0193e-02,  2.3884e-03, -7.0685e-03, -1.2962e-02,\n",
       "                      -3.4365e-04,  1.0568e-02,  3.8828e-03, -1.6666e-03,  2.6652e-03,\n",
       "                       5.0333e-02,  9.1508e-03, -6.8018e-03, -1.2557e-02, -3.0316e-02,\n",
       "                      -7.6796e-03, -1.5013e-02,  5.6976e-03, -3.7744e-02,  4.6106e-02,\n",
       "                       2.1530e-03,  3.6769e-04,  2.2205e-02, -2.7443e-02, -1.3026e-02,\n",
       "                       8.6303e-03, -2.2596e-03,  1.8931e-03,  2.7702e-03,  2.6900e-02,\n",
       "                       1.0671e-03,  1.6107e-02,  1.9970e-02, -4.4597e-03, -3.9116e-03,\n",
       "                       4.9573e-03,  4.0603e-03, -4.8045e-03, -9.1606e-03,  3.2684e-03,\n",
       "                       9.3202e-03, -1.2765e-02,  1.6661e-03,  3.7924e-02,  5.1221e-02,\n",
       "                      -1.3153e-02, -1.7259e-02, -8.5915e-04, -3.5864e-02, -4.4496e-02,\n",
       "                      -1.0373e-02,  2.4461e-02, -1.4089e-04, -8.1029e-03, -8.3374e-03,\n",
       "                       1.5562e-02,  3.0119e-02, -2.7318e-03,  7.1437e-03, -8.2767e-03,\n",
       "                      -1.6938e-03, -6.8909e-03,  5.5183e-03, -2.0665e-03,  1.8811e-02,\n",
       "                      -3.0584e-03,  5.7128e-03, -1.0369e-02,  7.7737e-03,  1.9930e-02,\n",
       "                      -4.7280e-03, -3.4843e-02, -1.6369e-02, -4.4654e-02, -4.8631e-03,\n",
       "                       1.0984e-02, -1.5505e-02, -5.2024e-03,  1.0867e-02,  5.5555e-03,\n",
       "                       2.8317e-03, -3.8683e-03, -3.8877e-04,  2.5451e-03, -4.5148e-03,\n",
       "                       9.2290e-03,  8.0716e-03, -5.6292e-03, -2.0006e-03, -1.2698e-02,\n",
       "                      -1.0742e-02, -1.7009e-02, -3.8558e-04, -1.3002e-02, -7.1345e-05,\n",
       "                       2.1222e-02, -5.8535e-03, -5.3322e-03, -1.5611e-02,  2.4683e-02,\n",
       "                       4.0145e-02,  2.2666e-04, -3.7282e-03, -2.4271e-02, -6.6814e-03,\n",
       "                       1.1668e-02,  3.4512e-02,  1.5859e-03,  1.2236e-02,  7.6477e-05,\n",
       "                       1.6798e-02,  9.9264e-03, -1.1853e-02, -5.7524e-03,  1.3634e-02,\n",
       "                      -1.6159e-02,  3.1174e-03,  6.2337e-03, -8.1357e-03,  1.5050e-02,\n",
       "                       2.4448e-03,  2.4860e-02,  1.1793e-02, -2.5192e-03,  1.1124e-02,\n",
       "                       6.9928e-03, -1.6517e-02, -6.2937e-03, -2.3632e-02, -4.2067e-03,\n",
       "                       7.9993e-03, -3.0144e-03, -1.9788e-03,  7.1632e-02,  4.2660e-02,\n",
       "                       1.3317e-02,  4.3381e-02, -1.1057e-03,  7.0495e-03,  7.0446e-03,\n",
       "                       6.4138e-03,  7.6731e-03, -4.5546e-03, -9.3834e-03,  3.4237e-02,\n",
       "                       2.2232e-02,  6.0292e-03, -4.8586e-03,  2.0352e-03, -6.5137e-03,\n",
       "                       1.1235e-02,  1.2232e-02,  9.3775e-03, -8.7847e-03, -2.3955e-02,\n",
       "                      -3.5733e-03,  1.3138e-02, -1.3171e-02, -8.9317e-03, -1.7957e-02,\n",
       "                      -8.1642e-03,  4.5631e-03, -1.6995e-03,  2.6420e-02,  3.4412e-02,\n",
       "                       8.3558e-04, -1.5352e-02,  2.8168e-03, -3.3684e-02,  6.6818e-03,\n",
       "                      -7.2084e-04, -1.1951e-02,  1.7742e-02, -1.4083e-02, -7.8707e-03,\n",
       "                      -1.5631e-02, -1.7903e-02,  1.0908e-02, -8.9580e-03, -4.6454e-04,\n",
       "                      -1.3548e-02, -1.4645e-02, -3.9785e-03, -1.4989e-02, -2.3067e-02,\n",
       "                       4.2784e-03, -1.1986e-02, -1.7303e-02, -3.4726e-02,  1.4300e-03,\n",
       "                       8.4295e-03, -2.7121e-02,  8.6106e-03, -1.9675e-02,  1.2190e-02,\n",
       "                      -7.4277e-03,  5.0227e-03, -2.4382e-02, -2.2719e-02, -3.1571e-02,\n",
       "                       1.4546e-02, -3.9153e-05, -8.0821e-03,  2.5303e-03, -1.2269e-03,\n",
       "                       2.0729e-02,  4.2484e-03, -1.3004e-02, -8.9005e-05,  9.7262e-03,\n",
       "                       1.1967e-02, -5.7223e-03, -4.1763e-04,  2.9492e-02, -1.9246e-02,\n",
       "                       7.7236e-03, -8.7909e-03,  5.6738e-03, -9.4157e-03, -6.5971e-03,\n",
       "                      -1.5731e-02,  1.3793e-02,  1.5450e-02,  3.2173e-03, -2.2713e-02,\n",
       "                      -3.3185e-03,  5.0734e-03, -3.1155e-02,  1.9183e-03,  4.9972e-02,\n",
       "                       5.2967e-02,  4.5080e-02, -6.5600e-03,  1.1936e-03,  6.9124e-03,\n",
       "                      -2.9263e-03,  7.1675e-03,  1.2202e-02, -6.2074e-03,  5.3216e-02,\n",
       "                      -2.0861e-03, -2.6448e-04,  6.0077e-03, -1.5086e-02, -6.3085e-03,\n",
       "                       1.5466e-03, -2.8052e-02,  5.7890e-03, -2.0732e-02,  3.2765e-02,\n",
       "                       3.2801e-03, -2.9662e-03, -1.0770e-02,  1.5262e-02,  7.3545e-03,\n",
       "                       1.2867e-03,  1.2210e-02,  3.3709e-03, -3.3705e-03, -2.6569e-02,\n",
       "                      -9.6980e-03,  4.5029e-03, -1.8446e-02,  6.7877e-03,  3.3936e-03,\n",
       "                      -3.8178e-03, -6.2146e-03, -6.3080e-03, -3.7384e-03, -5.4363e-05,\n",
       "                       4.3445e-03,  1.0617e-02, -2.0194e-02, -7.6743e-02,  1.6831e-03,\n",
       "                      -2.3786e-02,  7.5207e-02,  1.7562e-02, -7.2932e-03,  1.6118e-02,\n",
       "                      -4.9214e-02, -1.4645e-02])),\n",
       "             ('layer2.0.downsample.1.running_mean',\n",
       "              tensor([-2.3217e-03, -4.9534e-02,  2.3908e-02, -1.0399e-02, -3.3210e-02,\n",
       "                       2.2835e-02, -2.3348e-03, -5.9942e-02, -7.8947e-03, -2.0326e-02,\n",
       "                       1.2075e-02,  2.8068e-02, -5.8092e-02, -1.7718e-03,  1.8765e-02,\n",
       "                       1.5247e-02,  1.6072e-02,  1.1356e-01, -1.0233e-02,  6.6174e-02,\n",
       "                      -1.6162e-02, -4.7532e-02, -5.7027e-02,  7.8064e-03, -3.8980e-02,\n",
       "                      -5.1084e-02,  9.0536e-03,  3.5240e-02, -1.0120e-02, -1.1930e-01,\n",
       "                      -3.8748e-02,  4.5291e-02,  2.9943e-02, -4.2920e-03,  1.2290e-02,\n",
       "                      -7.7163e-02,  1.2283e-02, -2.7887e-03,  5.4036e-03, -6.4538e-03,\n",
       "                      -4.6899e-03,  2.7091e-03, -1.9598e-02,  8.6310e-02, -3.7231e-02,\n",
       "                      -2.2722e-02, -4.4949e-03,  3.4616e-04,  3.9777e-03,  6.4554e-03,\n",
       "                      -2.0886e-02, -5.2595e-03,  1.1465e-02,  4.5230e-02, -6.3647e-02,\n",
       "                       2.6392e-02,  1.7013e-02, -6.5506e-05,  2.6912e-03, -1.4924e-02,\n",
       "                       3.4129e-04, -3.4833e-02,  1.2141e-02, -2.8564e-03, -2.0310e-02,\n",
       "                      -2.1632e-02, -1.1162e-01,  9.5976e-03, -4.1978e-02, -2.9076e-02,\n",
       "                      -1.5547e-01,  2.4598e-03,  4.9311e-02, -9.6396e-03, -2.4693e-02,\n",
       "                      -3.2592e-02,  2.7618e-02, -4.4190e-03, -7.5354e-02, -7.1280e-02,\n",
       "                       3.6681e-02, -6.1553e-05,  7.2380e-03, -7.1654e-02, -5.9525e-02,\n",
       "                       7.9462e-03, -6.6541e-02, -1.0940e-02,  4.1387e-02,  7.0546e-05,\n",
       "                      -6.5525e-03, -8.1053e-03, -4.7232e-02, -3.5131e-02, -5.5347e-02,\n",
       "                      -2.8399e-02, -6.2070e-04, -1.2963e-03, -4.4411e-02, -1.7489e-02,\n",
       "                       3.6020e-03,  2.3892e-02,  7.6896e-03,  1.2033e-02,  1.2957e-02,\n",
       "                      -3.9518e-02,  6.0237e-03, -2.3442e-02, -6.1961e-02, -3.7751e-02,\n",
       "                       9.7179e-04,  2.4668e-02, -3.2248e-02,  1.4033e-02, -1.1669e-01,\n",
       "                       1.7910e-02,  5.1845e-04, -6.1897e-03,  5.5394e-03,  1.5942e-02,\n",
       "                      -9.4494e-03,  8.7423e-03, -5.2063e-03, -6.7941e-02,  1.2984e-02,\n",
       "                      -2.9976e-02,  2.2071e-02, -4.3812e-02,  2.2899e-03,  2.8376e-02,\n",
       "                      -1.6678e-03,  2.6899e-02,  1.0658e-02,  1.4478e-02, -5.9461e-03,\n",
       "                      -4.9292e-03, -1.3738e-02, -1.4857e-04,  1.2064e-02,  4.3836e-02,\n",
       "                      -2.0912e-02, -2.7694e-02,  1.7211e-02, -6.5215e-03, -6.2284e-02,\n",
       "                      -8.7351e-02,  2.3361e-02,  9.3345e-03, -9.5923e-03,  5.4066e-05,\n",
       "                       1.5460e-02, -7.0982e-03, -1.0338e-01, -2.8543e-03,  1.7632e-02,\n",
       "                      -3.8962e-02, -7.3087e-03, -2.9071e-02, -2.3987e-02,  9.7438e-02,\n",
       "                      -1.2082e-03, -1.9327e-02, -1.9877e-02, -1.5424e-03, -2.7119e-02,\n",
       "                      -5.1016e-02, -2.8489e-02,  2.7876e-02, -4.7558e-04, -8.1606e-02,\n",
       "                      -1.1957e-01, -3.5798e-04, -5.3683e-02, -4.4230e-02,  4.0396e-02,\n",
       "                       1.5220e-02, -9.0379e-03, -5.3749e-02,  5.3973e-03, -6.9502e-03,\n",
       "                       1.6938e-02,  5.2470e-03, -5.4364e-02,  1.5455e-02,  2.6009e-02,\n",
       "                      -2.9099e-02, -3.3895e-02,  5.9718e-02, -4.9928e-03, -4.7886e-02,\n",
       "                       8.8614e-03, -4.2501e-02,  2.9847e-02, -5.0103e-02, -1.6979e-02,\n",
       "                      -2.3852e-02,  2.1823e-02, -3.2952e-02, -3.7760e-02, -4.2248e-02,\n",
       "                       2.3480e-02, -3.1924e-02, -2.3379e-02,  1.0739e-02,  4.5378e-05,\n",
       "                      -1.1704e-02, -1.8748e-03,  1.5023e-02,  9.3734e-03,  5.9272e-02,\n",
       "                      -1.1808e-03, -5.9117e-03, -8.8750e-03,  6.6441e-02,  2.8592e-05,\n",
       "                       9.5364e-03,  4.1568e-02,  2.0034e-02,  1.7539e-02,  1.9602e-02,\n",
       "                       1.2176e-02, -1.1915e-02, -3.5208e-03,  1.5762e-03, -8.0413e-02,\n",
       "                      -2.5988e-02, -4.3153e-03,  2.1640e-02,  1.9409e-02, -1.0603e-02,\n",
       "                       3.5360e-03,  1.6833e-02, -1.1465e-02,  3.4887e-03,  3.1843e-02,\n",
       "                      -3.5781e-02,  1.4132e-02,  1.7736e-02,  5.5111e-04, -1.4960e-03,\n",
       "                      -4.2610e-02, -4.3756e-02, -7.2848e-03, -6.6250e-02, -5.5165e-02,\n",
       "                      -2.5407e-02, -1.2398e-03,  2.5644e-02,  3.9305e-02, -5.1592e-02,\n",
       "                       2.7728e-03,  5.5358e-03,  6.9644e-06,  8.8642e-03,  3.6940e-03,\n",
       "                      -5.5948e-02,  1.0745e-02, -1.2118e-02, -2.2938e-02, -3.6889e-02,\n",
       "                       2.6620e-02, -2.4957e-02, -1.2656e-02, -7.8978e-02,  7.5777e-03,\n",
       "                      -8.5510e-02, -5.4709e-02,  1.6660e-02,  1.8473e-02, -1.4166e-02,\n",
       "                       1.5071e-02,  7.7140e-03, -3.5946e-02, -1.5676e-02, -3.3764e-02,\n",
       "                      -2.7999e-02,  3.1257e-02,  2.5960e-02, -2.1754e-01,  9.2777e-02,\n",
       "                      -1.2781e-02, -6.4885e-02, -2.2661e-02,  7.4721e-03,  2.7618e-03,\n",
       "                      -6.0702e-02,  2.1670e-02, -2.3721e-02, -5.7429e-02,  1.4457e-02,\n",
       "                       1.6891e-02,  1.9753e-02, -5.4835e-02, -1.0861e-02, -1.3449e-02,\n",
       "                      -3.3753e-02,  4.5131e-03, -6.3979e-03,  1.0759e-02, -3.8343e-02,\n",
       "                      -8.2085e-03,  7.9863e-02,  3.5312e-02,  5.8503e-02,  3.1255e-03,\n",
       "                      -6.5257e-03, -2.0567e-02,  2.6953e-02, -3.6554e-02,  1.0300e-02,\n",
       "                       4.1912e-02, -7.6165e-03, -3.2050e-04, -2.5089e-02, -5.0026e-02,\n",
       "                       1.8432e-02, -1.7425e-02, -5.3930e-02,  1.7194e-03,  1.0291e-03,\n",
       "                      -3.0990e-02,  1.0301e-02, -1.1035e-02, -2.6285e-02, -1.1083e-05,\n",
       "                      -6.7311e-03,  8.9736e-03, -5.6593e-03,  2.1305e-02, -7.6300e-02,\n",
       "                      -4.1795e-02, -8.9080e-03, -7.7584e-02,  1.6488e-01,  4.5495e-02,\n",
       "                       1.4444e-02,  4.1538e-02,  5.3515e-03,  8.9021e-03,  1.7419e-02,\n",
       "                      -5.6587e-03,  3.6348e-02,  1.1209e-02,  2.0181e-03,  2.8121e-02,\n",
       "                       1.1077e-02,  9.2535e-03,  1.4418e-02, -5.5103e-03,  4.7806e-03,\n",
       "                       7.7958e-03, -2.9602e-02, -7.2654e-05,  3.2505e-02,  6.7849e-03,\n",
       "                      -2.8470e-02,  2.7650e-02,  6.3305e-03,  5.2185e-03, -1.2694e-03,\n",
       "                       1.9844e-02, -1.5662e-02,  3.4860e-02, -8.0124e-02, -7.0202e-02,\n",
       "                      -7.6210e-02, -5.2877e-03,  1.2964e-02,  1.2274e-03, -5.9246e-02,\n",
       "                       8.1886e-03,  1.1281e-01,  1.3992e-02,  3.9296e-02, -8.6331e-02,\n",
       "                      -5.6641e-02,  2.7617e-03, -6.0006e-03, -4.1117e-02, -1.6506e-02,\n",
       "                      -1.4729e-02, -7.8465e-03, -3.2896e-02, -4.1922e-03, -2.0431e-03,\n",
       "                       1.4260e-02, -6.4062e-02,  6.2995e-02,  4.4965e-03, -3.2167e-02,\n",
       "                       6.5120e-03,  2.5666e-02, -3.5614e-03, -6.9340e-02, -8.6495e-02,\n",
       "                      -1.5573e-02, -8.5186e-04, -5.1861e-02,  5.7491e-03,  9.6048e-03,\n",
       "                      -6.2913e-02,  5.5742e-02, -2.2589e-02, -4.6500e-05, -1.9263e-02,\n",
       "                       1.0611e-03,  2.6428e-02,  4.2127e-02,  3.2270e-02,  3.2288e-02,\n",
       "                       7.8618e-03,  1.1062e-01, -2.1989e-02, -3.0271e-02, -1.5240e-02,\n",
       "                      -3.9111e-02,  3.8504e-02,  1.6772e-02,  4.8027e-02, -1.6593e-02,\n",
       "                      -3.2375e-02,  2.9770e-02,  8.3467e-03, -1.1966e-02, -1.1316e-02,\n",
       "                       5.7775e-02,  2.5820e-03,  5.0346e-02,  8.9694e-03,  4.4443e-02,\n",
       "                      -2.0323e-02,  6.7316e-06, -1.6597e-02,  1.1636e-02,  7.6102e-04,\n",
       "                      -3.5044e-02,  2.5099e-02,  4.6967e-02, -3.1782e-05, -4.8033e-02,\n",
       "                      -1.0473e-02, -9.5784e-02, -3.1429e-05,  6.2728e-03, -9.2787e-03,\n",
       "                       3.6646e-03,  1.7668e-02, -2.2567e-02, -1.1099e-04, -5.3611e-03,\n",
       "                       9.0326e-02,  2.0375e-03,  6.1693e-03, -2.2657e-02,  6.4575e-03,\n",
       "                      -3.4839e-02,  6.0353e-03,  2.5692e-02,  8.5889e-04, -2.6564e-02,\n",
       "                      -6.0192e-03, -9.7421e-02,  3.5851e-02,  1.2012e-04,  6.8320e-03,\n",
       "                       1.8253e-03, -8.1294e-04,  7.3479e-03,  1.7185e-02, -8.4836e-02,\n",
       "                      -4.5490e-03, -3.1275e-04, -2.8481e-02, -1.8438e-02,  3.1326e-02,\n",
       "                       1.6798e-02,  2.1838e-02,  8.4741e-03,  5.9444e-02, -1.7255e-02,\n",
       "                       2.7625e-03,  2.7370e-02, -2.9042e-03, -1.2716e-02, -5.0622e-02,\n",
       "                      -2.4809e-02, -3.1609e-02,  3.1072e-02,  2.7949e-02,  1.0573e-03,\n",
       "                       2.0872e-02,  1.2792e-02, -2.3208e-02,  1.9396e-02,  5.2137e-03,\n",
       "                       1.7007e-02, -8.8370e-02,  7.2843e-04, -5.5702e-03, -3.6621e-05,\n",
       "                      -6.0517e-03,  6.5093e-03, -2.1651e-03,  3.0864e-02,  1.2896e-03,\n",
       "                      -1.3214e-02, -4.0626e-02, -1.1984e-02, -6.0834e-02, -4.1094e-02,\n",
       "                      -2.2546e-02, -7.3608e-03])),\n",
       "             ('layer2.0.downsample.1.running_var',\n",
       "              tensor([2.6112e-03, 1.5075e-03, 9.3819e-04, 1.1995e-03, 8.6754e-04, 2.3112e-03,\n",
       "                      3.8077e-03, 4.9457e-03, 3.0467e-04, 5.3744e-04, 9.0623e-04, 1.8888e-03,\n",
       "                      3.5089e-03, 1.5662e-03, 2.6818e-03, 4.8274e-04, 2.9850e-03, 1.4395e-02,\n",
       "                      3.8153e-03, 1.2636e-03, 2.2826e-03, 8.8295e-04, 2.9263e-03, 1.8732e-04,\n",
       "                      5.9160e-04, 3.1528e-03, 5.9188e-04, 1.2587e-03, 1.4767e-03, 7.0517e-03,\n",
       "                      1.1113e-03, 4.6778e-04, 4.3891e-04, 4.5580e-03, 2.1453e-04, 3.7211e-03,\n",
       "                      1.2209e-03, 1.1129e-04, 1.3926e-03, 3.4819e-04, 1.3402e-03, 2.3098e-04,\n",
       "                      6.3430e-04, 1.3118e-03, 1.7125e-03, 1.4436e-03, 1.3222e-03, 3.1239e-03,\n",
       "                      6.0127e-04, 3.1514e-04, 3.6116e-03, 7.0341e-03, 1.6598e-04, 5.0366e-04,\n",
       "                      1.7187e-03, 9.2982e-04, 3.9833e-04, 5.9505e-08, 1.0734e-03, 2.2376e-03,\n",
       "                      4.5431e-04, 1.6945e-03, 4.2632e-03, 2.6049e-04, 1.1734e-03, 5.0069e-03,\n",
       "                      4.5277e-03, 9.6316e-04, 1.0256e-03, 3.3028e-03, 7.2388e-03, 3.2899e-04,\n",
       "                      9.9342e-04, 5.5330e-03, 1.9763e-03, 3.1449e-03, 2.1752e-03, 5.0915e-04,\n",
       "                      2.6314e-03, 1.4332e-03, 1.3167e-03, 1.2313e-08, 2.6287e-04, 1.0928e-03,\n",
       "                      9.9440e-04, 6.4397e-04, 4.2560e-03, 1.6842e-03, 8.9104e-04, 2.2818e-07,\n",
       "                      5.7175e-04, 9.0372e-05, 1.2113e-03, 2.5407e-03, 2.9839e-03, 7.5836e-04,\n",
       "                      9.3276e-05, 4.3400e-04, 2.0007e-03, 6.3840e-04, 6.0818e-04, 1.7793e-03,\n",
       "                      5.1092e-03, 7.1040e-04, 3.1889e-03, 1.3668e-03, 1.9037e-03, 4.5138e-03,\n",
       "                      3.4177e-03, 1.4775e-03, 5.0871e-04, 5.8521e-04, 5.2261e-03, 3.3043e-04,\n",
       "                      4.3120e-03, 7.3003e-04, 2.8122e-04, 7.4566e-04, 4.0266e-04, 3.0416e-04,\n",
       "                      2.8805e-03, 1.0731e-03, 3.9340e-04, 4.0275e-03, 7.2719e-04, 1.6150e-03,\n",
       "                      3.1848e-03, 5.4150e-03, 8.9762e-04, 1.2878e-03, 4.2260e-04, 4.7142e-04,\n",
       "                      1.2735e-03, 6.3299e-04, 1.0476e-03, 3.3171e-04, 3.0062e-04, 4.3336e-07,\n",
       "                      2.3424e-04, 1.8395e-03, 3.6805e-03, 7.7883e-04, 9.2980e-04, 3.6938e-04,\n",
       "                      2.3785e-03, 4.1046e-03, 6.0711e-04, 9.6882e-04, 8.3185e-04, 3.3614e-07,\n",
       "                      4.4689e-04, 6.2441e-04, 2.8182e-03, 2.0210e-03, 6.5843e-04, 2.0716e-03,\n",
       "                      1.1517e-03, 1.9453e-03, 3.0653e-03, 3.8932e-03, 1.9099e-04, 1.1993e-03,\n",
       "                      8.4871e-04, 5.3684e-04, 1.7305e-03, 1.4766e-03, 3.8574e-03, 1.9671e-03,\n",
       "                      7.8455e-04, 1.6722e-03, 7.2904e-03, 2.1627e-05, 3.7545e-03, 8.0490e-04,\n",
       "                      2.2857e-03, 2.0202e-03, 1.2785e-03, 1.2535e-03, 4.9792e-04, 3.3633e-04,\n",
       "                      7.1709e-04, 1.4347e-03, 1.9651e-03, 2.0276e-04, 2.7002e-03, 2.5099e-04,\n",
       "                      1.2414e-03, 3.4694e-03, 8.4429e-04, 1.6372e-03, 2.2754e-03, 1.7135e-03,\n",
       "                      1.1111e-03, 7.8435e-03, 4.8170e-04, 1.5598e-03, 4.7314e-04, 4.1884e-04,\n",
       "                      1.0839e-03, 2.2222e-03, 2.8454e-03, 1.6668e-03, 1.3527e-03, 4.9180e-04,\n",
       "                      5.9897e-04, 6.6629e-03, 1.5395e-03, 2.8802e-03, 1.5461e-03, 8.4056e-03,\n",
       "                      2.8959e-04, 5.9888e-03, 1.7674e-03, 1.3867e-03, 4.1910e-09, 1.3688e-03,\n",
       "                      1.4275e-03, 1.3834e-03, 7.7227e-04, 3.9711e-04, 2.9921e-04, 4.7919e-04,\n",
       "                      4.7696e-04, 2.8995e-03, 2.2737e-03, 3.1859e-03, 5.2713e-04, 2.8315e-03,\n",
       "                      4.0539e-03, 8.2823e-04, 8.6858e-05, 1.0320e-03, 6.5398e-04, 5.6828e-04,\n",
       "                      2.5798e-03, 9.9705e-04, 6.1629e-04, 2.5045e-04, 1.6708e-06, 1.4208e-03,\n",
       "                      6.1306e-03, 1.5624e-03, 6.6528e-03, 3.4117e-03, 3.7689e-03, 3.8931e-04,\n",
       "                      2.5807e-03, 3.8690e-04, 1.0957e-03, 4.6592e-03, 3.7324e-04, 5.4068e-06,\n",
       "                      1.1976e-03, 1.7791e-03, 1.0970e-03, 1.9267e-03, 5.0527e-04, 9.9864e-04,\n",
       "                      9.3914e-04, 3.9745e-03, 7.7841e-04, 1.4282e-03, 1.4771e-03, 2.5530e-03,\n",
       "                      1.2099e-03, 2.0767e-03, 1.6564e-03, 7.2491e-04, 1.7402e-03, 5.6456e-04,\n",
       "                      2.4504e-03, 2.7328e-03, 1.2766e-03, 5.2072e-03, 7.3589e-03, 9.2568e-04,\n",
       "                      1.4278e-03, 8.0485e-04, 1.6926e-02, 2.7451e-03, 6.7108e-05, 2.5254e-03,\n",
       "                      1.7095e-03, 4.3312e-04, 7.5362e-04, 2.5055e-03, 2.4191e-03, 1.4932e-03,\n",
       "                      3.1780e-03, 1.7391e-03, 1.2736e-03, 6.4875e-04, 2.5268e-03, 9.0887e-04,\n",
       "                      4.2851e-03, 2.0037e-03, 1.4944e-03, 5.2239e-04, 7.3083e-04, 2.6502e-03,\n",
       "                      4.6925e-03, 3.5394e-03, 1.0370e-03, 3.0690e-03, 3.6835e-04, 1.2538e-03,\n",
       "                      1.0148e-03, 1.2800e-03, 2.1414e-03, 3.6678e-03, 2.4714e-03, 2.2272e-03,\n",
       "                      2.1696e-07, 1.0951e-03, 2.1174e-03, 1.2271e-03, 4.9484e-04, 3.2149e-03,\n",
       "                      2.6269e-04, 9.1346e-04, 2.0324e-03, 2.5724e-03, 1.3438e-03, 3.4222e-03,\n",
       "                      2.9181e-09, 1.6862e-03, 5.5746e-04, 1.2378e-04, 3.0416e-03, 3.1957e-03,\n",
       "                      3.0641e-03, 1.5808e-04, 2.3460e-03, 3.7524e-03, 2.2192e-03, 1.0455e-03,\n",
       "                      5.4901e-03, 4.9186e-03, 2.2178e-03, 1.1798e-03, 4.6931e-04, 1.5640e-03,\n",
       "                      3.1703e-03, 2.7155e-03, 9.6814e-04, 2.2486e-03, 2.3520e-04, 5.9222e-04,\n",
       "                      7.1841e-04, 3.2005e-03, 9.0007e-04, 1.4047e-03, 8.7689e-05, 1.0869e-03,\n",
       "                      3.4446e-04, 3.4397e-03, 7.1880e-04, 7.7912e-04, 6.1561e-04, 1.2116e-05,\n",
       "                      7.9489e-04, 2.0328e-04, 4.1929e-04, 7.6778e-03, 4.7110e-03, 1.4423e-03,\n",
       "                      5.5435e-03, 2.0652e-03, 6.3924e-04, 1.8240e-03, 5.3085e-04, 1.8259e-03,\n",
       "                      1.6766e-03, 1.6728e-03, 2.7598e-03, 2.0287e-03, 9.3342e-04, 2.4228e-04,\n",
       "                      1.0857e-03, 5.2967e-04, 1.5550e-03, 7.2423e-04, 2.3651e-03, 6.9559e-04,\n",
       "                      1.7260e-03, 2.0422e-03, 1.2618e-03, 2.6021e-03, 4.7594e-04, 1.9245e-03,\n",
       "                      5.2578e-04, 8.3450e-04, 2.8654e-03, 4.3430e-03, 4.4001e-03, 1.6254e-03,\n",
       "                      1.2551e-03, 1.4534e-03, 4.0546e-03, 2.3389e-04, 2.2949e-03, 2.5934e-03,\n",
       "                      4.0736e-03, 8.4645e-04, 1.1612e-03, 5.6488e-04, 1.3664e-03, 5.1265e-03,\n",
       "                      5.9797e-04, 3.8309e-04, 7.1193e-04, 2.0744e-03, 5.6924e-03, 1.2670e-03,\n",
       "                      5.3820e-04, 1.9588e-03, 5.0185e-04, 6.7034e-04, 5.0177e-03, 8.2329e-04,\n",
       "                      2.6797e-03, 3.1282e-03, 1.1050e-02, 3.2932e-03, 5.9401e-04, 1.1725e-03,\n",
       "                      5.0069e-04, 1.2713e-03, 1.3098e-03, 2.5141e-03, 1.4745e-03, 1.3358e-09,\n",
       "                      1.9815e-03, 5.1657e-04, 2.4119e-04, 6.9980e-03, 2.8733e-03, 1.5876e-03,\n",
       "                      2.8951e-09, 1.5023e-03, 7.9607e-04, 3.6223e-03, 8.4939e-08, 2.6281e-03,\n",
       "                      9.4851e-04, 1.0066e-03, 4.3909e-04, 2.8731e-03, 1.0313e-03, 2.2349e-04,\n",
       "                      1.8330e-03, 4.8578e-04, 5.4859e-04, 1.0549e-03, 3.8456e-04, 9.2504e-04,\n",
       "                      2.3091e-04, 2.1027e-03, 6.2134e-05, 4.6067e-03, 5.3458e-04, 1.0082e-02,\n",
       "                      3.8365e-03, 4.5611e-04, 3.5767e-03, 6.3477e-04, 1.8518e-04, 1.3203e-03,\n",
       "                      5.2792e-04, 6.6247e-03, 7.3331e-04, 3.3298e-08, 8.4957e-04, 2.3148e-03,\n",
       "                      1.1973e-03, 4.9311e-04, 5.5563e-04, 1.1093e-03, 2.2030e-03, 3.0611e-03,\n",
       "                      4.3134e-04, 1.0729e-03, 6.2610e-04, 1.8600e-03, 3.8406e-03, 1.1298e-03,\n",
       "                      1.2566e-03, 6.9269e-04, 5.7321e-04, 6.7158e-03, 6.8726e-04, 5.0361e-04,\n",
       "                      1.8947e-03, 4.5458e-04, 6.6649e-04, 3.5595e-04, 2.3651e-03, 2.6006e-04,\n",
       "                      1.2928e-03, 7.7212e-10, 2.8840e-03, 8.0230e-04, 1.1739e-03, 3.3928e-03,\n",
       "                      4.5000e-04, 1.4055e-03, 7.0170e-03, 1.6768e-03, 4.1029e-03, 2.6447e-03,\n",
       "                      4.0138e-03, 5.0911e-04])),\n",
       "             ('layer2.0.downsample.1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.1.conv1.weight',\n",
       "              tensor([[[[-0.0075]],\n",
       "              \n",
       "                       [[ 0.0076]],\n",
       "              \n",
       "                       [[ 0.0014]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0049]],\n",
       "              \n",
       "                       [[-0.0204]],\n",
       "              \n",
       "                       [[-0.0133]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0021]],\n",
       "              \n",
       "                       [[-0.0046]],\n",
       "              \n",
       "                       [[ 0.0044]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0115]],\n",
       "              \n",
       "                       [[ 0.0011]],\n",
       "              \n",
       "                       [[ 0.0178]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0101]],\n",
       "              \n",
       "                       [[-0.0005]],\n",
       "              \n",
       "                       [[ 0.0017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0003]],\n",
       "              \n",
       "                       [[ 0.0085]],\n",
       "              \n",
       "                       [[ 0.0187]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0047]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       [[-0.0017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0075]],\n",
       "              \n",
       "                       [[ 0.0114]],\n",
       "              \n",
       "                       [[ 0.0034]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0083]],\n",
       "              \n",
       "                       [[ 0.0156]],\n",
       "              \n",
       "                       [[-0.0244]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0140]],\n",
       "              \n",
       "                       [[-0.0148]],\n",
       "              \n",
       "                       [[ 0.0069]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0098]],\n",
       "              \n",
       "                       [[ 0.0027]],\n",
       "              \n",
       "                       [[-0.0079]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0007]],\n",
       "              \n",
       "                       [[ 0.0005]],\n",
       "              \n",
       "                       [[-0.0050]]]])),\n",
       "             ('layer2.1.bn1.weight',\n",
       "              tensor([0.1047, 0.0793, 0.0494, 0.0921, 0.0525, 0.0706, 0.0445, 0.0780, 0.0840,\n",
       "                      0.0784, 0.0593, 0.0500, 0.0441, 0.0767, 0.0357, 0.0543, 0.0263, 0.0367,\n",
       "                      0.0778, 0.0996, 0.0835, 0.0593, 0.0545, 0.0454, 0.0518, 0.0974, 0.0787,\n",
       "                      0.0563, 0.0659, 0.0774, 0.0363, 0.0509, 0.0419, 0.0668, 0.0437, 0.0631,\n",
       "                      0.0634, 0.1099, 0.0499, 0.0500, 0.0587, 0.1265, 0.0586, 0.0759, 0.0975,\n",
       "                      0.0700, 0.0770, 0.0595, 0.0708, 0.0877, 0.0904, 0.0725, 0.0551, 0.0653,\n",
       "                      0.0592, 0.0832, 0.0580, 0.0904, 0.0901, 0.0722, 0.0624, 0.0440, 0.0519,\n",
       "                      0.0565, 0.0650, 0.0612, 0.0709, 0.0892, 0.0246, 0.0293, 0.0953, 0.0704,\n",
       "                      0.0810, 0.0601, 0.0562, 0.0569, 0.0982, 0.0748, 0.0698, 0.0793, 0.0940,\n",
       "                      0.0469, 0.0588, 0.1025, 0.0772, 0.1174, 0.1152, 0.0965, 0.0417, 0.0325,\n",
       "                      0.1076, 0.1095, 0.0737, 0.0574, 0.0981, 0.0922, 0.1050, 0.0877, 0.0687,\n",
       "                      0.0371, 0.0630, 0.0994, 0.0955, 0.0515, 0.0330, 0.0456, 0.0518, 0.0855,\n",
       "                      0.0535, 0.0764, 0.0778, 0.0604, 0.0583, 0.0568, 0.0540, 0.0760, 0.1043,\n",
       "                      0.0500, 0.0618, 0.1414, 0.0773, 0.0312, 0.0404, 0.0707, 0.0475, 0.0454,\n",
       "                      0.0971, 0.0546])),\n",
       "             ('layer2.1.bn1.bias',\n",
       "              tensor([-3.8698e-02, -2.0999e-02,  4.8945e-02,  3.6610e-02,  1.7522e-02,\n",
       "                      -1.6778e-02,  4.4300e-02, -8.2070e-04, -1.3816e-02,  3.8299e-02,\n",
       "                       6.8811e-02,  5.4107e-03,  7.9873e-02,  2.3352e-02,  1.3816e-02,\n",
       "                       1.3082e-03,  2.4947e-03,  7.2156e-03, -1.5175e-02, -5.1230e-03,\n",
       "                       1.7610e-03,  3.6106e-02,  7.0984e-03,  5.6692e-02,  1.3099e-03,\n",
       "                      -3.8596e-03, -1.5868e-02, -4.2790e-04,  7.3412e-02,  5.1763e-03,\n",
       "                       6.3204e-02,  1.8409e-02,  4.4290e-02,  2.7727e-03,  4.0363e-03,\n",
       "                       2.8478e-02,  2.9711e-02, -2.8940e-02, -2.1964e-03,  7.1236e-02,\n",
       "                       4.0130e-02, -3.0710e-02, -4.7658e-03,  6.6725e-05, -6.2017e-03,\n",
       "                      -1.1114e-02, -2.0827e-02,  2.6143e-02, -8.5838e-03,  8.7954e-03,\n",
       "                       5.5221e-02,  6.2774e-02, -8.1737e-04,  2.9884e-03,  5.3626e-02,\n",
       "                      -2.7087e-02, -2.4981e-02, -3.9087e-02, -6.0586e-03, -5.3055e-03,\n",
       "                       5.0352e-02,  4.2496e-02, -2.0192e-02, -1.3538e-02,  2.6846e-02,\n",
       "                       1.4557e-02,  3.2275e-02, -1.6645e-02,  5.7921e-02,  3.9822e-02,\n",
       "                       2.8603e-02, -3.0437e-03, -5.7372e-03,  4.0187e-02,  1.8808e-02,\n",
       "                       2.1497e-02, -3.3001e-02,  1.4082e-02,  4.0231e-02,  8.7834e-02,\n",
       "                       9.1675e-02,  4.0406e-02,  4.9055e-02,  2.6141e-02,  2.9466e-03,\n",
       "                       4.0130e-03, -5.2558e-02, -3.4204e-02,  8.4827e-02,  4.8067e-02,\n",
       "                       3.7853e-02,  1.8819e-03, -2.5630e-03,  4.1638e-02, -2.1002e-02,\n",
       "                       1.8496e-03, -4.8037e-02,  1.3288e-02,  2.4957e-02, -8.0970e-03,\n",
       "                       1.8110e-02,  3.6847e-02,  6.2226e-02,  3.3914e-02,  1.8960e-03,\n",
       "                       3.6845e-02, -2.3347e-02, -6.3666e-03, -5.3613e-03,  1.2383e-02,\n",
       "                       5.6146e-02,  5.6264e-02,  4.1876e-03,  5.9922e-03, -6.6770e-03,\n",
       "                       2.2195e-02,  4.3788e-02,  1.2677e-02,  6.3702e-03, -5.7509e-02,\n",
       "                       9.8010e-03, -1.0891e-02,  1.2427e-03,  4.5110e-03,  1.1455e-01,\n",
       "                       3.1354e-02,  8.4730e-02,  1.8111e-02])),\n",
       "             ('layer2.1.bn1.running_mean',\n",
       "              tensor([ 0.0004, -0.0183,  0.0003,  0.0090, -0.0014, -0.0123,  0.0022,  0.0041,\n",
       "                      -0.0137,  0.0088,  0.0104,  0.0056, -0.0211,  0.0115, -0.0019,  0.0091,\n",
       "                       0.0076, -0.0021, -0.0117, -0.0336, -0.0274,  0.0059,  0.0314, -0.0282,\n",
       "                       0.0204, -0.0160, -0.0092,  0.0040, -0.0031,  0.0143,  0.0323,  0.0083,\n",
       "                      -0.0134,  0.0037, -0.0090, -0.0214, -0.0070,  0.0010,  0.0134,  0.0212,\n",
       "                       0.0125,  0.0634, -0.0116, -0.0258,  0.0138, -0.0217, -0.0036, -0.0247,\n",
       "                      -0.0152,  0.0297,  0.0072, -0.0072, -0.0179,  0.0066, -0.0078, -0.0005,\n",
       "                      -0.0054,  0.0221, -0.0055, -0.0137,  0.0152,  0.0102,  0.0132,  0.0134,\n",
       "                      -0.0301,  0.0117, -0.0137,  0.0206, -0.0171,  0.0058,  0.0263,  0.0292,\n",
       "                      -0.0154,  0.0033, -0.0102, -0.0182,  0.0224,  0.0229,  0.0059, -0.0384,\n",
       "                       0.0024, -0.0004,  0.0101, -0.0292,  0.0041, -0.0135, -0.0223, -0.0157,\n",
       "                      -0.0175, -0.0085, -0.0362,  0.0126, -0.0353,  0.0171, -0.0283,  0.0060,\n",
       "                      -0.0287, -0.0153, -0.0094,  0.0008,  0.0132,  0.0024,  0.0154,  0.0105,\n",
       "                       0.0052, -0.0057,  0.0186,  0.0059, -0.0053, -0.0030, -0.0157, -0.0203,\n",
       "                       0.0022, -0.0118, -0.0081, -0.0139, -0.0609,  0.0146, -0.0074, -0.0170,\n",
       "                      -0.0314,  0.0163,  0.0139, -0.0180, -0.0252, -0.0196, -0.0148,  0.0111])),\n",
       "             ('layer2.1.bn1.running_var',\n",
       "              tensor([0.0055, 0.0025, 0.0016, 0.0055, 0.0014, 0.0021, 0.0022, 0.0013, 0.0024,\n",
       "                      0.0052, 0.0051, 0.0011, 0.0045, 0.0026, 0.0014, 0.0007, 0.0003, 0.0011,\n",
       "                      0.0018, 0.0024, 0.0039, 0.0032, 0.0016, 0.0024, 0.0007, 0.0037, 0.0031,\n",
       "                      0.0008, 0.0046, 0.0021, 0.0020, 0.0018, 0.0032, 0.0011, 0.0017, 0.0025,\n",
       "                      0.0031, 0.0030, 0.0006, 0.0041, 0.0032, 0.0059, 0.0015, 0.0030, 0.0031,\n",
       "                      0.0035, 0.0015, 0.0032, 0.0016, 0.0025, 0.0036, 0.0039, 0.0008, 0.0011,\n",
       "                      0.0035, 0.0009, 0.0013, 0.0015, 0.0035, 0.0016, 0.0045, 0.0021, 0.0009,\n",
       "                      0.0010, 0.0041, 0.0012, 0.0028, 0.0023, 0.0020, 0.0017, 0.0057, 0.0011,\n",
       "                      0.0033, 0.0019, 0.0040, 0.0014, 0.0030, 0.0012, 0.0031, 0.0062, 0.0068,\n",
       "                      0.0016, 0.0031, 0.0067, 0.0021, 0.0041, 0.0031, 0.0019, 0.0044, 0.0024,\n",
       "                      0.0055, 0.0058, 0.0025, 0.0024, 0.0038, 0.0043, 0.0031, 0.0045, 0.0027,\n",
       "                      0.0002, 0.0020, 0.0061, 0.0073, 0.0019, 0.0003, 0.0024, 0.0012, 0.0018,\n",
       "                      0.0007, 0.0067, 0.0045, 0.0026, 0.0009, 0.0014, 0.0013, 0.0027, 0.0060,\n",
       "                      0.0009, 0.0018, 0.0048, 0.0031, 0.0003, 0.0003, 0.0018, 0.0060, 0.0015,\n",
       "                      0.0070, 0.0022])),\n",
       "             ('layer2.1.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.1.conv2.weight',\n",
       "              tensor([[[[ 2.9089e-03,  1.2093e-02,  6.6644e-03],\n",
       "                        [-5.4308e-03,  8.0658e-03,  1.0849e-02],\n",
       "                        [-1.7935e-03, -3.3647e-03,  1.6533e-02]],\n",
       "              \n",
       "                       [[ 3.6049e-03, -4.0724e-04,  1.1997e-02],\n",
       "                        [-6.0513e-03, -4.1061e-03,  1.0698e-02],\n",
       "                        [ 1.8762e-03, -2.8404e-03, -1.3033e-02]],\n",
       "              \n",
       "                       [[-3.5543e-03, -2.3038e-03,  1.5818e-03],\n",
       "                        [-4.5134e-03,  2.1168e-03, -2.2695e-03],\n",
       "                        [-1.0053e-02, -8.5767e-03, -5.2976e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.8160e-04, -2.1341e-03,  4.7607e-04],\n",
       "                        [ 2.9282e-03, -3.8864e-03,  5.1177e-04],\n",
       "                        [ 4.8832e-03,  3.7738e-03, -2.1880e-04]],\n",
       "              \n",
       "                       [[ 1.7114e-02, -5.0509e-03, -4.6209e-03],\n",
       "                        [-9.0432e-04,  1.0328e-02, -1.2330e-04],\n",
       "                        [ 2.4967e-02,  2.7833e-02, -2.2590e-02]],\n",
       "              \n",
       "                       [[-5.5315e-05,  4.2032e-03, -4.2211e-03],\n",
       "                        [-2.5469e-03,  3.5231e-03, -7.7517e-03],\n",
       "                        [-2.2764e-03, -7.7539e-03, -5.1414e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.9177e-03,  3.5734e-03,  4.7135e-03],\n",
       "                        [-3.8025e-03,  7.1334e-03,  4.6709e-04],\n",
       "                        [-4.6002e-04,  6.3381e-03, -1.9954e-04]],\n",
       "              \n",
       "                       [[-2.5707e-04, -1.0705e-02, -1.0311e-02],\n",
       "                        [-1.2218e-02, -1.8049e-02,  6.6577e-03],\n",
       "                        [-1.4490e-02,  9.0624e-03,  1.0181e-02]],\n",
       "              \n",
       "                       [[ 6.1899e-03, -1.1428e-03, -8.1266e-03],\n",
       "                        [ 1.2502e-02, -3.9362e-03, -1.4400e-02],\n",
       "                        [ 8.6887e-03, -4.8129e-03, -1.4926e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3241e-03,  1.4240e-03, -1.1718e-03],\n",
       "                        [-5.8460e-03, -4.8006e-03,  1.0189e-02],\n",
       "                        [-6.6770e-03, -1.2368e-03,  1.0861e-02]],\n",
       "              \n",
       "                       [[ 9.5945e-03, -1.9306e-02, -8.0624e-03],\n",
       "                        [-5.0671e-04, -3.4207e-03,  2.0604e-02],\n",
       "                        [-1.8156e-02,  6.8597e-03,  3.2570e-03]],\n",
       "              \n",
       "                       [[ 9.4748e-03, -2.2674e-04, -4.6865e-03],\n",
       "                        [ 1.5965e-02, -6.8915e-04, -1.4521e-02],\n",
       "                        [ 6.2332e-03, -4.2657e-03, -1.3780e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6310e-02,  2.7148e-02,  1.1339e-02],\n",
       "                        [ 2.7406e-02,  3.3823e-04, -1.8680e-02],\n",
       "                        [-5.0490e-03, -2.6259e-02, -2.3315e-02]],\n",
       "              \n",
       "                       [[ 1.0554e-04, -9.6051e-03, -3.0211e-03],\n",
       "                        [-3.0565e-03, -7.4532e-04,  7.3763e-03],\n",
       "                        [ 1.1053e-02,  7.7439e-03,  6.4416e-03]],\n",
       "              \n",
       "                       [[-6.0629e-03, -2.3443e-03,  9.6589e-03],\n",
       "                        [-7.3860e-03, -2.6577e-03,  1.2198e-02],\n",
       "                        [-1.3686e-03,  8.2234e-03,  1.5872e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.1957e-03, -9.5396e-03, -5.8149e-04],\n",
       "                        [-4.6073e-03,  5.4367e-03,  8.5478e-03],\n",
       "                        [ 9.2270e-03,  1.5914e-02,  9.1788e-03]],\n",
       "              \n",
       "                       [[ 7.8576e-03, -4.8200e-03, -1.6375e-02],\n",
       "                        [ 7.2432e-03,  1.9292e-03, -7.7678e-03],\n",
       "                        [ 3.2482e-02,  1.8008e-02,  9.6637e-04]],\n",
       "              \n",
       "                       [[ 6.9384e-03,  1.0493e-02,  7.2661e-03],\n",
       "                        [ 7.1989e-03, -8.1181e-03, -1.1355e-03],\n",
       "                        [-3.7836e-03, -6.4996e-03,  1.6600e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 4.6624e-03, -4.5490e-03, -1.7557e-03],\n",
       "                        [ 3.4720e-03, -1.2807e-02, -1.6544e-02],\n",
       "                        [ 5.7437e-03,  1.0736e-03,  3.9408e-03]],\n",
       "              \n",
       "                       [[-1.3947e-02, -4.4821e-03, -1.2455e-02],\n",
       "                        [-4.9881e-03,  3.7663e-03, -1.0666e-02],\n",
       "                        [-4.6279e-03,  1.0908e-02, -6.6974e-04]],\n",
       "              \n",
       "                       [[ 5.5829e-04, -3.6091e-03, -3.1582e-03],\n",
       "                        [-3.2343e-03, -1.0451e-02,  1.9678e-03],\n",
       "                        [ 9.9340e-04,  3.9444e-03, -2.9960e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.0449e-03, -4.1425e-03, -8.5901e-03],\n",
       "                        [-5.4828e-03,  4.2340e-03,  7.8770e-04],\n",
       "                        [-1.3988e-03, -3.4859e-04, -1.6274e-03]],\n",
       "              \n",
       "                       [[-6.3703e-04, -1.3028e-02, -6.8474e-03],\n",
       "                        [ 4.3026e-02,  1.0359e-02, -1.0279e-02],\n",
       "                        [ 9.7413e-04, -7.0917e-03, -3.8402e-03]],\n",
       "              \n",
       "                       [[ 1.8540e-02, -3.7799e-03,  5.8785e-03],\n",
       "                        [ 1.0331e-02, -1.9157e-02, -2.9962e-03],\n",
       "                        [ 3.1695e-03, -2.8840e-03,  3.8233e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7420e-03,  2.8103e-02, -9.2552e-04],\n",
       "                        [-3.1100e-02,  2.0681e-02,  3.2843e-02],\n",
       "                        [-1.6231e-02, -2.4303e-02,  9.9047e-03]],\n",
       "              \n",
       "                       [[-6.6938e-03, -1.0651e-02, -4.9340e-03],\n",
       "                        [ 1.1589e-02, -7.3440e-03, -9.4859e-03],\n",
       "                        [-6.3653e-03, -5.4250e-03, -1.1744e-02]],\n",
       "              \n",
       "                       [[ 3.2409e-03,  2.5466e-03,  6.2278e-03],\n",
       "                        [-3.7129e-03, -2.9789e-03, -1.6046e-03],\n",
       "                        [ 3.6342e-03, -6.1930e-04, -1.7942e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.1392e-03, -1.0720e-02, -9.4091e-03],\n",
       "                        [ 1.3564e-02, -9.1627e-03, -1.5896e-02],\n",
       "                        [ 1.5098e-02,  3.4472e-03,  3.9111e-03]],\n",
       "              \n",
       "                       [[-1.6329e-02, -1.8089e-02, -7.7144e-03],\n",
       "                        [ 5.5266e-03, -1.0564e-02, -5.3776e-03],\n",
       "                        [ 1.1210e-02, -1.4393e-04, -4.8359e-03]],\n",
       "              \n",
       "                       [[-8.5409e-03,  5.0451e-03, -4.2537e-03],\n",
       "                        [-9.8358e-03,  1.6094e-03,  4.9973e-03],\n",
       "                        [-4.7615e-03, -6.2982e-03, -6.5270e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3988e-02,  1.1836e-02, -4.7551e-03],\n",
       "                        [ 9.3889e-03,  3.8252e-03, -1.4146e-02],\n",
       "                        [-1.5131e-03,  1.6024e-03, -1.6992e-02]],\n",
       "              \n",
       "                       [[-3.7872e-03,  1.7396e-02,  5.9395e-03],\n",
       "                        [ 1.3386e-02,  1.0592e-02,  6.4985e-03],\n",
       "                        [ 2.4986e-02, -9.6426e-03,  1.3078e-03]],\n",
       "              \n",
       "                       [[-2.2621e-02, -1.1563e-02,  5.0934e-03],\n",
       "                        [-1.0432e-02,  1.8218e-03,  1.0542e-02],\n",
       "                        [-9.1108e-03,  1.9912e-03,  4.2832e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.2509e-03, -1.2510e-03, -2.8484e-05],\n",
       "                        [-1.0540e-03,  5.2852e-04,  5.7365e-03],\n",
       "                        [ 8.4035e-03, -2.9226e-03,  2.9493e-03]],\n",
       "              \n",
       "                       [[ 2.6959e-03, -1.0440e-03, -2.1540e-02],\n",
       "                        [ 3.0538e-02,  1.3179e-02, -1.1058e-02],\n",
       "                        [ 2.9949e-02, -1.1284e-03,  1.4337e-02]],\n",
       "              \n",
       "                       [[-2.5186e-03,  2.7872e-03, -7.2681e-04],\n",
       "                        [-5.2909e-03,  6.8257e-03,  5.6025e-03],\n",
       "                        [-8.4602e-03, -5.9910e-04,  7.4758e-03]]]])),\n",
       "             ('layer2.1.bn2.weight',\n",
       "              tensor([1.1372e-01, 9.9799e-02, 9.7327e-02, 2.2061e-01, 1.2555e-01, 1.1111e-01,\n",
       "                      6.4372e-02, 8.9852e-02, 7.7386e-02, 5.9894e-02, 8.7666e-02, 7.4906e-02,\n",
       "                      6.7808e-02, 1.0830e-01, 5.1117e-02, 9.3218e-02, 6.0839e-02, 9.3816e-02,\n",
       "                      1.2730e-01, 8.2008e-02, 8.5027e-02, 5.9539e-03, 9.8228e-02, 5.2987e-02,\n",
       "                      7.8839e-02, 1.0858e-01, 1.0333e-05, 9.2932e-02, 3.3134e-02, 2.1437e-02,\n",
       "                      6.9820e-02, 1.0511e-01, 4.1865e-02, 7.5485e-02, 7.1973e-02, 7.0021e-02,\n",
       "                      9.6231e-02, 1.5248e-01, 1.0175e-01, 8.6395e-02, 1.0773e-01, 1.1315e-01,\n",
       "                      1.0512e-01, 5.8212e-02, 7.0787e-02, 6.7694e-02, 5.4210e-02, 5.1715e-02,\n",
       "                      1.4023e-01, 9.0509e-02, 1.0285e-01, 8.9166e-02, 7.8651e-02, 6.0341e-02,\n",
       "                      1.0926e-01, 8.9539e-02, 6.4163e-02, 1.0601e-01, 7.3094e-02, 1.5007e-01,\n",
       "                      8.2376e-02, 1.2838e-01, 4.4199e-02, 1.2979e-01, 1.0743e-01, 1.1673e-01,\n",
       "                      7.4268e-02, 8.3405e-02, 1.1270e-01, 1.2285e-01, 7.6313e-02, 8.9690e-02,\n",
       "                      1.0902e-01, 9.1509e-02, 2.8014e-02, 9.8826e-02, 8.5323e-02, 1.1736e-01,\n",
       "                      9.0370e-02, 1.0753e-01, 5.4696e-02, 9.1642e-02, 1.0587e-01, 5.0899e-02,\n",
       "                      1.1743e-01, 4.1306e-02, 1.3782e-01, 8.8261e-02, 5.4966e-02, 9.1454e-02,\n",
       "                      1.2774e-02, 8.8658e-02, 9.0335e-02, 1.4266e-01, 1.0694e-01, 1.0734e-01,\n",
       "                      1.7967e-02, 2.7184e-02, 8.5034e-02, 9.6997e-02, 6.7133e-02, 8.9880e-02,\n",
       "                      1.0393e-01, 9.2564e-02, 7.0786e-02, 1.0211e-01, 9.5370e-02, 9.7577e-02,\n",
       "                      8.9615e-02, 9.1978e-02, 9.7665e-02, 1.0969e-01, 9.9069e-02, 1.2852e-01,\n",
       "                      1.1421e-01, 5.2653e-02, 9.0791e-02, 1.5874e-01, 2.1382e-02, 1.1925e-01,\n",
       "                      1.2097e-01, 9.6554e-02, 8.4297e-02, 1.2530e-01, 6.3890e-02, 1.0425e-01,\n",
       "                      1.0956e-01, 9.4326e-02])),\n",
       "             ('layer2.1.bn2.bias',\n",
       "              tensor([-6.8199e-02, -3.3408e-02, -9.7680e-04, -1.0154e-01, -3.5572e-02,\n",
       "                      -5.6569e-02, -4.2363e-02,  2.6065e-02, -2.1969e-02, -3.8687e-02,\n",
       "                      -1.4512e-02, -1.4860e-02, -3.5175e-02, -4.4255e-02,  1.1052e-02,\n",
       "                      -5.0348e-02, -6.4462e-03, -2.9880e-02, -7.5980e-02, -2.4756e-02,\n",
       "                       1.5564e-03, -3.0832e-02, -7.3125e-02, -1.6739e-02, -3.4914e-02,\n",
       "                      -4.0330e-02, -6.6468e-05, -1.7086e-02,  4.0542e-02,  5.7100e-04,\n",
       "                      -2.8347e-02,  3.1386e-03, -6.6446e-03, -3.4765e-02, -9.5764e-03,\n",
       "                      -2.6745e-02, -2.7136e-02, -1.5331e-02,  1.8939e-03, -2.1846e-02,\n",
       "                      -5.4032e-02, -1.8751e-02, -5.0654e-02, -3.3239e-02,  3.0064e-02,\n",
       "                       8.9371e-03, -1.0996e-02, -1.8753e-02, -4.5042e-02, -2.6286e-02,\n",
       "                      -6.3579e-02, -4.6283e-02, -3.4933e-02,  4.3215e-02, -2.1687e-03,\n",
       "                      -2.3702e-02,  2.8002e-03, -9.3364e-03,  1.8620e-02, -5.7186e-02,\n",
       "                      -3.4047e-02, -8.1526e-02, -1.3260e-02, -4.2555e-02, -4.4106e-02,\n",
       "                      -8.8944e-03, -1.6510e-02,  4.2434e-02, -3.4967e-02,  3.5052e-02,\n",
       "                       1.8288e-02,  3.5366e-02, -3.6597e-02, -5.5051e-02, -1.6934e-02,\n",
       "                      -6.0083e-02,  4.1106e-02, -4.7276e-02,  6.0572e-02, -4.9238e-02,\n",
       "                       3.7881e-03, -1.7622e-02, -5.9373e-02,  5.2014e-02, -6.1645e-02,\n",
       "                      -2.6617e-02, -6.6969e-02, -3.1682e-02, -2.8557e-02, -4.0270e-03,\n",
       "                      -4.0134e-02, -2.0291e-03, -3.9279e-02, -7.1184e-02, -5.5805e-02,\n",
       "                       2.1492e-03, -1.0334e-02, -3.1802e-03, -2.2296e-02, -3.1244e-02,\n",
       "                      -4.3745e-02, -8.1539e-02, -1.3870e-02, -4.9077e-02, -2.3420e-02,\n",
       "                       1.6316e-02, -6.8652e-02, -4.0759e-03, -6.2678e-02, -2.8756e-02,\n",
       "                      -6.4631e-04, -2.0930e-02, -3.3221e-02, -5.3656e-02, -1.8437e-02,\n",
       "                      -2.1433e-02, -4.5253e-04, -5.3548e-02,  6.2638e-03, -5.2343e-02,\n",
       "                      -1.0752e-01, -5.8523e-02, -2.7162e-02, -3.0435e-02, -2.4374e-02,\n",
       "                      -1.7214e-02, -8.4785e-02, -3.5149e-02])),\n",
       "             ('layer2.1.bn2.running_mean',\n",
       "              tensor([-4.8690e-03, -1.6632e-02, -2.7780e-03, -4.9333e-02,  1.0451e-02,\n",
       "                      -1.5184e-02,  1.0843e-02, -3.0593e-02,  3.8947e-02,  3.2036e-02,\n",
       "                      -6.5153e-02, -1.2436e-02, -1.1810e-02, -4.2754e-03, -3.0597e-03,\n",
       "                      -4.8035e-02, -2.7741e-02, -7.3517e-03, -2.8367e-02, -5.4884e-02,\n",
       "                       7.2035e-02, -7.5199e-03, -5.1646e-02,  4.0357e-03, -1.3842e-02,\n",
       "                       1.5460e-02,  1.0849e-05, -3.0244e-02, -7.1105e-03, -1.2813e-02,\n",
       "                      -1.1570e-02,  3.0950e-02,  6.3795e-02,  3.4729e-02,  1.0315e-02,\n",
       "                      -3.3138e-02, -2.9350e-03, -1.0283e-02, -3.5795e-02, -1.3130e-02,\n",
       "                      -2.9409e-02, -1.9462e-03, -2.3741e-02, -9.7536e-03, -2.2042e-02,\n",
       "                      -2.6444e-02, -3.1158e-03,  3.5547e-02, -6.9101e-02, -2.7273e-02,\n",
       "                      -5.1063e-02, -5.4797e-03,  2.5201e-02, -2.8432e-02, -1.2799e-02,\n",
       "                      -1.4601e-02, -1.6419e-02, -4.2380e-02, -3.5125e-02, -4.3670e-02,\n",
       "                      -2.3613e-02, -1.8334e-02, -2.7647e-02,  1.2146e-02, -2.3344e-02,\n",
       "                      -4.0185e-02, -5.0029e-03, -4.4500e-02, -4.3919e-02, -1.1107e-01,\n",
       "                      -4.5307e-03, -1.3185e-02, -3.0842e-02,  4.2658e-02,  4.4184e-03,\n",
       "                       2.5356e-02, -2.9609e-02, -3.2056e-02, -1.1916e-02, -7.1399e-02,\n",
       "                      -2.9754e-02, -5.7723e-02,  8.8667e-03, -1.5919e-02,  1.7067e-02,\n",
       "                       7.9105e-04, -1.5204e-02, -1.3830e-02, -1.4156e-02, -5.3530e-02,\n",
       "                      -1.9648e-02, -2.8756e-02, -5.9398e-03, -1.1794e-02, -7.8521e-02,\n",
       "                       4.1114e-02,  1.4201e-02, -8.0120e-03, -6.0584e-02, -9.5021e-03,\n",
       "                       3.5553e-02, -5.6653e-02, -2.0848e-02, -2.9752e-03, -1.3643e-03,\n",
       "                      -2.8340e-02,  3.4930e-02, -6.9795e-02, -2.8772e-02, -2.7526e-02,\n",
       "                       7.7753e-03,  3.8270e-02,  4.5365e-02, -3.6650e-02, -7.9739e-03,\n",
       "                       8.9420e-03, -3.7881e-02, -4.9068e-02, -1.4582e-02, -7.3317e-02,\n",
       "                       3.4632e-02, -6.2406e-02, -1.2458e-02,  2.0064e-02,  1.9751e-02,\n",
       "                      -8.1079e-02, -3.3343e-04, -4.0398e-02])),\n",
       "             ('layer2.1.bn2.running_var',\n",
       "              tensor([2.8671e-03, 3.5190e-03, 3.0055e-03, 5.0203e-03, 3.4443e-03, 2.5406e-03,\n",
       "                      8.9474e-04, 3.9485e-03, 1.8653e-03, 7.0643e-04, 1.1640e-03, 2.0248e-03,\n",
       "                      1.0611e-03, 3.3829e-03, 1.1519e-03, 1.8756e-03, 5.2621e-04, 3.1720e-03,\n",
       "                      1.8564e-03, 2.1335e-03, 3.5590e-03, 2.5628e-04, 2.1573e-03, 5.0738e-04,\n",
       "                      1.6127e-03, 3.4690e-03, 7.3364e-10, 2.1574e-03, 8.6159e-04, 1.0862e-04,\n",
       "                      1.1495e-03, 4.2323e-03, 1.2287e-03, 1.9374e-03, 2.1076e-03, 1.4962e-03,\n",
       "                      1.5801e-03, 4.6288e-03, 5.0500e-03, 2.7376e-03, 9.8158e-04, 4.2359e-03,\n",
       "                      1.0380e-03, 9.1065e-04, 2.5273e-03, 1.7036e-03, 7.9359e-04, 1.4915e-03,\n",
       "                      5.1459e-03, 1.7746e-03, 1.6821e-03, 2.1560e-03, 1.8787e-03, 2.4811e-03,\n",
       "                      3.0174e-03, 2.8676e-03, 1.1355e-03, 3.5010e-03, 1.8539e-03, 5.1357e-03,\n",
       "                      2.0162e-03, 4.0359e-03, 5.5693e-04, 3.2082e-03, 1.2222e-03, 3.1982e-03,\n",
       "                      1.0578e-03, 3.5050e-03, 2.9017e-03, 3.9199e-03, 2.8448e-03, 4.5498e-03,\n",
       "                      2.7072e-03, 1.8264e-03, 1.3661e-04, 1.9515e-03, 2.0717e-03, 2.4651e-03,\n",
       "                      3.1713e-03, 4.2823e-03, 6.7691e-04, 1.7937e-03, 2.1307e-03, 1.9078e-03,\n",
       "                      3.2342e-03, 4.3446e-04, 4.0512e-03, 1.8058e-03, 5.4697e-04, 2.9391e-03,\n",
       "                      3.0717e-04, 1.9381e-03, 2.7423e-03, 3.8604e-03, 2.3895e-03, 3.5114e-03,\n",
       "                      8.6636e-05, 2.1586e-04, 2.9983e-03, 2.9567e-03, 9.6438e-04, 1.2764e-03,\n",
       "                      2.4218e-03, 3.0845e-03, 1.2688e-03, 4.6720e-03, 1.4867e-03, 2.7476e-03,\n",
       "                      1.2209e-03, 2.0537e-03, 3.3185e-03, 4.0792e-03, 2.7788e-03, 2.9099e-03,\n",
       "                      3.7389e-03, 9.8917e-04, 2.4245e-03, 7.3447e-03, 1.8965e-04, 2.5577e-03,\n",
       "                      4.6246e-03, 2.4675e-03, 2.0310e-03, 3.3524e-03, 4.6097e-04, 2.3517e-03,\n",
       "                      2.5525e-03, 3.0126e-03])),\n",
       "             ('layer2.1.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.1.conv3.weight',\n",
       "              tensor([[[[ 0.0025]],\n",
       "              \n",
       "                       [[-0.0040]],\n",
       "              \n",
       "                       [[-0.0065]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0109]],\n",
       "              \n",
       "                       [[ 0.0063]],\n",
       "              \n",
       "                       [[-0.0048]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0112]],\n",
       "              \n",
       "                       [[ 0.0002]],\n",
       "              \n",
       "                       [[-0.0054]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0022]],\n",
       "              \n",
       "                       [[ 0.0109]],\n",
       "              \n",
       "                       [[ 0.0019]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0043]],\n",
       "              \n",
       "                       [[ 0.0192]],\n",
       "              \n",
       "                       [[-0.0096]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0086]],\n",
       "              \n",
       "                       [[ 0.0190]],\n",
       "              \n",
       "                       [[ 0.0024]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0053]],\n",
       "              \n",
       "                       [[ 0.0034]],\n",
       "              \n",
       "                       [[-0.0006]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0020]],\n",
       "              \n",
       "                       [[ 0.0013]],\n",
       "              \n",
       "                       [[ 0.0039]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0006]],\n",
       "              \n",
       "                       [[ 0.0023]],\n",
       "              \n",
       "                       [[ 0.0018]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0074]],\n",
       "              \n",
       "                       [[-0.0059]],\n",
       "              \n",
       "                       [[-0.0087]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0033]],\n",
       "              \n",
       "                       [[ 0.0023]],\n",
       "              \n",
       "                       [[ 0.0120]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0037]],\n",
       "              \n",
       "                       [[-0.0057]],\n",
       "              \n",
       "                       [[ 0.0338]]]])),\n",
       "             ('layer2.1.bn3.weight',\n",
       "              tensor([-2.2372e-02,  4.2819e-02, -7.0009e-02,  1.2035e-02, -8.4074e-03,\n",
       "                       1.7944e-02, -5.5431e-02,  4.4605e-02, -3.4707e-02,  6.2673e-02,\n",
       "                      -3.4551e-02,  4.1208e-02,  4.1752e-04,  5.1349e-02, -1.6426e-04,\n",
       "                       1.4652e-02, -1.4128e-03,  7.1377e-02,  2.5716e-02,  9.2853e-02,\n",
       "                      -2.2202e-04,  5.2917e-02, -3.9088e-02,  6.0542e-02,  3.1798e-02,\n",
       "                       1.1763e-01, -2.2183e-02,  3.4070e-03,  7.3585e-02,  3.5245e-03,\n",
       "                       6.8276e-02,  3.5458e-03,  7.2235e-02,  6.1910e-02, -1.3947e-02,\n",
       "                       1.9362e-02, -8.7460e-02, -4.5112e-02, -1.8943e-02,  3.3185e-02,\n",
       "                       5.1113e-02,  6.1710e-02,  7.8704e-04, -3.7600e-02,  5.0266e-02,\n",
       "                       9.7369e-02, -3.3411e-02, -5.6109e-02,  2.0882e-03, -5.0893e-02,\n",
       "                      -1.0132e-01,  1.2193e-01, -3.5203e-03, -6.1728e-03,  2.9239e-03,\n",
       "                      -5.0892e-03,  2.2909e-02, -1.6201e-04, -6.3167e-02,  4.5020e-02,\n",
       "                       3.5727e-03, -2.8098e-03,  1.2704e-02,  1.1072e-01, -5.2476e-03,\n",
       "                       7.5196e-02,  2.1371e-02, -9.2920e-02,  5.3448e-02, -4.6872e-03,\n",
       "                      -9.5323e-03,  1.0821e-02, -1.1987e-02,  8.3061e-02,  9.6561e-04,\n",
       "                       1.8215e-03, -2.1134e-03, -6.8839e-02,  1.1974e-01, -2.9834e-02,\n",
       "                       1.3269e-02, -9.3863e-02, -9.4868e-03, -2.0591e-02,  2.6293e-02,\n",
       "                       9.7170e-03, -2.2650e-03,  9.1945e-02,  2.0661e-02, -3.7375e-02,\n",
       "                       1.1784e-01,  7.0580e-02,  4.3540e-02,  7.5460e-02, -7.7325e-02,\n",
       "                      -2.3322e-02,  7.2064e-03,  4.8788e-02,  9.8148e-03, -2.7980e-02,\n",
       "                       1.7950e-03,  6.9320e-02, -6.9352e-04,  5.7999e-04,  2.2143e-03,\n",
       "                      -9.0507e-03,  1.0046e-01,  3.8607e-02, -5.5954e-03, -4.2393e-02,\n",
       "                       1.1749e-01, -7.2876e-02, -6.4156e-03,  2.6951e-03,  4.2757e-02,\n",
       "                       1.9320e-03,  1.8702e-02, -5.1258e-02, -4.2370e-04, -4.9920e-02,\n",
       "                      -2.3403e-02, -4.0103e-02,  3.2283e-02, -4.3492e-04, -1.2539e-01,\n",
       "                       4.7312e-02,  1.4076e-02, -2.3501e-03, -3.8020e-02,  3.2504e-02,\n",
       "                       1.3058e-02, -1.9863e-03,  1.2164e-01,  3.5050e-02,  4.5515e-03,\n",
       "                       4.0876e-02,  4.9155e-02,  1.7518e-04,  1.6694e-02, -1.0829e-01,\n",
       "                       5.5007e-02, -5.9958e-02,  2.6958e-02, -1.8905e-02, -8.9829e-02,\n",
       "                       1.2217e-01, -1.3934e-04,  1.0400e-01,  1.7504e-02,  1.3106e-01,\n",
       "                      -2.5388e-02,  1.0451e-01, -5.5612e-03, -4.1306e-03, -3.1071e-03,\n",
       "                      -1.6302e-02,  1.4932e-02,  3.2251e-02,  6.5653e-02,  3.5763e-02,\n",
       "                       2.0837e-02, -1.2760e-02, -1.0344e-01,  1.3340e-01,  1.0296e-01,\n",
       "                       9.6101e-04, -2.8911e-03,  4.4904e-03, -3.3759e-02,  4.5322e-02,\n",
       "                       1.3375e-03, -1.6447e-01, -4.4364e-02,  1.8491e-03, -7.7626e-02,\n",
       "                       8.3390e-02,  5.8833e-02, -8.5046e-02, -6.4772e-02,  1.6599e-02,\n",
       "                      -1.8583e-02,  5.3417e-03,  7.8394e-02,  3.5685e-02, -3.0339e-02,\n",
       "                       4.2197e-02,  2.3185e-02,  7.0549e-03,  7.1322e-02, -4.3368e-02,\n",
       "                      -7.4842e-02, -1.5326e-02,  3.9643e-04, -4.6560e-02,  4.4848e-02,\n",
       "                       2.7265e-02, -6.4794e-04,  6.1508e-02,  4.1348e-02, -3.2051e-02,\n",
       "                       7.8178e-03, -1.2792e-03, -6.7562e-03, -3.6805e-02, -9.4395e-03,\n",
       "                       5.9430e-02, -6.9789e-02,  5.0586e-04,  2.9049e-03, -1.3756e-01,\n",
       "                      -3.1149e-02, -1.1540e-02, -3.2744e-02, -1.6465e-02,  1.8615e-01,\n",
       "                      -1.2846e-01,  7.1723e-02, -6.8251e-02,  4.3392e-02,  1.5066e-02,\n",
       "                       9.2543e-03,  7.8983e-02, -2.6451e-02,  6.2916e-02, -5.2782e-03,\n",
       "                      -4.9458e-02,  4.9271e-02, -1.2620e-01, -6.1780e-04,  3.0478e-02,\n",
       "                       7.8602e-03, -6.0494e-04,  1.7734e-02,  3.8470e-03, -2.6802e-03,\n",
       "                       5.7460e-03,  2.1602e-02, -5.0068e-04, -9.2183e-04,  1.0659e-01,\n",
       "                      -5.5356e-03, -4.3215e-03, -1.3595e-03,  9.9641e-02, -8.2828e-04,\n",
       "                      -1.2729e-01, -4.8816e-02,  3.4414e-02, -8.9513e-02,  7.0745e-02,\n",
       "                       4.6725e-02,  1.9014e-03,  1.9504e-03,  8.3086e-04,  5.1357e-03,\n",
       "                       1.3642e-02, -2.2682e-02, -3.8934e-02,  8.6601e-03,  8.7838e-02,\n",
       "                      -2.5550e-03,  5.7011e-02,  7.7280e-02,  2.1120e-02, -9.3742e-02,\n",
       "                       1.3248e-02,  4.2565e-02,  2.6765e-03,  7.8672e-02, -6.6533e-03,\n",
       "                       5.9362e-03,  5.2325e-02, -2.2080e-05, -9.2910e-02, -7.4728e-03,\n",
       "                       4.3762e-02,  6.6699e-02, -6.1039e-02,  1.0484e-02, -3.5923e-03,\n",
       "                      -8.9524e-02, -8.8408e-02,  7.9886e-02,  5.3374e-02,  4.8088e-02,\n",
       "                       3.9462e-02,  4.8039e-02,  7.0727e-02,  1.1425e-02, -5.4436e-02,\n",
       "                      -1.0486e-03,  3.5088e-03, -3.0597e-02, -1.4684e-02, -6.7834e-02,\n",
       "                      -9.0357e-02,  9.2202e-03, -2.8540e-02,  5.2709e-02, -4.6337e-03,\n",
       "                       7.4176e-02,  7.7383e-03,  5.3438e-03,  5.1468e-03, -4.1224e-02,\n",
       "                       5.5303e-02, -2.2065e-02, -1.5645e-03,  9.5056e-02,  1.3242e-01,\n",
       "                      -3.3314e-02, -8.4855e-03,  6.3299e-02, -5.9590e-02,  8.9607e-02,\n",
       "                      -5.9456e-03, -2.3455e-02,  8.5166e-02,  1.2618e-02,  1.2327e-03,\n",
       "                       4.9041e-02,  6.6361e-04, -3.8168e-03,  5.1612e-02,  1.9243e-01,\n",
       "                       2.9936e-02, -3.0723e-02,  2.9218e-03,  2.8724e-03, -3.3501e-04,\n",
       "                      -7.2869e-02, -6.5117e-02, -3.6635e-02, -2.1623e-02, -3.2049e-02,\n",
       "                       3.5971e-02,  1.1258e-02, -1.8619e-03,  7.1656e-02,  7.9661e-02,\n",
       "                       6.7972e-02, -1.5984e-02, -1.1583e-03, -7.3504e-03, -1.2852e-02,\n",
       "                       9.1733e-03,  9.1239e-03,  1.1651e-01, -6.5334e-02, -2.8652e-02,\n",
       "                       8.6986e-02, -4.4125e-02, -3.1442e-02,  1.5535e-02,  3.3834e-04,\n",
       "                       7.6769e-03, -1.3708e-02, -7.5362e-02,  1.0652e-01, -1.3184e-01,\n",
       "                      -2.2121e-02,  2.3448e-02, -1.4085e-02,  4.1723e-03, -3.1482e-03,\n",
       "                      -5.0474e-02, -4.2306e-03, -7.5319e-02, -4.4932e-02, -6.4626e-02,\n",
       "                      -4.4091e-02, -5.6222e-02,  2.6484e-02,  1.4346e-02, -3.7753e-03,\n",
       "                       1.8210e-02,  3.5679e-04,  3.3887e-02, -2.4584e-02, -8.4354e-03,\n",
       "                      -7.5245e-02,  3.0438e-02,  3.4419e-05, -1.9891e-02,  9.9780e-03,\n",
       "                      -2.8036e-03, -1.2445e-02, -1.2476e-04,  6.5547e-02,  9.0879e-02,\n",
       "                      -1.5459e-02,  1.3287e-03,  8.2216e-03, -2.2604e-02, -8.5561e-02,\n",
       "                       1.9366e-03, -2.5929e-03, -1.5077e-03,  6.6809e-03, -2.6881e-02,\n",
       "                       5.0861e-02,  1.1754e-01, -1.2879e-03, -4.5677e-02, -1.0773e-01,\n",
       "                      -1.9262e-03,  4.8884e-02, -1.8064e-04, -6.4549e-03, -4.9919e-02,\n",
       "                       1.1143e-01,  3.3282e-02,  3.1173e-04,  8.9884e-02,  1.1549e-01,\n",
       "                      -8.6321e-02,  1.6444e-02, -9.1968e-02, -9.0475e-02,  2.5077e-02,\n",
       "                       1.0551e-01,  1.3955e-01, -4.5090e-03,  5.5863e-02,  3.8683e-02,\n",
       "                       1.6321e-02, -4.8994e-02,  2.8657e-02, -4.0022e-02, -3.5880e-02,\n",
       "                      -7.1903e-02, -1.4311e-01, -5.0510e-03,  1.8894e-02, -1.5531e-02,\n",
       "                      -1.9326e-02,  4.4600e-05,  1.3781e-02, -1.2869e-01,  5.9063e-03,\n",
       "                       2.7828e-03, -5.1358e-02, -9.6562e-02,  1.0936e-01, -3.1755e-02,\n",
       "                       3.0585e-02,  3.0552e-02,  8.4271e-02,  4.5842e-03, -2.5418e-02,\n",
       "                      -1.5683e-02, -1.8578e-02, -4.6032e-03, -7.8570e-03,  1.5782e-02,\n",
       "                      -1.4390e-01,  2.1064e-02,  1.7325e-02,  1.7723e-02,  3.6451e-03,\n",
       "                       5.3898e-02,  2.7880e-03,  2.1788e-02,  4.5509e-02, -1.7736e-03,\n",
       "                      -5.0248e-02, -7.9947e-03, -4.6832e-02,  9.5189e-03, -3.5758e-03,\n",
       "                       9.9681e-02,  5.5113e-02, -6.7659e-02, -3.2021e-02,  9.2805e-03,\n",
       "                       2.7545e-02,  6.6410e-02, -8.6494e-03,  4.3637e-02, -3.4180e-02,\n",
       "                      -7.1204e-03,  1.2876e-03,  8.3241e-03,  5.8313e-02, -2.9391e-02,\n",
       "                      -4.4161e-02,  1.7993e-03, -2.9702e-04,  2.6768e-03, -1.0618e-01,\n",
       "                      -8.6455e-02,  2.1110e-02,  4.6843e-02,  3.0683e-04,  2.9911e-02,\n",
       "                       1.2281e-03, -3.7329e-02,  2.4472e-03,  7.1950e-02, -9.0006e-02,\n",
       "                       1.6038e-01,  2.3303e-02, -7.1682e-02,  6.3098e-02,  4.3702e-02,\n",
       "                      -9.2216e-02, -3.2079e-03,  1.1677e-04,  4.5818e-03, -2.1677e-03,\n",
       "                       1.1205e-02,  6.4201e-02])),\n",
       "             ('layer2.1.bn3.bias',\n",
       "              tensor([-1.8906e-02, -1.5894e-02, -3.9652e-02, -2.3518e-02, -1.1541e-02,\n",
       "                      -1.3393e-02, -2.3202e-02, -6.3933e-02, -4.1505e-03, -1.4216e-02,\n",
       "                      -4.1917e-03, -2.7331e-02, -2.3399e-03, -3.2454e-02, -8.5107e-03,\n",
       "                      -5.2058e-03, -2.2295e-02, -1.3204e-01, -1.8237e-02, -2.4337e-02,\n",
       "                       2.1884e-02, -3.6447e-02, -3.1860e-02,  9.3371e-03, -1.6599e-03,\n",
       "                      -8.0450e-02, -2.2484e-03, -9.2832e-03, -2.1150e-02,  1.2099e-02,\n",
       "                      -2.1433e-02, -3.5439e-03,  2.5214e-03, -3.6263e-03, -3.7243e-03,\n",
       "                      -2.1271e-02, -3.5585e-02, -4.3424e-03, -1.6090e-02, -5.8108e-03,\n",
       "                      -1.3287e-02, -2.1691e-02,  6.9403e-04, -1.8464e-02, -6.0167e-02,\n",
       "                      -3.5248e-02, -3.8904e-02, -4.9246e-02,  7.7975e-03, -2.9298e-02,\n",
       "                      -4.1378e-02,  1.8231e-02, -5.8483e-03,  5.7640e-03,  3.6944e-03,\n",
       "                      -3.8361e-03, -1.5471e-02, -3.2577e-04, -2.3503e-02, -3.8697e-02,\n",
       "                      -1.1486e-02, -7.0493e-03, -2.7464e-02, -1.6407e-02, -2.3832e-02,\n",
       "                      -7.1973e-02, -2.2841e-03, -2.9031e-03, -6.1671e-02,  1.0620e-02,\n",
       "                       7.2300e-03, -1.1516e-02, -3.7254e-02, -6.8351e-02, -2.6772e-03,\n",
       "                       1.7553e-02,  1.6516e-02, -3.1862e-02, -1.9957e-02, -1.6036e-02,\n",
       "                      -2.0151e-02,  2.7199e-02, -4.5615e-03, -9.5022e-03, -2.7060e-02,\n",
       "                      -9.8581e-03, -6.1737e-03, -3.7738e-02, -2.2737e-02,  2.7678e-02,\n",
       "                      -5.5454e-02, -1.8811e-02, -3.2235e-02, -4.4771e-02, -3.2961e-02,\n",
       "                      -2.4770e-02, -6.0773e-03,  5.1479e-03, -3.4755e-03, -2.8670e-02,\n",
       "                      -1.1545e-02, -5.0848e-02,  8.6684e-03,  4.9650e-03,  1.3199e-02,\n",
       "                      -1.5253e-02,  1.0980e-02, -3.3061e-02, -1.0698e-02, -1.5567e-02,\n",
       "                      -2.2444e-02, -2.0451e-02,  2.8608e-03,  4.0652e-04, -7.3519e-03,\n",
       "                      -6.0917e-03, -1.3496e-03, -3.0214e-02, -3.9524e-03, -1.3186e-02,\n",
       "                      -5.3113e-03, -2.3177e-02, -1.0670e-03, -1.4392e-02, -1.7060e-02,\n",
       "                      -2.9161e-02, -2.0783e-02,  1.2407e-02, -3.0634e-02, -2.0847e-02,\n",
       "                      -5.5007e-03,  1.4101e-03, -4.6738e-02, -9.9963e-03, -8.7430e-03,\n",
       "                      -2.7246e-02, -2.2081e-02, -5.0948e-04, -6.8119e-03, -3.8615e-02,\n",
       "                      -6.1873e-02,  1.7188e-02, -4.3123e-02, -5.8456e-03, -4.4681e-02,\n",
       "                      -8.5054e-03, -1.3960e-03, -1.4756e-02, -3.0851e-02, -3.5160e-02,\n",
       "                      -2.9084e-02, -1.2673e-02,  1.1422e-02, -1.8011e-02, -2.8174e-03,\n",
       "                      -4.1310e-02, -2.0392e-02, -2.0528e-02, -4.6093e-02, -1.4573e-02,\n",
       "                      -1.2516e-02, -3.2620e-04, -3.6412e-03, -7.6024e-03, -1.6420e-02,\n",
       "                       6.2659e-04, -1.5360e-02, -2.5655e-02,  2.4018e-03, -5.6109e-03,\n",
       "                       8.3967e-03, -1.0890e-02, -3.4934e-02, -1.3862e-03, -4.8553e-02,\n",
       "                      -3.4031e-02, -2.1915e-03, -2.5826e-02, -2.2398e-03, -6.0027e-03,\n",
       "                      -5.7359e-03, -8.6351e-03, -3.2383e-03,  1.0515e-02, -2.7883e-02,\n",
       "                      -7.1690e-03, -2.0509e-02, -1.1785e-02, -3.1714e-02, -2.8827e-02,\n",
       "                      -5.5933e-02, -1.4147e-02, -2.8818e-03, -5.8213e-02, -2.2535e-02,\n",
       "                      -8.6872e-03,  6.8208e-03, -1.3785e-02, -1.7436e-02,  7.6414e-03,\n",
       "                      -1.6967e-02,  5.0704e-03, -7.4554e-03, -3.4472e-02, -1.7432e-02,\n",
       "                      -7.7334e-02, -2.3798e-02, -5.0813e-03,  1.1157e-02, -8.0549e-02,\n",
       "                      -1.6324e-02, -1.1051e-02, -1.2070e-02, -2.1020e-02, -2.4710e-02,\n",
       "                      -3.4797e-02, -1.6355e-02, -2.1192e-02,  7.0947e-03, -5.5108e-03,\n",
       "                      -6.6390e-03,  1.4696e-02, -5.9652e-03, -3.0531e-02, -6.6149e-03,\n",
       "                      -1.2879e-02,  2.5316e-03, -4.2611e-02,  7.6190e-03, -1.9364e-02,\n",
       "                       3.6256e-03, -1.6550e-02, -2.5760e-02, -8.6233e-03, -1.2165e-02,\n",
       "                      -1.4234e-03, -9.1386e-03,  1.6408e-03, -2.6336e-03, -1.7148e-02,\n",
       "                       1.4259e-02,  5.1650e-03,  2.6507e-03, -2.0171e-02, -2.1837e-02,\n",
       "                      -2.0537e-03, -2.4299e-02, -4.8095e-03, -4.6143e-02, -2.1282e-02,\n",
       "                      -1.4055e-02, -6.9552e-04, -1.4561e-03, -1.7963e-02, -1.6337e-02,\n",
       "                      -2.3411e-02, -1.0654e-02, -1.5184e-02, -1.9498e-02, -1.6919e-02,\n",
       "                      -7.5978e-05, -1.8425e-02, -1.7694e-02, -2.9038e-02, -3.9366e-02,\n",
       "                       8.2773e-03,  2.4123e-03, -1.3517e-02, -3.4785e-02, -2.1243e-02,\n",
       "                      -9.9295e-03, -4.2373e-02, -5.3275e-03, -5.8269e-02, -1.9800e-04,\n",
       "                      -1.9709e-02, -5.1137e-02, -7.8697e-03,  1.5335e-02, -5.8624e-02,\n",
       "                      -1.0625e-02, -6.0709e-02, -4.6433e-02, -2.1309e-02,  2.4219e-02,\n",
       "                      -4.2097e-02, -4.6119e-02, -2.7147e-02, -4.9440e-02, -1.8261e-02,\n",
       "                      -9.4223e-03, -5.2208e-03, -2.3935e-02, -1.2968e-02, -2.3786e-02,\n",
       "                      -3.1803e-02, -7.0993e-03, -1.8615e-02, -2.3043e-02,  2.6918e-03,\n",
       "                      -4.1404e-02,  1.2306e-02, -1.9373e-02, -3.7440e-02, -4.4508e-03,\n",
       "                      -8.3442e-03, -2.2295e-02, -1.1677e-02, -1.7964e-02, -4.5600e-02,\n",
       "                      -2.6605e-02,  1.7754e-04,  7.8523e-03, -7.7194e-03, -5.6543e-02,\n",
       "                      -4.5862e-04,  3.0319e-04,  2.4178e-04, -9.4336e-03, -3.2814e-02,\n",
       "                      -1.2110e-02,  3.7194e-03, -3.9290e-04,  2.3216e-02, -6.8161e-04,\n",
       "                       1.9482e-03,  3.3227e-03, -1.8446e-02, -1.0577e-02,  2.0957e-03,\n",
       "                      -3.7198e-02,  8.0148e-03, -1.2161e-02, -2.3427e-02, -6.0186e-02,\n",
       "                      -1.2785e-02, -2.8772e-02,  1.0274e-03, -4.3117e-02, -2.8505e-02,\n",
       "                      -3.4570e-02, -1.1789e-02, -1.2389e-02, -1.0663e-02, -6.4456e-03,\n",
       "                      -1.4886e-02, -3.4197e-03, -1.8247e-02, -1.1505e-02, -1.6795e-02,\n",
       "                      -7.7172e-03, -6.5522e-03, -2.0942e-02,  1.1738e-02,  3.4189e-03,\n",
       "                       1.0783e-02, -2.4268e-02, -3.3584e-02, -4.9407e-02, -8.5547e-03,\n",
       "                      -1.5619e-02,  1.1219e-03, -1.5644e-02, -1.3187e-03,  1.8526e-02,\n",
       "                      -2.0782e-02,  9.2462e-03, -2.0353e-02, -2.8308e-02, -3.0695e-03,\n",
       "                       1.3388e-03, -2.5129e-02, -3.5085e-03, -1.5823e-03,  5.3288e-03,\n",
       "                      -4.3432e-03,  1.3549e-02, -1.5378e-02, -2.1510e-02, -1.9604e-02,\n",
       "                      -4.6503e-02, -1.0895e-02,  1.3125e-02, -1.7287e-02, -1.7668e-02,\n",
       "                       1.2217e-02, -5.8250e-04,  3.0370e-03, -5.1398e-02, -6.7869e-02,\n",
       "                      -9.8194e-03, -4.7201e-03,  1.1161e-02, -1.3886e-02, -2.1332e-02,\n",
       "                      -3.5611e-03, -1.4553e-02,  6.4600e-04, -4.0417e-02, -4.0166e-03,\n",
       "                      -3.3999e-02, -2.2979e-02,  1.3048e-02, -2.2170e-02,  1.7979e-04,\n",
       "                      -2.7288e-02, -3.1072e-02,  1.2652e-02, -1.3642e-02,  2.8327e-02,\n",
       "                       4.3126e-02, -3.9881e-02,  5.6168e-03, -4.3559e-02, -3.7007e-02,\n",
       "                      -2.6328e-02, -9.1346e-03, -2.9087e-02, -4.8747e-02, -1.2847e-02,\n",
       "                      -6.9760e-02, -4.9291e-02,  5.9622e-03, -2.9538e-02, -1.8844e-02,\n",
       "                      -1.4328e-02, -2.4390e-02, -3.0038e-02, -4.3838e-02, -3.7481e-02,\n",
       "                      -3.9954e-02,  1.6160e-02, -8.3294e-03, -4.9967e-03, -4.0310e-03,\n",
       "                      -3.3777e-02, -1.6349e-02, -1.8268e-02,  1.0260e-02, -6.9805e-03,\n",
       "                      -3.5741e-03, -4.2774e-02,  1.9809e-02, -5.0186e-02, -2.0379e-02,\n",
       "                      -2.2740e-02, -1.6776e-02, -1.8432e-02,  4.8638e-03, -6.6422e-03,\n",
       "                      -1.4263e-02, -1.0758e-03,  3.0412e-03, -5.4104e-03, -2.3384e-02,\n",
       "                      -4.4464e-02, -6.1896e-03, -2.5427e-02, -1.5847e-03,  2.4695e-03,\n",
       "                      -2.0665e-02,  4.8366e-03, -2.8538e-02, -2.1137e-02,  3.3351e-03,\n",
       "                      -2.5299e-02, -3.5657e-03, -3.7895e-02,  1.7234e-03,  1.1886e-02,\n",
       "                      -1.7531e-03, -2.3895e-02, -3.9510e-02, -3.4864e-02,  2.6138e-03,\n",
       "                      -4.5204e-03, -5.1717e-02, -2.2099e-02, -5.2247e-02, -2.1020e-02,\n",
       "                      -9.2977e-03,  5.9374e-03, -2.3828e-02, -2.7479e-02,  4.1344e-03,\n",
       "                      -4.3894e-02,  5.0981e-03,  8.4613e-05,  2.8938e-03, -5.4933e-02,\n",
       "                      -8.3729e-03, -6.9928e-03, -2.6926e-02,  1.9218e-03, -7.3965e-03,\n",
       "                      -7.1368e-03, -2.8459e-02, -4.3970e-03, -9.7467e-03, -3.2290e-02,\n",
       "                      -4.8716e-02, -2.5461e-02, -2.6656e-02, -5.0360e-02, -2.2800e-02,\n",
       "                      -5.1081e-02, -2.7887e-03,  5.7662e-03, -2.0082e-02,  5.7006e-03,\n",
       "                      -4.5046e-02, -5.0581e-02])),\n",
       "             ('layer2.1.bn3.running_mean',\n",
       "              tensor([-3.1927e-03,  3.5342e-04, -7.2873e-03,  5.5006e-03,  2.4608e-03,\n",
       "                       3.6896e-03, -3.1134e-03, -4.4465e-04,  2.2339e-03,  1.9029e-03,\n",
       "                      -2.9032e-03, -4.3605e-03,  9.1155e-04,  5.8630e-03,  2.1960e-03,\n",
       "                       3.9487e-03,  3.0769e-03, -1.9695e-03,  6.3663e-03,  2.4336e-02,\n",
       "                      -5.3711e-04, -4.6629e-03, -1.5806e-03, -4.7811e-03, -7.8523e-04,\n",
       "                       2.1980e-02, -2.6300e-03,  1.0505e-03,  2.9582e-03, -4.8266e-03,\n",
       "                      -4.9522e-03,  9.8551e-04,  1.7474e-03, -1.8115e-02, -1.7539e-03,\n",
       "                       2.2779e-03,  3.0526e-03,  1.2820e-03, -7.8835e-03, -7.4159e-03,\n",
       "                       2.8578e-04,  3.6206e-03,  4.1644e-05, -8.9885e-03, -2.3258e-02,\n",
       "                       2.1335e-03, -4.0383e-04, -1.1155e-03,  1.2681e-03,  2.8010e-03,\n",
       "                      -5.5722e-03,  6.6999e-03, -4.6881e-04, -2.4394e-04, -1.3234e-03,\n",
       "                      -1.1276e-04,  6.2604e-03, -1.6266e-05, -1.0892e-03, -2.9309e-04,\n",
       "                       2.7688e-03,  2.8873e-03,  5.0607e-03, -1.1381e-02,  4.1557e-04,\n",
       "                       4.8005e-03, -1.2905e-02,  3.8169e-03,  4.6212e-03, -3.2269e-04,\n",
       "                      -3.9976e-03,  7.6303e-04,  2.1968e-03, -1.0223e-03, -2.5537e-03,\n",
       "                       8.9741e-04, -3.2122e-04,  6.0505e-03,  7.2989e-03, -3.6048e-03,\n",
       "                      -3.7350e-03,  9.5697e-03,  1.8360e-04,  1.5147e-02, -2.1358e-03,\n",
       "                      -9.3461e-04,  9.6093e-04, -6.5360e-03,  2.4560e-03,  7.7074e-03,\n",
       "                      -9.3668e-03, -8.6297e-03,  1.0054e-03, -3.5505e-03,  2.4356e-02,\n",
       "                       3.3700e-03,  1.5021e-03,  3.8876e-03, -2.8028e-03, -6.1657e-03,\n",
       "                      -8.7798e-04,  9.9176e-03,  4.0690e-03, -4.1163e-05, -1.4574e-03,\n",
       "                      -3.4784e-04, -5.2119e-03, -1.0237e-02,  1.2467e-03,  1.4856e-02,\n",
       "                       1.9793e-03,  8.0559e-04,  6.7129e-04, -4.5511e-05, -1.1323e-02,\n",
       "                      -2.2209e-04, -5.1856e-03, -3.5264e-03,  3.5424e-04, -1.2716e-02,\n",
       "                      -4.6972e-03,  3.4042e-03, -1.2027e-02,  7.3188e-03,  9.3004e-04,\n",
       "                       1.1729e-03,  2.2730e-03, -5.4944e-03,  3.3608e-04,  4.2408e-03,\n",
       "                      -5.5617e-04,  2.0105e-03,  7.7987e-03,  2.6662e-04, -1.4715e-03,\n",
       "                       8.3655e-03, -6.9430e-03, -6.2166e-05,  2.8342e-03, -6.4054e-03,\n",
       "                      -1.2826e-02,  5.0170e-03, -4.3520e-03,  1.0741e-03,  3.2839e-03,\n",
       "                      -4.3013e-02, -4.0588e-04, -7.4201e-03, -1.0983e-02, -9.2777e-03,\n",
       "                      -1.5508e-03, -9.6340e-03,  4.0503e-03, -5.7457e-04, -9.6491e-06,\n",
       "                       4.7828e-04,  3.5452e-03, -5.9365e-03, -1.2278e-03, -1.5646e-02,\n",
       "                      -3.2936e-03, -2.7024e-03,  1.0760e-02, -1.7904e-02, -7.3542e-03,\n",
       "                      -1.6385e-03,  2.2151e-04, -1.4843e-03, -1.0578e-03, -1.7282e-03,\n",
       "                       3.3839e-03,  1.1922e-02,  2.4337e-03, -1.3993e-04,  5.4254e-03,\n",
       "                      -1.1997e-02, -1.3171e-02,  1.8965e-02,  1.8644e-03,  9.8458e-04,\n",
       "                      -3.1675e-03, -1.6657e-05, -2.1560e-02, -4.6995e-03,  1.1249e-02,\n",
       "                      -1.2418e-02, -5.0219e-03, -3.1921e-03,  2.4728e-03, -4.7977e-03,\n",
       "                      -9.6236e-03,  4.6686e-03,  3.6565e-04, -2.9535e-03, -3.9251e-03,\n",
       "                      -1.6444e-03,  1.0697e-03, -1.8061e-02, -5.6664e-03, -3.1767e-03,\n",
       "                       3.7204e-04,  2.5605e-03, -7.4643e-05,  1.2508e-02,  1.3134e-03,\n",
       "                       3.2286e-03, -4.2616e-03,  2.2920e-03, -1.4324e-03, -1.2761e-04,\n",
       "                      -7.5333e-03,  1.2793e-03, -5.4966e-03, -6.7626e-03, -4.3064e-03,\n",
       "                      -4.0414e-03,  1.6668e-02, -9.4661e-03, -1.0578e-02,  2.6424e-03,\n",
       "                       1.1091e-04, -9.6069e-03, -3.3790e-03, -6.3149e-03,  3.6998e-03,\n",
       "                       1.2868e-02,  3.8117e-03,  2.9159e-04,  2.1333e-03,  4.6065e-03,\n",
       "                      -1.0530e-05,  7.5509e-04, -7.5010e-03,  1.5466e-03, -2.0488e-03,\n",
       "                       4.7530e-04,  5.6903e-03,  3.2981e-04,  2.5079e-04,  1.0868e-03,\n",
       "                       2.7899e-03,  2.6885e-03, -2.6828e-03, -1.1807e-02, -2.9977e-03,\n",
       "                       8.7414e-03, -4.5113e-03,  3.4237e-03,  7.2062e-03, -2.0829e-02,\n",
       "                       1.0535e-02,  3.9924e-04,  1.3858e-04,  8.2416e-04,  3.7752e-03,\n",
       "                      -8.9251e-04, -2.7968e-03,  2.2945e-03, -1.9543e-03, -4.0379e-03,\n",
       "                      -5.8858e-04,  3.3450e-03,  2.9457e-03, -7.3587e-03,  5.9817e-03,\n",
       "                       2.3019e-03, -3.9335e-03, -2.1926e-03,  1.8292e-03,  1.0966e-03,\n",
       "                       9.0545e-04, -4.5191e-03, -1.9060e-04,  2.2169e-04,  4.7383e-03,\n",
       "                      -1.4634e-02, -3.1216e-03,  1.0545e-02,  3.2133e-03, -3.4848e-04,\n",
       "                       1.6413e-03, -9.6931e-03,  2.5043e-04,  6.3285e-03, -7.0166e-03,\n",
       "                       4.0290e-03,  1.0091e-03,  1.3394e-02, -1.1008e-02,  2.1748e-03,\n",
       "                       3.1571e-04, -3.7373e-04,  1.1608e-03,  7.4865e-05, -1.8456e-03,\n",
       "                       2.0625e-02,  4.0979e-03,  5.4326e-03, -5.2925e-03, -2.3632e-03,\n",
       "                      -6.9820e-03,  4.3603e-04, -1.1994e-03,  4.5588e-04,  4.9791e-04,\n",
       "                      -6.9573e-03,  6.9272e-03, -4.8201e-04, -2.1880e-02, -1.2747e-03,\n",
       "                      -5.4863e-03,  3.1933e-03, -2.9003e-03, -4.4139e-03, -9.5624e-03,\n",
       "                      -5.0867e-03,  2.8521e-03, -6.4422e-03,  1.8562e-03,  5.1158e-04,\n",
       "                       1.3367e-03, -1.4747e-03, -1.6701e-03, -9.8604e-03, -1.2021e-02,\n",
       "                       1.4948e-03, -5.6453e-03, -3.3841e-04,  1.2258e-03,  7.5452e-04,\n",
       "                       7.9467e-03,  1.4135e-02,  6.6403e-03,  5.7770e-03,  8.5405e-03,\n",
       "                       6.8018e-03,  1.7272e-03,  1.7230e-03,  3.5712e-03, -2.7443e-03,\n",
       "                      -2.3880e-03, -5.9505e-04, -9.6646e-05, -4.1440e-03,  1.1803e-03,\n",
       "                      -6.7725e-04,  3.4888e-04, -1.0021e-02,  2.6959e-03, -1.3507e-02,\n",
       "                      -1.1461e-02, -8.8307e-04,  2.2567e-04, -4.0710e-03,  6.0882e-04,\n",
       "                       6.2823e-04,  1.5150e-03, -9.9341e-06,  1.8180e-03,  4.8642e-04,\n",
       "                       5.5848e-03, -9.9228e-03, -5.4263e-03,  6.1382e-03,  2.8553e-03,\n",
       "                       8.4556e-04,  2.8484e-03,  2.5921e-03,  8.4182e-03,  1.2859e-02,\n",
       "                       1.6595e-02, -1.0637e-02, -7.8382e-03, -3.3120e-03,  2.8840e-03,\n",
       "                      -2.0948e-03, -8.6498e-04,  5.5509e-03,  5.8815e-03, -1.6177e-03,\n",
       "                      -3.9477e-03,  4.7987e-03, -1.0389e-03, -7.0149e-05, -7.7898e-03,\n",
       "                       1.6811e-03,  3.0433e-03, -4.8901e-04,  5.4578e-03,  7.0461e-03,\n",
       "                      -1.8546e-03,  1.1604e-03, -1.5749e-03,  7.6854e-03,  1.8878e-02,\n",
       "                      -6.4374e-04,  6.8219e-04,  1.6996e-03, -7.6384e-04, -5.8380e-03,\n",
       "                       7.3565e-03, -1.4423e-02, -2.4093e-03,  5.0464e-04,  2.1280e-02,\n",
       "                      -2.3660e-03,  1.0771e-02, -1.8646e-03, -3.2418e-03,  9.4284e-03,\n",
       "                      -2.7602e-02,  8.7297e-03,  4.2488e-03, -1.4558e-02, -9.1119e-03,\n",
       "                       2.1341e-02,  2.7582e-03,  4.9209e-03,  3.4250e-02,  2.0267e-03,\n",
       "                       9.7634e-04,  4.4769e-03, -1.2272e-04,  8.4565e-03, -2.6390e-03,\n",
       "                      -1.1450e-03, -7.0468e-03,  2.7676e-03,  2.5561e-03, -7.5363e-03,\n",
       "                       5.6267e-03,  1.4464e-02,  4.2864e-04,  5.3869e-03, -3.0765e-03,\n",
       "                      -3.9420e-03,  1.3478e-03,  1.5731e-03, -4.3896e-04,  2.2040e-03,\n",
       "                       2.5814e-03,  1.7449e-02, -3.2647e-03,  1.0181e-02,  2.7082e-04,\n",
       "                       8.6963e-04, -5.2125e-04,  3.5072e-03,  6.6047e-03, -1.0435e-03,\n",
       "                       1.6396e-03,  2.5491e-03, -7.9833e-04, -8.6448e-04, -4.4443e-03,\n",
       "                       1.3140e-02,  5.1416e-03,  4.1446e-04,  3.8270e-03, -1.8391e-03,\n",
       "                      -1.5602e-02, -9.0405e-04, -1.3802e-03, -2.3128e-03,  5.2853e-04,\n",
       "                       4.5012e-03, -1.3640e-03, -7.2092e-03, -8.2150e-04, -6.4899e-05,\n",
       "                       3.8575e-03, -1.0200e-02, -7.1281e-03,  5.2401e-03, -2.0877e-03,\n",
       "                       6.9908e-03,  6.7621e-03, -9.8170e-04,  1.4607e-03, -2.0658e-03,\n",
       "                      -3.7026e-03, -7.0059e-04, -2.5679e-03, -5.3969e-03,  2.4014e-03,\n",
       "                      -4.7809e-03,  1.4555e-03,  1.3605e-03, -1.1693e-03,  6.8036e-03,\n",
       "                       3.6813e-03,  6.4592e-03, -1.1231e-03, -4.9509e-04,  4.6137e-03,\n",
       "                       2.6999e-04, -9.8198e-03,  3.4192e-04,  9.5618e-03,  1.1302e-02,\n",
       "                      -7.8702e-03,  1.9816e-03, -2.6249e-03,  1.1380e-03,  8.8498e-03,\n",
       "                       6.2617e-03, -4.2785e-04, -1.9899e-03,  2.2393e-03, -1.0585e-03,\n",
       "                       3.5389e-03,  1.0680e-03])),\n",
       "             ('layer2.1.bn3.running_var',\n",
       "              tensor([9.3110e-05, 6.0930e-05, 3.2052e-04, 5.4561e-05, 6.2170e-05, 9.5108e-05,\n",
       "                      1.5989e-04, 1.9828e-04, 3.6229e-05, 1.0033e-04, 8.5662e-05, 1.3704e-04,\n",
       "                      2.6155e-05, 8.1733e-05, 4.2773e-05, 1.7263e-05, 1.9768e-05, 2.7236e-04,\n",
       "                      7.1104e-05, 1.2922e-04, 2.5954e-05, 1.7710e-04, 1.1770e-04, 6.5721e-05,\n",
       "                      3.7385e-05, 3.7169e-04, 2.1599e-05, 1.8556e-05, 1.5373e-04, 5.7093e-05,\n",
       "                      1.5351e-04, 3.8540e-06, 1.5750e-04, 2.7308e-04, 7.3374e-06, 6.4545e-05,\n",
       "                      2.2523e-04, 2.7158e-05, 3.2173e-05, 2.8874e-05, 1.2338e-04, 4.1148e-05,\n",
       "                      1.0286e-05, 3.5337e-05, 2.0807e-04, 2.7697e-04, 9.7236e-05, 1.7324e-04,\n",
       "                      7.2255e-06, 9.5459e-05, 3.0022e-04, 6.8963e-04, 1.5071e-06, 9.6184e-06,\n",
       "                      2.2519e-05, 6.4672e-06, 9.7336e-06, 1.6356e-09, 1.5069e-04, 1.0208e-04,\n",
       "                      1.0674e-05, 3.6658e-05, 1.2613e-04, 6.3095e-04, 1.7116e-05, 1.8957e-04,\n",
       "                      1.3393e-04, 1.8091e-04, 1.4850e-04, 7.3655e-05, 7.1490e-05, 1.0141e-05,\n",
       "                      3.2384e-05, 2.4989e-04, 5.6814e-05, 3.9628e-05, 8.5749e-05, 1.2879e-04,\n",
       "                      3.1227e-04, 5.8194e-05, 3.1934e-05, 2.4606e-04, 5.7125e-06, 6.8854e-05,\n",
       "                      6.6829e-05, 1.4204e-05, 7.3325e-05, 1.4594e-04, 2.7040e-05, 9.9150e-05,\n",
       "                      3.6207e-04, 1.3647e-04, 7.7621e-05, 2.4715e-04, 2.2156e-04, 5.6124e-05,\n",
       "                      5.0482e-06, 8.1972e-05, 2.7958e-05, 2.6004e-05, 4.1800e-06, 1.0573e-04,\n",
       "                      8.5902e-05, 3.9290e-06, 4.9566e-05, 1.5972e-05, 3.4816e-04, 1.0900e-04,\n",
       "                      2.1087e-05, 1.2845e-04, 4.3150e-04, 9.0704e-05, 1.5068e-04, 2.6702e-06,\n",
       "                      2.3415e-04, 2.3500e-05, 1.6304e-05, 6.9126e-05, 5.6884e-06, 4.4049e-05,\n",
       "                      7.6531e-05, 4.7153e-05, 1.6189e-04, 6.6242e-05, 3.5278e-04, 1.1259e-04,\n",
       "                      2.2574e-05, 1.1211e-04, 7.5945e-05, 8.3718e-05, 2.8029e-05, 5.6845e-06,\n",
       "                      2.8487e-04, 8.2653e-05, 1.1836e-05, 7.4533e-05, 1.0231e-04, 9.2589e-09,\n",
       "                      1.5727e-05, 3.2801e-04, 1.7678e-04, 1.7644e-04, 4.0908e-05, 3.4213e-05,\n",
       "                      2.1610e-04, 6.3252e-04, 4.0537e-06, 4.4091e-04, 4.0769e-05, 2.5991e-04,\n",
       "                      1.8149e-05, 2.0921e-04, 1.1357e-04, 6.4899e-06, 3.2892e-06, 9.1888e-05,\n",
       "                      3.6821e-05, 1.0542e-04, 1.7401e-04, 8.7295e-05, 1.4131e-05, 1.8431e-05,\n",
       "                      3.1212e-04, 1.1236e-03, 4.0932e-04, 1.3176e-05, 4.0369e-05, 2.5430e-05,\n",
       "                      4.9425e-05, 1.1279e-04, 8.6280e-05, 3.4669e-04, 1.1696e-04, 1.3201e-05,\n",
       "                      2.0876e-04, 1.8467e-04, 1.0952e-04, 2.1199e-04, 9.7547e-05, 3.3322e-05,\n",
       "                      1.9906e-05, 1.7579e-05, 1.9090e-04, 3.5536e-05, 6.3727e-05, 6.8309e-05,\n",
       "                      6.8643e-05, 6.3544e-05, 1.2552e-04, 5.7309e-05, 3.5268e-04, 3.6134e-05,\n",
       "                      5.3696e-06, 3.4240e-04, 7.9765e-05, 4.5941e-05, 5.1769e-06, 1.1676e-04,\n",
       "                      7.4433e-05, 1.6782e-04, 1.5800e-05, 2.2793e-05, 2.5656e-05, 1.4171e-04,\n",
       "                      6.5255e-06, 2.0390e-04, 1.6581e-04, 4.4698e-05, 1.1741e-04, 3.6352e-04,\n",
       "                      3.4980e-05, 8.9858e-05, 1.6591e-04, 3.9418e-05, 6.5446e-04, 4.2528e-04,\n",
       "                      1.2573e-04, 7.0414e-05, 6.4456e-05, 3.2371e-05, 6.7182e-06, 1.2174e-04,\n",
       "                      2.5187e-05, 2.2805e-04, 6.8763e-05, 2.2146e-04, 6.2159e-05, 3.0868e-04,\n",
       "                      5.3772e-05, 4.8169e-05, 6.0173e-06, 2.2492e-05, 5.4566e-05, 6.4394e-06,\n",
       "                      3.7068e-05, 1.9737e-05, 3.6613e-05, 6.1822e-07, 1.3964e-07, 2.0591e-04,\n",
       "                      2.3313e-04, 2.4457e-05, 4.7239e-05, 4.2998e-04, 9.2478e-05, 3.6193e-04,\n",
       "                      1.7308e-04, 2.7435e-05, 2.0041e-04, 3.1786e-04, 3.7699e-05, 8.9418e-08,\n",
       "                      1.5132e-05, 2.0470e-05, 1.2922e-05, 8.9094e-05, 1.8241e-05, 1.0663e-04,\n",
       "                      3.9059e-05, 3.7707e-04, 1.0124e-05, 1.1376e-04, 3.9931e-04, 6.9774e-05,\n",
       "                      2.4515e-04, 1.1157e-04, 1.5669e-04, 8.0314e-06, 1.7320e-04, 3.8701e-05,\n",
       "                      3.1032e-05, 1.4439e-04, 2.4521e-06, 5.0660e-04, 1.2471e-04, 6.3877e-05,\n",
       "                      2.9177e-04, 7.7269e-05, 2.0279e-04, 2.4231e-05, 1.2939e-04, 2.7296e-04,\n",
       "                      2.2635e-04, 7.3728e-05, 1.4684e-04, 1.4543e-04, 1.8839e-04, 1.4791e-04,\n",
       "                      5.9827e-05, 1.5770e-04, 3.8940e-06, 5.2322e-06, 8.4217e-05, 1.0314e-05,\n",
       "                      1.9096e-04, 2.5441e-04, 2.2504e-05, 2.8440e-05, 9.2477e-05, 3.2579e-05,\n",
       "                      3.3823e-04, 3.8725e-05, 1.4005e-05, 8.1205e-05, 6.5115e-05, 1.5141e-04,\n",
       "                      6.9850e-05, 7.1617e-06, 4.5067e-04, 4.5459e-04, 7.8200e-05, 6.9159e-05,\n",
       "                      9.5288e-05, 1.0332e-04, 2.2867e-04, 1.3077e-05, 6.2629e-05, 2.9732e-04,\n",
       "                      9.0230e-06, 8.6956e-06, 9.1950e-05, 2.4426e-05, 1.8205e-05, 1.9180e-04,\n",
       "                      8.3942e-04, 6.3361e-05, 5.1989e-05, 6.9839e-06, 3.1290e-05, 3.6753e-05,\n",
       "                      2.6398e-04, 6.2918e-05, 8.9589e-05, 4.5494e-05, 8.7407e-05, 4.5817e-05,\n",
       "                      1.2090e-04, 4.0802e-05, 2.3988e-04, 1.5903e-04, 3.1638e-04, 1.5445e-05,\n",
       "                      2.6060e-05, 1.9069e-05, 9.4110e-06, 2.3970e-05, 5.3854e-06, 1.9840e-04,\n",
       "                      1.0133e-04, 1.2029e-04, 1.7547e-04, 1.2509e-04, 5.5543e-05, 1.7707e-05,\n",
       "                      5.8571e-06, 4.1156e-05, 2.2140e-05, 1.4635e-04, 1.2596e-04, 3.4536e-04,\n",
       "                      2.7625e-05, 1.4283e-05, 6.2999e-06, 1.7557e-04, 1.0337e-04, 1.5061e-04,\n",
       "                      1.1334e-04, 1.2035e-04, 7.0580e-05, 3.3666e-04, 1.9997e-04, 7.4572e-05,\n",
       "                      3.4243e-05, 4.1082e-05, 4.5591e-05, 8.9291e-05, 1.0748e-05, 3.4452e-05,\n",
       "                      6.5686e-05, 4.8073e-05, 2.3020e-04, 5.8182e-05, 3.6165e-05, 1.2024e-05,\n",
       "                      1.0783e-04, 1.5130e-05, 4.8711e-05, 2.8072e-05, 1.2407e-04, 2.3708e-04,\n",
       "                      5.7435e-06, 4.3824e-06, 5.8501e-05, 1.4596e-04, 3.6379e-04, 1.4846e-05,\n",
       "                      5.4908e-06, 2.1674e-05, 3.6942e-05, 1.8076e-05, 1.1276e-04, 3.0487e-04,\n",
       "                      1.4599e-04, 6.4466e-05, 3.5802e-04, 6.3922e-05, 9.4736e-05, 8.6426e-05,\n",
       "                      4.0086e-06, 7.3861e-05, 3.1155e-04, 4.1216e-05, 1.4496e-04, 1.9820e-04,\n",
       "                      3.0704e-04, 2.5493e-04, 9.0562e-06, 1.9735e-04, 2.6285e-04, 3.1123e-05,\n",
       "                      3.9468e-04, 5.0450e-04, 1.5254e-04, 1.8726e-04, 1.2319e-04, 1.7259e-05,\n",
       "                      5.9590e-05, 2.9580e-05, 1.0251e-04, 1.0581e-04, 1.2890e-04, 6.5411e-04,\n",
       "                      1.9197e-05, 1.0520e-05, 8.2318e-06, 8.1378e-05, 1.0906e-05, 3.2487e-05,\n",
       "                      2.1995e-04, 1.9082e-05, 5.0997e-06, 2.2389e-04, 2.0674e-04, 2.4666e-04,\n",
       "                      2.5313e-05, 1.0025e-04, 3.6258e-05, 2.2870e-04, 7.4254e-05, 1.6501e-05,\n",
       "                      3.5640e-05, 3.9147e-05, 2.0582e-05, 1.7595e-05, 1.1704e-05, 4.5218e-04,\n",
       "                      1.3690e-05, 1.5858e-05, 5.5530e-06, 1.1947e-04, 3.8616e-04, 1.5142e-04,\n",
       "                      1.2268e-04, 3.9641e-05, 4.6344e-05, 7.1670e-05, 4.6438e-06, 1.6416e-04,\n",
       "                      2.0537e-05, 1.9005e-04, 2.3346e-04, 8.1595e-05, 2.1183e-04, 1.3401e-04,\n",
       "                      1.8863e-05, 1.9823e-05, 1.5817e-04, 2.4329e-05, 1.2078e-04, 1.1512e-04,\n",
       "                      1.8356e-05, 1.1532e-05, 2.0490e-05, 1.6865e-04, 1.9776e-04, 1.1931e-04,\n",
       "                      1.6086e-05, 7.9545e-06, 2.2311e-06, 7.2220e-04, 1.2976e-04, 1.8345e-05,\n",
       "                      1.4451e-04, 3.3599e-06, 4.6916e-05, 2.7231e-06, 6.6566e-05, 3.8914e-06,\n",
       "                      1.9791e-04, 1.7084e-04, 5.5359e-04, 2.6241e-05, 1.1983e-04, 8.0149e-05,\n",
       "                      4.7470e-05, 2.0178e-04, 1.0823e-04, 2.0782e-05, 4.0852e-05, 3.5235e-05,\n",
       "                      4.6967e-05, 1.5251e-04])),\n",
       "             ('layer2.1.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.2.conv1.weight',\n",
       "              tensor([[[[-0.0072]],\n",
       "              \n",
       "                       [[ 0.0009]],\n",
       "              \n",
       "                       [[-0.0074]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0021]],\n",
       "              \n",
       "                       [[-0.0138]],\n",
       "              \n",
       "                       [[ 0.0056]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0039]],\n",
       "              \n",
       "                       [[ 0.0001]],\n",
       "              \n",
       "                       [[-0.0130]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0161]],\n",
       "              \n",
       "                       [[-0.0029]],\n",
       "              \n",
       "                       [[ 0.0150]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0053]],\n",
       "              \n",
       "                       [[-0.0049]],\n",
       "              \n",
       "                       [[-0.0076]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0071]],\n",
       "              \n",
       "                       [[-0.0025]],\n",
       "              \n",
       "                       [[ 0.0034]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0091]],\n",
       "              \n",
       "                       [[ 0.0095]],\n",
       "              \n",
       "                       [[-0.0013]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0219]],\n",
       "              \n",
       "                       [[-0.0007]],\n",
       "              \n",
       "                       [[-0.0012]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0040]],\n",
       "              \n",
       "                       [[-0.0095]],\n",
       "              \n",
       "                       [[-0.0236]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0164]],\n",
       "              \n",
       "                       [[ 0.0042]],\n",
       "              \n",
       "                       [[-0.0217]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0066]],\n",
       "              \n",
       "                       [[-0.0120]],\n",
       "              \n",
       "                       [[-0.0223]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0183]],\n",
       "              \n",
       "                       [[ 0.0069]],\n",
       "              \n",
       "                       [[ 0.0021]]]])),\n",
       "             ('layer2.2.bn1.weight',\n",
       "              tensor([0.0677, 0.0652, 0.0811, 0.0763, 0.0626, 0.0806, 0.0991, 0.0682, 0.1001,\n",
       "                      0.0876, 0.0666, 0.0558, 0.0895, 0.0846, 0.0592, 0.1075, 0.0988, 0.0919,\n",
       "                      0.1172, 0.0886, 0.0600, 0.0685, 0.1096, 0.0422, 0.0997, 0.0733, 0.0634,\n",
       "                      0.0720, 0.0245, 0.0741, 0.0599, 0.0781, 0.1210, 0.0695, 0.1062, 0.0569,\n",
       "                      0.0874, 0.0982, 0.0639, 0.0753, 0.1071, 0.0801, 0.1021, 0.0508, 0.1001,\n",
       "                      0.1106, 0.0300, 0.1141, 0.0866, 0.0984, 0.0470, 0.0986, 0.0575, 0.0787,\n",
       "                      0.0832, 0.0816, 0.0351, 0.1125, 0.0721, 0.0977, 0.0783, 0.0733, 0.0461,\n",
       "                      0.0865, 0.0615, 0.0730, 0.1115, 0.0623, 0.0511, 0.0578, 0.0950, 0.0684,\n",
       "                      0.0735, 0.0809, 0.0863, 0.1046, 0.0786, 0.0917, 0.0873, 0.0791, 0.0816,\n",
       "                      0.0580, 0.0466, 0.0804, 0.0903, 0.0887, 0.1096, 0.0523, 0.1385, 0.0811,\n",
       "                      0.0307, 0.0558, 0.0552, 0.0843, 0.0800, 0.0791, 0.0927, 0.0783, 0.0351,\n",
       "                      0.0730, 0.0735, 0.0971, 0.0860, 0.0690, 0.0969, 0.0274, 0.0796, 0.0880,\n",
       "                      0.0716, 0.1185, 0.0266, 0.0881, 0.0452, 0.0489, 0.0594, 0.0787, 0.0775,\n",
       "                      0.0740, 0.0927, 0.0857, 0.0874, 0.0917, 0.0792, 0.0327, 0.0771, 0.0562,\n",
       "                      0.0862, 0.1252])),\n",
       "             ('layer2.2.bn1.bias',\n",
       "              tensor([ 0.0018, -0.0232,  0.0448, -0.0344,  0.0198, -0.0307,  0.0248,  0.0168,\n",
       "                      -0.0020, -0.0186,  0.0474,  0.0839, -0.0267,  0.0012,  0.0271, -0.0107,\n",
       "                      -0.0188, -0.0060, -0.0314,  0.0014, -0.0289, -0.0030, -0.0423,  0.0715,\n",
       "                      -0.0166, -0.0099,  0.0462, -0.0350,  0.0227,  0.0096, -0.0185,  0.0574,\n",
       "                      -0.0080, -0.0063, -0.0113, -0.0244, -0.0174, -0.0191,  0.0967,  0.0310,\n",
       "                      -0.0182, -0.0324,  0.0519,  0.0232, -0.0092, -0.0189,  0.0055, -0.0145,\n",
       "                      -0.0011, -0.0351, -0.0199, -0.0798, -0.0032, -0.0210,  0.0020,  0.0215,\n",
       "                       0.0377, -0.0681,  0.0400,  0.0162,  0.0250,  0.0091,  0.0148,  0.0319,\n",
       "                      -0.0438, -0.0063, -0.0250,  0.0199, -0.0079, -0.0069, -0.0106, -0.0173,\n",
       "                       0.0045,  0.0029,  0.0187,  0.0174, -0.0136, -0.0102, -0.0430,  0.0043,\n",
       "                       0.0296,  0.0026,  0.0075, -0.0474,  0.0278,  0.0167, -0.0741, -0.0040,\n",
       "                      -0.0166,  0.0407,  0.0334, -0.0080,  0.0053,  0.0159, -0.0396,  0.0385,\n",
       "                       0.0049,  0.0764,  0.0064,  0.0190,  0.0149,  0.0071, -0.0025,  0.0081,\n",
       "                      -0.0244,  0.0083, -0.0562, -0.0040,  0.0238, -0.0596,  0.0017,  0.0159,\n",
       "                      -0.0197, -0.0162,  0.0186,  0.0370, -0.0085, -0.0189,  0.0769, -0.0010,\n",
       "                      -0.0002, -0.0089,  0.0120,  0.0439, -0.0102,  0.0464, -0.0002,  0.0555])),\n",
       "             ('layer2.2.bn1.running_mean',\n",
       "              tensor([-4.3950e-03,  5.6339e-03, -1.8946e-04,  6.1254e-03, -1.6081e-02,\n",
       "                      -1.0267e-02, -2.2204e-03, -1.1437e-02, -2.6320e-03, -5.3134e-02,\n",
       "                       6.5046e-03,  4.2810e-03,  6.9920e-03, -1.4101e-02, -1.7265e-02,\n",
       "                      -3.2387e-04, -1.9878e-02,  1.5945e-02,  9.3759e-03, -4.7624e-02,\n",
       "                      -5.5090e-07, -1.9953e-02,  2.4760e-02, -2.0802e-02, -1.2956e-03,\n",
       "                      -3.2133e-02, -2.1198e-02,  8.8987e-04, -2.9327e-03, -1.4115e-02,\n",
       "                      -1.1622e-03, -5.8744e-02, -2.3043e-02, -3.0631e-02, -4.5328e-02,\n",
       "                       2.9986e-02, -3.0837e-02, -3.6091e-02, -1.0381e-02,  3.4271e-02,\n",
       "                      -1.5849e-02, -2.2812e-02, -2.3045e-02,  7.4865e-03, -5.2208e-02,\n",
       "                       1.3833e-03,  1.2976e-02, -1.7427e-02, -3.4673e-02, -5.5374e-02,\n",
       "                      -2.5487e-03,  2.0723e-02,  1.7576e-02, -1.7486e-02, -1.6131e-02,\n",
       "                      -4.7255e-02,  1.8639e-02,  1.6196e-03, -8.4449e-03, -2.9198e-02,\n",
       "                      -1.4054e-02, -1.3170e-02, -3.1349e-02, -2.8161e-03,  2.9529e-02,\n",
       "                      -2.3530e-02, -4.1969e-03, -4.0982e-02,  1.6398e-02,  7.8101e-04,\n",
       "                      -3.2214e-02, -5.3200e-02, -2.2127e-02, -5.0080e-03, -1.7268e-02,\n",
       "                       5.6550e-03, -1.3851e-02, -3.8967e-02, -1.7917e-02, -4.1861e-02,\n",
       "                      -1.2832e-02,  2.1331e-02,  9.0565e-03,  1.6254e-02,  4.5178e-02,\n",
       "                      -4.7096e-02,  2.9441e-02, -1.5820e-02, -1.2811e-02, -4.2669e-02,\n",
       "                       1.0812e-02, -4.7748e-02,  1.8458e-02, -3.5055e-02, -2.6450e-02,\n",
       "                       1.3496e-02,  8.5098e-03,  9.9537e-03,  1.4163e-02, -4.1587e-02,\n",
       "                      -1.6648e-02, -2.2780e-02,  1.5528e-02, -5.5564e-02, -9.5865e-03,\n",
       "                      -6.3957e-03,  1.2811e-02, -1.5096e-02, -1.3270e-02, -1.9449e-02,\n",
       "                      -1.4412e-03, -3.4682e-02, -2.1217e-03,  1.6259e-02, -3.2665e-02,\n",
       "                      -4.5097e-02,  4.4420e-03, -1.4449e-02,  1.3862e-02, -2.8786e-02,\n",
       "                      -2.1578e-02,  1.6036e-02, -3.5454e-02, -1.0609e-02,  9.8526e-04,\n",
       "                      -9.8642e-03, -5.9316e-02, -2.2208e-02])),\n",
       "             ('layer2.2.bn1.running_var',\n",
       "              tensor([0.0020, 0.0014, 0.0039, 0.0016, 0.0029, 0.0030, 0.0041, 0.0024, 0.0049,\n",
       "                      0.0039, 0.0030, 0.0036, 0.0012, 0.0024, 0.0022, 0.0026, 0.0044, 0.0030,\n",
       "                      0.0061, 0.0023, 0.0015, 0.0030, 0.0045, 0.0027, 0.0033, 0.0028, 0.0025,\n",
       "                      0.0014, 0.0007, 0.0031, 0.0006, 0.0032, 0.0042, 0.0025, 0.0038, 0.0008,\n",
       "                      0.0023, 0.0036, 0.0064, 0.0027, 0.0032, 0.0021, 0.0045, 0.0016, 0.0045,\n",
       "                      0.0031, 0.0003, 0.0032, 0.0026, 0.0035, 0.0006, 0.0030, 0.0012, 0.0024,\n",
       "                      0.0031, 0.0037, 0.0011, 0.0017, 0.0037, 0.0027, 0.0027, 0.0037, 0.0019,\n",
       "                      0.0036, 0.0013, 0.0016, 0.0031, 0.0022, 0.0007, 0.0007, 0.0028, 0.0017,\n",
       "                      0.0018, 0.0037, 0.0037, 0.0024, 0.0020, 0.0028, 0.0027, 0.0017, 0.0046,\n",
       "                      0.0011, 0.0009, 0.0016, 0.0044, 0.0040, 0.0027, 0.0009, 0.0069, 0.0030,\n",
       "                      0.0010, 0.0008, 0.0010, 0.0034, 0.0013, 0.0043, 0.0043, 0.0041, 0.0005,\n",
       "                      0.0020, 0.0034, 0.0041, 0.0024, 0.0016, 0.0027, 0.0004, 0.0026, 0.0036,\n",
       "                      0.0029, 0.0030, 0.0003, 0.0037, 0.0004, 0.0007, 0.0014, 0.0038, 0.0025,\n",
       "                      0.0028, 0.0111, 0.0025, 0.0029, 0.0031, 0.0019, 0.0014, 0.0009, 0.0034,\n",
       "                      0.0025, 0.0047])),\n",
       "             ('layer2.2.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.2.conv2.weight',\n",
       "              tensor([[[[-4.3354e-04,  9.8884e-03, -6.6173e-03],\n",
       "                        [-9.2749e-03, -1.4058e-02, -6.0677e-04],\n",
       "                        [ 1.2998e-02, -2.8563e-03, -6.7136e-03]],\n",
       "              \n",
       "                       [[-3.7172e-03,  1.3033e-02,  5.7746e-03],\n",
       "                        [-9.0244e-03, -1.7235e-02, -7.3267e-03],\n",
       "                        [-8.8719e-03, -1.4859e-03,  3.2213e-03]],\n",
       "              \n",
       "                       [[-5.3441e-04,  1.2538e-02,  6.4231e-03],\n",
       "                        [-1.2117e-02, -1.4433e-02, -4.3686e-03],\n",
       "                        [-5.6292e-03,  9.1137e-03, -3.3944e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3778e-02, -1.7571e-02, -1.7248e-02],\n",
       "                        [ 1.3815e-02,  1.1478e-02,  1.5616e-02],\n",
       "                        [-4.7937e-03, -9.0506e-03, -3.9826e-03]],\n",
       "              \n",
       "                       [[ 1.4106e-03, -7.9025e-03, -1.5773e-02],\n",
       "                        [-1.5726e-03, -1.5509e-02,  2.7919e-03],\n",
       "                        [-7.2050e-03, -1.3563e-02,  6.5914e-03]],\n",
       "              \n",
       "                       [[-1.3886e-02, -1.9619e-02, -1.3605e-02],\n",
       "                        [ 2.6255e-02,  6.1339e-03,  1.8105e-02],\n",
       "                        [ 8.2216e-03, -1.1667e-02,  6.7000e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2259e-03,  9.2812e-03,  7.1135e-03],\n",
       "                        [-8.1834e-03,  9.1676e-03,  4.5741e-03],\n",
       "                        [-5.3734e-03, -5.5789e-03, -4.0631e-03]],\n",
       "              \n",
       "                       [[ 2.4544e-03, -3.1285e-03, -5.2244e-03],\n",
       "                        [ 3.8156e-03,  5.6971e-03,  7.3352e-03],\n",
       "                        [-3.6582e-03, -1.2091e-03, -4.8971e-04]],\n",
       "              \n",
       "                       [[-3.0960e-03, -1.5613e-02, -1.1929e-02],\n",
       "                        [ 2.9488e-03, -2.7642e-03,  9.1213e-03],\n",
       "                        [ 1.9361e-02,  1.2021e-02,  2.2620e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.4146e-03,  1.0530e-02,  1.0598e-02],\n",
       "                        [-4.9645e-03, -2.5430e-03, -6.1665e-03],\n",
       "                        [-7.1947e-03, -9.2371e-03, -8.2241e-03]],\n",
       "              \n",
       "                       [[-3.8849e-03, -5.5185e-03,  6.8851e-03],\n",
       "                        [-9.3086e-03, -6.2371e-04,  1.2528e-02],\n",
       "                        [-2.2584e-03,  9.6945e-03,  1.4023e-02]],\n",
       "              \n",
       "                       [[ 2.4290e-02,  7.8241e-03,  1.1234e-02],\n",
       "                        [ 1.0853e-02,  1.9204e-02,  1.9353e-02],\n",
       "                        [ 6.1211e-03,  6.9985e-03,  1.0778e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.5030e-03,  3.6348e-03,  7.6617e-03],\n",
       "                        [-6.5676e-03, -3.4747e-03, -2.3139e-03],\n",
       "                        [ 1.5905e-03, -6.1185e-03, -5.0470e-03]],\n",
       "              \n",
       "                       [[ 1.2043e-03, -4.2110e-04,  1.1576e-03],\n",
       "                        [ 8.4545e-03, -4.6154e-03, -3.6954e-03],\n",
       "                        [ 5.2314e-03, -1.5144e-02, -7.9565e-03]],\n",
       "              \n",
       "                       [[-6.2858e-06, -7.4691e-04,  4.7703e-03],\n",
       "                        [ 1.1354e-02, -9.5605e-03, -1.7978e-02],\n",
       "                        [ 2.5545e-02, -2.7352e-03, -2.1028e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.6733e-03,  1.4919e-02,  6.3850e-03],\n",
       "                        [-1.4756e-02,  1.8125e-03,  1.1556e-02],\n",
       "                        [-1.6215e-02, -1.3818e-02,  2.3990e-03]],\n",
       "              \n",
       "                       [[-1.6320e-02,  4.7134e-03,  5.3048e-03],\n",
       "                        [-2.6863e-02, -5.4614e-03,  2.5167e-02],\n",
       "                        [-2.2918e-02, -2.3808e-02,  2.3530e-02]],\n",
       "              \n",
       "                       [[ 6.4286e-03, -7.7368e-04, -2.7108e-02],\n",
       "                        [ 2.6068e-02,  1.7449e-02,  3.4283e-03],\n",
       "                        [-1.6559e-02,  1.9183e-02,  1.1206e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-5.4788e-03, -1.3011e-02,  1.2668e-03],\n",
       "                        [ 2.6770e-03,  4.4805e-03,  1.9116e-02],\n",
       "                        [ 9.6679e-03,  1.6184e-02,  1.2414e-02]],\n",
       "              \n",
       "                       [[-3.7876e-03, -7.5996e-03, -1.8303e-03],\n",
       "                        [-1.3103e-02, -8.7845e-03,  3.3805e-03],\n",
       "                        [ 1.7900e-03, -4.5460e-03, -1.8304e-03]],\n",
       "              \n",
       "                       [[ 2.6199e-02,  3.3081e-03, -1.3253e-02],\n",
       "                        [ 1.2665e-02, -1.1663e-02, -4.9402e-03],\n",
       "                        [-8.3564e-04, -2.9168e-03,  4.0277e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1556e-02,  5.6437e-03,  2.0847e-02],\n",
       "                        [-2.1098e-03,  1.2365e-02, -6.4818e-03],\n",
       "                        [-1.6872e-03, -5.7146e-03, -1.5479e-02]],\n",
       "              \n",
       "                       [[-9.0082e-03, -1.6482e-02, -1.9408e-02],\n",
       "                        [-1.6652e-02, -1.6626e-02,  3.0689e-03],\n",
       "                        [ 5.1739e-03,  5.7663e-03, -3.6882e-05]],\n",
       "              \n",
       "                       [[ 2.2135e-02,  1.8104e-02, -7.5466e-03],\n",
       "                        [ 1.8266e-02,  6.2660e-03,  5.9138e-03],\n",
       "                        [-1.5431e-02,  4.0780e-03, -1.0617e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.9444e-03, -1.0958e-02, -1.8893e-04],\n",
       "                        [-8.0470e-03, -4.0430e-03,  1.5565e-02],\n",
       "                        [-1.0745e-04, -3.7744e-03,  4.7241e-03]],\n",
       "              \n",
       "                       [[ 5.9910e-03,  2.0420e-03, -5.4177e-03],\n",
       "                        [ 6.5932e-03, -2.5059e-03, -1.2034e-03],\n",
       "                        [-3.9858e-03, -8.8939e-03, -5.2644e-03]],\n",
       "              \n",
       "                       [[-7.6227e-03, -8.9518e-03, -2.0386e-02],\n",
       "                        [-3.0925e-03,  8.7968e-04, -8.1589e-03],\n",
       "                        [-1.8169e-03,  1.4302e-02, -9.2739e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.1190e-03, -8.5570e-03,  3.4734e-03],\n",
       "                        [ 6.4004e-03,  4.2343e-03,  9.6335e-03],\n",
       "                        [-3.5709e-03, -3.8887e-03, -4.6512e-04]],\n",
       "              \n",
       "                       [[-2.6679e-02, -8.6446e-03,  4.7842e-03],\n",
       "                        [ 2.9094e-03, -9.4246e-03, -6.5185e-03],\n",
       "                        [ 1.7162e-02, -2.7733e-03, -1.0629e-02]],\n",
       "              \n",
       "                       [[ 8.3559e-03,  1.0656e-02, -2.1112e-03],\n",
       "                        [-3.0767e-02,  1.1193e-03, -1.0370e-02],\n",
       "                        [-2.7330e-02,  6.2973e-03, -9.2815e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2684e-02, -1.4705e-03,  1.0792e-02],\n",
       "                        [-5.7765e-03,  4.1548e-04,  1.2030e-02],\n",
       "                        [-6.1340e-03, -6.9750e-03,  1.0597e-02]],\n",
       "              \n",
       "                       [[-1.7472e-03,  1.1845e-03, -6.3741e-04],\n",
       "                        [-2.2611e-03,  2.0131e-03,  4.0054e-05],\n",
       "                        [ 6.5271e-03, -1.4265e-03, -1.4887e-03]],\n",
       "              \n",
       "                       [[-6.3943e-03, -6.9893e-03, -2.0589e-04],\n",
       "                        [ 5.7107e-03, -8.7439e-03, -5.5363e-03],\n",
       "                        [ 1.3415e-02, -1.0411e-02, -4.4380e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.2957e-03,  9.8779e-03,  7.4877e-03],\n",
       "                        [-1.2352e-02,  3.2140e-03,  5.0407e-03],\n",
       "                        [-1.2672e-02,  2.0960e-03,  1.7763e-03]],\n",
       "              \n",
       "                       [[-3.4515e-03,  3.3234e-03,  9.4729e-03],\n",
       "                        [-1.3750e-02,  5.7463e-05,  1.9213e-02],\n",
       "                        [-1.5643e-02, -1.3193e-02,  1.3645e-02]],\n",
       "              \n",
       "                       [[ 1.6769e-02, -7.4400e-03, -2.3952e-02],\n",
       "                        [ 1.3685e-02,  9.1389e-03, -2.2579e-03],\n",
       "                        [ 1.2917e-02,  1.7808e-02, -1.2790e-04]]]])),\n",
       "             ('layer2.2.bn2.weight',\n",
       "              tensor([1.4800e-01, 8.2125e-02, 1.4131e-01, 1.1099e-01, 1.6734e-01, 1.2437e-01,\n",
       "                      4.5359e-02, 5.1519e-02, 1.3486e-01, 4.0288e-02, 7.6676e-02, 9.7466e-02,\n",
       "                      5.5128e-02, 7.8022e-02, 5.1421e-02, 6.4308e-02, 6.8763e-02, 9.0598e-02,\n",
       "                      9.2588e-02, 1.3994e-01, 1.3968e-04, 9.1469e-02, 6.8713e-02, 6.2126e-03,\n",
       "                      8.8867e-02, 6.2159e-04, 4.9498e-02, 4.7160e-02, 1.3837e-01, 1.2984e-01,\n",
       "                      7.9321e-02, 2.0121e-03, 5.0191e-02, 9.6823e-02, 7.8746e-02, 6.3640e-02,\n",
       "                      9.9675e-02, 1.0738e-01, 1.2071e-01, 9.2816e-02, 1.4022e-01, 1.1760e-01,\n",
       "                      7.7151e-02, 8.7086e-02, 3.5412e-02, 8.4601e-02, 9.3285e-02, 1.3289e-01,\n",
       "                      1.0419e-01, 1.1750e-01, 1.1321e-01, 1.1279e-01, 9.6837e-02, 8.3963e-02,\n",
       "                      1.1531e-01, 1.0981e-01, 1.3841e-01, 1.0800e-01, 7.9068e-02, 7.5440e-02,\n",
       "                      1.3430e-01, 9.6547e-02, 1.3979e-01, 1.1548e-01, 1.0519e-01, 1.5924e-01,\n",
       "                      6.8347e-02, 1.2097e-01, 9.3356e-02, 1.0113e-01, 4.4777e-02, 9.1386e-02,\n",
       "                      9.2766e-02, 1.2854e-01, 9.5604e-02, 8.7214e-02, 8.9694e-02, 1.0199e-01,\n",
       "                      1.2842e-01, 8.1384e-02, 7.6230e-02, 1.0579e-01, 9.5199e-02, 1.1173e-01,\n",
       "                      1.1571e-01, 8.8381e-02, 9.8564e-02, 9.8103e-02, 7.7324e-02, 1.2227e-01,\n",
       "                      1.4148e-01, 1.2180e-01, 1.0728e-01, 1.1229e-01, 1.1196e-01, 5.9754e-02,\n",
       "                      1.0489e-01, 8.1841e-02, 1.0316e-01, 9.7787e-02, 8.9135e-02, 8.4949e-02,\n",
       "                      1.0267e-01, 8.4572e-02, 9.2809e-02, 1.1971e-01, 1.1766e-01, 1.4177e-01,\n",
       "                      1.0318e-01, 9.4043e-02, 1.2279e-01, 5.8315e-02, 2.1602e-01, 9.0315e-02,\n",
       "                      1.2575e-01, 1.5438e-01, 9.6199e-02, 7.5667e-02, 1.3187e-01, 9.5009e-02,\n",
       "                      3.9849e-02, 8.0866e-02, 8.9218e-02, 1.6610e-01, 6.5006e-02, 1.3111e-01,\n",
       "                      1.0428e-01, 8.4527e-02])),\n",
       "             ('layer2.2.bn2.bias',\n",
       "              tensor([-0.0567, -0.0372, -0.0808, -0.0613, -0.0678, -0.0769, -0.0264, -0.0072,\n",
       "                      -0.0716, -0.0112, -0.0342, -0.0600,  0.0033, -0.0418,  0.0024, -0.0415,\n",
       "                      -0.0395, -0.0383, -0.0480, -0.1130, -0.0013, -0.0652, -0.0266, -0.0216,\n",
       "                      -0.0388, -0.0031, -0.0406, -0.0306, -0.0455, -0.0244, -0.0459, -0.0115,\n",
       "                      -0.0132, -0.0382, -0.0251, -0.0250, -0.0359, -0.0336, -0.0983, -0.0876,\n",
       "                      -0.0132, -0.0582, -0.0032, -0.0563, -0.0064, -0.0340, -0.0529, -0.0810,\n",
       "                      -0.0286, -0.0687, -0.0056, -0.1242, -0.0816, -0.0668, -0.1029, -0.0802,\n",
       "                      -0.0990, -0.0672, -0.0414, -0.0051, -0.0853, -0.0526, -0.0963, -0.0598,\n",
       "                      -0.0654, -0.0667,  0.0136, -0.0546, -0.0095, -0.0675, -0.0228, -0.0127,\n",
       "                      -0.0395, -0.0819, -0.0475, -0.0021, -0.0432, -0.0383, -0.0495, -0.0544,\n",
       "                      -0.0309, -0.0219, -0.0527, -0.0629, -0.0458, -0.0409, -0.0814, -0.0509,\n",
       "                      -0.0333, -0.0635, -0.0681, -0.0876, -0.0135, -0.0397, -0.0605,  0.0011,\n",
       "                      -0.0643, -0.0005, -0.0637,  0.0140, -0.0499, -0.0320, -0.0870, -0.0270,\n",
       "                      -0.0241, -0.0790, -0.0560, -0.0748, -0.0681, -0.0380, -0.0863, -0.0188,\n",
       "                      -0.1097, -0.0483, -0.0663, -0.1012, -0.0273,  0.0062, -0.0914, -0.0503,\n",
       "                      -0.0072, -0.0502, -0.0611, -0.1378, -0.0146, -0.0818, -0.0733, -0.0662])),\n",
       "             ('layer2.2.bn2.running_mean',\n",
       "              tensor([-0.0659,  0.0335, -0.0280, -0.0028, -0.0350, -0.0552, -0.0317, -0.0204,\n",
       "                      -0.0411, -0.0010, -0.0276,  0.0114, -0.0170,  0.0011, -0.0307, -0.0282,\n",
       "                      -0.0188, -0.0373, -0.0009, -0.0769, -0.0002,  0.0486, -0.0127, -0.0012,\n",
       "                      -0.0348, -0.0017,  0.0389, -0.0185, -0.0096, -0.0472, -0.0124, -0.0171,\n",
       "                      -0.0175, -0.0605, -0.0409, -0.0021,  0.0028, -0.0343, -0.0138, -0.0189,\n",
       "                      -0.0332, -0.0431, -0.0514, -0.0450, -0.0007, -0.0244, -0.0033, -0.0756,\n",
       "                      -0.0203,  0.0169, -0.0592, -0.0984, -0.0116, -0.0234, -0.0713, -0.0182,\n",
       "                      -0.0152, -0.0237, -0.0307, -0.0513, -0.0575, -0.0046, -0.0727, -0.0621,\n",
       "                      -0.0091, -0.0110, -0.0034,  0.0017, -0.0130, -0.0283, -0.0241, -0.0232,\n",
       "                      -0.0444, -0.0636,  0.0134, -0.0322, -0.0111, -0.0481, -0.0384,  0.0080,\n",
       "                       0.0156, -0.0049, -0.0061, -0.0146, -0.1043, -0.0221,  0.0039, -0.0448,\n",
       "                       0.0078, -0.0368, -0.0744, -0.0023, -0.0587,  0.0076, -0.0139, -0.0582,\n",
       "                      -0.0310, -0.0402,  0.0391, -0.0506, -0.0460,  0.0013, -0.0336, -0.0012,\n",
       "                       0.0023, -0.0443,  0.0004, -0.0306, -0.0447, -0.0055, -0.0315, -0.0040,\n",
       "                      -0.0702, -0.0300, -0.0263,  0.0042, -0.0473, -0.0259, -0.0148, -0.0242,\n",
       "                       0.0072, -0.0106, -0.0437, -0.0629, -0.0042, -0.0260, -0.0538,  0.0115])),\n",
       "             ('layer2.2.bn2.running_var',\n",
       "              tensor([2.8743e-03, 1.9889e-03, 4.8497e-03, 1.4800e-03, 6.2025e-03, 3.3522e-03,\n",
       "                      3.2090e-04, 1.0598e-03, 7.2100e-03, 1.5453e-04, 1.3756e-03, 1.5211e-03,\n",
       "                      7.0745e-04, 2.1378e-03, 1.3361e-03, 1.3770e-03, 1.7624e-03, 1.5576e-03,\n",
       "                      2.2238e-03, 4.0900e-03, 3.3417e-07, 1.7147e-03, 6.4623e-04, 1.7104e-04,\n",
       "                      2.2049e-03, 6.8879e-06, 6.8544e-04, 3.7523e-04, 6.0536e-03, 4.7470e-03,\n",
       "                      1.8357e-03, 6.7740e-05, 1.3379e-03, 1.5993e-03, 2.1661e-03, 1.3339e-03,\n",
       "                      3.1329e-03, 3.2763e-03, 3.7532e-03, 1.6038e-03, 4.8289e-03, 4.1207e-03,\n",
       "                      3.1930e-03, 2.2116e-03, 2.0322e-04, 1.1394e-03, 2.3673e-03, 3.1496e-03,\n",
       "                      3.0495e-03, 3.0962e-03, 4.9692e-03, 4.1105e-03, 1.4967e-03, 1.5754e-03,\n",
       "                      1.8518e-03, 2.8403e-03, 6.1756e-03, 1.6876e-03, 1.4827e-03, 3.7431e-03,\n",
       "                      2.0079e-03, 1.7847e-03, 5.7601e-03, 3.7175e-03, 2.8652e-03, 5.2901e-03,\n",
       "                      1.7930e-03, 5.0790e-03, 5.8747e-03, 1.8105e-03, 3.2829e-04, 2.7691e-03,\n",
       "                      2.5632e-03, 2.4445e-03, 2.2758e-03, 2.1954e-03, 2.7726e-03, 2.3425e-03,\n",
       "                      5.8678e-03, 1.9115e-03, 9.5593e-04, 3.5869e-03, 1.3073e-03, 1.6645e-03,\n",
       "                      3.3378e-03, 2.0792e-03, 2.5974e-03, 1.7178e-03, 9.7611e-04, 3.3329e-03,\n",
       "                      3.9666e-03, 3.6178e-03, 3.8649e-03, 2.8497e-03, 2.9947e-03, 1.6591e-03,\n",
       "                      1.9251e-03, 2.5269e-03, 2.0626e-03, 4.1072e-03, 1.8113e-03, 2.6496e-03,\n",
       "                      3.7580e-03, 1.2255e-03, 1.9832e-03, 3.4989e-03, 3.4501e-03, 7.6118e-03,\n",
       "                      1.9699e-03, 3.2967e-03, 3.1822e-03, 7.8455e-04, 3.9870e-03, 1.7512e-03,\n",
       "                      1.3460e-03, 5.4257e-03, 3.9370e-03, 2.5741e-03, 4.8141e-03, 2.9463e-03,\n",
       "                      6.6812e-04, 1.5160e-03, 2.5534e-03, 4.2202e-03, 1.5543e-03, 3.3592e-03,\n",
       "                      2.6908e-03, 2.9423e-03])),\n",
       "             ('layer2.2.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.2.conv3.weight',\n",
       "              tensor([[[[-6.0140e-05]],\n",
       "              \n",
       "                       [[ 4.3357e-04]],\n",
       "              \n",
       "                       [[ 2.6959e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4673e-03]],\n",
       "              \n",
       "                       [[-3.1192e-05]],\n",
       "              \n",
       "                       [[ 3.2795e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.1635e-04]],\n",
       "              \n",
       "                       [[ 5.3275e-03]],\n",
       "              \n",
       "                       [[-5.6597e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8446e-03]],\n",
       "              \n",
       "                       [[-2.9398e-03]],\n",
       "              \n",
       "                       [[-2.5921e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.0839e-03]],\n",
       "              \n",
       "                       [[-2.1419e-02]],\n",
       "              \n",
       "                       [[-2.4408e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.8764e-02]],\n",
       "              \n",
       "                       [[-7.2389e-03]],\n",
       "              \n",
       "                       [[-3.2718e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-5.4693e-03]],\n",
       "              \n",
       "                       [[-5.4658e-04]],\n",
       "              \n",
       "                       [[-2.1165e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.1390e-03]],\n",
       "              \n",
       "                       [[-1.8334e-03]],\n",
       "              \n",
       "                       [[-1.2797e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3687e-02]],\n",
       "              \n",
       "                       [[ 2.3688e-03]],\n",
       "              \n",
       "                       [[ 1.4645e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1371e-02]],\n",
       "              \n",
       "                       [[ 4.2676e-03]],\n",
       "              \n",
       "                       [[ 1.3165e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.7343e-05]],\n",
       "              \n",
       "                       [[-1.0451e-02]],\n",
       "              \n",
       "                       [[ 2.4032e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.6627e-03]],\n",
       "              \n",
       "                       [[-2.1962e-03]],\n",
       "              \n",
       "                       [[ 5.2988e-03]]]])),\n",
       "             ('layer2.2.bn3.weight',\n",
       "              tensor([ 1.5829e-03,  4.0437e-03,  7.9018e-02, -3.3452e-02, -3.8362e-03,\n",
       "                      -4.0259e-02,  5.6479e-02, -5.2574e-02, -5.4821e-03,  9.8153e-02,\n",
       "                       7.4831e-02, -4.1457e-02,  6.4442e-03,  2.4705e-02,  4.2119e-03,\n",
       "                       8.1349e-03, -7.0708e-04,  6.3751e-02, -1.1563e-02, -3.6649e-02,\n",
       "                      -1.5035e-01,  1.0695e-01,  5.9250e-02,  7.9987e-03,  1.6767e-02,\n",
       "                      -1.8906e-02, -9.8717e-03,  3.2701e-03, -1.4381e-02, -6.6840e-03,\n",
       "                       5.9542e-02,  2.5946e-03,  1.3965e-02,  3.0240e-02,  3.8578e-02,\n",
       "                       8.9493e-03,  4.8468e-02, -3.8702e-03,  2.2036e-03, -3.3668e-02,\n",
       "                       3.7116e-02, -1.5955e-02,  9.3161e-04,  8.3122e-02,  5.0757e-02,\n",
       "                      -7.3959e-02, -1.2282e-01, -6.4725e-02, -1.5404e-02,  5.1301e-02,\n",
       "                       4.7618e-02, -4.8675e-02, -1.7186e-02,  1.0659e-02, -3.7929e-03,\n",
       "                      -3.2485e-02, -6.1401e-03,  1.2038e-01, -3.5274e-02, -1.1872e-01,\n",
       "                       5.1139e-02, -1.5709e-03, -1.1618e-02,  6.5530e-02,  6.4543e-02,\n",
       "                      -1.5042e-02,  4.5773e-02,  2.2373e-02, -1.6438e-01, -4.3549e-03,\n",
       "                       9.3170e-03, -9.8066e-03, -1.0458e-01,  3.2583e-02,  2.4457e-02,\n",
       "                      -2.0749e-04,  5.5162e-02, -8.3711e-02,  3.8844e-02, -4.7994e-02,\n",
       "                      -4.4053e-02, -5.2366e-02,  1.1795e-03, -4.4257e-02, -1.3640e-01,\n",
       "                      -4.4654e-02, -1.1747e-02, -8.7098e-02,  5.0487e-02, -1.0937e-01,\n",
       "                      -7.4689e-02,  3.6478e-02, -1.9747e-02, -1.1152e-02, -1.0650e-01,\n",
       "                      -2.7240e-03, -1.0373e-03,  4.9353e-02,  7.1201e-02, -2.2104e-02,\n",
       "                      -2.8073e-02, -2.5037e-02,  9.4156e-02,  1.4133e-03,  1.3810e-02,\n",
       "                       3.4151e-02, -1.1247e-01,  6.3775e-02, -6.9352e-03,  6.7968e-03,\n",
       "                       1.1309e-01,  2.2996e-02,  4.6319e-03, -3.5006e-03,  8.7288e-03,\n",
       "                      -4.6719e-02, -3.6751e-02,  1.1226e-02, -2.7166e-02,  3.3395e-02,\n",
       "                      -1.0715e-03, -3.3604e-02, -5.2038e-02, -4.0267e-04, -2.7295e-02,\n",
       "                       3.5035e-02, -2.9437e-02, -1.4287e-02, -1.1002e-01,  1.4737e-02,\n",
       "                      -2.9636e-02,  2.4018e-02, -3.2134e-03, -1.2396e-01, -5.7949e-03,\n",
       "                      -3.1232e-03, -3.7688e-02,  2.4344e-05,  2.1037e-02, -1.0925e-01,\n",
       "                       5.6896e-02,  1.7868e-02, -5.1204e-02, -5.3651e-02,  1.0153e-01,\n",
       "                       8.4627e-02,  3.3192e-03,  8.2375e-02, -1.7147e-01,  8.7208e-03,\n",
       "                       4.1965e-02,  4.9353e-03, -4.3558e-02,  6.5900e-02,  6.1482e-03,\n",
       "                       4.7150e-02, -2.6544e-02,  8.2614e-03,  6.1865e-02,  6.1811e-02,\n",
       "                      -5.5536e-02, -1.1983e-02, -8.1650e-02,  9.6898e-02,  9.0042e-02,\n",
       "                      -4.7739e-03,  9.6322e-03, -2.2633e-02,  2.3504e-02, -1.1557e-02,\n",
       "                      -8.2452e-03, -7.2066e-02, -4.3389e-02, -1.5566e-02,  1.0340e-01,\n",
       "                      -3.7579e-02,  8.2941e-02,  1.7058e-02,  3.9811e-02, -2.9540e-02,\n",
       "                      -1.8497e-02,  9.2129e-02,  4.9942e-02,  4.8889e-03, -3.6843e-02,\n",
       "                      -3.9036e-02, -6.3700e-02,  1.4278e-03, -3.3385e-02,  1.4760e-03,\n",
       "                       1.3775e-01,  3.0887e-02, -4.2267e-02,  2.4311e-02, -6.9307e-02,\n",
       "                      -1.8112e-02,  1.5081e-02, -7.8469e-02, -7.7433e-02, -3.9247e-02,\n",
       "                       2.2008e-02,  1.1176e-02,  9.5051e-02, -1.4900e-01, -6.2619e-02,\n",
       "                      -4.9418e-02, -7.3914e-02,  5.7702e-02, -8.4583e-02,  6.7998e-02,\n",
       "                      -7.5509e-02, -6.6179e-03,  1.2910e-01, -3.8610e-03, -7.8564e-02,\n",
       "                       8.3308e-03, -6.2040e-02, -3.4981e-02, -6.7767e-02, -1.1130e-04,\n",
       "                      -5.4493e-02,  4.4019e-03, -6.6009e-04,  6.9772e-02,  6.5309e-02,\n",
       "                      -7.3287e-02,  9.9644e-03, -3.5014e-02, -3.8998e-02,  5.4099e-03,\n",
       "                       2.3930e-02, -4.9644e-02, -7.6956e-02,  1.8026e-02,  1.5538e-02,\n",
       "                       2.0017e-02, -3.3916e-02,  4.8014e-04, -1.1347e-01,  2.6515e-02,\n",
       "                      -1.0853e-02,  1.0129e-03,  2.2172e-03,  2.7122e-02,  4.3629e-04,\n",
       "                      -1.1351e-03,  1.3814e-02, -9.2229e-03, -5.8520e-02, -8.8705e-02,\n",
       "                       2.0047e-02, -1.6106e-03, -2.8268e-02,  3.0472e-02, -6.6326e-02,\n",
       "                       4.9391e-02,  4.3098e-02,  5.4056e-02,  7.7297e-02, -4.0312e-03,\n",
       "                      -2.0513e-02,  1.8506e-02, -1.0970e-01, -8.9715e-02,  8.1095e-02,\n",
       "                       4.3587e-02,  3.5186e-02,  4.8570e-02,  1.6739e-01,  8.4981e-02,\n",
       "                      -7.5486e-02, -6.2515e-02,  2.6374e-02, -1.7484e-02,  9.0745e-02,\n",
       "                      -1.0824e-01, -1.0923e-01, -4.0532e-02,  6.8615e-03,  4.5870e-02,\n",
       "                      -3.3024e-03,  9.5369e-02,  1.0874e-02, -5.3625e-02, -4.6269e-02,\n",
       "                       3.7259e-03, -9.9354e-02,  2.6888e-02, -5.7416e-02,  1.1875e-04,\n",
       "                      -1.3073e-02,  3.9061e-03, -4.3728e-02,  5.4852e-02, -5.4855e-02,\n",
       "                       7.5377e-02,  3.2324e-04, -6.8670e-03, -8.5255e-02, -6.7882e-02,\n",
       "                       2.2904e-02,  1.0490e-02, -6.4107e-02,  1.1034e-02,  9.6505e-02,\n",
       "                      -5.1993e-02,  3.1586e-03, -1.1364e-02, -1.5949e-02,  4.9992e-02,\n",
       "                      -4.2084e-03,  8.3409e-02, -8.1363e-02, -1.0458e-02,  2.8923e-02,\n",
       "                       1.9837e-04,  1.1799e-01, -7.7097e-02, -8.8834e-05, -1.2722e-01,\n",
       "                       4.6381e-03, -1.2118e-04, -1.1136e-02, -4.8361e-02,  5.5958e-02,\n",
       "                      -1.4531e-02, -1.4035e-02, -8.3674e-02, -1.5793e-02,  7.4559e-03,\n",
       "                       4.8036e-02, -1.7034e-03, -2.2556e-02, -2.9093e-02,  1.2017e-01,\n",
       "                      -1.6008e-02,  6.1087e-03,  1.8592e-03,  1.2352e-01,  8.4984e-02,\n",
       "                      -7.4137e-02, -4.5293e-02, -7.2633e-02,  3.1042e-03, -1.4258e-02,\n",
       "                       7.8611e-03, -1.2131e-02, -2.4428e-02, -1.7103e-02,  1.2042e-03,\n",
       "                       6.3894e-02,  6.3542e-02,  5.3743e-02,  2.4950e-02,  1.0420e-03,\n",
       "                       2.0625e-02,  6.8959e-02,  1.1296e-01, -3.5913e-02, -2.0525e-02,\n",
       "                       1.6345e-02,  5.8566e-03,  1.9916e-04, -2.9885e-04,  2.3527e-03,\n",
       "                       4.4477e-02,  1.1423e-02, -8.7916e-02, -3.3727e-02, -4.0690e-03,\n",
       "                      -9.2252e-02,  3.7506e-02, -9.0929e-04,  2.1595e-02, -2.1786e-03,\n",
       "                      -4.0246e-02,  1.3993e-02,  1.8945e-02,  6.3931e-02,  7.2418e-02,\n",
       "                       6.5814e-02,  1.1509e-02,  2.8193e-03, -1.6138e-02,  6.1648e-02,\n",
       "                      -2.9999e-04, -3.8991e-03,  2.5421e-04,  5.7059e-02, -9.8740e-02,\n",
       "                      -3.6167e-02,  1.2112e-03, -2.0555e-03, -9.0794e-04,  4.2917e-02,\n",
       "                       4.8542e-02, -6.3328e-03,  1.6322e-02,  2.0731e-03,  3.3636e-02,\n",
       "                       2.0935e-03,  6.9116e-02,  7.5501e-03,  4.6836e-02, -5.2216e-02,\n",
       "                      -9.7399e-02,  4.0742e-02,  5.4802e-02, -1.4152e-02,  6.7916e-02,\n",
       "                      -2.7643e-02, -9.0476e-03,  2.2791e-03, -5.2197e-02,  8.5202e-02,\n",
       "                       1.1766e-01,  2.3816e-03, -9.2569e-02, -1.4479e-01, -7.0855e-03,\n",
       "                       7.2751e-02, -4.1513e-02, -4.4321e-03, -1.2287e-02, -3.8494e-02,\n",
       "                       3.9454e-02, -5.2048e-03, -1.7363e-02,  9.5823e-02,  4.6100e-02,\n",
       "                       2.5898e-02,  5.1789e-02, -1.5571e-02, -1.4936e-02, -3.7443e-03,\n",
       "                       1.5693e-02, -1.0856e-02, -4.6221e-02,  3.6886e-02, -5.0916e-03,\n",
       "                      -1.1586e-02, -7.8240e-02, -7.4325e-02, -3.4508e-02, -4.4422e-02,\n",
       "                      -7.9712e-02,  1.0462e-02, -1.8375e-02,  4.6809e-02, -1.2275e-02,\n",
       "                      -5.9055e-02,  2.1276e-02,  5.4480e-02, -3.1076e-03, -2.3088e-02,\n",
       "                      -8.6014e-02,  1.6246e-02,  3.1545e-04, -1.0434e-02,  6.1101e-03,\n",
       "                      -5.9160e-03, -2.5934e-03, -4.2152e-02, -1.9622e-02, -2.2270e-02,\n",
       "                       6.0079e-02,  1.0171e-04, -1.7854e-02,  1.5743e-02, -6.9336e-03,\n",
       "                      -1.0072e-02, -2.0913e-02,  1.0544e-02, -7.0895e-03,  3.9669e-02,\n",
       "                      -1.8048e-02,  1.1247e-01, -2.3560e-02, -7.9340e-03,  4.8141e-02,\n",
       "                      -4.4173e-04, -6.0957e-02, -2.9575e-02,  7.2800e-02,  3.6893e-02,\n",
       "                       9.5938e-02, -1.5224e-03, -1.1826e-03,  2.3038e-02, -6.1408e-03,\n",
       "                       1.4427e-02, -1.8679e-02, -3.6145e-03, -9.2930e-05, -2.6785e-03,\n",
       "                       1.1365e-02,  7.3722e-02, -6.4882e-03,  1.7558e-02,  1.0630e-01,\n",
       "                      -5.5126e-02, -2.4861e-02,  3.5640e-02, -7.0396e-02, -3.9603e-02,\n",
       "                      -1.5136e-01, -6.8999e-03, -6.3875e-03, -2.0353e-02, -2.3552e-02,\n",
       "                      -4.7827e-02,  1.1631e-02])),\n",
       "             ('layer2.2.bn3.bias',\n",
       "              tensor([-1.8744e-02, -2.4257e-02, -8.9681e-02, -2.6531e-02,  4.1337e-03,\n",
       "                      -2.9630e-02, -4.5003e-02, -5.8509e-02, -1.3556e-02, -3.3798e-02,\n",
       "                      -3.9863e-02, -3.5892e-02, -6.2526e-03, -2.4598e-02,  1.8117e-03,\n",
       "                       5.3594e-03,  6.0049e-03, -9.2376e-02, -1.2637e-02, -3.7918e-02,\n",
       "                      -2.8648e-03, -3.5929e-02, -1.9132e-02, -2.2163e-05, -1.6561e-02,\n",
       "                      -3.7659e-02, -1.6922e-02, -2.0701e-02, -1.0545e-02,  7.4528e-03,\n",
       "                      -3.6914e-02, -2.3959e-03, -1.2796e-02, -2.8137e-02, -1.8830e-02,\n",
       "                      -1.0010e-02, -5.6456e-02, -1.1099e-02,  5.0520e-03, -1.8661e-02,\n",
       "                      -1.9530e-02, -1.6407e-02,  8.9528e-04, -3.3169e-02, -4.0355e-02,\n",
       "                      -3.9479e-02, -1.3206e-02, -4.6608e-02, -3.0649e-04, -4.4972e-02,\n",
       "                      -5.0448e-02, -3.9609e-02, -7.1774e-03, -6.0099e-03,  7.9367e-04,\n",
       "                      -1.5192e-02, -9.4441e-03,  1.0390e-02,  3.2909e-02, -5.2805e-02,\n",
       "                      -9.8088e-03,  1.6495e-03, -3.4420e-02, -6.2973e-02, -6.0141e-03,\n",
       "                      -5.3385e-02, -1.8132e-02, -1.4544e-02, -8.8923e-02,  6.2377e-03,\n",
       "                      -1.6330e-03, -1.1541e-02,  3.9467e-02, -5.9977e-02, -2.1852e-03,\n",
       "                       1.6634e-02,  6.2117e-03, -4.6462e-02, -2.9762e-02, -1.9601e-02,\n",
       "                      -2.9809e-02, -2.2299e-03, -7.2845e-03, -1.1845e-02, -1.2651e-02,\n",
       "                      -1.1538e-02, -5.4408e-03, -6.0416e-02,  6.7348e-03,  4.7315e-03,\n",
       "                      -4.8426e-02, -8.0586e-03, -2.0576e-02, -2.2226e-02, -1.9175e-02,\n",
       "                      -4.3580e-03, -2.8801e-03, -1.1576e-02, -2.2331e-02, -2.7557e-02,\n",
       "                      -1.4157e-03, -5.5592e-02,  2.5682e-02,  5.6414e-03, -7.0235e-03,\n",
       "                      -2.3159e-02, -3.4776e-02, -4.9870e-02, -2.2990e-02,  8.1856e-03,\n",
       "                      -6.4365e-02, -1.9698e-02,  4.7561e-04, -5.7019e-03,  1.6008e-05,\n",
       "                      -4.7014e-02, -5.3650e-03, -3.3543e-02, -2.0875e-02, -2.1670e-02,\n",
       "                       9.0156e-03, -5.0817e-02, -1.3512e-02, -1.9200e-03, -3.2934e-02,\n",
       "                      -3.3910e-02, -1.7133e-02,  7.7914e-03, -2.5893e-02, -3.0165e-02,\n",
       "                       1.1013e-03, -1.0463e-02, -3.5074e-02, -1.5176e-02, -1.1020e-02,\n",
       "                      -3.3597e-02, -4.6652e-02, -7.3092e-04, -1.0722e-02, -4.7690e-02,\n",
       "                      -6.9984e-02, -8.7376e-04, -4.7592e-02, -2.2153e-02, -6.1340e-02,\n",
       "                      -4.2749e-02, -9.6033e-06, -3.5076e-02, -7.8742e-02, -2.4350e-02,\n",
       "                       1.2709e-02, -1.4113e-02,  9.8796e-03, -4.8039e-02, -4.3662e-03,\n",
       "                      -6.8509e-02, -1.5755e-02, -9.7471e-03, -3.5670e-02, -4.3754e-02,\n",
       "                       1.3903e-03, -3.4957e-02, -4.2217e-02, -9.4365e-02, -5.1441e-02,\n",
       "                      -1.5790e-02, -1.6739e-03, -2.1360e-02, -3.9486e-03, -6.6290e-03,\n",
       "                       7.6767e-03, -3.5377e-02, -3.9437e-02, -1.8031e-02, -7.8314e-02,\n",
       "                      -3.5269e-02, -1.0454e-02, -2.5297e-02, -1.5697e-02, -9.8893e-03,\n",
       "                       4.4165e-03, -3.6371e-02, -1.8590e-02,  9.9625e-04, -2.4411e-02,\n",
       "                      -1.0396e-02, -3.5838e-02,  5.6801e-03, -3.5826e-02, -9.0101e-03,\n",
       "                      -8.4645e-02,  6.6588e-03, -3.1893e-02, -1.7203e-02, -2.3646e-02,\n",
       "                      -1.2094e-02,  4.9268e-03, -3.0365e-02, -6.4098e-02, -2.3810e-02,\n",
       "                      -1.7650e-02, -1.1885e-02, -4.2660e-02, -9.5413e-02,  1.1614e-02,\n",
       "                      -3.4606e-03, -2.9480e-02, -4.3941e-02, -1.3204e-02, -7.0500e-02,\n",
       "                      -1.3970e-02, -2.4377e-02, -5.5281e-02,  6.6670e-03, -5.8377e-02,\n",
       "                      -3.9969e-02, -5.2050e-02, -4.0778e-02, -1.6957e-02,  2.0382e-03,\n",
       "                      -1.5194e-03, -2.1519e-03, -3.7700e-03, -3.2176e-02, -2.2991e-02,\n",
       "                      -3.5111e-02, -5.0024e-03, -5.8631e-02,  1.1370e-02, -6.3029e-03,\n",
       "                       2.9720e-03, -4.8644e-02, -5.0509e-02, -2.2140e-02, -1.3668e-02,\n",
       "                      -1.1893e-02, -3.0615e-03,  6.3193e-03, -1.8509e-04, -2.4437e-02,\n",
       "                      -6.7694e-03, -1.4520e-03,  9.3488e-04, -2.4002e-02,  4.7783e-03,\n",
       "                       4.0872e-03,  6.8956e-04, -1.4285e-02, -5.3687e-02, -5.7532e-02,\n",
       "                      -1.6360e-02, -6.5112e-04, -2.4706e-02, -2.6245e-02, -3.4944e-02,\n",
       "                      -2.3095e-02,  2.0028e-02, -3.4048e-02, -2.7483e-02,  6.6876e-03,\n",
       "                      -1.6266e-02, -2.3654e-02, -1.0119e-01, -4.9260e-02, -8.9008e-03,\n",
       "                      -3.1413e-03, -1.1126e-02,  1.7794e-03, -3.5042e-03, -5.5200e-02,\n",
       "                      -1.0668e-02, -2.2514e-02, -1.9532e-02, -5.0972e-02, -2.8533e-02,\n",
       "                      -3.5281e-02,  1.7609e-02, -3.4690e-02,  1.0645e-02, -5.7223e-02,\n",
       "                      -1.6130e-02, -6.4167e-02, -4.1951e-02,  3.2506e-02, -7.3646e-03,\n",
       "                       2.1955e-03, -1.0091e-01, -3.7755e-02, -3.6658e-02, -2.1247e-02,\n",
       "                      -2.5348e-03, -9.7105e-03, -3.2011e-02, -3.4886e-02, -4.1312e-02,\n",
       "                      -3.7917e-02,  3.3061e-03, -2.0055e-02, -3.8416e-02, -2.3202e-02,\n",
       "                      -5.6008e-02,  7.9957e-03,  1.6749e-03, -1.4902e-02, -9.3425e-03,\n",
       "                      -4.0433e-02, -9.3040e-03, -1.3454e-02, -1.2761e-02, -6.1502e-02,\n",
       "                      -1.6474e-02, -1.2370e-02,  2.0607e-03, -2.1907e-02, -5.3556e-02,\n",
       "                      -1.3099e-02, -1.4861e-02, -6.3123e-02,  3.6606e-04,  7.4243e-03,\n",
       "                      -3.1468e-03,  2.8614e-03, -7.5527e-03, -9.6547e-03, -3.5894e-02,\n",
       "                      -3.9362e-02, -1.3584e-02, -8.8369e-03, -4.3038e-02,  5.7653e-03,\n",
       "                       1.5693e-02,  2.6368e-03, -1.7855e-02, -4.4567e-02,  3.1776e-02,\n",
       "                      -1.6628e-02,  8.0298e-03, -2.2008e-03, -9.0399e-02, -3.6360e-02,\n",
       "                      -8.6437e-02, -2.5981e-03, -8.0556e-02, -3.0354e-03, -2.2552e-02,\n",
       "                      -2.4259e-02, -1.0142e-02, -1.3478e-02, -1.7483e-02,  8.0452e-03,\n",
       "                      -2.7129e-02, -3.1557e-02, -3.4817e-02,  7.6089e-03,  3.5009e-04,\n",
       "                      -5.6895e-04, -2.9472e-03, -4.8941e-02, -4.1436e-02, -6.4162e-03,\n",
       "                      -1.8918e-02,  1.2493e-03,  8.5783e-03, -3.6911e-02,  4.4288e-03,\n",
       "                      -3.2934e-02,  4.2827e-03, -7.3076e-02, -2.1353e-02,  1.1410e-03,\n",
       "                      -6.7582e-02, -4.1986e-02,  5.2526e-03, -2.1323e-02,  1.7008e-03,\n",
       "                      -1.3625e-02,  1.1159e-02, -6.8021e-03, -2.4619e-02, -2.8977e-02,\n",
       "                      -6.6424e-02, -1.1455e-02,  3.8447e-04, -1.0555e-02, -3.9325e-02,\n",
       "                       3.5850e-03, -3.1599e-03,  2.9311e-03, -5.5146e-02, -2.1986e-03,\n",
       "                      -6.5153e-03, -4.4599e-03,  1.7266e-02,  6.0034e-03, -3.9939e-02,\n",
       "                      -2.0941e-02, -4.9030e-03, -1.3614e-02, -2.3070e-02, -1.8452e-02,\n",
       "                      -4.4408e-02, -4.2828e-02,  9.5623e-03, -3.6728e-02,  7.1908e-04,\n",
       "                      -3.3726e-02, -5.6808e-02, -3.8349e-03, -8.7256e-04,  6.0766e-03,\n",
       "                       7.9199e-03, -2.1828e-02,  5.6991e-03, -5.3098e-02, -3.0807e-02,\n",
       "                      -5.9121e-02, -8.1353e-03,  1.0681e-02, -3.8410e-02, -2.6399e-02,\n",
       "                      -3.1759e-02, -7.0791e-02,  9.0542e-04, -1.2461e-02, -4.3210e-02,\n",
       "                       1.3339e-02, -2.4345e-02, -1.6197e-02, -5.5513e-02, -4.7456e-02,\n",
       "                      -4.3825e-02,  1.7301e-02, -2.1282e-02, -4.1521e-03, -6.7009e-03,\n",
       "                      -2.2255e-02, -2.5270e-02, -4.9815e-02,  3.9324e-03, -1.0321e-02,\n",
       "                      -6.3814e-03, -6.5245e-02, -1.4241e-03, -6.1984e-02, -1.3809e-02,\n",
       "                      -3.7635e-02, -3.1623e-02, -1.3858e-02, -4.9774e-03, -1.2814e-02,\n",
       "                      -3.6279e-02, -3.6395e-03, -1.2325e-02, -4.7667e-03, -3.4298e-02,\n",
       "                      -9.8060e-02, -6.7613e-04, -1.7858e-02, -2.1231e-03, -1.7780e-04,\n",
       "                      -1.0670e-01,  7.5097e-03, -4.2909e-02, -2.5111e-02, -2.6426e-02,\n",
       "                      -3.1636e-02, -1.3230e-03, -3.8035e-02, -1.8987e-02, -5.2120e-04,\n",
       "                       3.9376e-03, -2.8963e-02, -4.1115e-02, -3.1050e-02,  8.7939e-03,\n",
       "                      -2.0530e-02, -4.3885e-02, -1.9208e-02, -4.2716e-02, -5.2999e-02,\n",
       "                      -8.6666e-03,  4.2907e-03, -3.1820e-02, -5.2249e-02, -1.0152e-02,\n",
       "                      -6.0884e-02,  6.8777e-03, -4.4137e-03, -4.0533e-03, -8.5555e-03,\n",
       "                      -1.7592e-02,  1.8743e-03, -2.0008e-02, -9.2651e-04, -1.1894e-02,\n",
       "                      -7.9233e-03,  2.9386e-02, -3.9404e-03, -2.6324e-02, -1.3843e-02,\n",
       "                      -6.3500e-02, -2.3366e-02, -2.9084e-02, -5.6786e-02, -2.7411e-02,\n",
       "                      -2.4817e-02,  5.2742e-03,  3.0612e-03,  4.3082e-03, -2.0590e-02,\n",
       "                      -3.3681e-02, -3.9172e-02])),\n",
       "             ('layer2.2.bn3.running_mean',\n",
       "              tensor([-3.2376e-04,  5.0708e-04, -3.7842e-03, -2.6476e-03, -7.5071e-04,\n",
       "                       5.2575e-03, -4.1611e-03, -2.4044e-03,  1.1734e-04, -2.8984e-03,\n",
       "                       5.0237e-03,  7.3204e-03, -3.2001e-03, -2.5414e-03,  8.9020e-04,\n",
       "                       8.0549e-05, -1.3939e-03, -6.5171e-03,  1.3419e-03, -8.8291e-03,\n",
       "                       4.4467e-03, -2.8761e-03, -2.4285e-03, -1.2390e-03, -2.5118e-04,\n",
       "                       1.9221e-03, -3.3768e-04, -4.4470e-04,  1.9176e-03,  3.1598e-03,\n",
       "                      -1.4619e-03,  5.2364e-04, -4.0607e-04, -1.3700e-03,  7.3974e-04,\n",
       "                      -8.5205e-04, -1.9021e-03,  7.1181e-04,  2.7666e-04,  2.8564e-03,\n",
       "                      -1.4099e-03, -2.7043e-04,  5.8735e-04,  1.3282e-02, -1.1320e-02,\n",
       "                       1.2822e-02,  1.8415e-02,  9.6158e-03,  3.1520e-03, -1.4419e-03,\n",
       "                      -4.8299e-04,  5.5002e-03, -1.3082e-03, -2.0697e-04,  9.3533e-04,\n",
       "                       4.4747e-03, -9.9144e-04,  6.5032e-03,  4.6249e-03,  8.2191e-03,\n",
       "                      -3.7134e-03, -9.0334e-04, -2.1945e-03, -6.4712e-03, -5.3350e-03,\n",
       "                       1.8602e-03, -9.5416e-03, -2.2710e-03, -3.3044e-03,  6.0411e-04,\n",
       "                       1.5197e-03,  4.5459e-04,  1.9150e-02, -1.9111e-03, -5.1794e-03,\n",
       "                      -1.1046e-03, -6.5707e-03, -2.7885e-03,  1.5199e-03, -5.6116e-04,\n",
       "                       4.8170e-03, -1.9028e-03, -1.5249e-04,  6.6531e-03,  7.4471e-03,\n",
       "                       5.8613e-03,  3.4483e-03,  9.1577e-03, -1.2032e-04,  4.8489e-03,\n",
       "                      -4.2915e-03, -7.0389e-03,  1.3439e-03,  3.2191e-03,  6.1558e-03,\n",
       "                       4.8032e-04, -9.5097e-05, -2.8876e-03,  3.2916e-03,  1.2300e-03,\n",
       "                      -7.5098e-03,  2.0867e-03, -1.8855e-02,  1.7602e-04, -1.1710e-03,\n",
       "                      -1.5301e-03,  2.2168e-02, -3.1284e-03, -2.6769e-03,  1.8488e-03,\n",
       "                       7.8035e-03, -1.8749e-03,  1.2173e-03,  1.6378e-04,  2.0685e-03,\n",
       "                       6.9002e-03,  5.7808e-03, -2.9827e-03, -5.3747e-03,  7.9410e-03,\n",
       "                      -3.5085e-03,  5.2684e-03, -9.1493e-03, -8.4465e-03,  2.8025e-03,\n",
       "                      -2.7627e-03, -3.5479e-03, -1.1361e-03,  5.8624e-03,  1.5662e-03,\n",
       "                       1.3630e-03, -1.8134e-03, -6.9598e-04,  1.0530e-02,  1.2852e-04,\n",
       "                      -5.3079e-04,  2.0298e-03, -1.9321e-05, -1.5653e-03,  8.2497e-03,\n",
       "                      -1.1029e-02, -1.1708e-03,  2.2873e-03,  5.2469e-03,  2.6032e-04,\n",
       "                      -4.8044e-03,  4.4743e-04, -1.7748e-02,  2.6520e-03,  9.3158e-04,\n",
       "                      -5.2378e-03, -8.0255e-04, -8.2461e-04, -4.5515e-03,  7.0009e-04,\n",
       "                       9.6304e-03,  1.3031e-03, -3.4060e-04,  3.1023e-03, -1.2239e-02,\n",
       "                       2.2155e-03, -3.0370e-03,  1.0401e-03, -4.2985e-03, -8.4325e-03,\n",
       "                       1.5220e-03,  2.0178e-03,  5.7247e-03, -1.6874e-03,  3.3929e-03,\n",
       "                       3.3818e-03,  1.0554e-02,  1.3909e-03,  2.1424e-03, -1.2373e-02,\n",
       "                       5.5078e-03, -9.5570e-03,  1.1804e-03, -3.8276e-03, -2.1977e-03,\n",
       "                      -2.8634e-03, -4.2859e-03, -6.7210e-03, -2.4842e-03,  7.7012e-03,\n",
       "                      -3.4469e-04,  4.5943e-03,  1.4013e-05,  4.7264e-03, -2.2259e-04,\n",
       "                       3.7135e-03, -2.5214e-03,  1.9734e-03,  3.3698e-03, -2.1005e-03,\n",
       "                      -3.7719e-03, -3.3785e-03, -5.3190e-03, -8.2122e-03, -2.2155e-03,\n",
       "                      -2.8902e-03,  1.4729e-03, -4.4267e-04,  5.5904e-03,  4.3896e-03,\n",
       "                       7.6234e-03,  8.7985e-03, -3.0123e-03,  5.0843e-03,  1.1210e-03,\n",
       "                       1.6979e-03, -1.1369e-03,  6.1111e-03,  1.2071e-03,  7.5216e-03,\n",
       "                      -2.2350e-03, -1.5315e-02, -3.5204e-03,  1.1522e-02,  3.3638e-04,\n",
       "                      -1.2192e-03, -1.4957e-03, -1.0932e-04, -5.1460e-03, -6.9007e-03,\n",
       "                       8.8123e-03, -8.4841e-04,  4.3358e-04,  2.9949e-03, -1.5122e-03,\n",
       "                      -5.0209e-05, -2.0840e-03,  4.6044e-03, -1.5619e-03,  4.5754e-04,\n",
       "                      -1.7295e-03, -4.9250e-03, -4.2259e-05, -7.7886e-03, -2.4562e-03,\n",
       "                       5.7431e-03, -1.8376e-05, -2.8909e-03, -1.6289e-03, -3.3157e-04,\n",
       "                      -1.2378e-03, -2.5459e-03, -8.9040e-04,  2.4824e-03,  1.2410e-02,\n",
       "                       1.7993e-03, -1.7234e-04,  1.1298e-03,  2.7069e-04,  4.2024e-03,\n",
       "                      -1.0454e-03, -8.0591e-03, -3.0096e-03, -7.2100e-03, -8.7605e-04,\n",
       "                       3.5673e-03, -1.7930e-03, -7.0078e-03,  3.9949e-04, -5.1084e-03,\n",
       "                      -2.8288e-03, -1.0072e-03, -4.2978e-03, -2.5196e-02,  8.2973e-04,\n",
       "                       5.7833e-04,  3.8067e-03,  5.0638e-03, -6.6718e-04, -2.1377e-02,\n",
       "                       8.8550e-03, -1.0693e-02,  2.9218e-03, -2.5725e-03,  2.5419e-03,\n",
       "                      -3.9253e-04,  1.3013e-03, -1.8842e-03,  1.0761e-02,  5.5648e-03,\n",
       "                      -6.4182e-04, -3.8579e-03,  6.6234e-04,  1.0523e-02, -1.0942e-03,\n",
       "                       1.0344e-03,  1.8087e-04,  8.7379e-03,  4.8627e-03,  4.3955e-03,\n",
       "                       5.9883e-03, -8.7374e-04,  1.4304e-03,  6.0137e-03, -2.6289e-03,\n",
       "                      -5.4063e-03,  1.5147e-03,  6.2023e-03,  4.7197e-05, -2.7005e-03,\n",
       "                       1.1155e-02,  4.3409e-04,  8.5884e-04,  2.3397e-04, -4.3289e-03,\n",
       "                      -2.3161e-03, -8.1995e-04,  3.3281e-03,  2.6829e-03, -5.2103e-03,\n",
       "                       2.7358e-04, -2.6164e-03, -5.3878e-04,  1.2004e-05,  1.9847e-02,\n",
       "                       2.3671e-04,  5.7528e-04,  9.0079e-04, -3.0855e-03, -1.4165e-02,\n",
       "                       1.3564e-03,  2.8043e-03, -3.9279e-04,  5.0553e-04, -9.0018e-04,\n",
       "                      -7.3284e-04,  2.7725e-04,  4.9727e-03,  2.1107e-03, -2.2401e-02,\n",
       "                      -2.4165e-03,  2.3672e-03, -1.9659e-03,  1.1479e-02,  1.7239e-03,\n",
       "                       2.1294e-03, -1.1381e-02,  6.9834e-05,  5.8475e-05,  1.5803e-03,\n",
       "                      -2.0508e-04, -2.3977e-04,  6.9719e-03,  1.4565e-03,  6.8780e-04,\n",
       "                      -1.0644e-02, -5.4067e-03,  2.3387e-03, -4.1255e-04, -1.8144e-04,\n",
       "                      -2.6848e-03, -9.1969e-03, -4.0230e-03,  2.7839e-03,  8.2381e-03,\n",
       "                      -2.2583e-03, -2.7134e-04, -1.0396e-04, -3.8159e-03, -5.0974e-04,\n",
       "                      -2.0547e-04, -1.8837e-03, -3.0281e-03,  6.5900e-03,  1.3151e-04,\n",
       "                       1.4320e-02,  8.1426e-03,  5.6775e-04, -4.6056e-03,  1.2350e-03,\n",
       "                      -1.0257e-03,  8.7305e-04,  3.7256e-03, -4.9146e-03, -2.5868e-03,\n",
       "                      -7.9864e-03,  3.7749e-04,  3.2164e-05,  3.1331e-04, -5.5108e-03,\n",
       "                      -2.9129e-03, -8.8283e-04,  1.1265e-03, -9.9386e-05,  1.6435e-02,\n",
       "                      -4.8037e-03,  8.6594e-04,  2.0974e-04, -1.8386e-04, -1.1379e-03,\n",
       "                      -7.8700e-03, -1.5139e-03, -5.8334e-03,  9.4052e-04,  6.9003e-03,\n",
       "                       6.0175e-04, -7.1944e-03, -2.2751e-03, -4.4898e-03,  9.2445e-03,\n",
       "                      -1.3687e-03,  1.4978e-03, -2.0623e-03, -1.4645e-03, -1.0147e-02,\n",
       "                      -7.3217e-04, -1.6007e-03, -6.5013e-04, -1.4308e-03, -3.6782e-03,\n",
       "                      -3.8992e-03, -1.8348e-03,  4.9938e-03,  2.3121e-02,  4.4127e-03,\n",
       "                       2.6766e-03,  9.8386e-03,  2.2843e-03,  5.6896e-04, -3.6972e-03,\n",
       "                       1.2107e-03, -3.3995e-04, -1.1639e-03, -2.7306e-03, -4.8917e-04,\n",
       "                      -2.6856e-03, -3.9725e-03,  2.9738e-06, -7.3805e-04, -5.1083e-05,\n",
       "                      -5.4687e-06,  4.4146e-03, -3.8830e-03, -3.9830e-03, -4.5294e-04,\n",
       "                       7.7515e-04,  6.6310e-03,  5.9848e-03,  4.5363e-03, -5.4758e-04,\n",
       "                       3.3469e-03, -1.4572e-03, -4.2585e-04,  3.2351e-03,  2.1232e-03,\n",
       "                       9.8420e-05, -1.9314e-03, -1.4538e-03, -6.8829e-05,  3.3735e-03,\n",
       "                      -4.2795e-03,  3.7142e-03,  1.8295e-03, -1.7318e-03,  1.7775e-03,\n",
       "                       1.8315e-03, -2.6054e-03, -2.9184e-03, -1.2919e-04,  6.2289e-03,\n",
       "                      -3.8155e-03,  3.7468e-06,  2.5711e-03, -3.0137e-03,  1.6257e-03,\n",
       "                       2.9709e-03, -1.9566e-03,  4.4549e-04,  2.8288e-03,  6.4997e-04,\n",
       "                      -3.4828e-03,  2.3864e-03, -2.1760e-04, -1.0274e-03, -4.5016e-03,\n",
       "                      -1.9037e-04, -3.7086e-03,  1.5151e-03, -1.5611e-02, -5.4301e-03,\n",
       "                      -1.3156e-04,  1.0922e-03, -4.7201e-04,  5.4061e-05, -1.0287e-03,\n",
       "                      -2.3278e-03, -3.2997e-03,  1.1530e-03,  2.7296e-04,  4.1878e-04,\n",
       "                       2.4806e-04,  1.6989e-02, -2.1626e-04, -5.8856e-04,  1.8301e-03,\n",
       "                       7.4653e-03,  2.8418e-03, -5.6235e-03, -5.7167e-04, -6.9410e-03,\n",
       "                      -7.0535e-04, -2.7407e-03,  2.7586e-04, -1.4033e-03,  2.2537e-03,\n",
       "                      -2.6079e-03, -3.4433e-03])),\n",
       "             ('layer2.2.bn3.running_var',\n",
       "              tensor([5.4826e-06, 1.0040e-05, 2.3260e-04, 5.5944e-05, 1.3856e-05, 4.7272e-05,\n",
       "                      7.5632e-05, 1.0121e-04, 3.7269e-06, 1.1624e-04, 1.2489e-04, 5.3999e-05,\n",
       "                      3.4771e-05, 2.0695e-05, 1.7845e-05, 5.8536e-06, 2.4251e-05, 1.1687e-04,\n",
       "                      3.3310e-05, 4.3950e-05, 4.3644e-04, 1.9195e-04, 8.3401e-05, 5.5733e-06,\n",
       "                      1.3746e-05, 4.3702e-05, 2.4468e-06, 3.3975e-06, 1.5755e-05, 3.7474e-05,\n",
       "                      6.3800e-05, 7.4492e-07, 2.0322e-05, 8.0743e-05, 2.6239e-05, 2.2347e-05,\n",
       "                      8.4773e-05, 1.6464e-06, 6.5351e-06, 2.2506e-05, 5.3938e-05, 1.5823e-05,\n",
       "                      2.3163e-06, 8.8671e-05, 1.1444e-04, 1.1202e-04, 2.0290e-04, 6.6782e-05,\n",
       "                      1.7321e-05, 4.5259e-05, 7.5283e-05, 8.6143e-05, 5.2742e-06, 9.1071e-06,\n",
       "                      1.4291e-05, 1.4037e-05, 1.6993e-06, 3.0420e-04, 9.6620e-05, 2.1336e-04,\n",
       "                      4.0625e-05, 1.0414e-05, 6.2574e-05, 1.5464e-04, 9.0868e-05, 1.4005e-05,\n",
       "                      1.5893e-04, 2.3708e-05, 3.0194e-04, 4.1482e-05, 6.4623e-05, 3.3682e-06,\n",
       "                      2.1371e-04, 5.2139e-05, 1.0828e-04, 2.0442e-05, 2.2553e-04, 8.9558e-05,\n",
       "                      4.9168e-05, 2.8158e-05, 3.5361e-05, 7.1169e-05, 2.3011e-07, 8.6309e-05,\n",
       "                      2.5375e-04, 4.5696e-05, 3.2744e-05, 1.3493e-04, 4.8957e-05, 1.7061e-04,\n",
       "                      1.2014e-04, 4.4213e-05, 1.1644e-05, 2.2854e-05, 2.3212e-04, 1.2802e-05,\n",
       "                      2.9880e-08, 4.2807e-05, 1.0857e-04, 9.7882e-06, 3.7152e-05, 3.2965e-05,\n",
       "                      3.9744e-04, 1.1085e-06, 4.7047e-05, 2.5764e-05, 3.0458e-04, 1.3751e-04,\n",
       "                      4.4492e-05, 1.4055e-05, 2.7411e-04, 2.4082e-05, 5.0760e-05, 4.8469e-07,\n",
       "                      3.5926e-05, 8.8931e-05, 1.9800e-05, 1.1117e-05, 1.9241e-05, 1.8765e-05,\n",
       "                      2.8718e-05, 2.8405e-05, 1.3501e-04, 6.8600e-05, 3.7641e-05, 3.5449e-05,\n",
       "                      2.4656e-05, 3.3711e-05, 1.3309e-04, 1.6451e-05, 3.4290e-05, 1.2825e-05,\n",
       "                      1.1156e-05, 1.6895e-04, 2.7748e-06, 4.8601e-06, 4.9291e-05, 2.4395e-09,\n",
       "                      7.7361e-06, 2.7388e-04, 1.5649e-04, 2.6484e-05, 6.0108e-05, 4.1708e-05,\n",
       "                      1.9289e-04, 2.7999e-04, 3.8832e-06, 1.8320e-04, 3.2377e-04, 1.9629e-05,\n",
       "                      2.9452e-05, 6.7609e-06, 1.0209e-04, 6.1177e-05, 3.3160e-06, 1.6806e-04,\n",
       "                      1.8873e-05, 1.5908e-05, 1.0438e-04, 1.3493e-04, 5.7289e-05, 1.7995e-05,\n",
       "                      1.2010e-04, 2.8217e-04, 1.1482e-04, 1.2379e-05, 1.8703e-05, 4.4982e-05,\n",
       "                      2.6957e-05, 2.9979e-05, 6.0259e-05, 1.2982e-04, 9.2164e-05, 1.2343e-05,\n",
       "                      2.2173e-04, 5.7964e-05, 8.6157e-05, 1.9211e-05, 5.3288e-05, 2.5902e-05,\n",
       "                      2.4351e-05, 1.3072e-04, 9.8291e-05, 3.1494e-06, 6.2173e-05, 3.6001e-05,\n",
       "                      1.0848e-04, 2.1221e-05, 1.9809e-05, 5.4107e-06, 3.0643e-04, 3.9465e-05,\n",
       "                      3.8380e-05, 6.6197e-05, 5.2172e-05, 2.8581e-05, 6.7140e-06, 1.1794e-04,\n",
       "                      1.5856e-04, 8.4609e-05, 2.4210e-05, 1.7053e-05, 8.4358e-05, 4.0379e-04,\n",
       "                      5.9011e-05, 1.4220e-04, 1.0070e-04, 8.8317e-05, 4.0649e-04, 9.0260e-05,\n",
       "                      8.0510e-05, 3.1066e-05, 2.1449e-04, 4.7782e-06, 1.3457e-04, 2.0827e-05,\n",
       "                      9.9780e-05, 3.8685e-05, 9.3515e-05, 2.4859e-07, 2.9128e-05, 7.0700e-06,\n",
       "                      1.2300e-06, 1.7552e-04, 1.2712e-04, 1.7460e-04, 6.8970e-06, 5.6408e-05,\n",
       "                      5.5458e-05, 4.8575e-06, 1.0406e-05, 4.5605e-05, 1.1354e-04, 8.0065e-06,\n",
       "                      3.9236e-05, 2.9893e-05, 3.5322e-05, 2.3144e-07, 1.2065e-04, 3.9273e-05,\n",
       "                      1.1598e-04, 6.7456e-06, 3.4955e-05, 6.3303e-05, 2.7270e-05, 2.0365e-05,\n",
       "                      2.8959e-05, 4.4767e-06, 1.0284e-04, 2.0666e-04, 7.1514e-06, 4.7737e-08,\n",
       "                      3.2628e-05, 5.4450e-05, 3.8546e-05, 7.6561e-05, 3.9357e-05, 5.7587e-05,\n",
       "                      1.3280e-04, 1.3718e-05, 3.5305e-05, 1.9781e-05, 2.0669e-04, 9.5877e-05,\n",
       "                      1.4820e-04, 1.1517e-04, 7.4348e-05, 5.2704e-05, 3.3908e-04, 1.6478e-04,\n",
       "                      1.2023e-04, 1.0634e-04, 2.3880e-05, 4.2189e-05, 3.7071e-04, 2.3932e-04,\n",
       "                      3.3158e-04, 5.4548e-05, 9.1034e-05, 4.2375e-05, 3.5217e-06, 1.7657e-04,\n",
       "                      1.0373e-05, 8.6496e-05, 6.0805e-05, 1.8615e-05, 2.6612e-04, 4.0398e-05,\n",
       "                      1.2181e-04, 1.5336e-05, 8.3189e-06, 1.6567e-06, 9.1516e-05, 6.3321e-05,\n",
       "                      1.1945e-04, 1.1544e-04, 6.4316e-06, 3.1115e-06, 8.8546e-05, 1.2247e-04,\n",
       "                      5.3330e-05, 2.8316e-05, 9.6902e-05, 3.8479e-05, 1.1728e-04, 1.2866e-04,\n",
       "                      7.9775e-06, 8.5522e-06, 5.9075e-05, 7.0620e-05, 1.9392e-05, 2.0873e-04,\n",
       "                      1.0136e-04, 1.2400e-05, 5.3030e-05, 7.0742e-06, 1.9517e-04, 1.5815e-04,\n",
       "                      1.0165e-08, 2.0026e-04, 2.6472e-05, 9.2872e-06, 1.0666e-05, 9.3272e-05,\n",
       "                      2.0719e-04, 4.6134e-05, 1.4614e-05, 5.6853e-05, 7.6325e-05, 3.7840e-05,\n",
       "                      8.1739e-05, 1.6824e-06, 3.2970e-05, 3.7895e-05, 2.1811e-04, 2.0097e-05,\n",
       "                      4.7214e-05, 2.3108e-05, 2.3928e-04, 9.0333e-05, 1.6915e-04, 6.5192e-05,\n",
       "                      1.1279e-04, 1.4014e-05, 1.7921e-05, 1.5311e-05, 3.8229e-06, 4.2799e-05,\n",
       "                      1.3682e-05, 1.4120e-05, 8.7477e-05, 8.1540e-05, 7.3957e-05, 3.1985e-05,\n",
       "                      2.2198e-06, 7.2652e-05, 6.8862e-05, 1.5896e-04, 3.8392e-05, 4.3772e-05,\n",
       "                      8.9689e-06, 2.3566e-06, 9.1940e-08, 1.0203e-04, 5.5923e-05, 5.7914e-05,\n",
       "                      5.1912e-05, 1.4335e-04, 3.5546e-05, 3.2117e-05, 2.6333e-04, 4.9920e-05,\n",
       "                      1.8564e-05, 2.2605e-05, 2.1635e-05, 6.4106e-05, 2.0973e-05, 8.6265e-06,\n",
       "                      4.3797e-05, 2.2199e-04, 1.2795e-04, 7.4982e-06, 1.2263e-05, 1.1180e-05,\n",
       "                      1.0982e-04, 1.5387e-05, 9.8644e-06, 1.0609e-05, 6.1937e-05, 2.0391e-04,\n",
       "                      2.4267e-05, 7.2090e-06, 1.5741e-05, 2.5492e-05, 5.6138e-05, 7.1378e-05,\n",
       "                      6.5114e-06, 4.2722e-05, 2.0146e-05, 2.2228e-05, 3.4340e-05, 8.5016e-05,\n",
       "                      4.0153e-05, 3.7873e-05, 1.0794e-04, 1.7514e-04, 6.1527e-05, 1.7429e-04,\n",
       "                      1.4535e-05, 6.4370e-05, 8.7514e-05, 1.1544e-05, 1.0126e-04, 1.1928e-04,\n",
       "                      1.6527e-04, 2.2168e-04, 4.7465e-06, 1.5756e-04, 2.6065e-04, 6.0784e-06,\n",
       "                      1.3206e-04, 7.3202e-05, 6.0090e-05, 4.2466e-05, 5.7469e-05, 5.0760e-05,\n",
       "                      1.5209e-06, 3.8453e-05, 1.3350e-04, 5.0189e-05, 4.2560e-05, 1.3051e-04,\n",
       "                      1.6051e-05, 9.5997e-06, 7.2658e-07, 4.2036e-05, 1.5557e-05, 1.0281e-04,\n",
       "                      4.6386e-05, 1.4396e-05, 6.0718e-06, 1.6995e-04, 1.2199e-04, 4.7110e-05,\n",
       "                      2.7298e-05, 1.2975e-04, 8.4782e-06, 3.6181e-05, 1.4906e-04, 6.4030e-06,\n",
       "                      1.0939e-04, 2.3267e-05, 3.8721e-05, 3.7601e-06, 1.8417e-05, 1.8855e-04,\n",
       "                      8.2888e-06, 1.2622e-05, 2.0992e-06, 7.8994e-05, 2.4960e-05, 6.1748e-05,\n",
       "                      8.5107e-05, 1.0340e-05, 1.3081e-04, 5.1774e-05, 1.0788e-09, 2.4092e-05,\n",
       "                      1.2823e-05, 8.7902e-05, 2.0192e-05, 2.3861e-05, 2.0080e-05, 2.9430e-05,\n",
       "                      5.2376e-05, 2.7630e-05, 1.8587e-04, 3.9259e-05, 1.4005e-05, 5.3161e-05,\n",
       "                      4.7047e-07, 1.1183e-04, 5.3214e-05, 1.0923e-04, 1.0715e-04, 1.3133e-04,\n",
       "                      7.1842e-06, 3.7722e-06, 1.2069e-05, 3.8924e-05, 1.0835e-05, 2.0935e-05,\n",
       "                      9.4425e-06, 1.1779e-06, 5.8163e-06, 2.8042e-06, 1.7684e-04, 1.2072e-06,\n",
       "                      2.5825e-05, 1.7742e-04, 1.0133e-04, 1.4864e-05, 2.1024e-05, 1.1368e-04,\n",
       "                      2.4774e-05, 4.7758e-04, 6.5248e-05, 1.1283e-05, 1.0836e-04, 8.0031e-05,\n",
       "                      9.7118e-05, 1.5546e-05])),\n",
       "             ('layer2.2.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.3.conv1.weight',\n",
       "              tensor([[[[-0.0107]],\n",
       "              \n",
       "                       [[-0.0066]],\n",
       "              \n",
       "                       [[ 0.0033]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0045]],\n",
       "              \n",
       "                       [[-0.0142]],\n",
       "              \n",
       "                       [[-0.0125]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0023]],\n",
       "              \n",
       "                       [[ 0.0167]],\n",
       "              \n",
       "                       [[-0.0124]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0042]],\n",
       "              \n",
       "                       [[-0.0044]],\n",
       "              \n",
       "                       [[-0.0101]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0070]],\n",
       "              \n",
       "                       [[-0.0045]],\n",
       "              \n",
       "                       [[-0.0102]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0107]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       [[ 0.0039]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0004]],\n",
       "              \n",
       "                       [[ 0.0117]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0142]],\n",
       "              \n",
       "                       [[ 0.0018]],\n",
       "              \n",
       "                       [[-0.0036]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0106]],\n",
       "              \n",
       "                       [[ 0.0007]],\n",
       "              \n",
       "                       [[-0.0164]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0012]],\n",
       "              \n",
       "                       [[-0.0133]],\n",
       "              \n",
       "                       [[ 0.0187]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0081]],\n",
       "              \n",
       "                       [[-0.0022]],\n",
       "              \n",
       "                       [[-0.0034]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0004]],\n",
       "              \n",
       "                       [[ 0.0004]]]])),\n",
       "             ('layer2.3.bn1.weight',\n",
       "              tensor([0.0810, 0.1049, 0.0705, 0.0815, 0.1365, 0.0634, 0.1283, 0.1195, 0.1227,\n",
       "                      0.1148, 0.0614, 0.1183, 0.0900, 0.0810, 0.0756, 0.1156, 0.0969, 0.0935,\n",
       "                      0.0982, 0.1049, 0.0779, 0.0803, 0.1575, 0.0274, 0.1111, 0.0511, 0.1265,\n",
       "                      0.0834, 0.0420, 0.0770, 0.0852, 0.0334, 0.1000, 0.0546, 0.0545, 0.0719,\n",
       "                      0.0782, 0.0936, 0.1001, 0.0779, 0.0753, 0.0568, 0.1088, 0.0915, 0.0506,\n",
       "                      0.0482, 0.0694, 0.0695, 0.1226, 0.1160, 0.0652, 0.0787, 0.0882, 0.1049,\n",
       "                      0.1016, 0.1305, 0.0845, 0.1198, 0.0413, 0.0804, 0.1159, 0.0758, 0.0559,\n",
       "                      0.0778, 0.0872, 0.0660, 0.1397, 0.0791, 0.1180, 0.0641, 0.0594, 0.0430,\n",
       "                      0.0935, 0.1192, 0.0822, 0.0552, 0.1274, 0.0777, 0.1250, 0.0551, 0.0777,\n",
       "                      0.0635, 0.0596, 0.0256, 0.1038, 0.0427, 0.0947, 0.0372, 0.1203, 0.1041,\n",
       "                      0.0547, 0.1208, 0.0815, 0.0854, 0.0536, 0.0600, 0.1335, 0.1029, 0.0740,\n",
       "                      0.0974, 0.0789, 0.1152, 0.0532, 0.0451, 0.0711, 0.0887, 0.0450, 0.0671,\n",
       "                      0.0409, 0.0836, 0.1184, 0.0915, 0.0537, 0.1016, 0.0372, 0.0970, 0.0462,\n",
       "                      0.0924, 0.0784, 0.0740, 0.0643, 0.0610, 0.0889, 0.0685, 0.0920, 0.0632,\n",
       "                      0.0800, 0.0431])),\n",
       "             ('layer2.3.bn1.bias',\n",
       "              tensor([-0.0407,  0.0353, -0.0376, -0.0445, -0.0727, -0.0327, -0.0553, -0.1045,\n",
       "                      -0.0602, -0.0676, -0.0225, -0.0597, -0.0107, -0.0580, -0.0066, -0.0489,\n",
       "                       0.0059, -0.0415, -0.0400, -0.0648, -0.0026, -0.0200, -0.0831,  0.0006,\n",
       "                      -0.0657,  0.0272, -0.0692, -0.0141,  0.0155, -0.0322, -0.0102, -0.0155,\n",
       "                      -0.0622, -0.0147,  0.0182, -0.0553,  0.0230, -0.0707, -0.0091, -0.0470,\n",
       "                      -0.0169, -0.0053, -0.0777, -0.0138,  0.0071, -0.0169, -0.0151, -0.0073,\n",
       "                      -0.0698, -0.0439,  0.0329, -0.0076, -0.0523, -0.0800, -0.0333, -0.0280,\n",
       "                      -0.0278, -0.0909, -0.0003, -0.0271, -0.0346, -0.0065,  0.0492, -0.0098,\n",
       "                      -0.0180, -0.0375, -0.0905, -0.0350, -0.0565, -0.0359, -0.0184, -0.0072,\n",
       "                      -0.0354, -0.0134, -0.0090, -0.0183, -0.0858, -0.0219, -0.0555, -0.0317,\n",
       "                       0.0117, -0.0156, -0.0021, -0.0100, -0.0431, -0.0224, -0.0629, -0.0023,\n",
       "                       0.0207, -0.0697, -0.0490, -0.0517, -0.0051, -0.0308, -0.0032,  0.0161,\n",
       "                       0.0077, -0.0184,  0.0173, -0.0714,  0.0021, -0.1049,  0.0182, -0.0059,\n",
       "                      -0.0291, -0.0161, -0.0161, -0.0120, -0.0225, -0.0128, -0.0594, -0.0217,\n",
       "                       0.0199, -0.0469, -0.0017, -0.0165, -0.0124, -0.0327, -0.0150, -0.0170,\n",
       "                      -0.0104, -0.0412,  0.0027, -0.0225, -0.0391,  0.0261, -0.0242, -0.0070])),\n",
       "             ('layer2.3.bn1.running_mean',\n",
       "              tensor([-3.2122e-02, -6.4645e-02, -2.2134e-03, -9.7273e-03, -7.2408e-02,\n",
       "                       2.5225e-02, -6.0510e-02, -3.3641e-02, -4.5446e-05, -6.0594e-02,\n",
       "                      -2.4668e-02,  1.3326e-02,  2.1946e-02, -2.4208e-02,  1.2579e-02,\n",
       "                       3.7372e-02, -5.3829e-02, -1.2968e-02, -1.4215e-02,  5.8087e-04,\n",
       "                      -2.8625e-02,  2.5783e-02,  2.0221e-02, -1.2220e-02,  3.9726e-02,\n",
       "                      -1.7466e-02, -3.5398e-02, -1.6683e-02, -2.4557e-02, -3.5535e-02,\n",
       "                      -6.0839e-03, -1.6400e-02, -1.4539e-02, -5.2504e-04, -2.1799e-02,\n",
       "                      -1.2532e-02, -6.4301e-02, -5.2381e-02, -6.7006e-02,  1.5108e-02,\n",
       "                      -9.6650e-03, -1.7186e-03, -3.1151e-02,  1.3374e-02, -2.1696e-02,\n",
       "                       2.8659e-02, -7.9610e-03, -4.4030e-02,  6.6675e-03, -5.2077e-02,\n",
       "                      -6.9790e-02,  1.8338e-02, -7.6176e-03, -1.4861e-02, -4.6394e-02,\n",
       "                      -9.5382e-03,  1.0075e-02, -1.3964e-02,  1.1027e-02, -1.1934e-02,\n",
       "                      -5.7649e-02,  2.7981e-04, -4.7482e-02, -2.2494e-02, -1.7142e-02,\n",
       "                       9.1567e-04,  1.1688e-02, -2.1206e-02,  2.2345e-02, -8.0597e-04,\n",
       "                       9.3323e-04, -1.8392e-02, -1.5119e-02, -3.5556e-02,  1.2156e-02,\n",
       "                      -2.8236e-02,  4.9162e-02,  1.9986e-03, -5.1212e-02,  8.9078e-03,\n",
       "                      -2.6014e-02, -4.1587e-03, -2.5749e-02, -1.3701e-02, -4.9115e-02,\n",
       "                      -1.7739e-02,  1.0764e-02, -2.3547e-02, -3.7199e-02, -3.4415e-02,\n",
       "                      -4.0284e-03,  3.1348e-02, -4.4275e-02, -6.0270e-04, -1.1203e-03,\n",
       "                      -2.9057e-02, -3.2690e-02, -4.8314e-02, -3.8228e-02, -3.0149e-02,\n",
       "                      -3.9992e-02, -1.6706e-02,  7.0871e-03, -9.7063e-03, -1.4343e-02,\n",
       "                      -3.4090e-02, -1.5620e-02, -1.1901e-02, -6.4896e-04, -2.1426e-02,\n",
       "                      -2.9395e-02,  3.9259e-03, -3.9872e-03, -3.6776e-02, -2.2141e-02,\n",
       "                       2.9669e-02, -2.6301e-02, -6.0665e-03, -2.9148e-03,  1.2305e-02,\n",
       "                      -1.8832e-02, -1.0875e-02, -2.1579e-02, -3.1454e-02, -5.4720e-03,\n",
       "                       5.1889e-03, -3.7309e-02, -1.9783e-02])),\n",
       "             ('layer2.3.bn1.running_var',\n",
       "              tensor([0.0027, 0.0107, 0.0010, 0.0016, 0.0045, 0.0017, 0.0029, 0.0029, 0.0050,\n",
       "                      0.0034, 0.0010, 0.0039, 0.0047, 0.0019, 0.0012, 0.0047, 0.0036, 0.0017,\n",
       "                      0.0027, 0.0026, 0.0025, 0.0023, 0.0074, 0.0003, 0.0043, 0.0012, 0.0027,\n",
       "                      0.0043, 0.0010, 0.0013, 0.0019, 0.0002, 0.0027, 0.0010, 0.0016, 0.0008,\n",
       "                      0.0044, 0.0019, 0.0064, 0.0026, 0.0021, 0.0005, 0.0029, 0.0018, 0.0013,\n",
       "                      0.0005, 0.0010, 0.0016, 0.0045, 0.0021, 0.0068, 0.0013, 0.0024, 0.0034,\n",
       "                      0.0029, 0.0045, 0.0015, 0.0028, 0.0007, 0.0015, 0.0038, 0.0024, 0.0018,\n",
       "                      0.0018, 0.0015, 0.0011, 0.0075, 0.0013, 0.0029, 0.0013, 0.0007, 0.0008,\n",
       "                      0.0020, 0.0063, 0.0019, 0.0009, 0.0029, 0.0013, 0.0027, 0.0010, 0.0013,\n",
       "                      0.0022, 0.0024, 0.0002, 0.0028, 0.0005, 0.0029, 0.0004, 0.0075, 0.0032,\n",
       "                      0.0007, 0.0034, 0.0067, 0.0014, 0.0007, 0.0014, 0.0065, 0.0040, 0.0033,\n",
       "                      0.0018, 0.0016, 0.0037, 0.0013, 0.0005, 0.0011, 0.0011, 0.0004, 0.0006,\n",
       "                      0.0004, 0.0018, 0.0027, 0.0029, 0.0012, 0.0032, 0.0004, 0.0030, 0.0007,\n",
       "                      0.0021, 0.0023, 0.0011, 0.0014, 0.0009, 0.0017, 0.0016, 0.0028, 0.0020,\n",
       "                      0.0024, 0.0009])),\n",
       "             ('layer2.3.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.3.conv2.weight',\n",
       "              tensor([[[[-7.3569e-03, -1.0722e-02, -6.7934e-03],\n",
       "                        [-4.7375e-03,  1.2484e-02, -3.0341e-04],\n",
       "                        [-6.3413e-03, -8.9185e-03, -1.3451e-02]],\n",
       "              \n",
       "                       [[-1.6060e-02, -1.2419e-02, -2.4540e-02],\n",
       "                        [-8.5753e-03, -3.9463e-03, -7.9599e-04],\n",
       "                        [-3.5456e-03, -3.3107e-03,  8.4624e-03]],\n",
       "              \n",
       "                       [[ 1.5454e-02,  2.1846e-02,  2.0862e-02],\n",
       "                        [ 1.0544e-03, -6.7264e-03,  4.3052e-03],\n",
       "                        [-2.1131e-03, -1.2435e-02, -6.0997e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9445e-03,  2.7160e-03, -4.2073e-03],\n",
       "                        [-2.3062e-03, -6.3552e-03, -9.4569e-03],\n",
       "                        [ 1.6871e-03,  3.6803e-03,  7.7003e-03]],\n",
       "              \n",
       "                       [[-1.7164e-03, -1.1047e-02, -9.0785e-03],\n",
       "                        [ 2.6527e-03, -9.2838e-03,  1.1102e-02],\n",
       "                        [-4.6875e-04,  5.9330e-03,  1.3296e-02]],\n",
       "              \n",
       "                       [[ 5.8359e-04, -8.0582e-05, -1.7054e-03],\n",
       "                        [-1.0236e-03,  2.2122e-04, -5.9813e-03],\n",
       "                        [ 1.5155e-03,  1.1265e-03,  4.3512e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1851e-03, -1.0851e-02,  1.0571e-03],\n",
       "                        [-3.6148e-03,  8.2450e-03,  6.0329e-03],\n",
       "                        [ 1.5796e-02,  1.1201e-02,  9.6757e-03]],\n",
       "              \n",
       "                       [[ 6.7612e-03,  8.7198e-03,  6.2462e-03],\n",
       "                        [ 2.0548e-03,  3.6902e-03, -1.4533e-02],\n",
       "                        [-2.1176e-03, -4.6131e-03, -1.8799e-04]],\n",
       "              \n",
       "                       [[-1.1132e-03,  8.0342e-04,  7.5135e-04],\n",
       "                        [ 1.0290e-03, -2.0182e-03,  4.6621e-03],\n",
       "                        [ 6.1683e-04,  8.4045e-04, -7.4350e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.9185e-03,  7.1351e-04,  9.3223e-04],\n",
       "                        [-4.8690e-03, -3.8801e-03, -8.5153e-03],\n",
       "                        [ 8.2906e-03,  5.9919e-04,  8.3479e-04]],\n",
       "              \n",
       "                       [[ 4.5604e-03, -2.5754e-03, -3.9610e-04],\n",
       "                        [-1.7799e-03,  4.7370e-04, -4.6054e-04],\n",
       "                        [ 3.4959e-03,  2.8457e-03, -2.0907e-03]],\n",
       "              \n",
       "                       [[ 8.1741e-04, -1.5553e-03,  1.2201e-03],\n",
       "                        [ 9.4101e-04, -3.5546e-03, -1.5465e-03],\n",
       "                        [-1.5363e-03,  7.6930e-04,  5.5768e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.1736e-03, -9.0146e-04,  1.1367e-02],\n",
       "                        [-2.9405e-03,  1.8896e-03,  1.2226e-02],\n",
       "                        [-1.6813e-02, -1.2569e-02, -1.5274e-03]],\n",
       "              \n",
       "                       [[ 3.0632e-03, -6.9816e-03, -1.3658e-03],\n",
       "                        [-1.5613e-02, -3.9714e-03, -5.5043e-04],\n",
       "                        [-1.1105e-02, -9.1483e-04, -8.7686e-03]],\n",
       "              \n",
       "                       [[ 6.0544e-03,  5.9714e-04,  5.6344e-03],\n",
       "                        [ 1.9462e-03,  7.7119e-03,  7.6530e-04],\n",
       "                        [-2.7865e-03, -9.3632e-04,  4.5682e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3874e-02, -1.7219e-02, -1.4799e-02],\n",
       "                        [-5.7933e-03,  6.9189e-03,  8.3909e-03],\n",
       "                        [ 1.7277e-02,  1.6720e-02,  9.5635e-03]],\n",
       "              \n",
       "                       [[ 1.3145e-03, -1.7875e-03, -1.8307e-03],\n",
       "                        [ 1.5622e-03, -4.3892e-03, -3.1964e-03],\n",
       "                        [ 2.9748e-03, -1.8708e-04,  5.0233e-03]],\n",
       "              \n",
       "                       [[ 1.9268e-04, -3.1278e-03,  1.3295e-04],\n",
       "                        [ 2.9932e-03,  4.3556e-04,  6.0182e-03],\n",
       "                        [ 7.2921e-03,  7.5526e-03,  1.2214e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1444e-03,  2.3098e-03,  3.0060e-03],\n",
       "                        [ 1.7959e-02,  1.0503e-02,  1.0200e-02],\n",
       "                        [-4.9627e-03, -4.1679e-03, -1.7609e-02]],\n",
       "              \n",
       "                       [[ 3.6956e-03, -3.8471e-03, -2.5179e-02],\n",
       "                        [ 4.0079e-03,  1.1453e-02, -1.9869e-02],\n",
       "                        [-1.5078e-02, -1.4879e-02, -6.8388e-03]],\n",
       "              \n",
       "                       [[ 4.9363e-03,  3.0961e-03,  3.8422e-03],\n",
       "                        [-5.9544e-03, -1.2381e-03, -1.8251e-03],\n",
       "                        [ 1.9525e-03, -1.3016e-03, -2.4096e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0036e-02, -7.1879e-03, -1.5259e-02],\n",
       "                        [-2.4024e-02, -3.6848e-03, -1.2887e-02],\n",
       "                        [-1.5471e-02, -2.2391e-03, -6.8314e-03]],\n",
       "              \n",
       "                       [[-1.7008e-02, -7.9882e-03, -2.6633e-02],\n",
       "                        [-1.9475e-03,  2.4731e-03,  2.8976e-03],\n",
       "                        [-1.0392e-02, -1.4133e-02, -2.7740e-03]],\n",
       "              \n",
       "                       [[-1.6564e-03,  1.2945e-03,  3.6667e-03],\n",
       "                        [ 1.2716e-03,  2.3629e-03,  4.7153e-03],\n",
       "                        [-5.0540e-03, -4.0941e-03, -1.8998e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.1270e-03, -9.2725e-04,  1.6003e-03],\n",
       "                        [-1.1043e-02, -4.0097e-03, -4.8456e-03],\n",
       "                        [-4.6054e-03, -2.7210e-03, -1.0370e-02]],\n",
       "              \n",
       "                       [[ 2.6823e-03, -4.4357e-03,  8.3024e-03],\n",
       "                        [-3.6768e-03,  3.9241e-03,  5.7624e-03],\n",
       "                        [ 8.7609e-04, -6.5991e-03, -1.0417e-02]],\n",
       "              \n",
       "                       [[ 2.2458e-03,  4.5093e-03, -3.6127e-03],\n",
       "                        [ 4.4093e-03, -3.0284e-04, -1.9074e-03],\n",
       "                        [-1.6135e-03, -1.0849e-03, -1.1272e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.0796e-03,  4.2121e-03,  1.4908e-02],\n",
       "                        [ 2.6388e-03,  4.2109e-03, -4.7109e-04],\n",
       "                        [-2.4193e-03, -8.1706e-03, -1.2703e-02]],\n",
       "              \n",
       "                       [[-1.5121e-02, -1.4332e-02, -5.6527e-03],\n",
       "                        [-1.1491e-03,  5.4472e-03,  8.1601e-03],\n",
       "                        [ 1.1479e-02,  1.3346e-02,  1.3208e-02]],\n",
       "              \n",
       "                       [[ 4.6696e-03,  6.3047e-03,  5.1845e-03],\n",
       "                        [-7.2390e-04, -9.7027e-04, -3.4463e-03],\n",
       "                        [-2.7475e-03, -4.3850e-03, -9.2309e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3379e-03,  1.4883e-03,  8.6645e-03],\n",
       "                        [-6.7080e-03,  5.1875e-03,  1.2960e-02],\n",
       "                        [-3.1695e-04,  6.9143e-03,  1.9519e-02]],\n",
       "              \n",
       "                       [[ 9.7803e-03,  2.5992e-02,  1.7494e-02],\n",
       "                        [ 1.0840e-02,  7.8249e-03,  7.2129e-03],\n",
       "                        [ 7.8497e-03,  9.0367e-03,  1.7551e-03]],\n",
       "              \n",
       "                       [[-4.0817e-03, -5.1843e-03, -5.1176e-03],\n",
       "                        [-1.9806e-03, -2.8139e-03, -3.1285e-03],\n",
       "                        [-3.9847e-03, -6.9047e-03, -7.1823e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3092e-03,  8.4078e-03,  7.2967e-03],\n",
       "                        [-8.8437e-03, -6.3398e-04, -7.8692e-04],\n",
       "                        [ 1.0742e-04,  5.2453e-03,  2.6988e-03]],\n",
       "              \n",
       "                       [[-4.7676e-03, -9.4658e-03, -9.5954e-03],\n",
       "                        [-1.1033e-02, -7.5715e-03, -1.1317e-02],\n",
       "                        [-4.5619e-03, -7.9033e-03, -5.8446e-03]],\n",
       "              \n",
       "                       [[-5.1100e-04, -3.4654e-03, -2.9914e-03],\n",
       "                        [-6.6429e-04, -7.5744e-04, -8.0216e-04],\n",
       "                        [-6.9123e-04, -1.8888e-03,  3.6080e-04]]]])),\n",
       "             ('layer2.3.bn2.weight',\n",
       "              tensor([0.0936, 0.0546, 0.0976, 0.0623, 0.1233, 0.1226, 0.1157, 0.0793, 0.0967,\n",
       "                      0.0774, 0.1111, 0.0681, 0.0831, 0.0954, 0.0995, 0.1352, 0.1010, 0.1209,\n",
       "                      0.0106, 0.0894, 0.1099, 0.0641, 0.0736, 0.1143, 0.0359, 0.0667, 0.0897,\n",
       "                      0.1587, 0.0020, 0.0699, 0.0345, 0.0873, 0.0945, 0.0572, 0.1153, 0.1039,\n",
       "                      0.1212, 0.1091, 0.1150, 0.1356, 0.0794, 0.0666, 0.1248, 0.1087, 0.0822,\n",
       "                      0.0842, 0.1293, 0.1104, 0.1060, 0.0420, 0.0641, 0.0461, 0.1049, 0.1076,\n",
       "                      0.0584, 0.0653, 0.0558, 0.1051, 0.0825, 0.0593, 0.0624, 0.0638, 0.0850,\n",
       "                      0.0692, 0.0678, 0.0274, 0.1318, 0.1075, 0.1272, 0.2401, 0.1242, 0.0843,\n",
       "                      0.0598, 0.0506, 0.0610, 0.0881, 0.1402, 0.1053, 0.0004, 0.0763, 0.0608,\n",
       "                      0.0951, 0.0850, 0.0848, 0.0960, 0.1388, 0.0582, 0.0460, 0.0655, 0.0760,\n",
       "                      0.1037, 0.1061, 0.0633, 0.0780, 0.0650, 0.0892, 0.1305, 0.0007, 0.1788,\n",
       "                      0.1045, 0.1315, 0.1021, 0.0969, 0.0457, 0.0915, 0.0923, 0.1341, 0.0965,\n",
       "                      0.0997, 0.0846, 0.0885, 0.0800, 0.1030, 0.1253, 0.1642, 0.1097, 0.0833,\n",
       "                      0.0865, 0.1019, 0.0671, 0.0483, 0.0782, 0.1381, 0.0486, 0.0559, 0.1143,\n",
       "                      0.0944, 0.0565])),\n",
       "             ('layer2.3.bn2.bias',\n",
       "              tensor([-8.2520e-02, -3.2150e-02, -6.2900e-02, -5.6477e-02, -9.3770e-02,\n",
       "                      -9.6310e-02, -8.0114e-02, -4.3721e-02, -5.8287e-02, -2.9108e-02,\n",
       "                      -5.9287e-02, -2.2655e-02, -6.0896e-02, -5.4549e-02, -5.4873e-02,\n",
       "                      -8.6751e-02, -9.2101e-02, -5.9274e-02, -7.6479e-03, -5.1791e-02,\n",
       "                      -8.2501e-02, -1.9656e-02, -2.1784e-02, -9.7491e-02, -5.3827e-03,\n",
       "                      -4.1494e-02, -6.4562e-02, -1.0609e-01, -1.4911e-02, -2.3241e-02,\n",
       "                      -1.1187e-02, -8.8361e-02, -3.1516e-02, -3.7528e-02, -5.2688e-02,\n",
       "                      -6.7179e-02, -4.9972e-02, -3.8682e-02, -5.3665e-02, -1.0289e-01,\n",
       "                      -6.1855e-02,  4.4201e-03, -9.0414e-02, -2.0843e-02, -1.5039e-02,\n",
       "                      -3.2822e-02, -6.6072e-02, -1.2263e-01, -2.3984e-02, -3.1640e-02,\n",
       "                      -2.0223e-02, -1.1831e-04, -4.3008e-02, -7.0759e-02, -8.1418e-03,\n",
       "                      -3.9963e-02, -2.6871e-02, -5.8048e-02, -6.0836e-02, -4.9252e-02,\n",
       "                      -5.2205e-02, -1.3869e-02, -2.9489e-02, -1.0469e-02, -8.8808e-02,\n",
       "                      -1.5279e-02, -6.3887e-02, -6.4437e-02, -1.0620e-01, -1.1816e-01,\n",
       "                      -6.0405e-02, -3.9954e-02, -3.0873e-02, -1.1582e-02, -3.0834e-02,\n",
       "                      -3.0406e-02, -1.1672e-01, -8.0658e-02, -4.6846e-03, -4.3972e-02,\n",
       "                      -3.9382e-02, -4.1428e-02, -7.5190e-02, -6.0767e-02, -5.3949e-02,\n",
       "                      -8.3535e-02, -1.0906e-02, -2.2326e-02, -3.5003e-02, -2.9335e-02,\n",
       "                      -1.0467e-01, -5.0836e-02,  8.3766e-03, -5.4008e-02, -3.0787e-02,\n",
       "                      -6.9922e-02, -1.1087e-01, -4.5451e-03, -1.3219e-01, -5.7870e-02,\n",
       "                      -4.9518e-02, -5.2905e-02, -4.9485e-02, -1.5898e-02, -3.5173e-02,\n",
       "                      -1.1378e-01, -1.3204e-01, -3.5640e-02, -4.2917e-02, -2.5610e-02,\n",
       "                      -8.9689e-02, -6.0538e-02, -4.3963e-02, -8.7499e-02, -1.5631e-01,\n",
       "                      -4.7503e-02, -1.1981e-02, -8.0307e-02, -6.7963e-02, -4.4310e-02,\n",
       "                      -2.6690e-03, -4.7879e-02, -1.3205e-01, -2.4221e-02, -2.3192e-02,\n",
       "                      -5.9965e-02, -3.5586e-02, -2.3985e-02])),\n",
       "             ('layer2.3.bn2.running_mean',\n",
       "              tensor([-5.2316e-03, -2.1822e-04, -1.7203e-02, -4.4753e-03,  6.4687e-03,\n",
       "                       4.2660e-02, -1.8520e-02,  4.0238e-03, -2.7182e-03, -3.7243e-03,\n",
       "                      -1.6632e-02, -2.4344e-02, -4.0504e-04, -3.1509e-04, -2.9973e-02,\n",
       "                      -4.3872e-02, -1.7389e-02, -3.2965e-02,  5.6206e-05, -5.8037e-03,\n",
       "                      -1.2239e-02, -5.4147e-04, -1.9135e-02, -4.5956e-02,  3.5381e-03,\n",
       "                       2.1545e-03, -7.5471e-03, -4.2307e-02, -1.6281e-03, -1.0572e-02,\n",
       "                      -2.8666e-03, -1.5764e-02, -2.8588e-03, -8.4694e-03,  1.1848e-03,\n",
       "                       1.6418e-03, -2.2172e-02, -2.6210e-02, -2.0659e-04, -4.0751e-02,\n",
       "                       1.0023e-02, -1.6750e-02, -2.2979e-02, -2.6265e-02, -1.6414e-03,\n",
       "                      -1.3512e-02, -3.3091e-02,  3.5254e-02, -3.4816e-02,  3.1689e-04,\n",
       "                      -1.5163e-02,  9.9976e-03, -2.4704e-02, -1.6839e-02,  2.0111e-02,\n",
       "                      -1.5714e-02, -7.8530e-03, -2.0514e-02,  7.9056e-03, -3.8158e-03,\n",
       "                      -1.0638e-02, -4.6303e-02, -1.0286e-02, -1.7977e-02,  1.7980e-02,\n",
       "                       4.4678e-04,  6.2757e-04, -8.2885e-03, -2.8647e-02, -2.9758e-02,\n",
       "                      -4.8562e-02, -2.9463e-02,  7.9026e-03, -1.2636e-02, -1.7159e-02,\n",
       "                      -7.3794e-03, -6.1234e-02,  1.0155e-02, -1.0654e-04, -7.4154e-03,\n",
       "                       6.9320e-03, -2.1966e-03, -1.9151e-02,  8.8885e-04, -6.8023e-03,\n",
       "                      -2.0087e-02, -5.9843e-03,  7.2474e-03,  7.1910e-03, -2.5213e-02,\n",
       "                       2.1099e-02, -5.2665e-02, -2.3552e-02,  2.5725e-02, -2.2077e-02,\n",
       "                      -2.6204e-02, -1.1721e-02, -1.1496e-03, -2.4484e-02, -1.4857e-02,\n",
       "                      -2.5597e-02, -1.6248e-02, -2.3725e-02, -7.5342e-03, -7.1252e-03,\n",
       "                       8.5075e-03, -6.3527e-02, -5.5184e-02, -1.2287e-02, -8.3748e-03,\n",
       "                      -3.3025e-03,  5.1766e-03, -1.2140e-02, -4.4616e-02, -5.8856e-02,\n",
       "                      -5.3245e-02, -4.4007e-03, -2.2512e-02, -8.7548e-03, -1.2437e-02,\n",
       "                      -4.5775e-03, -1.3018e-02, -2.0934e-02,  3.3024e-03, -1.5897e-04,\n",
       "                      -7.7823e-02, -1.3132e-02, -1.6646e-02])),\n",
       "             ('layer2.3.bn2.running_var',\n",
       "              tensor([1.3089e-03, 5.0206e-04, 1.1947e-03, 4.4917e-04, 2.0096e-03, 1.3632e-03,\n",
       "                      2.0941e-03, 1.0499e-03, 1.4428e-03, 6.6202e-04, 1.5771e-03, 8.9886e-04,\n",
       "                      5.8559e-04, 1.1380e-03, 1.7749e-03, 3.2523e-03, 1.2166e-03, 3.5404e-03,\n",
       "                      1.1683e-05, 1.3439e-03, 1.8555e-03, 8.5153e-04, 3.9896e-04, 1.7750e-03,\n",
       "                      1.4731e-04, 4.8153e-04, 9.7479e-04, 3.2701e-03, 9.4777e-05, 7.6086e-04,\n",
       "                      2.1415e-04, 9.5809e-04, 1.0809e-03, 3.8201e-04, 1.4629e-03, 1.1541e-03,\n",
       "                      2.5553e-03, 3.8002e-03, 1.6472e-03, 2.4574e-03, 6.2909e-04, 8.8001e-04,\n",
       "                      2.1222e-03, 1.5592e-03, 1.2796e-03, 5.6694e-04, 3.7345e-03, 1.5956e-03,\n",
       "                      1.6233e-03, 1.8988e-04, 4.2965e-04, 7.0211e-04, 1.5506e-03, 1.3641e-03,\n",
       "                      1.1011e-03, 4.3197e-04, 7.0169e-04, 1.6261e-03, 1.8240e-03, 1.0246e-03,\n",
       "                      4.4824e-04, 6.0773e-04, 1.3208e-03, 8.5503e-04, 6.4291e-04, 1.5971e-04,\n",
       "                      3.8808e-03, 1.9231e-03, 1.6082e-03, 3.9591e-03, 2.8885e-03, 1.4170e-03,\n",
       "                      6.3924e-04, 9.3738e-04, 3.1128e-04, 1.8290e-03, 3.0137e-03, 9.3147e-04,\n",
       "                      2.3973e-06, 6.5606e-04, 8.1689e-04, 1.5910e-03, 5.4591e-04, 7.5913e-04,\n",
       "                      1.3741e-03, 2.1792e-03, 4.9516e-04, 2.2962e-04, 7.0826e-04, 3.5806e-04,\n",
       "                      1.2269e-03, 1.7247e-03, 6.4897e-04, 9.5934e-04, 4.2643e-04, 1.0467e-03,\n",
       "                      1.9215e-03, 7.3241e-06, 3.4800e-03, 1.6804e-03, 2.2938e-03, 1.5714e-03,\n",
       "                      1.1712e-03, 8.7938e-04, 6.8323e-04, 2.1179e-03, 2.5721e-03, 1.1085e-03,\n",
       "                      1.4306e-03, 1.3133e-03, 1.6811e-03, 5.6066e-04, 1.7853e-03, 2.1533e-03,\n",
       "                      3.4323e-03, 2.2563e-03, 1.3459e-03, 9.7763e-04, 1.2099e-03, 5.2412e-04,\n",
       "                      5.1758e-04, 7.8366e-04, 2.7898e-03, 2.7758e-04, 3.7419e-04, 2.6583e-03,\n",
       "                      8.0101e-04, 2.0050e-03])),\n",
       "             ('layer2.3.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer2.3.conv3.weight',\n",
       "              tensor([[[[ 0.0034]],\n",
       "              \n",
       "                       [[-0.0014]],\n",
       "              \n",
       "                       [[ 0.0047]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0073]],\n",
       "              \n",
       "                       [[ 0.0021]],\n",
       "              \n",
       "                       [[-0.0022]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0032]],\n",
       "              \n",
       "                       [[ 0.0011]],\n",
       "              \n",
       "                       [[-0.0011]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0014]],\n",
       "              \n",
       "                       [[ 0.0084]],\n",
       "              \n",
       "                       [[-0.0019]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0075]],\n",
       "              \n",
       "                       [[ 0.0011]],\n",
       "              \n",
       "                       [[-0.0070]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0093]],\n",
       "              \n",
       "                       [[ 0.0011]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0076]],\n",
       "              \n",
       "                       [[-0.0002]],\n",
       "              \n",
       "                       [[-0.0140]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0093]],\n",
       "              \n",
       "                       [[ 0.0085]],\n",
       "              \n",
       "                       [[-0.0007]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0020]],\n",
       "              \n",
       "                       [[ 0.0028]],\n",
       "              \n",
       "                       [[ 0.0112]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0128]],\n",
       "              \n",
       "                       [[-0.0157]],\n",
       "              \n",
       "                       [[ 0.0035]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0038]],\n",
       "              \n",
       "                       [[-0.0026]],\n",
       "              \n",
       "                       [[ 0.0030]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0124]],\n",
       "              \n",
       "                       [[ 0.0026]],\n",
       "              \n",
       "                       [[ 0.0007]]]])),\n",
       "             ('layer2.3.bn3.weight',\n",
       "              tensor([ 1.7122e-02, -1.2234e-02,  1.5618e-02, -8.0287e-02,  8.7820e-03,\n",
       "                      -2.1223e-02, -4.7843e-02,  8.1989e-02,  2.1662e-04, -8.6665e-02,\n",
       "                       5.4648e-02,  4.8814e-02,  2.0108e-02, -2.1369e-03,  3.0128e-02,\n",
       "                       5.3056e-02,  3.7622e-04, -7.7217e-03, -8.1147e-02,  5.2449e-02,\n",
       "                      -6.3120e-03,  3.0420e-02, -4.3341e-02, -6.2958e-03, -2.1437e-03,\n",
       "                      -5.0535e-02,  2.3256e-02, -3.8051e-03,  1.2712e-02,  3.3781e-04,\n",
       "                       1.3869e-02,  8.5704e-03, -1.2471e-03, -9.5364e-02,  9.3787e-02,\n",
       "                      -4.6004e-02, -5.3412e-02, -5.9079e-03, -4.6692e-03, -3.7554e-02,\n",
       "                      -2.4865e-02, -3.8269e-02,  8.8055e-04,  4.1910e-02, -1.7449e-02,\n",
       "                       3.7213e-02, -5.7562e-03, -1.1191e-02, -1.8755e-02, -7.0573e-03,\n",
       "                       5.7149e-02,  5.7041e-02,  2.2172e-02, -7.7145e-04,  1.0185e-04,\n",
       "                      -2.4118e-02, -2.1066e-02, -6.6663e-02,  5.9887e-02,  5.5808e-02,\n",
       "                       9.4565e-02,  9.0244e-02, -7.9740e-03, -3.5808e-03, -7.2692e-02,\n",
       "                      -8.4657e-03,  8.9892e-03,  2.4525e-02,  5.7984e-02, -1.2491e-02,\n",
       "                      -4.6815e-03,  3.5536e-03, -4.7313e-02,  5.0766e-02, -7.4733e-03,\n",
       "                       3.9407e-02,  1.8621e-03,  5.7550e-03,  1.5726e-02,  2.2207e-02,\n",
       "                       2.7826e-02, -7.8947e-03,  2.4360e-04, -3.8927e-02, -1.7928e-02,\n",
       "                      -2.7315e-02, -5.9059e-02, -8.1953e-02,  5.5476e-02,  1.3857e-03,\n",
       "                       7.1586e-02,  2.3795e-02, -4.3222e-02, -5.7686e-03, -8.3269e-02,\n",
       "                      -1.2243e-02,  2.6455e-02,  2.7708e-03,  4.8038e-02, -5.4920e-02,\n",
       "                      -3.3337e-02,  3.0073e-02, -8.0246e-03, -9.1998e-04, -7.7193e-03,\n",
       "                      -7.5503e-02,  5.6844e-02,  6.1479e-03, -6.5889e-02, -4.9747e-03,\n",
       "                       1.9248e-02,  5.0187e-02,  5.3797e-03, -2.5732e-02,  6.0479e-02,\n",
       "                      -8.0518e-02, -3.3984e-02,  1.0285e-01, -2.4222e-03, -3.7300e-02,\n",
       "                       8.5125e-03, -8.5083e-02, -2.2096e-02,  1.0910e-02,  1.8807e-02,\n",
       "                       4.4160e-02, -7.1064e-02, -6.8418e-03, -1.6309e-02, -2.7474e-02,\n",
       "                      -1.2658e-03, -3.0485e-02,  5.0075e-02, -1.5021e-02,  3.7545e-02,\n",
       "                      -1.2603e-01,  8.4219e-02, -8.7643e-02, -2.6967e-02,  2.9513e-03,\n",
       "                      -1.0720e-01,  3.5492e-02, -1.0903e-01, -3.2599e-04,  2.3899e-02,\n",
       "                      -4.8164e-03,  2.5502e-02, -4.6958e-02, -1.4010e-02, -8.0094e-02,\n",
       "                       2.9669e-02,  1.2362e-02,  7.2105e-03, -3.4583e-02, -7.0063e-04,\n",
       "                      -9.5877e-03, -1.9180e-02,  3.1429e-02,  8.1176e-02, -5.1398e-02,\n",
       "                      -1.6665e-02, -7.4117e-02, -2.1762e-02,  5.0092e-03,  1.6698e-03,\n",
       "                       2.1340e-02, -5.1166e-02,  2.3186e-03,  3.0818e-03, -5.1702e-02,\n",
       "                       6.4112e-03,  6.7391e-02,  4.3409e-02,  1.4754e-02, -1.4752e-01,\n",
       "                      -1.2454e-01, -5.9338e-02,  1.8425e-03,  2.2055e-03,  4.4066e-02,\n",
       "                      -4.6887e-02,  4.2363e-02,  6.0035e-03, -5.4146e-03, -7.8249e-02,\n",
       "                      -4.8482e-02,  5.7217e-02,  2.1986e-03,  1.2124e-02,  2.6461e-02,\n",
       "                       8.0219e-02, -3.5394e-02,  1.4769e-02,  3.5034e-04,  3.1103e-02,\n",
       "                      -8.5081e-03, -8.2414e-02,  7.2759e-03,  2.4312e-02,  9.0006e-03,\n",
       "                      -4.3917e-02, -3.7558e-02, -1.1559e-02, -6.3185e-02, -3.4862e-03,\n",
       "                       6.2211e-02, -8.8131e-04, -8.1295e-02,  1.5043e-03, -7.5948e-02,\n",
       "                      -1.6176e-02,  1.2266e-02, -1.0722e-01,  3.2717e-02, -4.9891e-05,\n",
       "                       6.6238e-03,  7.3763e-02, -1.3181e-01,  3.0937e-03,  1.1396e-03,\n",
       "                      -2.8461e-02,  4.4815e-03,  4.1663e-02,  4.6653e-03, -2.5473e-02,\n",
       "                      -1.3000e-01, -3.0953e-02, -1.9788e-03,  4.0463e-02, -5.8251e-03,\n",
       "                       2.6408e-03, -7.9086e-02, -1.0257e-02,  6.9046e-02,  1.3345e-02,\n",
       "                      -2.9684e-02,  3.4873e-02,  1.6452e-02,  4.3584e-03, -4.6394e-02,\n",
       "                       1.1290e-02,  4.5990e-04,  1.4410e-03,  1.4321e-02, -6.9693e-02,\n",
       "                       1.8971e-02, -6.3353e-02, -3.3959e-02, -1.1379e-01,  7.7910e-02,\n",
       "                       6.3216e-02, -2.7444e-05,  1.5994e-02, -1.7824e-02,  7.3259e-02,\n",
       "                      -1.0293e-03, -2.3522e-02, -4.3420e-02, -3.8112e-02, -2.1880e-04,\n",
       "                      -2.3367e-04,  8.6750e-03, -3.6211e-02,  4.4651e-02, -3.0234e-04,\n",
       "                       1.7435e-03, -1.8022e-02,  1.9883e-02, -2.5398e-02,  2.9919e-03,\n",
       "                       2.5311e-02,  9.0482e-02, -4.2737e-02,  3.2269e-02,  1.7747e-03,\n",
       "                       3.2587e-02, -1.3054e-02,  4.7272e-02, -6.8758e-03, -4.9565e-02,\n",
       "                      -7.4664e-02,  9.8131e-02,  1.0673e-02,  4.0123e-02,  4.5173e-03,\n",
       "                      -1.5480e-03, -8.3821e-03, -2.6830e-02, -5.0081e-02, -1.0022e-04,\n",
       "                      -3.7590e-02,  1.0369e-02,  2.9558e-03,  8.4367e-02,  1.4251e-02,\n",
       "                       8.2855e-02, -7.9653e-03,  5.8490e-02, -3.3093e-02,  1.8419e-02,\n",
       "                       5.2348e-02, -6.8716e-03,  4.8143e-03, -2.4262e-02, -2.3001e-03,\n",
       "                       3.2663e-03, -1.4379e-02, -2.5254e-03, -6.7464e-03,  5.7135e-03,\n",
       "                       5.0692e-02, -5.3025e-03,  3.4746e-02,  1.2732e-03, -2.2318e-02,\n",
       "                       3.1688e-02, -5.7244e-02, -2.2215e-02,  3.2190e-04,  5.3298e-02,\n",
       "                      -2.1018e-02,  1.8292e-03,  6.3904e-02,  3.5159e-03, -6.2025e-02,\n",
       "                       5.0360e-02, -2.7827e-03, -3.7964e-02, -3.1628e-02,  3.2008e-03,\n",
       "                      -2.5648e-02,  3.5581e-02,  4.5189e-02, -5.5781e-02, -1.5957e-03,\n",
       "                      -5.2560e-02,  3.2971e-03, -9.2809e-03, -8.5053e-03, -1.5495e-02,\n",
       "                       1.6181e-02,  2.2778e-02, -1.0207e-01, -7.7431e-02,  8.7234e-03,\n",
       "                      -5.2901e-02, -2.7011e-02, -7.4570e-02, -2.2029e-03,  8.3633e-02,\n",
       "                      -9.4262e-04, -1.8220e-03, -1.3066e-04,  4.9629e-02, -4.3870e-02,\n",
       "                       2.6421e-03, -3.4957e-02,  2.2263e-02, -4.7769e-02, -8.9474e-03,\n",
       "                      -2.1724e-02, -3.8332e-02,  1.7988e-02,  1.1180e-02, -6.2881e-03,\n",
       "                      -3.4881e-02,  7.8772e-03, -4.4589e-02,  3.1756e-03,  4.5210e-02,\n",
       "                      -3.2255e-02,  5.0413e-02, -7.5401e-03,  5.8621e-02, -1.5423e-03,\n",
       "                       1.7415e-03, -2.1475e-02,  3.8998e-02, -6.2399e-02, -6.9490e-03,\n",
       "                      -8.1410e-02, -8.8043e-02,  1.2670e-03,  1.6447e-02,  8.1437e-02,\n",
       "                       3.4012e-02, -4.8680e-02, -2.7750e-03,  6.7251e-02,  2.8913e-02,\n",
       "                      -3.9802e-02, -1.5461e-03, -5.0954e-02,  2.7402e-04,  8.4726e-04,\n",
       "                       1.0825e-03, -1.2633e-02,  4.7664e-03,  1.4997e-02,  1.5524e-02,\n",
       "                       9.8025e-02, -2.2250e-02,  6.2364e-03,  3.5412e-03, -9.9901e-03,\n",
       "                      -5.1120e-02,  1.2762e-01, -6.0433e-03,  2.8250e-02,  2.8917e-02,\n",
       "                       7.7286e-04, -5.5745e-02, -8.7109e-04, -1.4499e-01, -6.6701e-02,\n",
       "                       3.6822e-02,  3.0268e-02, -5.9998e-03,  7.4250e-02, -7.1495e-02,\n",
       "                       3.1073e-02,  1.0108e-03,  3.2087e-03, -6.8806e-02, -5.2786e-03,\n",
       "                       3.4227e-02, -2.2349e-02, -9.4618e-02,  4.0336e-02, -9.2628e-02,\n",
       "                      -7.1470e-02,  6.7064e-02, -7.6312e-02,  3.8906e-02,  3.4375e-02,\n",
       "                      -4.5583e-02, -9.2694e-03, -8.7914e-03,  5.4601e-02,  2.4172e-02,\n",
       "                       3.0827e-03, -6.9414e-02,  1.5975e-02,  6.4854e-02,  5.0928e-02,\n",
       "                      -6.7052e-03,  5.1449e-02,  1.4395e-02, -1.2553e-03, -3.6843e-02,\n",
       "                       4.6273e-02, -3.7018e-02,  5.5076e-02, -2.3000e-02,  7.6401e-02,\n",
       "                       5.1618e-02, -1.8951e-02, -6.0831e-02, -1.3815e-02, -6.1279e-03,\n",
       "                      -2.7138e-03, -4.8309e-03,  9.2685e-02, -5.0907e-02, -1.7312e-02,\n",
       "                       7.6071e-02, -1.7572e-02, -3.7981e-02, -5.7857e-04,  8.8207e-03,\n",
       "                      -2.5390e-02,  7.4492e-02, -5.3063e-02, -6.9753e-03,  8.9334e-02,\n",
       "                       2.5847e-02, -7.4367e-02,  4.2687e-02, -1.0464e-01, -4.2807e-02,\n",
       "                       8.8150e-04, -6.2035e-02, -7.1089e-02, -8.0920e-02,  5.9855e-04,\n",
       "                       1.6180e-02,  1.0168e-03,  4.1149e-03,  3.0823e-02, -1.0017e-01,\n",
       "                       1.2985e-02,  3.3224e-02, -3.3127e-03,  2.0527e-03, -4.7864e-02,\n",
       "                      -2.4437e-02, -3.7138e-03,  3.3135e-02, -4.5220e-02, -1.7823e-03,\n",
       "                      -1.0940e-01, -5.5410e-03,  4.4366e-02,  8.5259e-02, -1.6698e-02,\n",
       "                       3.3781e-03,  9.5508e-04,  7.1363e-04, -3.6419e-03,  5.0770e-02,\n",
       "                       3.2577e-02, -1.0373e-02])),\n",
       "             ('layer2.3.bn3.bias',\n",
       "              tensor([-2.1431e-02, -2.5716e-02, -6.3690e-02, -4.0164e-02,  4.3767e-03,\n",
       "                      -3.6036e-02, -4.7499e-02, -6.6932e-02, -1.0945e-02, -1.5590e-02,\n",
       "                      -5.9625e-02, -2.5938e-02, -1.3831e-02, -1.7505e-02,  4.2500e-03,\n",
       "                      -7.0963e-03,  2.4293e-03, -3.0296e-02, -4.9161e-02, -5.3716e-02,\n",
       "                       2.7065e-03, -2.6437e-02, -4.2218e-02,  4.3272e-03, -1.9130e-02,\n",
       "                      -4.7764e-02, -2.0041e-02, -1.7587e-02, -2.5882e-02,  8.8031e-03,\n",
       "                      -4.3781e-02, -8.4509e-03,  2.4263e-03, -4.1919e-02, -4.9634e-03,\n",
       "                      -2.5083e-02, -2.8159e-04, -1.0308e-02,  1.4884e-04, -1.7396e-02,\n",
       "                      -3.8764e-02,  8.0305e-03,  1.0424e-03, -5.1640e-02,  7.5564e-03,\n",
       "                      -3.8486e-02, -1.2159e-02, -4.5489e-02, -5.9391e-04, -3.2825e-02,\n",
       "                      -4.1672e-02, -4.6752e-02, -2.5766e-03,  1.7361e-03,  2.0113e-03,\n",
       "                      -3.1544e-02, -8.6218e-04, -4.3789e-02,  2.1680e-02, -7.3161e-02,\n",
       "                      -4.0588e-02, -3.6924e-02,  3.5843e-03, -2.1636e-02, -4.4977e-02,\n",
       "                      -4.8196e-02,  8.3979e-03, -2.0207e-03, -6.0556e-02,  1.4539e-02,\n",
       "                       5.9547e-03, -1.3311e-02,  1.4589e-02, -5.9930e-02,  2.9269e-03,\n",
       "                       2.6926e-02,  4.9684e-03, -3.7802e-02, -3.0120e-03, -1.2598e-02,\n",
       "                      -3.4677e-02, -1.2502e-03, -7.9464e-03, -8.1880e-03, -3.1902e-02,\n",
       "                      -1.9658e-02, -1.7701e-02, -5.6602e-02,  1.5669e-02,  9.4990e-03,\n",
       "                      -6.6899e-02,  9.3460e-03, -1.4245e-02, -3.2511e-02, -1.5830e-02,\n",
       "                      -9.8381e-03,  4.1567e-03,  1.9010e-03, -2.4375e-02,  2.1965e-02,\n",
       "                      -5.3036e-03, -3.9713e-02, -4.3654e-03, -5.0937e-03,  5.8281e-03,\n",
       "                      -4.0182e-02, -4.9682e-02, -3.4858e-02, -1.0554e-02, -4.6897e-03,\n",
       "                      -5.0065e-02,  1.7312e-02,  4.9911e-03,  2.2508e-03, -4.9438e-02,\n",
       "                      -1.7724e-04, -1.8994e-02, -3.8447e-02, -2.3815e-03, -3.9075e-02,\n",
       "                       2.7400e-03, -7.4752e-02, -4.2805e-03, -6.7826e-03,  1.5142e-02,\n",
       "                      -4.6342e-02,  6.3504e-02,  3.1679e-03, -2.8292e-02, -2.1186e-02,\n",
       "                       9.6300e-04, -4.1519e-03, -7.6381e-04, -2.1750e-02, -3.5783e-02,\n",
       "                      -1.6354e-02,  1.8441e-02, -6.7552e-03, -1.2264e-02, -5.8437e-02,\n",
       "                      -6.1806e-02, -1.1449e-03, -3.2418e-02, -2.2948e-02, -6.6844e-02,\n",
       "                      -3.7326e-02, -1.3530e-02, -5.8710e-02, -5.1893e-02,  8.3393e-03,\n",
       "                      -2.9547e-03, -4.7305e-03,  6.1995e-03, -2.4292e-02,  1.8302e-03,\n",
       "                      -5.7449e-02, -1.2506e-02, -1.3937e-02, -3.1731e-02, -4.5323e-02,\n",
       "                      -6.2816e-03, -3.9782e-03,  7.9648e-03, -7.0316e-02, -5.1958e-02,\n",
       "                      -3.7214e-02, -5.4723e-02, -1.9657e-02, -1.2394e-02, -3.0072e-02,\n",
       "                      -9.7924e-03, -1.2638e-02, -2.1875e-02, -6.1724e-03, -7.2274e-02,\n",
       "                      -6.4830e-03, -1.9085e-02,  6.0189e-03, -1.4159e-02, -1.5540e-02,\n",
       "                      -1.2234e-02, -3.5238e-02,  2.5240e-03, -3.0194e-04,  4.4632e-02,\n",
       "                      -1.3139e-02, -5.5201e-02, -4.0127e-03, -1.9167e-02, -6.7687e-03,\n",
       "                      -6.6221e-02, -1.9664e-03, -2.4367e-02,  6.0405e-03, -1.6857e-02,\n",
       "                      -1.4203e-02, -4.5186e-03, -3.0050e-02, -5.9243e-02, -1.9758e-02,\n",
       "                      -3.6220e-02, -8.9463e-03, -4.1290e-02, -5.0138e-02, -3.9120e-03,\n",
       "                      -5.0226e-02, -2.7922e-02, -3.1229e-02, -9.1188e-03, -5.9381e-02,\n",
       "                      -1.4835e-02,  9.7960e-03, -5.3708e-02, -1.3343e-02, -1.4775e-02,\n",
       "                       2.7576e-03,  2.8475e-02, -6.0302e-02,  2.3362e-03, -4.3104e-05,\n",
       "                      -3.8416e-03, -1.4631e-02, -1.9206e-02, -2.6968e-02, -4.2299e-02,\n",
       "                      -7.5566e-02, -1.4978e-02, -4.3185e-02, -1.2401e-02, -9.8431e-03,\n",
       "                       2.4385e-03, -1.9471e-02, -2.5793e-02,  2.3474e-02, -4.5447e-02,\n",
       "                      -1.1037e-02, -1.7451e-02,  7.3714e-03,  3.8355e-03, -1.1171e-02,\n",
       "                       8.7706e-03,  1.5091e-03,  4.1463e-03, -3.4258e-02, -8.0538e-03,\n",
       "                       6.7498e-03, -3.2622e-02, -1.9670e-02, -1.4672e-01, -2.4573e-02,\n",
       "                      -1.9532e-02,  1.1836e-06, -4.9307e-03, -3.0688e-03,  1.0917e-02,\n",
       "                      -2.4438e-02, -6.7551e-03, -1.0284e-02, -2.1209e-02,  7.6570e-03,\n",
       "                      -1.0214e-02, -2.5439e-02, -8.4311e-02, -6.4445e-02, -1.4870e-02,\n",
       "                       2.9578e-03, -1.8131e-02,  1.1472e-02, -2.7312e-02, -3.9815e-02,\n",
       "                      -1.8444e-02, -4.1989e-02, -1.7150e-02, -6.3172e-02,  5.3725e-03,\n",
       "                      -5.0818e-02,  1.6512e-05, -4.6992e-02, -1.1956e-03, -6.8854e-02,\n",
       "                      -2.4916e-02, -3.3829e-02, -5.1342e-02,  1.1458e-02, -2.7663e-03,\n",
       "                       3.6743e-03, -7.8966e-02, -4.2423e-02,  1.3778e-02, -1.8260e-02,\n",
       "                      -2.6413e-02, -9.2355e-03, -2.5782e-02, -5.6882e-02, -4.1202e-02,\n",
       "                      -6.0154e-02,  8.6115e-04,  1.4204e-02, -3.6796e-02, -1.2928e-02,\n",
       "                      -4.9283e-02,  2.2633e-03,  6.7282e-03, -1.3349e-02, -3.6672e-03,\n",
       "                      -4.0975e-02, -5.0027e-03, -8.3819e-03,  2.0653e-03, -5.3544e-02,\n",
       "                       2.0987e-02,  1.0226e-03, -1.4784e-02, -2.0130e-02, -5.0628e-02,\n",
       "                      -1.2501e-02, -4.6471e-02, -4.7957e-02,  3.7473e-04, -5.7503e-03,\n",
       "                       7.8505e-03,  2.3181e-03, -2.0118e-02,  2.6616e-03, -2.9657e-03,\n",
       "                      -4.2073e-02, -1.1144e-02, -2.7239e-02, -4.2283e-02,  1.4877e-03,\n",
       "                       4.3722e-03,  4.6379e-03, -3.2328e-02, -5.1696e-02,  2.5410e-03,\n",
       "                      -8.6000e-03,  3.1247e-03,  1.0088e-02, -6.9165e-02, -4.4037e-02,\n",
       "                      -8.3829e-02,  1.9171e-02, -7.0937e-03, -3.6972e-02, -2.9579e-02,\n",
       "                      -6.1513e-02,  7.5242e-03, -1.1205e-02, -1.7482e-02, -1.8139e-03,\n",
       "                      -3.0740e-02, -2.3837e-02, -2.1687e-02,  9.8598e-03, -5.3865e-03,\n",
       "                       1.1298e-03, -1.3628e-02, -4.4025e-02, -4.5098e-02,  3.1772e-03,\n",
       "                       1.2979e-03,  1.0529e-02,  8.5833e-03, -7.3064e-04,  1.7243e-03,\n",
       "                      -2.6860e-02,  4.0202e-03, -4.7419e-02, -2.4927e-02, -2.6403e-02,\n",
       "                      -7.6310e-02, -4.4690e-02,  1.2362e-02, -6.6835e-03, -1.9054e-02,\n",
       "                      -8.2872e-03,  1.4219e-02,  1.9285e-03, -2.6636e-02, -2.7563e-02,\n",
       "                      -6.2157e-02, -4.5242e-03, -1.0632e-02, -2.2139e-02, -1.5144e-02,\n",
       "                       1.8883e-02, -2.6717e-02,  2.9764e-03, -6.9790e-03, -1.3810e-02,\n",
       "                      -2.2187e-02, -5.7777e-03,  1.4476e-02,  3.8380e-03, -3.6143e-02,\n",
       "                      -2.3961e-02, -5.7384e-03, -1.2674e-03, -7.7567e-03,  1.4043e-02,\n",
       "                      -1.7874e-02, -4.2381e-02,  1.5121e-03, -3.1895e-02, -1.3149e-03,\n",
       "                       1.5847e-02,  1.8918e-02, -6.3389e-03,  1.1465e-02, -3.3084e-03,\n",
       "                       2.4241e-03, -4.0034e-02,  3.5150e-03, -1.8876e-02, -4.0245e-02,\n",
       "                      -5.2069e-02, -6.1520e-03,  2.3712e-03,  3.4180e-03, -1.7135e-02,\n",
       "                      -5.9191e-02,  3.5359e-03,  3.0561e-03, -3.3335e-02, -3.9864e-02,\n",
       "                      -3.1496e-03, -6.9077e-03, -4.5979e-02, -5.8591e-03, -2.2520e-02,\n",
       "                      -6.4367e-02, -3.9773e-03, -5.4941e-02, -4.0406e-03, -9.4201e-04,\n",
       "                      -7.6445e-02, -1.3728e-02, -3.7319e-02, -2.2582e-02, -1.8507e-03,\n",
       "                      -1.3504e-02, -5.7444e-02,  7.5654e-04, -2.4116e-02,  1.6027e-02,\n",
       "                      -3.9929e-02,  1.9579e-02, -1.2905e-02, -1.5137e-02,  5.1593e-05,\n",
       "                       1.3490e-02, -5.7329e-03, -1.7744e-02, -6.2294e-03, -8.5515e-03,\n",
       "                      -8.5643e-02, -1.1214e-02,  2.4094e-02, -8.4294e-03, -1.7318e-03,\n",
       "                      -9.4014e-02,  1.0344e-02, -4.4911e-02, -6.4094e-03, -2.7031e-02,\n",
       "                      -1.5136e-02, -2.7027e-03, -2.5089e-02, -8.1558e-03, -4.1017e-04,\n",
       "                       6.8220e-03, -8.1937e-02,  1.0542e-02, -1.0830e-03, -1.0672e-02,\n",
       "                       1.1617e-02, -2.8914e-02, -3.6339e-02, -9.9233e-02, -5.3854e-02,\n",
       "                      -5.6811e-03,  1.1256e-02, -1.7377e-02, -4.4052e-02, -1.7164e-02,\n",
       "                      -4.7094e-02,  6.7912e-04, -4.6022e-03,  7.6383e-03, -4.7448e-02,\n",
       "                      -2.5280e-02, -1.4747e-02, -2.5182e-02,  4.0010e-04, -9.2456e-03,\n",
       "                       6.4018e-04, -7.3553e-03, -1.4147e-02,  1.1804e-02, -2.8889e-02,\n",
       "                      -5.9345e-02, -2.4266e-02, -3.6466e-02, -5.8635e-02,  9.5444e-03,\n",
       "                       4.1952e-03,  5.8752e-03,  1.9725e-03, -1.4843e-02, -8.6561e-03,\n",
       "                      -4.2111e-02, -3.0300e-02])),\n",
       "             ('layer2.3.bn3.running_mean',\n",
       "              tensor([-8.8094e-04, -2.9788e-04,  2.1665e-03, -8.8872e-04,  5.3767e-05,\n",
       "                       2.8791e-03,  5.1532e-03,  4.3556e-03, -6.1866e-05,  5.6242e-03,\n",
       "                      -6.2244e-03,  1.0585e-04, -1.7194e-03, -4.7484e-04,  4.6142e-03,\n",
       "                       5.5013e-03, -1.0163e-03,  1.5675e-04,  8.7022e-03,  1.7179e-03,\n",
       "                       1.3955e-03, -2.6260e-03, -2.4981e-05,  3.5618e-04,  1.5844e-04,\n",
       "                      -1.6093e-03, -1.7471e-03, -8.0610e-04, -8.8994e-06,  5.2564e-04,\n",
       "                      -1.3273e-03,  1.1527e-03,  1.1882e-04,  6.0465e-04, -2.0717e-03,\n",
       "                      -4.1473e-03,  1.0796e-03, -2.8785e-04, -9.0228e-04, -3.8319e-04,\n",
       "                      -2.5092e-03,  3.5579e-03, -9.2945e-05,  5.5626e-03,  4.6433e-04,\n",
       "                      -2.3588e-03,  2.3881e-03, -5.0287e-04, -3.3894e-03, -7.8441e-04,\n",
       "                      -4.9491e-03, -3.9771e-03,  2.2114e-03,  6.0907e-04, -1.4846e-05,\n",
       "                       2.8115e-04, -3.3444e-04,  2.4719e-03, -4.2172e-03, -3.9675e-03,\n",
       "                       4.6392e-05,  3.8795e-03,  5.5773e-03,  5.4150e-04, -7.7981e-04,\n",
       "                      -6.5779e-05,  1.5547e-03, -3.1751e-03, -6.8874e-04,  7.8500e-04,\n",
       "                       1.4142e-03,  5.5754e-04,  1.5277e-03,  1.7072e-03, -4.6804e-04,\n",
       "                       1.6623e-03,  1.0192e-03,  7.2083e-04, -2.3100e-04,  4.3088e-03,\n",
       "                      -9.0802e-04,  3.3806e-05,  2.1400e-04,  4.6913e-03,  3.8280e-03,\n",
       "                       3.2727e-03, -3.5006e-03, -2.9651e-04,  1.4433e-03, -1.9269e-04,\n",
       "                      -2.6731e-03,  7.9665e-04, -6.2562e-03, -1.2063e-04,  1.5133e-02,\n",
       "                      -2.9739e-03,  2.7871e-03,  3.0122e-04,  2.3660e-04, -1.3380e-04,\n",
       "                      -3.7777e-03, -2.9224e-03,  1.5392e-03,  5.7715e-05, -8.1397e-04,\n",
       "                      -1.1101e-02, -7.1907e-03, -3.3378e-03, -4.8098e-03,  9.7925e-04,\n",
       "                      -2.6369e-03, -5.1836e-03, -6.7285e-04, -1.3711e-03, -1.4247e-02,\n",
       "                      -7.6445e-04,  3.2470e-04, -1.9194e-03,  1.1298e-03, -4.5833e-03,\n",
       "                       7.1093e-04,  4.7704e-03,  4.0688e-03, -9.4563e-04, -5.3216e-04,\n",
       "                       4.9045e-03, -4.4581e-03,  1.0279e-03,  3.9187e-03,  3.4891e-03,\n",
       "                      -3.1280e-04,  1.7047e-03,  1.5578e-03,  4.4944e-03, -8.2508e-04,\n",
       "                       7.7342e-03, -2.5262e-04, -3.6254e-04,  1.2958e-03,  1.4393e-03,\n",
       "                       2.5964e-03, -6.7764e-03,  4.7671e-03, -7.1917e-04, -2.7927e-03,\n",
       "                       9.0859e-04,  1.3706e-03,  1.1201e-02, -3.2437e-03, -3.1098e-03,\n",
       "                       1.6424e-03, -1.4195e-03, -1.1507e-03,  4.9599e-03, -2.2986e-04,\n",
       "                       1.0218e-03,  1.4550e-03, -1.6678e-03, -3.3420e-03,  6.8063e-03,\n",
       "                      -1.5269e-03,  5.1144e-05,  3.3350e-03,  1.2331e-04, -7.0568e-04,\n",
       "                       4.8711e-04,  1.1994e-03, -4.0107e-04, -4.7337e-04,  9.7175e-03,\n",
       "                      -1.8967e-03, -8.9997e-03, -4.2287e-03,  2.3833e-04,  6.1885e-03,\n",
       "                       6.1193e-04,  7.8112e-03,  7.8210e-04, -1.6224e-04, -4.9427e-05,\n",
       "                      -6.0914e-03, -6.2696e-03, -2.0584e-03,  1.0771e-03,  8.1849e-03,\n",
       "                      -1.2406e-03, -9.7406e-03,  3.1031e-04, -2.5206e-04,  5.9448e-04,\n",
       "                      -5.4489e-03,  1.0042e-02, -1.7294e-03, -9.4456e-04, -4.9993e-03,\n",
       "                       1.9829e-04, -4.0720e-03, -1.2648e-03,  1.2013e-03,  1.1183e-03,\n",
       "                      -2.4366e-03, -2.5625e-03, -1.8566e-03,  1.1229e-03,  5.7163e-04,\n",
       "                      -5.5702e-03,  5.2730e-04,  9.3718e-03, -1.8743e-03,  1.0657e-02,\n",
       "                      -1.6427e-03,  2.1568e-03,  2.2571e-03, -4.8011e-04, -1.2512e-03,\n",
       "                      -6.3985e-04,  3.0499e-03, -4.1042e-03,  4.6400e-04,  6.9906e-05,\n",
       "                      -4.0888e-03, -2.9462e-03,  1.3327e-04, -2.5887e-03, -1.6977e-03,\n",
       "                       6.1933e-03, -1.3205e-03, -5.1659e-04, -2.0512e-03, -4.2588e-04,\n",
       "                       7.6794e-05, -5.7527e-04,  5.8477e-04,  1.9614e-03,  2.2217e-04,\n",
       "                      -5.6448e-04,  4.6374e-03,  1.4201e-03,  1.0927e-03, -2.9383e-03,\n",
       "                       1.9072e-03,  1.8896e-04,  1.8539e-03,  1.1261e-04,  1.3058e-02,\n",
       "                       1.2225e-03,  2.3843e-04, -2.6234e-03,  2.8529e-03, -5.3409e-03,\n",
       "                       1.8054e-03, -2.7868e-06,  1.9768e-03, -6.7890e-04, -1.0818e-02,\n",
       "                       5.2047e-04,  6.7244e-04,  4.5984e-03, -1.9944e-03,  4.8515e-04,\n",
       "                       6.3928e-04, -1.3808e-03,  3.2471e-03, -2.6212e-03,  1.5906e-03,\n",
       "                      -7.3509e-04, -1.9429e-03,  2.5104e-03,  6.2664e-03,  6.2086e-04,\n",
       "                       2.4376e-03, -3.6753e-03, -7.9424e-03, -1.1544e-03,  1.6103e-03,\n",
       "                      -8.2238e-04,  2.3876e-03,  6.9489e-04,  1.9309e-03, -3.3215e-03,\n",
       "                       2.4850e-03, -2.7576e-03,  2.9694e-04, -6.3101e-03, -6.6061e-04,\n",
       "                      -1.1597e-04,  2.7142e-03, -4.5345e-04, -3.4023e-03, -1.8412e-03,\n",
       "                       1.6756e-03, -3.1205e-04,  8.4708e-04,  4.7174e-03, -1.6668e-04,\n",
       "                      -2.5534e-03,  9.9138e-05, -4.3278e-03,  2.3717e-03,  3.3993e-04,\n",
       "                      -3.2908e-03,  3.1725e-03,  1.2060e-03, -1.6579e-03, -5.0890e-04,\n",
       "                       1.8417e-03, -9.9099e-04, -3.0612e-05, -1.6979e-03, -3.1630e-05,\n",
       "                       1.7043e-03,  1.9267e-05, -7.9690e-04,  2.0478e-04, -2.3257e-03,\n",
       "                       1.4358e-03,  4.0504e-03,  6.6703e-03, -1.0075e-04, -1.3359e-02,\n",
       "                      -2.2939e-03,  6.8654e-04,  5.5276e-03,  1.1679e-03,  7.2993e-03,\n",
       "                      -3.0346e-03, -6.1998e-04,  2.5870e-03,  1.1815e-03, -3.9094e-04,\n",
       "                       7.3813e-03,  2.4694e-04,  1.4733e-03,  2.3962e-03, -5.9245e-04,\n",
       "                      -5.5982e-03,  3.1397e-03, -5.0445e-05,  1.7968e-03,  2.0622e-03,\n",
       "                       7.7766e-04,  6.1553e-03,  4.8093e-03,  1.1142e-02, -1.5076e-03,\n",
       "                      -3.4481e-04, -2.1099e-03,  3.3400e-03, -3.3002e-05, -4.1402e-03,\n",
       "                       3.1172e-04,  1.5387e-03, -1.6172e-03, -2.1976e-03,  3.8279e-03,\n",
       "                      -8.3748e-04, -2.6146e-03, -1.9762e-03,  1.9730e-03,  5.4353e-03,\n",
       "                       3.6203e-04,  1.4658e-03,  2.8958e-05, -3.2065e-03, -1.6309e-03,\n",
       "                       2.1168e-03,  1.3920e-03,  8.1939e-04, -9.8839e-04, -5.4513e-03,\n",
       "                      -2.4106e-04,  2.9708e-03, -2.3026e-03,  9.1215e-04,  6.4507e-04,\n",
       "                      -5.8936e-04, -3.5321e-05,  3.2792e-03,  5.0953e-04,  1.2879e-03,\n",
       "                       2.4995e-03, -3.3667e-03, -1.0182e-03,  1.4089e-04,  3.9400e-03,\n",
       "                       2.8922e-03, -1.6193e-03, -1.0571e-04, -1.0702e-03, -3.8278e-03,\n",
       "                      -4.6063e-03, -1.2926e-05, -3.7563e-03, -2.0202e-04, -1.3169e-03,\n",
       "                      -1.9855e-05, -2.3253e-03, -1.1633e-04,  5.0784e-04,  2.0701e-03,\n",
       "                      -5.0983e-04,  3.3938e-03,  2.1229e-03, -1.7393e-04, -1.8716e-04,\n",
       "                       3.3877e-03, -2.7983e-03,  1.6401e-03,  1.6480e-04,  2.0277e-03,\n",
       "                       2.5959e-04,  2.4199e-03,  2.4699e-03,  1.4135e-03,  1.2637e-02,\n",
       "                      -8.9997e-04,  2.5780e-03,  1.6812e-05, -1.4578e-02,  2.5915e-03,\n",
       "                       3.9973e-03, -2.8131e-04,  2.0349e-03,  2.1389e-03, -2.4377e-04,\n",
       "                       2.6045e-04,  1.7147e-03, -2.2964e-03, -2.3288e-03,  1.9266e-04,\n",
       "                       1.1838e-03, -7.0376e-03, -1.4501e-02,  4.9576e-03,  4.2674e-03,\n",
       "                       2.4623e-03,  1.3654e-03,  8.6872e-04, -4.4679e-03,  1.1630e-03,\n",
       "                      -1.8628e-05,  7.4047e-03,  1.1217e-03, -5.6167e-03, -2.8238e-03,\n",
       "                       5.0102e-03,  1.3129e-03, -3.0243e-03,  6.5608e-04, -7.3581e-04,\n",
       "                      -3.2988e-03, -5.0193e-03,  2.8932e-03, -1.4423e-03, -2.8124e-04,\n",
       "                      -8.5516e-03, -2.5084e-03, -7.8713e-04, -1.5940e-03,  3.4000e-03,\n",
       "                      -2.6805e-04, -1.5903e-03,  8.1561e-03,  2.7251e-03, -7.2165e-04,\n",
       "                      -2.1933e-03, -9.0189e-04,  3.3605e-03, -1.6188e-04, -7.4405e-04,\n",
       "                      -4.8757e-04, -2.6901e-03, -2.0234e-03, -3.0866e-04,  2.1808e-04,\n",
       "                       1.0870e-03,  2.7053e-03,  3.1887e-03, -2.9666e-03, -4.7016e-03,\n",
       "                       2.1781e-05, -2.4992e-03,  2.1993e-03,  7.9392e-03,  2.1106e-04,\n",
       "                      -5.2147e-04, -8.6099e-04, -8.5435e-04,  9.0313e-04,  1.1626e-02,\n",
       "                      -1.8784e-03,  4.3453e-03,  4.4193e-04,  1.0166e-04, -4.6259e-03,\n",
       "                      -6.8391e-05, -3.0913e-04,  1.6952e-04,  2.5336e-03,  3.8347e-03,\n",
       "                       7.8339e-03, -4.8654e-04, -3.7139e-03,  3.9801e-04, -1.0391e-03,\n",
       "                      -1.3617e-03, -2.2177e-03,  5.9005e-04,  3.4200e-03, -6.8885e-04,\n",
       "                      -7.0597e-04,  1.3688e-03])),\n",
       "             ('layer2.3.bn3.running_var',\n",
       "              tensor([3.9218e-05, 1.6909e-05, 3.2802e-05, 1.1225e-04, 6.2057e-05, 1.1080e-05,\n",
       "                      1.1492e-04, 2.0028e-04, 1.1495e-08, 1.0241e-04, 5.1253e-05, 4.6393e-05,\n",
       "                      4.5524e-05, 1.5146e-06, 1.0827e-04, 4.2587e-05, 1.5412e-05, 3.4913e-05,\n",
       "                      1.1012e-04, 4.8661e-05, 4.7186e-05, 4.6256e-05, 4.8208e-05, 8.9156e-06,\n",
       "                      2.7572e-06, 9.2049e-05, 7.5321e-06, 3.5561e-06, 1.6114e-05, 5.3372e-05,\n",
       "                      9.8915e-06, 2.6620e-06, 7.9901e-06, 1.1876e-04, 1.0208e-04, 8.8061e-05,\n",
       "                      8.0424e-05, 2.2590e-06, 5.8986e-06, 1.9463e-05, 3.6930e-05, 2.5734e-05,\n",
       "                      1.9408e-06, 4.7610e-05, 4.7604e-05, 4.2502e-05, 1.8056e-05, 7.2965e-06,\n",
       "                      3.1693e-05, 1.0540e-05, 1.1072e-04, 7.9428e-05, 9.4004e-06, 1.3344e-06,\n",
       "                      1.1946e-05, 1.3924e-05, 9.9605e-06, 1.0012e-04, 1.0663e-04, 1.3528e-04,\n",
       "                      1.0450e-04, 1.1034e-04, 9.4704e-05, 3.2211e-05, 1.5436e-04, 1.1729e-05,\n",
       "                      6.2530e-05, 3.2013e-05, 1.1198e-04, 1.0349e-04, 4.4662e-05, 9.8363e-07,\n",
       "                      1.6111e-04, 1.0588e-04, 4.1471e-05, 1.4765e-04, 2.9070e-05, 7.7965e-06,\n",
       "                      2.6302e-05, 3.6884e-05, 2.0036e-05, 1.3115e-05, 2.4445e-07, 4.9035e-05,\n",
       "                      3.6221e-05, 2.6043e-05, 9.7464e-05, 1.1640e-04, 8.3620e-05, 1.4625e-05,\n",
       "                      1.1384e-04, 5.7817e-05, 6.8041e-05, 4.2763e-05, 1.4716e-04, 1.7402e-05,\n",
       "                      1.0065e-05, 1.8112e-06, 4.5347e-05, 4.8295e-05, 4.6428e-05, 3.7151e-05,\n",
       "                      5.2834e-05, 1.7646e-06, 3.4201e-05, 9.4097e-05, 1.3346e-04, 8.3310e-05,\n",
       "                      8.0224e-05, 2.6619e-05, 2.1277e-05, 5.4927e-05, 3.5629e-05, 1.7638e-05,\n",
       "                      1.5556e-04, 1.4957e-04, 1.9578e-05, 8.0747e-05, 1.4412e-06, 1.2422e-05,\n",
       "                      2.9857e-05, 5.5361e-05, 6.7406e-05, 1.4631e-05, 5.4135e-05, 6.2110e-05,\n",
       "                      1.3509e-04, 4.9745e-05, 2.4363e-05, 9.6558e-06, 1.0776e-06, 2.4251e-05,\n",
       "                      5.8488e-05, 1.7557e-05, 2.5345e-05, 2.1224e-04, 1.7336e-04, 1.4252e-04,\n",
       "                      8.4178e-06, 1.5270e-05, 1.6083e-04, 4.0666e-05, 1.3592e-04, 5.7414e-07,\n",
       "                      3.6431e-05, 2.3320e-05, 2.3178e-05, 1.1207e-04, 3.5485e-05, 1.5153e-04,\n",
       "                      2.1590e-05, 2.8790e-05, 2.7064e-05, 6.1352e-05, 2.1265e-06, 1.6394e-05,\n",
       "                      2.4912e-05, 5.3812e-05, 1.0522e-04, 7.0947e-05, 1.7062e-05, 5.6920e-05,\n",
       "                      3.5584e-05, 5.0556e-05, 1.4440e-05, 2.7015e-05, 7.3488e-05, 4.9335e-06,\n",
       "                      2.1123e-06, 5.8988e-05, 1.1167e-04, 1.1942e-04, 4.5449e-05, 2.7595e-05,\n",
       "                      2.3019e-04, 2.2075e-04, 8.9950e-05, 1.5126e-05, 7.8757e-06, 2.6479e-05,\n",
       "                      3.5187e-05, 3.2026e-05, 3.0032e-05, 6.1071e-06, 1.9922e-04, 6.7441e-05,\n",
       "                      9.0529e-05, 1.9580e-05, 5.4063e-06, 3.9112e-05, 1.3015e-04, 5.1974e-05,\n",
       "                      1.4871e-05, 4.4445e-05, 3.4429e-05, 1.3805e-05, 9.6084e-05, 1.3527e-05,\n",
       "                      1.8577e-05, 3.0569e-05, 8.9892e-05, 5.0013e-05, 6.9066e-06, 1.2388e-04,\n",
       "                      1.1644e-05, 1.8415e-04, 4.4646e-06, 9.8581e-05, 4.9930e-05, 1.0071e-04,\n",
       "                      9.9621e-06, 6.1112e-05, 1.5605e-04, 4.9743e-05, 2.1970e-05, 2.1504e-05,\n",
       "                      1.3843e-04, 2.6351e-04, 9.1529e-06, 1.3770e-07, 1.8154e-05, 1.3010e-05,\n",
       "                      2.2228e-05, 4.5907e-05, 4.7698e-05, 2.4839e-04, 2.0521e-05, 6.9852e-06,\n",
       "                      5.5564e-05, 2.7042e-06, 1.1523e-06, 7.9332e-05, 1.6939e-05, 9.1077e-05,\n",
       "                      1.0770e-05, 4.3576e-05, 2.2151e-05, 7.4888e-06, 9.8137e-06, 1.3417e-04,\n",
       "                      1.3017e-04, 8.3091e-06, 3.7249e-05, 4.0854e-05, 1.1795e-04, 3.4220e-05,\n",
       "                      8.8750e-05, 1.2164e-05, 1.5238e-04, 2.9608e-04, 6.2361e-05, 1.7020e-11,\n",
       "                      2.7397e-05, 6.4315e-05, 1.0008e-04, 6.9999e-06, 1.3918e-05, 3.4511e-05,\n",
       "                      9.9167e-05, 1.2244e-05, 2.4152e-06, 7.7589e-06, 6.0891e-05, 5.0703e-05,\n",
       "                      1.2400e-05, 2.2084e-05, 2.2276e-05, 4.4338e-05, 6.3243e-05, 1.3346e-05,\n",
       "                      5.1735e-05, 9.0589e-05, 5.8397e-05, 4.6503e-05, 6.8059e-05, 6.3669e-05,\n",
       "                      4.6298e-05, 2.5729e-05, 8.5051e-05, 7.8792e-05, 7.0654e-05, 1.4524e-04,\n",
       "                      7.6258e-06, 6.6981e-05, 8.0200e-06, 1.4510e-05, 3.1414e-05, 2.3723e-05,\n",
       "                      7.8931e-05, 2.2716e-05, 3.0506e-05, 4.4578e-06, 1.3922e-05, 1.2095e-04,\n",
       "                      2.9848e-05, 1.4528e-04, 1.0368e-05, 3.8856e-05, 1.5735e-05, 4.7113e-05,\n",
       "                      4.3217e-05, 2.8353e-05, 2.1197e-05, 8.9221e-05, 7.2071e-06, 1.5159e-05,\n",
       "                      1.5979e-05, 6.7399e-06, 3.0490e-05, 6.4801e-06, 8.3770e-05, 1.2249e-05,\n",
       "                      4.0980e-05, 3.0326e-06, 3.3744e-05, 2.7898e-05, 6.1872e-05, 3.5376e-05,\n",
       "                      2.4576e-08, 1.1576e-04, 3.2482e-05, 1.0505e-05, 8.4364e-05, 1.5515e-05,\n",
       "                      2.4707e-04, 6.6937e-05, 1.1756e-06, 3.7204e-05, 6.5667e-05, 2.4600e-05,\n",
       "                      8.0084e-05, 2.6808e-05, 6.6551e-05, 6.2080e-05, 2.0716e-05, 4.2992e-05,\n",
       "                      7.0921e-05, 8.0027e-05, 1.3162e-05, 2.2764e-05, 2.0444e-05, 4.9065e-05,\n",
       "                      1.2801e-04, 1.0351e-04, 6.4981e-06, 5.3527e-05, 1.4665e-05, 8.0397e-05,\n",
       "                      6.6975e-07, 1.3008e-04, 1.1846e-05, 8.9818e-06, 6.6457e-06, 4.9730e-05,\n",
       "                      4.2513e-05, 2.5750e-05, 4.4763e-05, 1.7895e-05, 6.9304e-05, 2.7566e-05,\n",
       "                      2.2399e-05, 1.9846e-05, 9.6130e-06, 1.0131e-04, 7.1600e-05, 5.1203e-05,\n",
       "                      6.2536e-05, 6.9419e-05, 9.3660e-06, 1.2659e-04, 8.0055e-05, 5.5054e-05,\n",
       "                      4.5030e-05, 1.0422e-04, 1.5576e-05, 8.7969e-06, 4.0123e-05, 2.2727e-05,\n",
       "                      5.5139e-05, 5.7979e-05, 1.1918e-04, 5.3493e-05, 2.6431e-05, 4.5268e-06,\n",
       "                      1.4452e-04, 6.3314e-05, 6.7578e-05, 1.1681e-05, 6.0159e-05, 3.4753e-05,\n",
       "                      1.8519e-05, 4.1932e-06, 1.1084e-04, 2.6040e-05, 1.3631e-05, 1.2291e-05,\n",
       "                      1.4616e-05, 9.1829e-06, 5.4358e-05, 1.3609e-05, 1.3113e-04, 2.6382e-05,\n",
       "                      5.8665e-05, 6.4315e-06, 3.6489e-05, 1.3342e-04, 1.8328e-04, 4.3343e-05,\n",
       "                      2.9062e-05, 2.4762e-05, 1.0815e-05, 3.0181e-05, 8.1088e-05, 2.6270e-04,\n",
       "                      1.1353e-04, 3.5698e-05, 2.3672e-05, 3.6842e-05, 1.5713e-04, 4.5475e-05,\n",
       "                      4.7314e-05, 1.9430e-05, 1.0681e-04, 7.1512e-05, 3.5572e-06, 3.0112e-05,\n",
       "                      1.3581e-05, 1.0046e-04, 4.5973e-05, 1.6802e-04, 6.2926e-05, 2.6765e-04,\n",
       "                      9.8643e-05, 2.1756e-05, 1.6635e-05, 6.5876e-05, 1.3874e-05, 1.2957e-05,\n",
       "                      8.0877e-05, 2.8733e-05, 1.4066e-06, 1.7357e-04, 4.6948e-05, 5.9135e-05,\n",
       "                      4.3505e-05, 3.2570e-05, 8.1123e-05, 3.3574e-05, 2.0672e-05, 1.7786e-05,\n",
       "                      9.7109e-05, 4.4867e-05, 5.2887e-05, 2.1209e-05, 8.2771e-05, 9.1791e-05,\n",
       "                      6.2433e-06, 9.7611e-05, 2.1966e-06, 1.1975e-04, 8.6989e-06, 8.2662e-05,\n",
       "                      1.1972e-04, 3.7723e-05, 1.1104e-04, 8.2370e-05, 4.8816e-06, 3.5092e-05,\n",
       "                      1.7024e-06, 1.1383e-04, 4.9772e-05, 5.1929e-05, 1.0679e-04, 1.6107e-05,\n",
       "                      8.9323e-05, 2.5080e-05, 9.2151e-05, 4.3611e-05, 1.8875e-04, 6.6041e-05,\n",
       "                      4.4725e-07, 8.7008e-05, 7.8560e-05, 1.0354e-04, 2.6524e-05, 1.6443e-05,\n",
       "                      4.1232e-06, 5.9899e-06, 1.7140e-05, 2.3689e-04, 2.3282e-05, 1.6495e-05,\n",
       "                      7.7258e-06, 9.5050e-07, 5.2892e-05, 1.4681e-05, 2.0946e-05, 1.0542e-05,\n",
       "                      6.6371e-05, 2.5119e-05, 1.2982e-04, 1.8865e-06, 3.7183e-05, 9.2517e-05,\n",
       "                      1.7756e-05, 2.3994e-05, 5.9822e-05, 7.8385e-06, 3.2124e-05, 9.3501e-05,\n",
       "                      4.5909e-05, 2.3308e-05])),\n",
       "             ('layer2.3.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.0.conv1.weight',\n",
       "              tensor([[[[ 2.9959e-04]],\n",
       "              \n",
       "                       [[-6.0943e-03]],\n",
       "              \n",
       "                       [[-1.1900e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.1821e-03]],\n",
       "              \n",
       "                       [[ 9.7294e-03]],\n",
       "              \n",
       "                       [[ 1.5060e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.9574e-03]],\n",
       "              \n",
       "                       [[ 6.9012e-03]],\n",
       "              \n",
       "                       [[-1.8818e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.8561e-03]],\n",
       "              \n",
       "                       [[-1.2904e-02]],\n",
       "              \n",
       "                       [[ 5.7749e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.2717e-05]],\n",
       "              \n",
       "                       [[-2.0981e-03]],\n",
       "              \n",
       "                       [[-4.8983e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3172e-02]],\n",
       "              \n",
       "                       [[ 2.0097e-04]],\n",
       "              \n",
       "                       [[-2.5511e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-7.4010e-03]],\n",
       "              \n",
       "                       [[ 1.9130e-03]],\n",
       "              \n",
       "                       [[-1.0337e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3273e-02]],\n",
       "              \n",
       "                       [[-8.1771e-03]],\n",
       "              \n",
       "                       [[-2.2129e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.1404e-03]],\n",
       "              \n",
       "                       [[-4.0594e-03]],\n",
       "              \n",
       "                       [[-9.0241e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0022e-02]],\n",
       "              \n",
       "                       [[-1.0572e-04]],\n",
       "              \n",
       "                       [[ 1.0518e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8423e-03]],\n",
       "              \n",
       "                       [[ 6.4203e-03]],\n",
       "              \n",
       "                       [[ 7.6690e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1973e-02]],\n",
       "              \n",
       "                       [[ 7.0963e-03]],\n",
       "              \n",
       "                       [[-4.2810e-03]]]])),\n",
       "             ('layer3.0.bn1.weight',\n",
       "              tensor([1.1482e-01, 1.1859e-01, 8.3628e-02, 5.6589e-02, 9.9791e-02, 1.1886e-01,\n",
       "                      8.1790e-02, 9.6089e-02, 6.5366e-02, 1.1084e-01, 4.4668e-02, 1.4861e-01,\n",
       "                      1.2887e-01, 6.7883e-02, 8.3320e-02, 9.3840e-02, 9.7619e-02, 1.2883e-01,\n",
       "                      2.9324e-02, 1.1305e-01, 1.0229e-01, 7.1134e-02, 1.0904e-01, 9.3652e-02,\n",
       "                      7.6352e-02, 5.1161e-02, 1.0105e-01, 6.9500e-02, 7.3253e-02, 1.3368e-01,\n",
       "                      1.3137e-01, 7.3219e-02, 9.2654e-02, 7.8324e-02, 1.0617e-01, 1.0567e-01,\n",
       "                      9.4249e-02, 1.5036e-01, 5.6175e-02, 1.3828e-01, 8.7031e-02, 1.2517e-01,\n",
       "                      7.5022e-02, 1.2319e-01, 8.3966e-02, 1.2173e-01, 8.1430e-02, 5.4151e-02,\n",
       "                      1.1707e-01, 1.2796e-01, 9.3403e-02, 9.7870e-02, 1.0940e-01, 1.2837e-01,\n",
       "                      1.5649e-01, 8.0890e-02, 1.1532e-01, 1.3485e-01, 1.5452e-01, 1.0619e-01,\n",
       "                      9.9504e-02, 4.5885e-02, 7.0262e-02, 1.2030e-01, 1.2439e-01, 7.7887e-02,\n",
       "                      9.1679e-02, 1.0699e-01, 5.0336e-02, 1.3819e-01, 6.4740e-02, 8.6812e-02,\n",
       "                      5.3903e-02, 1.1574e-01, 1.5019e-01, 1.0281e-01, 1.3720e-01, 9.3537e-02,\n",
       "                      7.3315e-02, 1.0445e-01, 1.8685e-01, 1.2792e-01, 1.1612e-01, 1.4239e-01,\n",
       "                      7.2164e-02, 9.0262e-02, 1.0773e-01, 6.9908e-02, 8.5661e-02, 8.4988e-02,\n",
       "                      1.3717e-01, 1.1479e-01, 8.3493e-02, 1.0967e-01, 1.1220e-01, 1.4500e-01,\n",
       "                      7.7125e-02, 1.2602e-01, 9.7671e-02, 1.1111e-01, 1.0273e-01, 8.5319e-02,\n",
       "                      6.1465e-02, 9.7266e-02, 7.4631e-02, 1.1016e-01, 1.0152e-01, 1.1460e-01,\n",
       "                      1.4461e-01, 1.2946e-01, 1.0638e-01, 6.8328e-02, 8.4017e-02, 1.5197e-01,\n",
       "                      8.2654e-02, 8.7219e-02, 1.3560e-01, 1.4036e-01, 7.1761e-02, 1.0760e-01,\n",
       "                      9.8644e-02, 7.8301e-02, 1.4239e-01, 8.7475e-02, 8.2175e-02, 5.7658e-02,\n",
       "                      1.1191e-01, 1.3224e-01, 8.9105e-02, 1.1913e-01, 1.2450e-01, 1.1549e-01,\n",
       "                      1.7115e-01, 9.9369e-02, 9.2128e-02, 6.8466e-02, 1.3383e-01, 1.0475e-01,\n",
       "                      1.1118e-01, 9.5805e-02, 5.5489e-02, 7.8952e-02, 1.5712e-01, 8.7024e-02,\n",
       "                      1.0937e-01, 1.4911e-01, 1.1234e-01, 1.6140e-01, 1.0386e-01, 4.7493e-02,\n",
       "                      1.1542e-01, 7.5579e-02, 1.3624e-01, 1.5119e-01, 1.0943e-01, 7.2594e-02,\n",
       "                      1.0230e-01, 1.5196e-01, 3.8638e-02, 1.1448e-01, 9.7609e-02, 8.9229e-02,\n",
       "                      7.3391e-02, 5.7155e-06, 5.0261e-02, 1.3843e-01, 8.0967e-02, 6.7985e-02,\n",
       "                      8.8380e-02, 1.2486e-01, 1.7045e-01, 7.9310e-02, 9.8446e-02, 6.1524e-02,\n",
       "                      1.7613e-01, 1.7491e-01, 6.5964e-02, 1.3051e-01, 6.8957e-02, 9.8137e-02,\n",
       "                      1.0046e-01, 1.3380e-01, 4.7673e-02, 9.1825e-02, 1.0713e-01, 6.9448e-02,\n",
       "                      1.1404e-01, 7.2104e-02, 1.5634e-01, 6.8032e-02, 1.6278e-01, 1.5990e-01,\n",
       "                      7.7523e-02, 8.6351e-02, 1.0210e-01, 1.1610e-01, 1.3479e-01, 9.5615e-02,\n",
       "                      8.0835e-02, 9.4379e-02, 9.9265e-02, 1.3638e-01, 9.7847e-02, 7.1763e-02,\n",
       "                      9.8985e-02, 1.1087e-01, 1.2031e-01, 8.6072e-02, 5.5119e-02, 1.0923e-01,\n",
       "                      5.6843e-02, 7.2727e-02, 7.4370e-02, 1.1644e-01, 1.2417e-01, 4.2632e-02,\n",
       "                      1.3230e-01, 1.3455e-01, 7.2736e-02, 1.0388e-01, 8.4545e-02, 6.0979e-02,\n",
       "                      6.7048e-02, 1.1829e-01, 1.9323e-01, 7.0687e-02, 1.0201e-01, 9.9627e-02,\n",
       "                      7.1763e-02, 9.9141e-02, 7.1159e-02, 8.2729e-02, 1.1737e-01, 1.0841e-01,\n",
       "                      6.5894e-02, 1.0760e-01, 6.8742e-02, 1.3257e-01, 3.5190e-02, 1.0497e-01,\n",
       "                      1.3214e-01, 1.0111e-01, 1.0036e-01, 9.6738e-02, 1.0458e-01, 1.2582e-01,\n",
       "                      5.0534e-02, 9.9136e-02, 9.3266e-02, 7.7820e-02, 1.1662e-01, 6.8149e-02,\n",
       "                      8.0858e-02, 8.0156e-02, 1.1158e-01, 1.4769e-01])),\n",
       "             ('layer3.0.bn1.bias',\n",
       "              tensor([-8.1548e-02, -2.6107e-02, -5.5689e-02, -2.2262e-02, -1.1648e-02,\n",
       "                      -4.4581e-02,  1.6188e-03, -4.5779e-02, -3.1679e-02, -6.3325e-02,\n",
       "                      -2.4770e-02, -1.0053e-01, -6.8927e-02, -2.5099e-02,  1.6485e-02,\n",
       "                      -3.3843e-02, -6.0616e-02, -8.5248e-02,  5.9460e-03, -1.1835e-01,\n",
       "                      -6.1497e-02,  3.2756e-02, -4.4187e-02, -6.3621e-02,  1.7117e-02,\n",
       "                       4.2591e-02, -4.9105e-02, -1.4289e-02, -2.2262e-02, -6.9455e-02,\n",
       "                      -7.2984e-02, -1.4537e-02, -1.0662e-02, -2.3432e-02, -4.5251e-02,\n",
       "                      -3.0986e-02, -3.8953e-02, -3.2009e-02,  1.2680e-03, -6.3915e-02,\n",
       "                      -2.4055e-02, -9.5809e-02, -1.6349e-02, -6.8704e-02, -4.9951e-02,\n",
       "                      -5.5801e-02, -4.4443e-02, -1.0878e-02, -6.0385e-02, -6.6516e-02,\n",
       "                      -5.8532e-02, -3.0374e-03, -8.0730e-02,  2.0472e-03, -5.0116e-02,\n",
       "                      -1.6696e-02, -4.9755e-02, -8.5689e-02, -5.1056e-02, -1.6627e-04,\n",
       "                      -5.7446e-02,  9.4940e-03, -6.3148e-03, -9.3606e-02, -6.0145e-02,\n",
       "                      -3.4320e-02, -6.7000e-02, -5.5955e-02, -1.3289e-02, -8.1459e-02,\n",
       "                      -1.7407e-02, -1.3341e-02,  8.1589e-03, -9.0305e-02, -1.1178e-01,\n",
       "                      -4.2136e-02, -1.2948e-01, -5.6893e-02, -3.7606e-02, -5.5946e-02,\n",
       "                      -9.2583e-02, -3.7260e-02, -5.8893e-02, -6.3261e-02, -1.8121e-02,\n",
       "                      -2.6518e-02, -3.2867e-02, -1.5330e-02,  1.5501e-02, -2.6899e-02,\n",
       "                      -9.4782e-02, -4.7616e-02, -1.3187e-02, -6.1234e-02, -3.0297e-02,\n",
       "                      -7.7604e-02, -6.2969e-02, -7.2373e-02, -3.1855e-02,  1.8466e-03,\n",
       "                      -6.8336e-02, -1.7520e-02,  3.7432e-02, -1.5817e-02, -1.9967e-02,\n",
       "                      -6.7257e-03, -4.3942e-02, -3.7831e-02, -1.1985e-01, -1.6652e-02,\n",
       "                      -3.6734e-02, -4.6844e-02,  8.6001e-03, -6.3114e-02, -2.8416e-02,\n",
       "                      -3.9760e-02, -6.5858e-02, -8.0616e-02, -1.7336e-02, -2.5520e-02,\n",
       "                      -5.2211e-02, -2.9989e-02, -6.4638e-02, -3.1647e-02, -1.6925e-02,\n",
       "                      -4.7178e-02, -5.7755e-02, -3.6552e-02, -5.7516e-04,  4.0855e-02,\n",
       "                      -8.7080e-02, -3.9584e-02, -8.6786e-02, -7.6197e-03, -2.8686e-02,\n",
       "                      -3.5493e-02, -6.2939e-02, -8.7158e-02, -7.7850e-02, -2.8284e-02,\n",
       "                      -1.8036e-02, -2.5451e-02, -1.2052e-01, -3.9430e-02, -5.1679e-02,\n",
       "                      -8.4258e-02, -8.6161e-02, -1.1350e-01, -4.0830e-02, -1.1215e-02,\n",
       "                      -5.3256e-02, -4.8353e-03, -6.3637e-02, -8.1965e-02, -2.9016e-02,\n",
       "                      -5.7535e-03, -5.1644e-02, -7.1814e-02, -9.2026e-04, -5.5152e-02,\n",
       "                      -9.2235e-02, -5.4508e-02, -3.6649e-02, -6.3101e-05, -8.2630e-03,\n",
       "                      -7.9485e-02, -3.4689e-02, -2.8292e-02,  1.3628e-02, -2.0527e-02,\n",
       "                      -6.5153e-02, -4.0240e-02, -3.8852e-02, -3.4334e-02, -1.4177e-01,\n",
       "                      -7.0490e-02, -1.7610e-02, -8.8384e-02, -1.9870e-02, -9.5377e-03,\n",
       "                      -1.3418e-02, -8.2090e-02,  1.4970e-02, -1.8712e-02, -4.4991e-02,\n",
       "                      -9.2893e-03, -7.3306e-02, -3.9780e-02, -7.7336e-02, -1.2354e-02,\n",
       "                      -1.3418e-01, -1.4919e-01, -1.0053e-02, -2.0545e-02, -1.1198e-02,\n",
       "                      -7.0297e-02, -2.7983e-02, -7.1150e-02, -4.9583e-02, -3.9292e-02,\n",
       "                      -5.2056e-02, -2.1347e-02, -3.9761e-02, -2.6800e-02, -6.7998e-02,\n",
       "                      -1.1566e-02, -6.2320e-02, -3.4593e-03,  3.7539e-03, -1.0083e-02,\n",
       "                      -5.0930e-03, -4.7487e-03, -2.3910e-02, -5.4343e-02, -6.9798e-02,\n",
       "                       6.6306e-03, -4.4632e-02, -7.5204e-02, -2.0754e-02,  8.6252e-03,\n",
       "                       1.5467e-02, -4.3688e-03,  4.4348e-03, -9.5199e-02, -1.1304e-01,\n",
       "                      -1.9225e-02, -2.8066e-02, -4.9769e-02, -1.1042e-02, -3.8528e-02,\n",
       "                       3.9012e-03,  1.2375e-04, -5.8793e-02, -3.8324e-02, -2.1007e-02,\n",
       "                      -1.9081e-02, -3.0382e-02, -7.6345e-02,  4.8620e-03, -6.2839e-02,\n",
       "                      -4.1560e-02, -3.1580e-02, -3.0650e-02, -4.2526e-02, -3.6722e-02,\n",
       "                      -2.9358e-02, -2.8481e-02, -4.2583e-02, -7.3705e-03, -1.0577e-02,\n",
       "                      -4.3929e-02, -8.4879e-03, -8.9111e-02, -2.0609e-02, -4.5215e-02,\n",
       "                      -9.0846e-02])),\n",
       "             ('layer3.0.bn1.running_mean',\n",
       "              tensor([ 7.4145e-03, -2.1149e-02, -3.5281e-02,  7.0007e-03, -2.8155e-02,\n",
       "                       6.7831e-03, -3.9629e-02, -2.0682e-02, -1.5861e-02, -1.3214e-02,\n",
       "                      -2.9453e-02, -5.7633e-02, -5.2729e-02, -3.0403e-02, -3.4747e-02,\n",
       "                       1.3964e-02, -5.2451e-02, -6.0080e-02, -2.1064e-02, -4.5614e-02,\n",
       "                      -4.1900e-02, -2.0090e-02, -6.7065e-02, -1.6944e-02, -3.1942e-02,\n",
       "                      -3.0898e-02, -2.8665e-02, -1.9065e-02, -3.3823e-02, -9.2443e-02,\n",
       "                      -5.9252e-02, -2.5765e-02, -4.5012e-02, -4.7393e-02, -3.6997e-02,\n",
       "                       9.9539e-03, -8.5474e-03, -6.2897e-02,  2.2702e-02, -2.6786e-02,\n",
       "                      -5.0716e-02, -1.8480e-03, -3.1189e-02, -7.9731e-03, -3.1966e-02,\n",
       "                      -2.2377e-02,  3.1845e-03, -3.2452e-02, -6.9806e-02, -8.0709e-02,\n",
       "                      -2.0832e-02, -3.9063e-02, -5.4390e-02, -7.5655e-02, -3.9340e-02,\n",
       "                      -2.0558e-03, -3.2584e-02, -1.0904e-02, -4.4951e-02,  9.8417e-03,\n",
       "                      -8.3372e-02, -2.1081e-02, -1.6325e-02, -5.2517e-02, -2.7307e-02,\n",
       "                      -5.4574e-02, -1.2262e-02, -5.2947e-02, -1.9824e-02, -2.8135e-02,\n",
       "                      -4.8336e-03,  9.6826e-03, -3.8794e-02, -2.1429e-02, -4.7883e-02,\n",
       "                      -2.9233e-02,  2.6036e-03,  3.4994e-02,  2.1071e-03, -7.0429e-02,\n",
       "                      -8.0255e-02, -1.7817e-02, -8.7964e-03, -5.9089e-02, -2.8799e-02,\n",
       "                      -1.0867e-02, -2.8818e-02, -7.8064e-03, -2.8622e-02, -2.2661e-02,\n",
       "                       1.1995e-02, -6.2671e-02, -3.8009e-02, -5.8287e-03, -7.2886e-02,\n",
       "                      -2.7384e-02, -2.3547e-02, -1.9701e-02, -2.5506e-02, -1.3872e-02,\n",
       "                       4.5561e-02, -1.8688e-02, -1.8213e-02, -2.3984e-02, -2.6193e-02,\n",
       "                      -4.5590e-02, -1.4058e-02, -3.2922e-02,  6.8967e-03,  1.9179e-02,\n",
       "                      -6.9352e-02, -4.1033e-02, -3.2382e-02, -1.3506e-02, -4.6275e-02,\n",
       "                      -4.6463e-03, -7.3017e-02, -4.4127e-02, -5.2231e-02, -5.0545e-02,\n",
       "                      -8.2744e-03, -3.5197e-02, -7.1235e-02, -6.7430e-02, -6.6235e-02,\n",
       "                      -2.8989e-02, -4.0669e-04, -1.7866e-02, -5.5403e-03, -8.0302e-02,\n",
       "                      -5.0319e-02, -3.5947e-02, -2.2966e-02, -3.8102e-02, -3.4854e-02,\n",
       "                       9.7340e-03, -7.1328e-02, -3.3096e-02, -4.7060e-02, -7.3039e-02,\n",
       "                      -6.0644e-02, -7.4861e-02, -8.2310e-03, -2.4138e-02, -2.2171e-03,\n",
       "                      -1.3691e-02,  5.0543e-02, -1.6556e-03, -4.1609e-02,  1.1342e-02,\n",
       "                      -5.2125e-02, -5.6312e-02,  9.6196e-03, -5.2400e-02, -2.7212e-02,\n",
       "                      -7.5383e-02, -2.4102e-02, -6.0787e-02, -2.8389e-02,  8.2572e-03,\n",
       "                       5.6977e-03, -3.5124e-02, -1.5920e-02, -6.4004e-05, -3.6937e-02,\n",
       "                      -5.9492e-02,  2.7157e-02, -2.3267e-02, -2.6648e-02, -8.9029e-02,\n",
       "                      -3.9639e-02, -3.0047e-02, -2.7069e-02, -3.1254e-02, -8.0055e-02,\n",
       "                      -1.1020e-01, -3.8158e-02, -5.8748e-02, -3.1867e-02, -1.7542e-02,\n",
       "                      -6.4751e-02, -1.0593e-02, -2.2152e-02, -5.5012e-02,  4.1101e-03,\n",
       "                      -2.8767e-02, -3.6552e-02, -9.0012e-03, -3.1668e-02, -1.2936e-03,\n",
       "                      -6.4933e-02, -5.1238e-02, -1.9179e-02, -5.0563e-02, -5.8705e-02,\n",
       "                      -4.9892e-02, -4.4077e-02, -1.2642e-02, -2.1092e-02, -4.2231e-02,\n",
       "                      -5.9683e-02, -8.3673e-02, -5.4790e-02, -5.4467e-02, -2.7144e-02,\n",
       "                      -4.7312e-02, -2.0071e-02, -6.1257e-02, -3.6749e-02, -4.1196e-02,\n",
       "                      -2.6618e-02, -3.2399e-02, -2.9924e-02,  1.7485e-02, -2.4242e-02,\n",
       "                      -2.3837e-02, -4.0436e-02, -3.6859e-02, -8.5237e-02, -3.7379e-02,\n",
       "                      -3.3520e-02, -2.2742e-02, -2.4024e-02, -5.3239e-02, -3.0805e-02,\n",
       "                      -1.1561e-02, -5.5195e-02, -4.3489e-02,  1.7696e-03, -1.6010e-02,\n",
       "                      -2.8977e-02, -1.4047e-02, -8.2222e-02, -1.3761e-02, -3.3763e-02,\n",
       "                      -3.9774e-02,  2.1758e-02, -4.3597e-02, -8.4264e-03, -4.8300e-02,\n",
       "                      -4.8790e-02, -5.9351e-02, -5.3137e-02, -2.2016e-02, -1.9723e-02,\n",
       "                      -5.3799e-02, -1.7975e-02, -4.0868e-02, -3.3045e-02, -4.3371e-02,\n",
       "                      -5.7741e-02,  1.1001e-02, -2.5078e-02, -4.6116e-02, -7.0565e-02,\n",
       "                      -7.5229e-02])),\n",
       "             ('layer3.0.bn1.running_var',\n",
       "              tensor([2.8294e-03, 3.5993e-03, 1.2080e-03, 5.3987e-04, 2.5046e-03, 2.9309e-03,\n",
       "                      6.2409e-03, 2.3798e-03, 1.0664e-03, 3.1081e-03, 5.2339e-04, 3.2670e-03,\n",
       "                      3.2269e-03, 1.8028e-03, 6.6796e-03, 2.7606e-03, 1.7150e-03, 3.4299e-03,\n",
       "                      1.9698e-04, 3.6827e-03, 3.2129e-03, 3.6911e-03, 1.5201e-03, 3.6767e-03,\n",
       "                      1.2614e-03, 2.2532e-03, 1.9241e-03, 7.1503e-04, 2.3941e-03, 4.7949e-03,\n",
       "                      4.1991e-03, 2.1398e-03, 1.9030e-03, 2.6984e-03, 2.0536e-03, 2.1778e-03,\n",
       "                      1.7766e-03, 6.2414e-03, 1.2162e-03, 3.7656e-03, 2.6369e-03, 2.4418e-03,\n",
       "                      2.1787e-03, 3.2558e-03, 1.6237e-03, 2.6759e-03, 1.3008e-03, 9.3251e-04,\n",
       "                      6.4515e-03, 2.5774e-03, 2.5454e-03, 2.6338e-03, 3.9833e-03, 5.0074e-03,\n",
       "                      4.4826e-03, 3.3048e-03, 2.5803e-03, 2.5924e-03, 3.3357e-03, 8.9760e-03,\n",
       "                      3.3172e-03, 1.7340e-03, 9.9977e-04, 2.9976e-03, 3.8071e-03, 2.5247e-03,\n",
       "                      1.7347e-03, 1.9019e-03, 5.0805e-04, 2.6164e-03, 7.6636e-04, 2.0161e-03,\n",
       "                      1.0030e-03, 2.6925e-03, 6.6898e-03, 4.5961e-03, 2.4266e-03, 1.6072e-03,\n",
       "                      9.2046e-04, 3.6540e-03, 7.5480e-03, 4.0888e-03, 2.2872e-03, 5.2536e-03,\n",
       "                      2.7494e-03, 3.4777e-03, 2.0226e-03, 1.9105e-03, 4.3000e-03, 2.5137e-03,\n",
       "                      2.7544e-03, 2.6235e-03, 1.6686e-03, 2.7327e-03, 2.4488e-03, 4.6760e-03,\n",
       "                      1.0185e-03, 3.3680e-03, 2.6620e-03, 9.2804e-03, 1.8134e-03, 1.7212e-03,\n",
       "                      2.2945e-03, 3.9459e-03, 1.9191e-03, 3.3029e-03, 1.6336e-03, 3.3602e-03,\n",
       "                      4.5195e-03, 3.5472e-03, 3.4457e-03, 1.3422e-03, 2.3569e-03, 3.9506e-03,\n",
       "                      2.8291e-03, 2.2668e-03, 4.2447e-03, 5.5962e-03, 1.3753e-03, 3.4195e-03,\n",
       "                      2.7300e-03, 2.1765e-03, 3.3375e-03, 2.1778e-03, 4.5269e-03, 9.8620e-04,\n",
       "                      3.6789e-03, 2.3811e-03, 5.7804e-03, 7.6174e-03, 2.7251e-03, 2.8972e-03,\n",
       "                      7.7035e-03, 4.5949e-03, 2.6735e-03, 1.1301e-03, 2.6133e-03, 4.0734e-03,\n",
       "                      4.5067e-03, 3.7830e-03, 1.1539e-03, 2.4696e-03, 5.6902e-03, 2.1050e-03,\n",
       "                      2.6583e-03, 5.3454e-03, 1.9309e-03, 6.0382e-03, 2.9110e-03, 9.4929e-04,\n",
       "                      2.5245e-03, 3.1135e-03, 4.7452e-03, 5.1437e-03, 3.4973e-03, 3.1842e-03,\n",
       "                      3.4511e-03, 3.2001e-03, 6.9370e-04, 2.0809e-03, 1.5723e-03, 1.7058e-03,\n",
       "                      1.3393e-03, 2.2874e-09, 1.5850e-03, 5.2335e-03, 2.0271e-03, 7.6351e-04,\n",
       "                      6.3647e-03, 3.0662e-03, 4.4744e-03, 1.6384e-03, 2.4845e-03, 8.0677e-04,\n",
       "                      5.3316e-03, 4.1209e-03, 1.1133e-03, 3.8016e-03, 8.3488e-04, 3.4289e-03,\n",
       "                      2.9027e-03, 4.1427e-03, 1.3713e-03, 2.7436e-03, 1.6386e-03, 1.4363e-03,\n",
       "                      3.4978e-03, 9.2769e-04, 3.2384e-03, 1.3134e-03, 4.3171e-03, 5.0055e-03,\n",
       "                      3.2939e-03, 2.7227e-03, 5.6040e-03, 2.9913e-03, 4.9847e-03, 2.5247e-03,\n",
       "                      1.3750e-03, 2.6866e-03, 2.0748e-03, 4.2771e-03, 3.1186e-03, 1.9077e-03,\n",
       "                      2.2277e-03, 3.8615e-03, 2.0014e-03, 2.6945e-03, 1.0839e-03, 4.9027e-03,\n",
       "                      1.8260e-03, 2.0912e-03, 9.8516e-04, 2.6271e-03, 3.3752e-03, 4.7305e-04,\n",
       "                      2.6162e-03, 3.4188e-03, 2.6532e-03, 7.2754e-03, 3.7178e-03, 2.0224e-03,\n",
       "                      1.1791e-03, 2.6992e-03, 6.3072e-03, 2.4180e-03, 2.9678e-03, 2.0322e-03,\n",
       "                      1.7633e-03, 2.2861e-03, 4.2382e-03, 3.1071e-03, 2.8247e-03, 4.0005e-03,\n",
       "                      2.1066e-03, 3.8704e-03, 1.0902e-03, 2.3869e-03, 1.0392e-03, 2.9487e-03,\n",
       "                      3.0522e-03, 2.4115e-03, 2.1707e-03, 2.2862e-03, 2.2322e-03, 4.0443e-03,\n",
       "                      1.1929e-03, 2.9208e-03, 1.5769e-03, 1.8558e-03, 3.2566e-03, 1.9213e-03,\n",
       "                      1.0107e-03, 2.3274e-03, 2.3136e-03, 4.1419e-03])),\n",
       "             ('layer3.0.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.0.conv2.weight',\n",
       "              tensor([[[[ 7.7599e-05,  1.1975e-03,  7.0305e-03],\n",
       "                        [-5.4750e-03,  9.9014e-06, -8.7780e-03],\n",
       "                        [-1.4895e-02, -1.3976e-02, -1.4862e-02]],\n",
       "              \n",
       "                       [[ 2.2087e-03, -2.2510e-03,  7.8901e-03],\n",
       "                        [ 5.1742e-03,  9.7857e-03,  1.2319e-03],\n",
       "                        [-7.5905e-04, -8.2854e-04,  7.9384e-03]],\n",
       "              \n",
       "                       [[ 1.3026e-03,  2.6400e-03,  1.2132e-03],\n",
       "                        [-4.5927e-03,  2.2631e-03, -3.0788e-03],\n",
       "                        [-8.0999e-03, -6.0265e-03, -1.0090e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.0690e-03,  9.6645e-03,  1.6361e-02],\n",
       "                        [ 1.0102e-02,  4.2799e-03,  1.3308e-02],\n",
       "                        [ 1.7682e-02,  9.8197e-03,  1.5046e-02]],\n",
       "              \n",
       "                       [[ 1.3439e-03, -5.8983e-03, -2.3493e-03],\n",
       "                        [-4.4266e-03,  3.3292e-03, -7.7120e-03],\n",
       "                        [-4.4224e-03,  1.2587e-04,  4.3682e-03]],\n",
       "              \n",
       "                       [[ 3.0724e-03,  4.0735e-03,  1.2728e-03],\n",
       "                        [ 5.4849e-03,  3.0347e-04, -2.8731e-03],\n",
       "                        [ 1.0152e-02, -2.9922e-03, -5.6540e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.4160e-04, -2.3844e-04,  6.2358e-03],\n",
       "                        [ 2.6593e-03, -9.1455e-03,  4.1060e-03],\n",
       "                        [ 1.0980e-02,  4.8238e-03,  8.7099e-03]],\n",
       "              \n",
       "                       [[ 1.1165e-02,  8.3439e-03,  1.2814e-02],\n",
       "                        [ 2.3333e-04,  8.0291e-03,  1.0824e-02],\n",
       "                        [-8.5553e-03,  5.5547e-03,  5.6615e-03]],\n",
       "              \n",
       "                       [[-6.4550e-04, -1.9423e-03, -1.3977e-03],\n",
       "                        [-7.5667e-03, -2.1231e-03, -4.7623e-03],\n",
       "                        [-5.4715e-03,  6.3905e-05,  7.1816e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.2708e-05, -6.5524e-03, -3.7400e-03],\n",
       "                        [-1.3722e-03, -3.3324e-03, -5.7524e-03],\n",
       "                        [-7.5262e-03, -8.8981e-03, -8.4372e-03]],\n",
       "              \n",
       "                       [[-8.2317e-03, -4.2582e-03, -1.3610e-02],\n",
       "                        [-1.2622e-02,  1.9534e-03, -1.6802e-03],\n",
       "                        [-3.4181e-03, -1.3417e-02, -1.7493e-03]],\n",
       "              \n",
       "                       [[-6.9334e-03,  3.6739e-03, -1.4885e-02],\n",
       "                        [-1.1210e-02, -7.2616e-04, -4.1264e-03],\n",
       "                        [ 2.9303e-03,  7.8971e-03,  7.6457e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.8525e-03,  5.0959e-04, -6.9208e-03],\n",
       "                        [ 5.5867e-03,  4.3457e-05, -5.0063e-03],\n",
       "                        [ 3.8780e-03, -1.1134e-03, -4.5644e-03]],\n",
       "              \n",
       "                       [[ 2.7623e-03,  3.5360e-03,  7.2219e-04],\n",
       "                        [ 3.2825e-03,  1.7851e-03,  4.2304e-03],\n",
       "                        [ 3.8374e-03,  1.0213e-02,  2.2217e-03]],\n",
       "              \n",
       "                       [[ 7.9317e-04, -7.3069e-03, -5.4567e-03],\n",
       "                        [-4.9677e-04, -2.4799e-03, -8.9466e-03],\n",
       "                        [ 8.5166e-04, -6.4267e-04, -8.3088e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.5981e-03,  3.9764e-03,  4.7309e-03],\n",
       "                        [ 1.1776e-03, -8.7998e-04,  9.9877e-04],\n",
       "                        [ 3.8850e-03, -1.3920e-03, -8.6924e-06]],\n",
       "              \n",
       "                       [[ 5.7951e-03,  3.6837e-03, -7.8281e-04],\n",
       "                        [-6.7237e-04,  3.1189e-04, -7.9946e-03],\n",
       "                        [ 6.7227e-03,  1.8772e-03, -3.5866e-04]],\n",
       "              \n",
       "                       [[ 2.4415e-03,  6.3031e-03,  3.7176e-03],\n",
       "                        [ 1.9141e-03,  5.2307e-03, -1.8543e-03],\n",
       "                        [ 1.1534e-02,  7.0959e-03,  7.5263e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.6844e-03, -1.0778e-02, -6.4555e-03],\n",
       "                        [-1.7080e-03, -1.2314e-03, -4.7255e-03],\n",
       "                        [-1.7576e-03, -1.0531e-02, -5.7377e-03]],\n",
       "              \n",
       "                       [[-2.2737e-02, -1.0421e-02, -1.3909e-02],\n",
       "                        [-6.7276e-03, -2.9460e-03,  4.2215e-04],\n",
       "                        [ 8.5046e-03,  4.2644e-03,  3.9177e-03]],\n",
       "              \n",
       "                       [[-5.3647e-03, -1.8494e-03, -9.6871e-03],\n",
       "                        [-1.0618e-02, -6.1439e-03, -2.0863e-03],\n",
       "                        [ 5.7361e-03,  4.6438e-03,  3.3283e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.8765e-03,  6.9860e-03,  1.0589e-02],\n",
       "                        [ 8.6398e-03,  4.2896e-03,  1.0398e-02],\n",
       "                        [-5.0669e-04,  7.1606e-03,  1.0646e-02]],\n",
       "              \n",
       "                       [[-9.9951e-03, -2.3063e-03, -3.4267e-03],\n",
       "                        [-5.9254e-03,  8.8737e-04, -1.3801e-03],\n",
       "                        [-2.7781e-04, -2.4152e-03, -2.1144e-03]],\n",
       "              \n",
       "                       [[-3.0111e-03, -6.5831e-04, -4.7914e-03],\n",
       "                        [ 6.1143e-04,  4.3042e-03,  2.3651e-03],\n",
       "                        [-3.9304e-03,  6.3712e-03,  2.1778e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.9903e-04,  2.4740e-03,  1.0793e-02],\n",
       "                        [ 6.3350e-03, -3.3735e-03, -6.7397e-03],\n",
       "                        [ 4.7644e-03, -2.4166e-03, -1.8966e-03]],\n",
       "              \n",
       "                       [[-2.4851e-03,  3.3347e-04, -5.3414e-03],\n",
       "                        [-5.7831e-04,  4.8400e-03, -1.0214e-03],\n",
       "                        [-7.1788e-03, -1.5682e-03, -6.7406e-05]],\n",
       "              \n",
       "                       [[ 2.6778e-03,  2.4590e-03,  3.5164e-03],\n",
       "                        [ 1.0943e-02,  3.2990e-04,  6.5569e-03],\n",
       "                        [ 1.4531e-02,  4.7159e-03, -2.0158e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.4800e-05,  7.3446e-03,  2.9608e-03],\n",
       "                        [ 8.3092e-03,  3.1464e-03,  1.2884e-02],\n",
       "                        [-8.5187e-04,  1.1874e-02,  1.0823e-02]],\n",
       "              \n",
       "                       [[ 2.5632e-03, -3.8044e-03, -4.5370e-03],\n",
       "                        [ 4.5922e-04,  6.3855e-03, -6.7906e-03],\n",
       "                        [-1.7127e-03,  3.0943e-03, -4.1589e-03]],\n",
       "              \n",
       "                       [[ 8.5966e-03, -2.4950e-03, -2.8662e-03],\n",
       "                        [ 4.0441e-03,  9.2433e-04, -1.3789e-03],\n",
       "                        [ 1.6420e-03,  4.8329e-03,  7.2453e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5955e-03,  1.0250e-02,  8.7030e-03],\n",
       "                        [ 5.1110e-03,  5.4913e-03,  1.3886e-02],\n",
       "                        [-1.5065e-03,  2.2414e-03,  3.3431e-03]],\n",
       "              \n",
       "                       [[-5.0868e-03, -4.2391e-03, -5.3533e-03],\n",
       "                        [ 8.2342e-03, -3.9521e-03,  2.1366e-03],\n",
       "                        [-1.0294e-03,  3.1664e-03,  5.6713e-03]],\n",
       "              \n",
       "                       [[ 3.1784e-03,  2.3520e-03, -2.9492e-04],\n",
       "                        [ 7.6237e-03, -2.8956e-03, -4.4920e-04],\n",
       "                        [ 5.1698e-03, -1.4103e-03,  5.7958e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4788e-04, -2.9220e-03,  2.6775e-03],\n",
       "                        [ 1.9723e-03,  4.2407e-03,  5.5252e-03],\n",
       "                        [ 9.5077e-03,  5.5642e-03,  7.6630e-03]],\n",
       "              \n",
       "                       [[ 1.5689e-03,  9.9030e-03,  9.9299e-03],\n",
       "                        [ 7.2147e-03, -5.3050e-03,  1.0341e-02],\n",
       "                        [ 4.4934e-03,  8.9678e-04,  1.7951e-03]],\n",
       "              \n",
       "                       [[-1.0154e-02, -1.1591e-02, -8.5539e-03],\n",
       "                        [-1.8892e-03, -5.3165e-03, -8.0981e-03],\n",
       "                        [-3.8575e-03, -8.4126e-03, -2.8192e-03]]]])),\n",
       "             ('layer3.0.bn2.weight',\n",
       "              tensor([6.9130e-02, 1.0994e-01, 3.8934e-02, 6.4578e-02, 1.1271e-01, 9.0274e-02,\n",
       "                      9.5509e-02, 7.7692e-02, 4.9669e-02, 8.2703e-02, 1.4359e-01, 9.6069e-02,\n",
       "                      7.1769e-02, 1.0993e-01, 1.3421e-01, 9.1244e-02, 1.0675e-01, 1.4222e-01,\n",
       "                      1.0004e-01, 1.1060e-01, 8.9486e-02, 9.3366e-02, 1.0796e-01, 1.0958e-01,\n",
       "                      1.5493e-01, 1.0376e-01, 7.2632e-02, 1.1490e-01, 5.3698e-02, 6.3290e-02,\n",
       "                      1.0259e-01, 1.3049e-01, 1.3487e-01, 1.4520e-01, 9.2235e-02, 1.2443e-01,\n",
       "                      1.1979e-01, 9.0214e-02, 1.6526e-01, 9.9254e-02, 1.1648e-01, 6.8122e-02,\n",
       "                      8.6138e-02, 8.5419e-02, 1.2080e-01, 1.0398e-01, 1.0753e-01, 1.3471e-01,\n",
       "                      1.1884e-01, 1.2793e-01, 8.3279e-02, 9.1159e-02, 6.7709e-02, 7.6994e-02,\n",
       "                      1.1383e-01, 8.3901e-02, 9.3120e-02, 1.0633e-01, 1.2734e-01, 7.2284e-02,\n",
       "                      1.3845e-01, 9.9513e-02, 1.0689e-01, 1.0555e-01, 8.9515e-02, 7.9831e-02,\n",
       "                      9.9140e-02, 1.1131e-01, 7.2704e-02, 1.1930e-01, 9.0931e-02, 1.4011e-01,\n",
       "                      1.3108e-01, 8.2198e-02, 1.2812e-01, 9.7419e-02, 1.1904e-01, 9.3735e-02,\n",
       "                      9.1432e-02, 1.0292e-01, 1.1082e-01, 1.2386e-01, 1.3284e-01, 9.1852e-02,\n",
       "                      1.0640e-01, 8.9958e-02, 9.7586e-02, 8.8669e-02, 1.2599e-01, 8.5364e-02,\n",
       "                      1.0035e-01, 1.2335e-01, 8.4768e-02, 1.2734e-01, 1.1587e-01, 1.0504e-01,\n",
       "                      1.2796e-01, 9.7522e-02, 7.6404e-02, 7.9905e-02, 1.1455e-01, 1.2152e-01,\n",
       "                      9.4033e-02, 6.4693e-02, 1.0071e-01, 9.1577e-02, 1.0884e-01, 1.0529e-01,\n",
       "                      8.9664e-02, 1.3633e-01, 1.1463e-01, 1.0592e-01, 8.0699e-02, 8.8119e-02,\n",
       "                      1.0611e-01, 9.9857e-02, 1.1550e-01, 1.2713e-01, 9.4941e-02, 1.2081e-01,\n",
       "                      7.9356e-02, 1.1557e-01, 9.7777e-02, 1.2073e-01, 1.4730e-01, 1.2392e-01,\n",
       "                      8.9746e-02, 1.0881e-01, 8.3083e-02, 8.1481e-02, 1.2752e-01, 8.7449e-02,\n",
       "                      1.2158e-01, 1.2233e-04, 5.3375e-02, 1.1113e-01, 1.0039e-01, 9.8086e-02,\n",
       "                      1.6590e-01, 8.3070e-02, 1.2101e-01, 1.0913e-01, 9.3019e-02, 1.3199e-01,\n",
       "                      1.0057e-01, 8.4772e-02, 8.0715e-02, 6.9727e-02, 1.0029e-01, 8.2431e-02,\n",
       "                      1.0857e-01, 1.2237e-01, 9.1855e-02, 8.0309e-02, 1.0949e-01, 8.6528e-02,\n",
       "                      1.2928e-01, 8.3926e-02, 8.4102e-02, 9.0625e-02, 1.2564e-01, 1.0930e-01,\n",
       "                      7.5360e-02, 1.3657e-01, 1.2498e-01, 7.6836e-02, 5.0267e-04, 1.1304e-01,\n",
       "                      1.0377e-01, 9.9963e-02, 7.1980e-02, 1.0580e-01, 1.0858e-01, 8.9058e-02,\n",
       "                      1.1190e-01, 1.0167e-01, 1.2193e-01, 7.2137e-02, 1.0743e-01, 1.1082e-01,\n",
       "                      9.5801e-02, 1.0430e-01, 4.8066e-02, 9.5522e-02, 1.2992e-01, 8.5077e-02,\n",
       "                      1.0484e-01, 1.2558e-01, 7.3980e-02, 1.0141e-01, 1.3367e-01, 9.8136e-02,\n",
       "                      8.6202e-02, 1.1440e-01, 1.0558e-01, 8.6577e-02, 1.6357e-01, 9.9373e-02,\n",
       "                      7.7977e-02, 9.2590e-02, 1.0275e-01, 1.1501e-01, 7.3806e-02, 9.3874e-02,\n",
       "                      6.1774e-02, 8.3862e-02, 6.2849e-02, 8.3943e-02, 9.3141e-02, 6.4610e-02,\n",
       "                      1.0633e-01, 1.3672e-01, 1.1944e-01, 1.0420e-01, 9.4581e-02, 1.2296e-01,\n",
       "                      1.6336e-01, 1.0739e-01, 1.1485e-01, 8.4660e-02, 8.3793e-02, 1.1341e-01,\n",
       "                      1.0747e-01, 1.0281e-01, 6.8020e-02, 8.3797e-02, 1.0124e-01, 1.3384e-01,\n",
       "                      7.1577e-02, 1.0572e-01, 7.5631e-02, 1.7565e-01, 7.7788e-02, 1.3569e-01,\n",
       "                      4.6573e-02, 1.0078e-01, 9.7807e-02, 1.1754e-01, 1.2277e-01, 1.1277e-01,\n",
       "                      7.1964e-02, 1.1275e-01, 1.1733e-01, 7.2239e-02, 6.7219e-02, 1.0786e-01,\n",
       "                      9.5715e-02, 5.6236e-02, 1.4153e-01, 9.9235e-02, 9.8836e-02, 8.9646e-02,\n",
       "                      1.1937e-01, 9.0453e-02, 8.1734e-02, 1.1923e-01])),\n",
       "             ('layer3.0.bn2.bias',\n",
       "              tensor([ 0.0630,  0.0236,  0.0373,  0.0248, -0.0335,  0.0209, -0.0172,  0.0050,\n",
       "                      -0.0176,  0.0109, -0.0035,  0.0183, -0.0040, -0.0125, -0.0358, -0.0250,\n",
       "                      -0.0636,  0.0183, -0.0004, -0.0059,  0.0123, -0.0498, -0.0218, -0.0497,\n",
       "                      -0.0731,  0.0023, -0.0392, -0.0507,  0.0241, -0.0170, -0.0182,  0.0079,\n",
       "                      -0.0189,  0.0071,  0.0247, -0.0386, -0.0740,  0.0227, -0.0643,  0.0355,\n",
       "                       0.0295,  0.0167, -0.0023, -0.0111, -0.0363, -0.0388,  0.0166,  0.0003,\n",
       "                      -0.0357,  0.0561, -0.0107, -0.0068, -0.0142, -0.0015,  0.0056, -0.0158,\n",
       "                       0.0309,  0.0026, -0.0407, -0.0102, -0.0816, -0.0256, -0.0367,  0.0085,\n",
       "                      -0.0174,  0.0126, -0.0085,  0.0072, -0.0281,  0.0163, -0.0215, -0.0581,\n",
       "                       0.0567, -0.0071, -0.0264, -0.0519, -0.0648,  0.0011, -0.0202,  0.0046,\n",
       "                       0.0093, -0.0179, -0.0001,  0.0467, -0.0441,  0.0540,  0.0088, -0.0553,\n",
       "                       0.0143, -0.0336,  0.0430, -0.0274,  0.0374,  0.0288,  0.0023, -0.0020,\n",
       "                      -0.0217, -0.0355, -0.0198,  0.0178, -0.0080,  0.0073, -0.0030,  0.0090,\n",
       "                       0.0372, -0.0079, -0.0014,  0.0002, -0.0157, -0.0516, -0.0004, -0.0006,\n",
       "                      -0.0208, -0.0599,  0.0022,  0.0047, -0.0499, -0.0569, -0.0207, -0.0282,\n",
       "                      -0.0243, -0.0128,  0.0482,  0.0297, -0.0099, -0.0675,  0.0175, -0.0359,\n",
       "                      -0.0201,  0.0380, -0.0121,  0.0572, -0.0310, -0.0008,  0.0529, -0.0417,\n",
       "                      -0.0273,  0.0103, -0.0616,  0.0108,  0.0435, -0.0017, -0.0156,  0.0214,\n",
       "                      -0.0349,  0.0164,  0.0409, -0.0184, -0.0582, -0.0159, -0.0299, -0.0367,\n",
       "                      -0.0197,  0.0109, -0.0262,  0.0510, -0.0468,  0.0151, -0.0504, -0.0450,\n",
       "                      -0.0830,  0.0213, -0.0394, -0.0452, -0.0388,  0.0306, -0.0356, -0.0143,\n",
       "                      -0.0371,  0.0306, -0.0374,  0.0011, -0.0524, -0.0528, -0.0284, -0.0609,\n",
       "                      -0.0018, -0.0513, -0.0738,  0.0345,  0.0493, -0.0179,  0.0195, -0.0033,\n",
       "                       0.0137,  0.0267,  0.0213, -0.0142,  0.0579,  0.0273, -0.0004,  0.0332,\n",
       "                       0.0074,  0.0128,  0.0043,  0.0147, -0.0613,  0.0157,  0.0227,  0.0564,\n",
       "                      -0.0318, -0.0454, -0.0350, -0.0116,  0.0258,  0.0518,  0.0253, -0.0081,\n",
       "                      -0.0154, -0.0117,  0.0250, -0.1022, -0.0228, -0.0082, -0.0526, -0.0111,\n",
       "                      -0.0398, -0.0192,  0.0586,  0.0595,  0.0246, -0.0237,  0.0219, -0.0017,\n",
       "                      -0.0334,  0.0402,  0.0385,  0.0294,  0.0403, -0.0360,  0.0181, -0.1164,\n",
       "                       0.0428, -0.0745,  0.0244, -0.0295,  0.0616,  0.0032, -0.0026, -0.0123,\n",
       "                       0.0008, -0.0095, -0.0209,  0.0272,  0.0190,  0.0409,  0.0478,  0.0292,\n",
       "                       0.0060,  0.0386,  0.0081, -0.0202,  0.0064, -0.0050,  0.0442, -0.0495])),\n",
       "             ('layer3.0.bn2.running_mean',\n",
       "              tensor([-1.5159e-02, -8.2322e-02, -2.5694e-02, -1.5562e-02, -1.6721e-04,\n",
       "                      -2.2821e-02, -1.4695e-02, -2.3027e-02, -4.2168e-03, -2.1465e-02,\n",
       "                      -3.3596e-02, -3.8199e-02, -4.5630e-02,  8.1346e-03, -3.2522e-03,\n",
       "                      -7.7663e-03,  3.0225e-02, -7.3567e-02, -4.2423e-02,  1.4481e-02,\n",
       "                      -2.5024e-02,  1.4807e-02, -2.3987e-02, -1.4488e-02, -5.4683e-02,\n",
       "                      -8.9345e-03, -1.3186e-02, -4.2966e-02,  8.5408e-03, -2.7003e-02,\n",
       "                      -8.3811e-03, -1.0914e-02,  3.9117e-02, -1.1690e-01, -6.8784e-02,\n",
       "                      -4.3716e-02,  1.7900e-02, -3.0347e-02, -3.7245e-02, -1.8656e-02,\n",
       "                      -4.8884e-02, -2.0833e-02, -3.6500e-02, -1.0646e-02,  1.5061e-02,\n",
       "                      -1.8063e-02, -4.6354e-02, -5.0306e-02, -5.7345e-03, -4.0251e-02,\n",
       "                      -1.0924e-03,  1.2443e-02, -4.8895e-04, -2.3874e-02, -6.5369e-02,\n",
       "                       1.6119e-02, -5.9179e-02, -2.0220e-02,  7.9341e-02, -8.5210e-03,\n",
       "                      -2.3104e-02,  2.2215e-02,  1.1457e-03, -2.6715e-02,  1.8746e-02,\n",
       "                      -3.4614e-02,  3.2049e-02, -2.5807e-02, -3.2390e-02, -1.2105e-02,\n",
       "                      -2.7036e-02, -5.6761e-03, -3.0767e-02,  6.3775e-03,  9.5924e-03,\n",
       "                       1.8965e-02,  4.4413e-02, -1.6734e-02,  3.6457e-02, -3.5091e-02,\n",
       "                      -2.5618e-02, -1.8515e-02, -1.7817e-02, -4.2602e-02, -3.8606e-02,\n",
       "                      -6.2489e-02, -4.1053e-02,  7.2792e-03, -7.0804e-02,  1.7658e-02,\n",
       "                      -3.3279e-02,  2.6955e-02, -1.7358e-02, -1.3354e-02, -4.9669e-02,\n",
       "                       6.5836e-03, -2.0371e-02,  4.3233e-02, -1.3801e-02, -5.1813e-02,\n",
       "                      -4.2256e-02, -3.9231e-02, -4.4855e-02,  1.5837e-02, -6.4377e-02,\n",
       "                       4.6676e-02, -2.8091e-02, -3.3269e-02,  2.6538e-02,  2.1829e-02,\n",
       "                       1.4505e-02,  2.8773e-02, -4.4648e-03,  6.2708e-03, -5.0461e-02,\n",
       "                      -2.7837e-02,  1.4618e-02, -3.3673e-02,  3.5393e-02, -6.0180e-02,\n",
       "                       1.0613e-02, -3.3272e-02, -4.9040e-02, -2.5014e-02, -8.3176e-02,\n",
       "                      -1.6457e-02, -3.9478e-02, -2.0871e-02,  3.3765e-02, -2.6143e-02,\n",
       "                      -9.4225e-03, -6.4761e-02,  1.9758e-02,  3.3910e-04, -6.7200e-02,\n",
       "                       7.0646e-03, -1.9543e-02, -3.1851e-02, -1.0597e-01, -4.0074e-03,\n",
       "                      -5.9758e-02, -9.1055e-02, -2.3552e-02, -4.3483e-02, -2.7245e-02,\n",
       "                      -7.3476e-02, -5.9085e-02,  1.6113e-02,  1.2007e-02,  2.2884e-02,\n",
       "                       2.2689e-02, -3.1557e-02, -2.1232e-02, -1.3663e-03,  3.0356e-02,\n",
       "                      -1.8449e-02, -1.9315e-02, -3.8073e-02, -3.7871e-02,  3.8682e-03,\n",
       "                       7.1281e-03, -5.6184e-02, -8.1955e-04, -1.8222e-04, -4.1537e-03,\n",
       "                      -5.0364e-02,  3.1755e-03,  3.8683e-02,  7.4961e-04, -2.0491e-02,\n",
       "                      -9.2691e-03,  7.1294e-03,  7.3347e-04,  1.7934e-02,  1.9939e-02,\n",
       "                      -1.3814e-02, -4.1647e-02,  6.0815e-03,  1.2962e-02, -7.1883e-02,\n",
       "                      -2.2013e-02, -3.5298e-02, -2.8115e-02, -4.8843e-02, -1.2960e-02,\n",
       "                       2.7215e-04, -4.6367e-02, -3.2687e-02, -2.3338e-02, -5.4024e-02,\n",
       "                       4.4470e-02,  6.1530e-03, -9.2917e-03, -2.0350e-02, -5.7386e-02,\n",
       "                      -3.4853e-02,  2.6679e-02, -2.2999e-02, -1.1433e-02, -2.7841e-02,\n",
       "                      -2.6252e-02,  2.3128e-02, -1.6237e-02,  2.1828e-02, -6.2888e-03,\n",
       "                      -5.5960e-02, -3.5398e-02, -7.9671e-03, -5.3746e-02,  2.7338e-02,\n",
       "                      -6.7294e-03, -2.7718e-02,  3.2166e-02, -3.7857e-02, -6.2132e-03,\n",
       "                      -9.9944e-04, -3.4037e-02, -2.4343e-03, -4.4872e-02, -6.0139e-02,\n",
       "                      -3.0732e-02,  2.4348e-02, -2.9274e-02,  2.0760e-02,  2.8233e-02,\n",
       "                      -2.7320e-02, -5.4634e-02, -4.3847e-02,  7.4970e-03,  1.1399e-05,\n",
       "                      -6.9042e-02,  5.1069e-02, -3.8505e-02, -5.2045e-02, -1.3762e-02,\n",
       "                       3.6442e-02, -2.1475e-02, -1.0502e-03, -3.9834e-02, -1.2077e-02,\n",
       "                      -2.0608e-03, -3.3810e-02, -4.8368e-02, -4.4465e-02,  6.1017e-03,\n",
       "                      -5.3693e-03, -2.4989e-02, -7.7519e-03, -3.9603e-02, -6.8463e-02,\n",
       "                      -1.4429e-03,  4.4946e-02, -9.1721e-03, -1.5698e-02, -4.1885e-02,\n",
       "                       1.3914e-02])),\n",
       "             ('layer3.0.bn2.running_var',\n",
       "              tensor([5.3755e-03, 9.2717e-03, 3.1778e-03, 3.8905e-03, 4.8476e-03, 5.9040e-03,\n",
       "                      2.7017e-03, 2.3478e-03, 2.3485e-03, 5.1915e-03, 6.4654e-03, 3.9898e-03,\n",
       "                      3.2478e-03, 4.6815e-03, 5.2310e-03, 6.6405e-03, 5.4597e-03, 6.8510e-03,\n",
       "                      3.5345e-03, 4.3466e-03, 4.2156e-03, 3.6748e-03, 5.4680e-03, 5.0538e-03,\n",
       "                      4.9460e-03, 4.4481e-03, 3.2249e-03, 5.0358e-03, 2.6080e-03, 2.5992e-03,\n",
       "                      4.2675e-03, 7.9903e-03, 7.3105e-03, 1.2100e-02, 7.8093e-03, 4.5189e-03,\n",
       "                      6.1648e-03, 4.9391e-03, 7.5291e-03, 4.3526e-03, 6.4849e-03, 3.2113e-03,\n",
       "                      3.1538e-03, 4.4484e-03, 4.4019e-03, 4.1582e-03, 6.9126e-03, 8.4834e-03,\n",
       "                      4.9668e-03, 8.9729e-03, 3.3572e-03, 3.5075e-03, 2.8276e-03, 2.9839e-03,\n",
       "                      5.5169e-03, 3.3547e-03, 5.6967e-03, 5.9815e-03, 1.2275e-02, 2.5160e-03,\n",
       "                      5.3588e-03, 4.9082e-03, 4.5357e-03, 3.8444e-03, 4.7329e-03, 3.1378e-03,\n",
       "                      4.6934e-03, 3.7337e-03, 2.1045e-03, 5.7955e-03, 2.7416e-03, 4.8927e-03,\n",
       "                      8.2434e-03, 2.6999e-03, 4.0488e-03, 5.0870e-03, 5.0361e-03, 6.4429e-03,\n",
       "                      5.8695e-03, 5.5220e-03, 6.1728e-03, 4.9884e-03, 5.7015e-03, 8.0656e-03,\n",
       "                      5.2277e-03, 5.7627e-03, 4.2336e-03, 3.9819e-03, 9.0359e-03, 2.7946e-03,\n",
       "                      1.2579e-02, 3.3553e-03, 4.0159e-03, 1.1212e-02, 3.4153e-03, 1.3047e-02,\n",
       "                      6.1623e-03, 3.2287e-03, 2.6751e-03, 5.4299e-03, 7.2236e-03, 7.2777e-03,\n",
       "                      3.5113e-03, 4.1140e-03, 5.5447e-03, 6.2756e-03, 2.7374e-03, 6.4614e-03,\n",
       "                      4.6640e-03, 6.6968e-03, 6.3152e-03, 5.9138e-03, 2.6168e-03, 3.5674e-03,\n",
       "                      4.5378e-03, 5.8761e-03, 6.6064e-03, 5.2394e-03, 3.8043e-03, 6.7757e-03,\n",
       "                      1.9277e-03, 3.4974e-03, 6.9180e-03, 5.6290e-03, 8.1147e-03, 5.1367e-03,\n",
       "                      3.6320e-03, 4.2347e-03, 4.7455e-03, 6.1105e-03, 5.2321e-03, 6.7243e-03,\n",
       "                      6.4239e-03, 4.1339e-07, 7.3089e-03, 4.4631e-03, 3.5150e-03, 5.7521e-03,\n",
       "                      1.0095e-02, 4.3361e-03, 8.5258e-03, 8.2498e-03, 6.0914e-03, 1.0815e-02,\n",
       "                      3.0679e-03, 4.3643e-03, 5.4074e-03, 3.4825e-03, 4.3398e-03, 4.4619e-03,\n",
       "                      5.3834e-03, 5.1095e-03, 4.5809e-03, 2.4677e-03, 5.3052e-03, 7.2735e-03,\n",
       "                      5.6233e-03, 6.2999e-03, 4.4000e-03, 2.5238e-03, 7.0443e-03, 1.0328e-02,\n",
       "                      3.2705e-03, 5.2105e-03, 3.6886e-03, 7.3283e-03, 3.0648e-04, 4.8674e-03,\n",
       "                      5.3280e-03, 9.9942e-03, 2.8760e-03, 7.6492e-03, 3.9109e-03, 5.2351e-03,\n",
       "                      6.8538e-03, 4.2928e-03, 5.2924e-03, 4.0107e-03, 9.4313e-03, 9.0182e-03,\n",
       "                      6.2232e-03, 4.7467e-03, 2.2904e-03, 4.6168e-03, 5.3642e-03, 3.3203e-03,\n",
       "                      7.5114e-03, 6.0891e-03, 6.1199e-03, 5.9917e-03, 8.4952e-03, 4.8497e-03,\n",
       "                      3.6764e-03, 5.9516e-03, 5.5733e-03, 3.8960e-03, 7.8759e-03, 6.5160e-03,\n",
       "                      4.3288e-03, 7.8454e-03, 4.9613e-03, 6.1870e-03, 2.4047e-03, 3.2971e-03,\n",
       "                      3.6559e-03, 9.7768e-03, 4.5979e-03, 2.3822e-03, 3.8505e-03, 1.9784e-03,\n",
       "                      5.9130e-03, 9.8265e-03, 4.7003e-03, 4.3197e-03, 4.8761e-03, 5.9573e-03,\n",
       "                      6.4444e-03, 4.1108e-03, 8.5304e-03, 9.1979e-03, 5.3913e-03, 1.0480e-02,\n",
       "                      5.1667e-03, 3.3978e-03, 4.2937e-03, 5.0875e-03, 4.7190e-03, 8.5642e-03,\n",
       "                      3.1452e-03, 3.8452e-03, 3.8645e-03, 1.0204e-02, 7.0357e-03, 4.9276e-03,\n",
       "                      3.0514e-03, 3.2920e-03, 7.3330e-03, 4.7175e-03, 8.5191e-03, 4.1633e-03,\n",
       "                      2.5868e-03, 5.0949e-03, 3.8557e-03, 4.1825e-03, 1.7048e-03, 5.9431e-03,\n",
       "                      5.3831e-03, 2.7196e-03, 6.3971e-03, 6.4393e-03, 3.8160e-03, 3.9724e-03,\n",
       "                      5.6058e-03, 6.5415e-03, 8.2318e-03, 3.9924e-03])),\n",
       "             ('layer3.0.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.0.conv3.weight',\n",
       "              tensor([[[[ 1.3868e-02]],\n",
       "              \n",
       "                       [[ 2.7103e-03]],\n",
       "              \n",
       "                       [[-6.0017e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0375e-02]],\n",
       "              \n",
       "                       [[ 2.5722e-03]],\n",
       "              \n",
       "                       [[-3.9342e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3069e-02]],\n",
       "              \n",
       "                       [[-7.0837e-04]],\n",
       "              \n",
       "                       [[ 2.1819e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.8408e-03]],\n",
       "              \n",
       "                       [[-5.9491e-03]],\n",
       "              \n",
       "                       [[ 7.9828e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.8535e-04]],\n",
       "              \n",
       "                       [[-1.3550e-02]],\n",
       "              \n",
       "                       [[-6.3890e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.2595e-03]],\n",
       "              \n",
       "                       [[-2.3154e-02]],\n",
       "              \n",
       "                       [[-9.6463e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.1677e-02]],\n",
       "              \n",
       "                       [[-1.2029e-02]],\n",
       "              \n",
       "                       [[ 2.0411e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2679e-03]],\n",
       "              \n",
       "                       [[-1.6643e-02]],\n",
       "              \n",
       "                       [[ 6.3457e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6586e-03]],\n",
       "              \n",
       "                       [[-1.0580e-02]],\n",
       "              \n",
       "                       [[-4.3881e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1773e-03]],\n",
       "              \n",
       "                       [[ 2.3497e-03]],\n",
       "              \n",
       "                       [[ 3.2721e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9447e-05]],\n",
       "              \n",
       "                       [[-4.9647e-05]],\n",
       "              \n",
       "                       [[ 7.7879e-06]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9883e-05]],\n",
       "              \n",
       "                       [[-1.6815e-05]],\n",
       "              \n",
       "                       [[ 2.3595e-05]]]])),\n",
       "             ('layer3.0.bn3.weight',\n",
       "              tensor([ 0.0836, -0.1112,  0.1254,  ...,  0.0582, -0.0545,  0.0001])),\n",
       "             ('layer3.0.bn3.bias',\n",
       "              tensor([-0.0161, -0.0052, -0.0104,  ..., -0.0173,  0.0039, -0.0003])),\n",
       "             ('layer3.0.bn3.running_mean',\n",
       "              tensor([-9.1463e-05,  1.8549e-02, -7.2431e-02,  ...,  2.0183e-02,\n",
       "                       1.2344e-02,  8.2238e-06])),\n",
       "             ('layer3.0.bn3.running_var',\n",
       "              tensor([3.3713e-04, 6.9776e-04, 5.1994e-04,  ..., 5.8222e-04, 1.5416e-04,\n",
       "                      8.0530e-09])),\n",
       "             ('layer3.0.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.0.downsample.0.weight',\n",
       "              tensor([[[[-1.4171e-03]],\n",
       "              \n",
       "                       [[-3.3692e-03]],\n",
       "              \n",
       "                       [[ 1.0864e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.2359e-04]],\n",
       "              \n",
       "                       [[-9.3093e-03]],\n",
       "              \n",
       "                       [[-2.3677e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8766e-04]],\n",
       "              \n",
       "                       [[ 4.9127e-03]],\n",
       "              \n",
       "                       [[-2.2664e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.9909e-03]],\n",
       "              \n",
       "                       [[-3.7438e-03]],\n",
       "              \n",
       "                       [[-2.3469e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0388e-04]],\n",
       "              \n",
       "                       [[-1.9415e-03]],\n",
       "              \n",
       "                       [[ 2.3946e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9527e-03]],\n",
       "              \n",
       "                       [[-3.3829e-03]],\n",
       "              \n",
       "                       [[-3.6587e-04]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.4776e-03]],\n",
       "              \n",
       "                       [[ 1.3430e-04]],\n",
       "              \n",
       "                       [[-7.5681e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.2413e-03]],\n",
       "              \n",
       "                       [[ 1.1233e-02]],\n",
       "              \n",
       "                       [[-6.0349e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0838e-03]],\n",
       "              \n",
       "                       [[-7.2506e-03]],\n",
       "              \n",
       "                       [[-1.2889e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1767e-05]],\n",
       "              \n",
       "                       [[ 8.0860e-03]],\n",
       "              \n",
       "                       [[ 1.0801e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3733e-05]],\n",
       "              \n",
       "                       [[-4.4916e-05]],\n",
       "              \n",
       "                       [[-6.0333e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6089e-05]],\n",
       "              \n",
       "                       [[-8.3312e-05]],\n",
       "              \n",
       "                       [[-1.0435e-04]]]])),\n",
       "             ('layer3.0.downsample.1.weight',\n",
       "              tensor([0.0603, 0.0160, 0.0131,  ..., 0.0458, 0.0559, 0.0005])),\n",
       "             ('layer3.0.downsample.1.bias',\n",
       "              tensor([-0.0161, -0.0052, -0.0104,  ..., -0.0173,  0.0039, -0.0003])),\n",
       "             ('layer3.0.downsample.1.running_mean',\n",
       "              tensor([ 0.0316, -0.0313, -0.0059,  ...,  0.0292, -0.0194, -0.0006])),\n",
       "             ('layer3.0.downsample.1.running_var',\n",
       "              tensor([6.5542e-04, 7.3903e-04, 4.4277e-04,  ..., 9.9203e-04, 5.5311e-04,\n",
       "                      4.3389e-07])),\n",
       "             ('layer3.0.downsample.1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.1.conv1.weight',\n",
       "              tensor([[[[ 3.7097e-03]],\n",
       "              \n",
       "                       [[ 3.8365e-04]],\n",
       "              \n",
       "                       [[-5.0570e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.5785e-03]],\n",
       "              \n",
       "                       [[-4.3239e-03]],\n",
       "              \n",
       "                       [[-2.2392e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6178e-03]],\n",
       "              \n",
       "                       [[ 1.1393e-02]],\n",
       "              \n",
       "                       [[ 2.7228e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.6002e-03]],\n",
       "              \n",
       "                       [[-2.0335e-03]],\n",
       "              \n",
       "                       [[ 7.7481e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.4106e-04]],\n",
       "              \n",
       "                       [[ 2.8873e-03]],\n",
       "              \n",
       "                       [[ 1.2967e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.6725e-03]],\n",
       "              \n",
       "                       [[-2.6624e-03]],\n",
       "              \n",
       "                       [[-2.7069e-05]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3118e-03]],\n",
       "              \n",
       "                       [[-2.4647e-03]],\n",
       "              \n",
       "                       [[ 5.0082e-06]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.0713e-04]],\n",
       "              \n",
       "                       [[-2.7681e-03]],\n",
       "              \n",
       "                       [[ 1.1016e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8686e-03]],\n",
       "              \n",
       "                       [[ 1.7146e-02]],\n",
       "              \n",
       "                       [[-6.9749e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.9212e-03]],\n",
       "              \n",
       "                       [[-7.7028e-04]],\n",
       "              \n",
       "                       [[-1.1676e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3374e-04]],\n",
       "              \n",
       "                       [[-2.2242e-03]],\n",
       "              \n",
       "                       [[-3.8426e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.2733e-03]],\n",
       "              \n",
       "                       [[-8.7824e-04]],\n",
       "              \n",
       "                       [[ 2.7167e-05]]]])),\n",
       "             ('layer3.1.bn1.weight',\n",
       "              tensor([0.0556, 0.0633, 0.0379, 0.0456, 0.0569, 0.0344, 0.0321, 0.0483, 0.0305,\n",
       "                      0.0608, 0.0303, 0.1061, 0.0519, 0.0345, 0.0516, 0.0368, 0.0903, 0.0381,\n",
       "                      0.0611, 0.0769, 0.0387, 0.0553, 0.0382, 0.0548, 0.0539, 0.0438, 0.0562,\n",
       "                      0.0656, 0.0421, 0.0210, 0.0808, 0.0361, 0.0484, 0.0386, 0.0568, 0.0425,\n",
       "                      0.0528, 0.0574, 0.0499, 0.0590, 0.0665, 0.0311, 0.0414, 0.0403, 0.1032,\n",
       "                      0.0477, 0.0401, 0.0273, 0.0542, 0.0545, 0.0453, 0.0571, 0.0598, 0.0358,\n",
       "                      0.0617, 0.0379, 0.0602, 0.0390, 0.0422, 0.0290, 0.0671, 0.0643, 0.0537,\n",
       "                      0.0832, 0.0530, 0.0470, 0.0139, 0.0429, 0.0450, 0.0332, 0.0467, 0.0591,\n",
       "                      0.0476, 0.0716, 0.0444, 0.0517, 0.0768, 0.0429, 0.0331, 0.0648, 0.0429,\n",
       "                      0.0605, 0.0312, 0.0572, 0.0138, 0.0426, 0.0918, 0.0426, 0.0262, 0.0375,\n",
       "                      0.0754, 0.0846, 0.0268, 0.0632, 0.0480, 0.0703, 0.0671, 0.0267, 0.0240,\n",
       "                      0.0321, 0.0267, 0.0445, 0.0673, 0.0227, 0.0517, 0.0657, 0.0747, 0.0960,\n",
       "                      0.0523, 0.0419, 0.0260, 0.0581, 0.0283, 0.0675, 0.0301, 0.0469, 0.0573,\n",
       "                      0.1116, 0.0427, 0.0686, 0.0567, 0.0696, 0.0493, 0.0473, 0.0329, 0.0323,\n",
       "                      0.1008, 0.0419, 0.0794, 0.0375, 0.0367, 0.0494, 0.0454, 0.0494, 0.0429,\n",
       "                      0.0705, 0.0662, 0.0363, 0.0385, 0.0254, 0.0427, 0.0345, 0.0636, 0.0328,\n",
       "                      0.0507, 0.0384, 0.0761, 0.0176, 0.0607, 0.0501, 0.0416, 0.0624, 0.0638,\n",
       "                      0.0305, 0.0485, 0.0349, 0.0364, 0.0722, 0.0536, 0.0919, 0.0346, 0.0510,\n",
       "                      0.0520, 0.0593, 0.0464, 0.0626, 0.0539, 0.0539, 0.0291, 0.0419, 0.0883,\n",
       "                      0.0271, 0.0524, 0.0637, 0.0510, 0.0425, 0.1029, 0.0621, 0.0401, 0.0520,\n",
       "                      0.0475, 0.0272, 0.0289, 0.0637, 0.0460, 0.0657, 0.0518, 0.0245, 0.0529,\n",
       "                      0.0523, 0.0427, 0.0456, 0.0486, 0.0941, 0.0437, 0.0656, 0.0276, 0.0304,\n",
       "                      0.0577, 0.0348, 0.0562, 0.0221, 0.0338, 0.0393, 0.0760, 0.0490, 0.0717,\n",
       "                      0.0488, 0.0733, 0.0978, 0.0503, 0.0485, 0.0299, 0.0533, 0.0603, 0.0739,\n",
       "                      0.0383, 0.0879, 0.0717, 0.0430, 0.0720, 0.0530, 0.0397, 0.0449, 0.0297,\n",
       "                      0.0352, 0.0549, 0.0927, 0.0303, 0.0666, 0.0371, 0.0668, 0.0461, 0.0337,\n",
       "                      0.0517, 0.0406, 0.0515, 0.0310, 0.0529, 0.0199, 0.0440, 0.0336, 0.0387,\n",
       "                      0.0743, 0.0365, 0.1080, 0.0735, 0.0472, 0.0436, 0.0331, 0.0639, 0.0557,\n",
       "                      0.1113, 0.0500, 0.0725, 0.0379])),\n",
       "             ('layer3.1.bn1.bias',\n",
       "              tensor([-1.4527e-02,  3.0981e-03,  1.1295e-02, -1.2855e-02, -7.3255e-03,\n",
       "                       1.1025e-02,  5.8332e-03,  2.2594e-02, -4.1870e-03, -3.6932e-02,\n",
       "                       5.4828e-03, -7.1008e-02, -5.8958e-03,  2.3319e-03, -6.6405e-03,\n",
       "                       3.4247e-03, -1.6815e-02,  1.1020e-02, -1.1648e-02,  1.5736e-02,\n",
       "                       1.1962e-02, -3.1800e-03,  1.2418e-04, -1.3732e-02, -1.3129e-02,\n",
       "                       3.6450e-04,  2.2808e-02, -1.6173e-02, -2.2553e-03,  8.7701e-03,\n",
       "                      -1.3697e-02,  6.1819e-03,  1.6904e-02, -7.6789e-03,  9.9689e-03,\n",
       "                       1.8727e-02,  3.0435e-03, -2.5148e-03,  2.6357e-03, -2.7038e-03,\n",
       "                      -4.2529e-03,  8.6759e-03,  1.1952e-02,  5.7435e-03, -3.5988e-02,\n",
       "                       5.3091e-03,  4.8984e-03,  3.2529e-03,  4.4680e-04, -6.4183e-03,\n",
       "                       3.0599e-03, -1.8425e-02,  3.6581e-02, -7.2880e-04, -7.0947e-04,\n",
       "                       4.7498e-04,  6.6350e-03,  9.9609e-03, -1.3213e-02, -2.1805e-03,\n",
       "                      -2.4483e-02, -1.3310e-02,  1.2032e-03, -2.7757e-02,  2.3549e-02,\n",
       "                       1.6375e-03,  1.3662e-02, -3.8958e-04,  2.5810e-02,  4.4980e-03,\n",
       "                      -3.0418e-03,  1.5348e-03,  1.4354e-02, -2.8310e-02,  3.9018e-03,\n",
       "                      -1.5110e-02, -1.8020e-02,  2.3984e-02,  2.8787e-03, -2.5655e-02,\n",
       "                       2.9956e-02, -2.1943e-02,  1.7815e-02, -1.9801e-02,  1.5790e-03,\n",
       "                       1.6917e-02, -5.2993e-02,  3.3449e-02,  2.2637e-02,  1.0647e-02,\n",
       "                      -4.7396e-03, -4.4208e-03,  3.0732e-03, -1.5883e-02,  5.9229e-03,\n",
       "                       3.0315e-02, -2.5961e-02, -1.3818e-03, -5.2636e-03,  1.0058e-02,\n",
       "                       8.3271e-03,  1.1785e-02,  4.5666e-03, -1.1405e-03, -4.9562e-03,\n",
       "                       9.8407e-03,  2.7862e-02, -2.1326e-02, -2.0359e-03, -1.4716e-02,\n",
       "                       3.3173e-03,  2.5731e-03,  8.8602e-03,  1.8593e-02,  2.3988e-02,\n",
       "                       1.7410e-02, -8.4275e-03, -2.7496e-03,  3.1322e-03,  3.6334e-03,\n",
       "                      -6.6692e-03, -1.6418e-02, -4.3976e-03,  2.4072e-02,  2.3490e-02,\n",
       "                       2.1678e-02, -3.9392e-03,  2.0841e-02, -2.8616e-02, -1.2919e-02,\n",
       "                       7.8153e-03, -2.3604e-03, -8.9475e-03,  1.3675e-02,  1.1988e-03,\n",
       "                       2.9618e-03, -6.1971e-03,  6.0031e-03,  2.9619e-02, -1.9010e-03,\n",
       "                      -4.4366e-03, -1.4705e-02, -1.1214e-02, -8.5647e-03,  2.7634e-03,\n",
       "                       5.1871e-03,  1.6731e-02, -9.5737e-05,  3.4092e-03, -1.0414e-02,\n",
       "                      -9.5969e-03, -3.3444e-02, -1.0430e-02,  5.7890e-03,  1.2290e-02,\n",
       "                       5.4676e-04, -1.3866e-02,  2.3480e-04, -1.4517e-02,  2.1553e-02,\n",
       "                       5.5524e-03,  6.3469e-03, -1.0140e-02,  5.9750e-03, -2.1348e-02,\n",
       "                      -6.7177e-03, -6.5379e-03, -1.9867e-03,  8.0780e-03, -2.6736e-03,\n",
       "                      -2.1051e-04, -4.4445e-03,  1.0233e-02, -2.2600e-02, -1.2142e-02,\n",
       "                      -5.1956e-03,  2.5180e-02,  7.0902e-03, -2.5221e-03, -2.7329e-02,\n",
       "                       2.3733e-03, -5.2043e-04, -4.3911e-03, -1.0424e-02,  2.4526e-03,\n",
       "                       2.0877e-03, -1.0162e-02, -1.6346e-03, -6.7647e-03, -9.2867e-03,\n",
       "                       6.8722e-03, -2.3450e-03, -5.5245e-03, -4.7474e-02,  1.2611e-02,\n",
       "                       2.2321e-03,  1.0947e-03,  2.1085e-02, -4.3176e-04,  7.6010e-03,\n",
       "                      -6.9645e-03,  1.1512e-02,  5.6347e-03,  2.1801e-02,  2.7109e-02,\n",
       "                       6.6035e-03, -3.3340e-02,  3.4926e-02, -1.5292e-02, -2.0548e-02,\n",
       "                       2.2462e-02, -3.3854e-03,  8.1292e-03,  1.7526e-02, -8.4986e-03,\n",
       "                      -1.4062e-02,  1.5059e-03,  1.4452e-03,  1.9189e-02,  8.2627e-03,\n",
       "                      -3.0861e-02, -3.2784e-03,  1.4791e-02, -2.2722e-03,  3.3532e-02,\n",
       "                       1.8418e-02,  2.1790e-03, -1.2672e-02,  2.5183e-03, -4.0108e-02,\n",
       "                      -8.8239e-03,  1.9233e-02,  8.0644e-03, -1.6731e-03,  1.6231e-02,\n",
       "                       5.3831e-03, -1.1566e-02,  9.3049e-03, -1.3309e-02,  5.9822e-03,\n",
       "                      -5.7840e-03,  6.6028e-03, -1.0618e-02, -1.4955e-02,  9.0482e-03,\n",
       "                       2.1836e-02, -2.2422e-02,  2.1796e-02, -1.4508e-02,  3.0027e-03,\n",
       "                      -2.9160e-02, -6.8636e-03, -5.4835e-02, -4.9610e-03,  2.1243e-03,\n",
       "                       2.1198e-03])),\n",
       "             ('layer3.1.bn1.running_mean',\n",
       "              tensor([ 0.0014,  0.0007, -0.0002,  0.0007,  0.0266, -0.0115, -0.0052,  0.0144,\n",
       "                       0.0147,  0.0289, -0.0111,  0.0273,  0.0146, -0.0046, -0.0266, -0.0047,\n",
       "                      -0.0151,  0.0245,  0.0209, -0.0125, -0.0218,  0.0104, -0.0173, -0.0200,\n",
       "                       0.0040,  0.0017, -0.0058,  0.0237,  0.0109, -0.0162,  0.0075, -0.0076,\n",
       "                       0.0029, -0.0030,  0.0151,  0.0074, -0.0036,  0.0009,  0.0197,  0.0212,\n",
       "                      -0.0187,  0.0300,  0.0250, -0.0189, -0.0222, -0.0148,  0.0202,  0.0142,\n",
       "                      -0.0129, -0.0112,  0.0359, -0.0087,  0.0602,  0.0008,  0.0150,  0.0199,\n",
       "                       0.0208,  0.0001,  0.0085, -0.0015, -0.0107,  0.0004,  0.0219,  0.0076,\n",
       "                       0.0184, -0.0215,  0.0006,  0.0037,  0.0032,  0.0130,  0.0204, -0.0275,\n",
       "                       0.0167,  0.0044, -0.0021, -0.0009,  0.0072,  0.0278, -0.0135, -0.0162,\n",
       "                       0.0294,  0.0259,  0.0253,  0.0156, -0.0067,  0.0244,  0.0100,  0.0372,\n",
       "                       0.0130,  0.0001,  0.0127, -0.0201, -0.0104, -0.0044,  0.0136, -0.0430,\n",
       "                       0.0316, -0.0133, -0.0003,  0.0050, -0.0194, -0.0148,  0.0074, -0.0063,\n",
       "                       0.0159, -0.0252,  0.0203,  0.0197,  0.0103,  0.0082,  0.0070,  0.0043,\n",
       "                       0.0269, -0.0006,  0.0112,  0.0450,  0.0145, -0.0164, -0.0066,  0.0006,\n",
       "                       0.0011, -0.0249,  0.0287, -0.0095,  0.0072, -0.0248, -0.0466,  0.0137,\n",
       "                       0.0012, -0.0021,  0.0233, -0.0219,  0.0051,  0.0013,  0.0162, -0.0380,\n",
       "                       0.0130, -0.0026,  0.0171,  0.0048, -0.0074,  0.0090,  0.0223, -0.0062,\n",
       "                      -0.0245,  0.0134, -0.0410,  0.0101,  0.0258,  0.0033, -0.0293,  0.0186,\n",
       "                      -0.0245,  0.0061,  0.0544,  0.0101,  0.0202,  0.0101, -0.0033, -0.0927,\n",
       "                       0.0114, -0.0016, -0.0160, -0.0162,  0.0081, -0.0398, -0.0022, -0.0065,\n",
       "                      -0.0014,  0.0012, -0.0333,  0.0094, -0.0464,  0.0275,  0.0098, -0.0107,\n",
       "                      -0.0975, -0.0111,  0.0176, -0.0120,  0.0314,  0.0136,  0.0024, -0.0011,\n",
       "                       0.0141, -0.0143,  0.0126,  0.0117,  0.0360, -0.0163,  0.0055, -0.0057,\n",
       "                       0.0062,  0.0282,  0.0304, -0.0302, -0.0068, -0.0022,  0.0046,  0.0054,\n",
       "                      -0.0010,  0.0259,  0.0069, -0.0182,  0.0125,  0.0398,  0.0089,  0.0181,\n",
       "                       0.0021, -0.0482,  0.0301,  0.0170, -0.0163,  0.0078, -0.0041, -0.0022,\n",
       "                       0.0016, -0.0187, -0.0704, -0.0280,  0.0076,  0.0150,  0.0440,  0.0113,\n",
       "                       0.0203, -0.0230,  0.0214, -0.0249, -0.0103,  0.0217,  0.0046,  0.0191,\n",
       "                       0.0105,  0.0091,  0.0094, -0.0024,  0.0152, -0.0004,  0.0144, -0.0036,\n",
       "                       0.0081, -0.0102, -0.0018,  0.0042, -0.0008, -0.0914,  0.0337, -0.0262,\n",
       "                       0.0050, -0.0235,  0.0099,  0.0244, -0.0258, -0.0232, -0.0085, -0.0263])),\n",
       "             ('layer3.1.bn1.running_var',\n",
       "              tensor([0.0014, 0.0018, 0.0033, 0.0009, 0.0015, 0.0004, 0.0013, 0.0025, 0.0005,\n",
       "                      0.0012, 0.0005, 0.0028, 0.0012, 0.0004, 0.0011, 0.0008, 0.0024, 0.0021,\n",
       "                      0.0019, 0.0025, 0.0026, 0.0015, 0.0004, 0.0006, 0.0006, 0.0023, 0.0034,\n",
       "                      0.0012, 0.0023, 0.0009, 0.0019, 0.0010, 0.0024, 0.0005, 0.0021, 0.0010,\n",
       "                      0.0013, 0.0010, 0.0024, 0.0040, 0.0027, 0.0012, 0.0045, 0.0006, 0.0035,\n",
       "                      0.0012, 0.0019, 0.0016, 0.0044, 0.0015, 0.0035, 0.0016, 0.0040, 0.0021,\n",
       "                      0.0012, 0.0013, 0.0018, 0.0018, 0.0011, 0.0010, 0.0029, 0.0012, 0.0016,\n",
       "                      0.0020, 0.0027, 0.0024, 0.0006, 0.0021, 0.0014, 0.0020, 0.0018, 0.0012,\n",
       "                      0.0022, 0.0017, 0.0008, 0.0007, 0.0017, 0.0023, 0.0004, 0.0017, 0.0015,\n",
       "                      0.0010, 0.0037, 0.0009, 0.0001, 0.0045, 0.0028, 0.0024, 0.0007, 0.0033,\n",
       "                      0.0020, 0.0018, 0.0010, 0.0019, 0.0020, 0.0035, 0.0018, 0.0005, 0.0003,\n",
       "                      0.0009, 0.0005, 0.0031, 0.0020, 0.0002, 0.0016, 0.0018, 0.0037, 0.0041,\n",
       "                      0.0009, 0.0004, 0.0006, 0.0019, 0.0019, 0.0040, 0.0015, 0.0029, 0.0028,\n",
       "                      0.0027, 0.0023, 0.0038, 0.0016, 0.0017, 0.0009, 0.0022, 0.0011, 0.0017,\n",
       "                      0.0044, 0.0017, 0.0018, 0.0007, 0.0035, 0.0022, 0.0019, 0.0016, 0.0031,\n",
       "                      0.0025, 0.0013, 0.0012, 0.0023, 0.0004, 0.0009, 0.0004, 0.0011, 0.0004,\n",
       "                      0.0016, 0.0014, 0.0047, 0.0001, 0.0022, 0.0023, 0.0012, 0.0015, 0.0025,\n",
       "                      0.0010, 0.0028, 0.0013, 0.0009, 0.0013, 0.0012, 0.0036, 0.0011, 0.0011,\n",
       "                      0.0009, 0.0027, 0.0006, 0.0019, 0.0017, 0.0013, 0.0005, 0.0019, 0.0025,\n",
       "                      0.0004, 0.0011, 0.0015, 0.0005, 0.0013, 0.0040, 0.0023, 0.0009, 0.0008,\n",
       "                      0.0039, 0.0006, 0.0010, 0.0016, 0.0013, 0.0017, 0.0010, 0.0003, 0.0022,\n",
       "                      0.0013, 0.0011, 0.0012, 0.0009, 0.0020, 0.0026, 0.0013, 0.0005, 0.0028,\n",
       "                      0.0015, 0.0005, 0.0014, 0.0016, 0.0021, 0.0023, 0.0042, 0.0031, 0.0013,\n",
       "                      0.0016, 0.0021, 0.0041, 0.0030, 0.0008, 0.0003, 0.0020, 0.0008, 0.0027,\n",
       "                      0.0011, 0.0023, 0.0023, 0.0040, 0.0026, 0.0022, 0.0041, 0.0020, 0.0013,\n",
       "                      0.0030, 0.0027, 0.0027, 0.0006, 0.0008, 0.0007, 0.0028, 0.0009, 0.0006,\n",
       "                      0.0011, 0.0033, 0.0013, 0.0015, 0.0018, 0.0006, 0.0018, 0.0017, 0.0008,\n",
       "                      0.0020, 0.0013, 0.0042, 0.0022, 0.0017, 0.0013, 0.0004, 0.0012, 0.0013,\n",
       "                      0.0044, 0.0014, 0.0033, 0.0012])),\n",
       "             ('layer3.1.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.1.conv2.weight',\n",
       "              tensor([[[[-3.9256e-03,  2.9606e-03,  1.4462e-03],\n",
       "                        [-8.5083e-03,  2.4937e-03,  3.6892e-04],\n",
       "                        [-6.5904e-03, -3.4515e-03,  1.0764e-03]],\n",
       "              \n",
       "                       [[ 4.2809e-03,  7.2374e-03,  5.1179e-03],\n",
       "                        [-2.5341e-03, -1.6127e-03, -2.1344e-03],\n",
       "                        [ 9.2724e-04,  3.6535e-03,  1.1780e-03]],\n",
       "              \n",
       "                       [[-6.5153e-03,  8.3244e-04,  9.4584e-05],\n",
       "                        [-7.9588e-03,  3.0103e-03,  3.2606e-03],\n",
       "                        [-7.1730e-03, -2.1179e-04,  5.5719e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.2961e-04,  3.8139e-04,  1.3874e-03],\n",
       "                        [-4.6129e-03,  4.9362e-03,  1.1489e-03],\n",
       "                        [-1.7357e-03, -2.2798e-03, -1.4518e-03]],\n",
       "              \n",
       "                       [[-1.0681e-02, -3.9238e-03,  4.3391e-03],\n",
       "                        [-7.7706e-03,  3.3953e-03,  3.0743e-03],\n",
       "                        [-1.0323e-02,  1.3082e-03, -5.4338e-04]],\n",
       "              \n",
       "                       [[-1.2721e-03, -1.2058e-03, -4.0915e-04],\n",
       "                        [-1.3002e-03, -1.0108e-03, -2.8480e-04],\n",
       "                        [ 7.7279e-04,  1.0842e-03,  3.6144e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.7913e-03,  3.6839e-03,  2.3215e-03],\n",
       "                        [-2.4435e-03, -1.7409e-04,  2.1882e-03],\n",
       "                        [-3.6663e-03, -5.8164e-03,  1.2520e-04]],\n",
       "              \n",
       "                       [[ 3.7665e-03,  1.0104e-03,  9.2355e-03],\n",
       "                        [ 8.6763e-04, -5.2064e-03,  6.1549e-03],\n",
       "                        [-2.1129e-03, -1.2714e-02,  4.7801e-03]],\n",
       "              \n",
       "                       [[-2.2797e-03, -2.7816e-03,  1.4653e-02],\n",
       "                        [-3.5363e-03, -4.3312e-03,  1.0210e-02],\n",
       "                        [-3.5407e-03, -5.3874e-03,  6.9441e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2317e-05,  6.2939e-03,  4.5363e-03],\n",
       "                        [-2.1128e-03, -1.5563e-03, -3.9689e-04],\n",
       "                        [ 5.4017e-04,  5.0693e-04,  3.4506e-04]],\n",
       "              \n",
       "                       [[ 1.5983e-03, -5.8243e-03, -3.0389e-03],\n",
       "                        [ 5.6332e-03,  5.8319e-03, -2.4420e-03],\n",
       "                        [ 5.4919e-03,  2.9686e-03, -7.8674e-03]],\n",
       "              \n",
       "                       [[-1.4739e-03, -1.8265e-03,  5.8593e-03],\n",
       "                        [-4.7814e-03, -3.6755e-03,  1.3724e-03],\n",
       "                        [-4.8924e-03, -1.8682e-03,  2.0342e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5191e-04, -6.1665e-03,  3.4029e-03],\n",
       "                        [-6.3087e-03, -6.6374e-04, -2.2989e-03],\n",
       "                        [-5.6161e-03, -4.2596e-03,  5.0623e-04]],\n",
       "              \n",
       "                       [[ 8.2790e-03, -2.8258e-03, -3.3923e-03],\n",
       "                        [-5.7232e-04, -2.9815e-03,  3.2353e-03],\n",
       "                        [-6.6102e-03, -1.7617e-03, -4.7959e-03]],\n",
       "              \n",
       "                       [[ 9.4792e-04, -4.6051e-03, -4.2376e-03],\n",
       "                        [ 4.6792e-03,  3.0933e-03,  2.3140e-03],\n",
       "                        [-1.1857e-03, -2.1065e-03,  5.2795e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.2072e-03, -1.1383e-02, -4.2137e-03],\n",
       "                        [-7.9695e-03, -4.6435e-04, -4.0353e-03],\n",
       "                        [-6.3092e-03, -7.3748e-03, -9.7057e-03]],\n",
       "              \n",
       "                       [[ 3.6791e-03,  2.1519e-03,  6.7700e-03],\n",
       "                        [-4.3977e-03,  1.2895e-05, -2.0049e-03],\n",
       "                        [-1.4470e-02, -4.3936e-03, -1.2867e-02]],\n",
       "              \n",
       "                       [[ 1.1659e-03, -4.2276e-04,  1.8061e-03],\n",
       "                        [ 1.1390e-03, -1.7681e-04,  4.6494e-03],\n",
       "                        [-8.5678e-04, -2.9760e-03, -4.2765e-04]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.2251e-03,  5.0883e-03,  1.3434e-02],\n",
       "                        [ 2.6983e-03, -3.2501e-04,  4.7507e-03],\n",
       "                        [ 7.0445e-03,  4.2045e-03,  7.0319e-04]],\n",
       "              \n",
       "                       [[-5.8642e-03,  4.8846e-04,  2.9646e-04],\n",
       "                        [-7.4423e-03, -7.7919e-03, -1.0361e-02],\n",
       "                        [-7.9048e-03, -7.9428e-03, -9.9307e-03]],\n",
       "              \n",
       "                       [[-1.3189e-03, -5.6767e-05, -3.6184e-04],\n",
       "                        [-3.3725e-03, -4.9002e-03, -4.9023e-03],\n",
       "                        [ 6.1865e-03,  5.7491e-03,  3.4961e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.5014e-03,  9.1682e-03,  1.4420e-02],\n",
       "                        [-1.0025e-03, -5.5418e-03,  3.2147e-03],\n",
       "                        [-2.0673e-03, -3.0302e-03, -9.4339e-04]],\n",
       "              \n",
       "                       [[-2.4408e-03, -1.8214e-02, -1.1476e-02],\n",
       "                        [ 1.2208e-02,  1.8536e-03,  4.1183e-03],\n",
       "                        [ 6.1167e-04, -3.6793e-03, -1.7289e-03]],\n",
       "              \n",
       "                       [[ 4.0559e-04,  3.4200e-03, -9.5340e-04],\n",
       "                        [ 4.0165e-03, -2.6073e-03, -2.5751e-03],\n",
       "                        [ 6.0069e-03,  2.1686e-03,  4.3591e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3978e-03, -6.8449e-04,  5.0874e-03],\n",
       "                        [ 4.1707e-03, -1.9115e-03,  1.2668e-03],\n",
       "                        [ 1.7272e-03,  2.3828e-03,  6.6192e-03]],\n",
       "              \n",
       "                       [[-2.2345e-03, -1.5395e-03, -3.1338e-03],\n",
       "                        [ 2.3014e-03, -8.0402e-04, -3.9203e-03],\n",
       "                        [ 1.1346e-02,  9.4937e-03,  6.2009e-03]],\n",
       "              \n",
       "                       [[-2.9639e-03, -2.0649e-03,  8.2386e-03],\n",
       "                        [ 1.6653e-03, -5.7990e-04,  9.9702e-04],\n",
       "                        [ 5.0077e-03,  5.2962e-03,  1.7410e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.5712e-04, -3.7692e-03,  4.2606e-03],\n",
       "                        [ 4.0385e-03,  1.1938e-03,  1.9630e-03],\n",
       "                        [ 1.5053e-03,  2.3413e-03,  6.7480e-03]],\n",
       "              \n",
       "                       [[ 1.2126e-02,  7.1009e-03, -4.6781e-03],\n",
       "                        [ 1.2028e-03,  4.5317e-03, -1.0631e-04],\n",
       "                        [ 3.3221e-03,  6.6644e-04, -6.4517e-04]],\n",
       "              \n",
       "                       [[-2.2935e-03, -2.3698e-03,  2.1757e-03],\n",
       "                        [ 2.9584e-04,  8.6615e-04,  1.7125e-03],\n",
       "                        [ 7.2771e-04,  3.0231e-03,  4.4482e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.7653e-03,  2.5989e-04, -1.8740e-03],\n",
       "                        [ 6.9814e-03, -1.8712e-03, -3.2685e-03],\n",
       "                        [ 2.2134e-03,  2.7521e-04,  1.3153e-03]],\n",
       "              \n",
       "                       [[ 4.8223e-03, -2.7671e-03,  1.2173e-03],\n",
       "                        [ 1.6608e-02, -1.7956e-03, -4.8282e-03],\n",
       "                        [ 7.0570e-03,  5.2529e-04,  1.1404e-03]],\n",
       "              \n",
       "                       [[ 6.9944e-03, -7.8995e-03, -2.3626e-03],\n",
       "                        [ 8.9442e-03, -4.2028e-03, -4.6666e-03],\n",
       "                        [ 3.4598e-03, -8.0969e-03,  4.0674e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2718e-03, -1.1588e-03, -4.1118e-03],\n",
       "                        [ 6.9187e-03, -3.1895e-03, -8.6499e-03],\n",
       "                        [ 3.2459e-03,  6.6524e-04, -4.3024e-04]],\n",
       "              \n",
       "                       [[-9.2391e-03,  1.5823e-03,  1.2071e-02],\n",
       "                        [-7.6604e-03,  6.0857e-03,  1.2739e-02],\n",
       "                        [-6.8307e-03,  9.7672e-03, -2.9434e-03]],\n",
       "              \n",
       "                       [[ 4.5573e-03, -1.7386e-03, -2.8704e-03],\n",
       "                        [ 5.3682e-03, -1.3729e-03, -4.4966e-03],\n",
       "                        [ 5.9857e-03, -4.1411e-03,  1.4434e-04]]]])),\n",
       "             ('layer3.1.bn2.weight',\n",
       "              tensor([7.2207e-02, 7.3728e-02, 7.0536e-02, 5.6500e-02, 5.4341e-02, 1.0903e-01,\n",
       "                      3.9480e-02, 1.1336e-01, 8.8166e-06, 5.1269e-02, 5.4760e-02, 4.3303e-02,\n",
       "                      6.7654e-02, 1.4710e-03, 2.9399e-02, 6.1845e-02, 1.0058e-01, 5.3694e-02,\n",
       "                      8.3350e-02, 1.7491e-02, 5.1112e-02, 7.5662e-02, 7.1957e-02, 5.2754e-02,\n",
       "                      6.4169e-02, 3.6422e-02, 7.6019e-02, 1.0835e-01, 4.7584e-02, 5.5010e-02,\n",
       "                      4.4041e-02, 6.9959e-02, 9.2561e-02, 5.6193e-02, 6.4862e-02, 6.7941e-02,\n",
       "                      1.0090e-01, 5.5561e-02, 7.1419e-02, 3.8930e-02, 4.7133e-02, 4.8707e-02,\n",
       "                      6.6934e-02, 8.1489e-02, 3.9029e-02, 8.0740e-02, 5.4141e-02, 5.7108e-02,\n",
       "                      9.4629e-02, 6.8213e-02, 9.5006e-02, 9.1970e-02, 4.0545e-02, 6.4834e-02,\n",
       "                      6.5371e-02, 4.4531e-02, 9.8143e-02, 7.8440e-02, 4.8644e-02, 5.3452e-02,\n",
       "                      3.7348e-02, 1.1278e-01, 6.2304e-02, 7.2660e-02, 8.1367e-02, 6.4891e-02,\n",
       "                      8.1572e-02, 1.1076e-01, 7.6649e-02, 1.0531e-01, 8.4727e-02, 1.0860e-01,\n",
       "                      5.6547e-02, 7.9896e-02, 4.8346e-02, 8.9819e-02, 5.4153e-02, 5.9643e-02,\n",
       "                      9.1752e-02, 8.6319e-02, 9.5551e-02, 8.4497e-02, 6.6991e-02, 9.0675e-02,\n",
       "                      5.5035e-02, 6.3755e-02, 5.1891e-02, 6.5592e-02, 4.0589e-02, 8.7474e-02,\n",
       "                      7.2704e-02, 6.9761e-02, 1.8987e-02, 6.6566e-02, 5.3383e-02, 4.4266e-02,\n",
       "                      9.3471e-02, 6.4829e-02, 4.9569e-02, 9.5223e-02, 5.9008e-02, 5.0726e-02,\n",
       "                      3.2119e-02, 8.3472e-02, 5.0549e-02, 4.2765e-02, 7.8086e-02, 1.0780e-01,\n",
       "                      1.0495e-01, 6.1004e-02, 6.6990e-02, 5.8677e-02, 4.5656e-02, 5.7506e-02,\n",
       "                      5.4978e-02, 3.0699e-02, 8.6747e-02, 7.0042e-02, 6.9338e-02, 6.4664e-02,\n",
       "                      4.0586e-02, 8.5124e-02, 5.6452e-02, 3.8534e-02, 5.7043e-02, 8.5950e-02,\n",
       "                      7.2543e-02, 9.1765e-02, 8.9075e-02, 5.3055e-02, 4.2071e-02, 5.0840e-02,\n",
       "                      9.1118e-02, 6.7569e-02, 8.3164e-02, 2.9281e-02, 5.6318e-02, 6.8068e-02,\n",
       "                      4.4774e-02, 6.4444e-02, 6.9705e-02, 8.1586e-02, 7.6588e-02, 7.2323e-02,\n",
       "                      6.6478e-02, 8.2414e-02, 7.1860e-02, 5.7206e-02, 7.7443e-02, 4.6568e-02,\n",
       "                      9.6642e-02, 7.6452e-02, 4.9807e-02, 4.0354e-02, 3.9161e-02, 8.7303e-02,\n",
       "                      1.0683e-01, 8.4706e-02, 4.6222e-02, 8.2426e-02, 5.3036e-02, 4.7885e-02,\n",
       "                      5.4720e-02, 5.2519e-02, 3.9054e-02, 5.1802e-02, 7.6875e-02, 7.2911e-02,\n",
       "                      6.2507e-02, 4.4148e-02, 5.6933e-02, 1.3532e-01, 5.8955e-02, 2.6424e-02,\n",
       "                      6.7033e-02, 9.3412e-02, 8.1412e-02, 5.9889e-02, 7.3973e-02, 8.2055e-02,\n",
       "                      7.2268e-02, 5.3075e-02, 3.2432e-02, 7.5699e-02, 2.2241e-02, 5.1980e-02,\n",
       "                      9.8378e-02, 6.8137e-02, 9.2575e-02, 6.4648e-02, 7.6879e-02, 5.2158e-02,\n",
       "                      6.7317e-02, 8.1442e-02, 7.0285e-02, 4.6030e-02, 7.2145e-02, 3.9578e-02,\n",
       "                      6.7528e-02, 2.8160e-02, 4.2724e-02, 4.2633e-02, 1.2204e-01, 1.0085e-01,\n",
       "                      6.2587e-02, 1.0699e-01, 5.6560e-02, 6.3657e-02, 3.8237e-02, 8.5130e-02,\n",
       "                      7.5330e-02, 5.8320e-02, 7.3286e-02, 3.4874e-02, 6.4458e-02, 8.1664e-02,\n",
       "                      5.0881e-02, 8.1600e-02, 1.5695e-01, 5.6160e-02, 4.0938e-02, 1.3571e-01,\n",
       "                      5.3202e-02, 2.6613e-02, 6.7158e-02, 4.8635e-02, 5.1956e-02, 9.3148e-02,\n",
       "                      7.4727e-02, 5.9755e-02, 6.9094e-02, 5.5774e-02, 4.8576e-02, 9.4176e-02,\n",
       "                      5.6785e-02, 6.9054e-02, 7.4007e-02, 9.3728e-02, 7.4181e-02, 5.9113e-02,\n",
       "                      7.0206e-02, 4.7632e-02, 3.7939e-02, 3.0989e-02, 5.9457e-02, 6.5780e-02,\n",
       "                      4.1433e-02, 4.3995e-02, 7.9734e-02, 6.6482e-02, 8.2713e-02, 6.8303e-02,\n",
       "                      5.1351e-02, 6.8293e-02, 6.4327e-02, 9.2303e-02])),\n",
       "             ('layer3.1.bn2.bias',\n",
       "              tensor([-1.1007e-02, -5.3992e-02, -4.1426e-02, -5.4148e-03, -4.4007e-03,\n",
       "                      -3.6520e-02,  4.5976e-03, -5.3301e-02, -4.4758e-05,  1.0516e-03,\n",
       "                       7.5617e-03,  7.7261e-03,  1.5658e-02, -6.8076e-03, -7.2684e-04,\n",
       "                       4.5405e-03, -3.8291e-02, -4.8433e-02, -3.7043e-02, -9.6216e-03,\n",
       "                       2.0329e-03, -4.9244e-02, -1.7171e-02, -3.0591e-02, -2.2811e-02,\n",
       "                      -3.6602e-02, -2.2697e-02, -2.8508e-02, -9.5656e-03, -1.1671e-02,\n",
       "                      -1.7287e-02, -1.5581e-02, -3.3694e-02, -2.3856e-02, -2.5099e-02,\n",
       "                      -1.3765e-03, -3.8737e-02, -2.3685e-02, -2.6399e-02, -3.0306e-02,\n",
       "                      -7.4849e-03, -5.7100e-02, -2.3811e-02, -5.0632e-02, -1.7395e-02,\n",
       "                      -5.7628e-02, -3.1342e-02, -8.9189e-03, -1.5217e-02, -2.4000e-02,\n",
       "                      -4.8211e-02, -5.0541e-02, -1.0116e-02, -1.2354e-02, -4.7767e-02,\n",
       "                      -9.4325e-04, -4.8159e-02, -4.2815e-02, -8.5550e-03, -2.1682e-02,\n",
       "                      -1.0389e-02, -7.1823e-02, -3.4336e-02,  7.0163e-03, -2.9692e-02,\n",
       "                      -5.4589e-02, -2.5240e-02, -5.8521e-02,  1.6362e-02, -5.9444e-02,\n",
       "                      -6.4605e-02, -6.7058e-02, -3.5550e-02, -3.1603e-02,  3.1484e-03,\n",
       "                      -5.2531e-02, -3.5499e-02, -3.2044e-02, -4.0607e-02, -9.0822e-02,\n",
       "                      -6.3147e-02, -2.3791e-02, -2.0419e-02, -4.4444e-02, -1.0927e-02,\n",
       "                      -3.1421e-02,  1.2351e-02, -3.6041e-02, -5.0890e-03, -3.2719e-02,\n",
       "                      -1.7875e-02, -2.0692e-02, -6.5173e-03, -7.6117e-02, -1.7722e-02,\n",
       "                      -1.9748e-02, -2.0593e-02, -9.2630e-03, -1.9463e-02, -3.6048e-02,\n",
       "                      -3.7278e-02, -2.6582e-02,  5.4169e-03, -5.1944e-02,  2.1101e-02,\n",
       "                      -3.9241e-02, -2.1527e-02, -8.5029e-02, -4.3441e-02, -2.2130e-02,\n",
       "                       1.1178e-02, -4.1243e-02, -4.4358e-03, -5.9804e-02, -1.8606e-02,\n",
       "                       2.2923e-03, -3.6562e-02, -5.3445e-02, -5.9089e-03, -3.8480e-02,\n",
       "                      -9.7259e-03, -7.8078e-04, -3.2280e-02, -7.6480e-03, -1.5965e-02,\n",
       "                      -2.3896e-02, -4.0702e-02, -7.2105e-02, -2.6517e-02, -3.1817e-03,\n",
       "                      -2.1039e-02, -9.4448e-03, -6.0172e-02, -3.5303e-02, -1.2315e-02,\n",
       "                       9.2057e-03, -4.0258e-02, -4.7723e-03, -8.6632e-03, -2.9551e-02,\n",
       "                      -2.0993e-02, -5.5593e-02, -1.4859e-02, -1.3385e-02, -4.6583e-02,\n",
       "                      -2.6087e-02, -6.5554e-02, -2.8346e-02, -5.1526e-02, -1.7914e-02,\n",
       "                      -4.5573e-02, -3.3239e-02, -3.4788e-02,  2.2138e-04, -7.2668e-03,\n",
       "                      -5.4341e-02, -8.0351e-02, -1.9705e-02, -3.0334e-02, -5.3677e-02,\n",
       "                      -8.6954e-03, -6.1155e-03, -1.4104e-02,  8.0835e-03, -1.8066e-02,\n",
       "                      -3.7378e-02, -4.2884e-02, -2.8289e-02, -2.1314e-02, -1.1427e-02,\n",
       "                      -2.6666e-03, -7.1002e-02, -2.7447e-02, -8.1061e-03, -1.1340e-02,\n",
       "                      -5.8015e-02, -5.7040e-02, -3.6944e-02, -1.6421e-02, -4.2020e-03,\n",
       "                       4.1189e-03, -3.7194e-02, -2.3272e-03, -8.1398e-03,  4.7592e-05,\n",
       "                      -5.1901e-02, -5.4717e-02, -5.2906e-02, -4.1785e-02, -2.6204e-02,\n",
       "                      -3.8282e-02, -3.1949e-02, -2.1447e-02, -2.5335e-02, -1.4731e-02,\n",
       "                      -5.8051e-03, -1.7602e-02, -7.3732e-03, -2.4595e-02, -7.2248e-03,\n",
       "                      -2.1091e-02, -5.3135e-03, -1.1530e-01, -6.3901e-02, -5.0711e-02,\n",
       "                      -3.0754e-02, -1.2611e-02, -3.1159e-02, -9.8999e-03,  9.4057e-03,\n",
       "                      -3.0269e-02, -4.1583e-03, -3.7786e-02, -1.6713e-02, -2.7596e-02,\n",
       "                      -6.2021e-03, -3.6655e-02, -2.2415e-02, -9.7793e-02, -1.4477e-03,\n",
       "                      -2.8561e-02, -1.0436e-01, -1.4132e-02, -3.0532e-03, -2.5911e-02,\n",
       "                      -3.6741e-02,  5.0637e-03, -4.7743e-02, -4.1602e-02, -5.0867e-02,\n",
       "                      -2.7714e-02, -1.1190e-02, -2.0473e-02, -3.3403e-02, -8.9711e-04,\n",
       "                      -4.2774e-02,  1.0476e-02, -6.0665e-02, -2.1249e-02, -3.6824e-02,\n",
       "                      -6.9380e-02, -4.2956e-02, -1.1745e-02, -1.3519e-02, -3.0849e-02,\n",
       "                      -3.2216e-02, -2.0373e-02, -1.6080e-02, -4.8476e-02, -8.2454e-03,\n",
       "                      -2.2151e-02, -1.4557e-03, -3.2396e-02, -3.8620e-02, -5.3794e-02,\n",
       "                      -4.6804e-02])),\n",
       "             ('layer3.1.bn2.running_mean',\n",
       "              tensor([-3.7214e-02,  1.4403e-02,  6.4294e-03, -1.0828e-02, -2.6501e-02,\n",
       "                       1.0427e-02, -5.0409e-03, -4.0863e-04,  4.0788e-05, -3.3698e-02,\n",
       "                      -3.7737e-02, -2.8409e-02, -2.4120e-02, -1.0497e-03, -1.2773e-02,\n",
       "                      -2.0454e-02, -2.5733e-02,  5.6237e-03,  8.7063e-03, -2.7901e-03,\n",
       "                      -3.7296e-02,  2.5056e-02, -3.4863e-02,  7.1446e-03,  2.2522e-03,\n",
       "                      -2.3360e-02, -1.8873e-02, -5.5455e-02, -2.3718e-02,  1.6590e-02,\n",
       "                       2.2660e-02, -3.1607e-02, -1.9535e-02, -8.7497e-03, -1.8667e-02,\n",
       "                      -2.3536e-02, -5.8782e-02, -4.4206e-03, -2.2203e-02,  7.8340e-03,\n",
       "                      -2.2267e-02,  1.1191e-02, -1.4674e-02, -2.8087e-02,  5.6558e-03,\n",
       "                       6.3855e-04,  1.5178e-04, -2.8764e-02, -2.9686e-02, -6.4826e-03,\n",
       "                      -2.7523e-02, -1.6769e-03, -1.8103e-02, -1.2465e-02,  5.3476e-03,\n",
       "                      -2.0855e-03, -2.6807e-02, -1.5397e-02,  5.4176e-03, -1.2283e-02,\n",
       "                      -7.5969e-03, -4.2815e-02,  1.4448e-02, -4.3533e-02, -3.0751e-02,\n",
       "                       1.2981e-02, -1.9801e-02,  2.3405e-04, -1.8750e-02, -5.2105e-03,\n",
       "                       1.9434e-02, -2.3384e-02,  6.1938e-03, -1.6748e-03, -4.6737e-03,\n",
       "                      -1.1777e-02,  2.7043e-02,  3.6850e-03, -2.6268e-02,  1.0170e-02,\n",
       "                       1.5517e-02, -1.5863e-02,  5.7503e-03, -9.0758e-03, -1.6173e-02,\n",
       "                       7.9576e-03,  4.5853e-02, -2.3959e-03, -2.0397e-02, -2.2729e-02,\n",
       "                       1.1340e-02, -8.6006e-04,  1.6276e-02,  1.0163e-02, -7.0505e-03,\n",
       "                       1.0643e-02, -3.8330e-02,  3.0817e-03, -3.3878e-02, -3.9026e-02,\n",
       "                       1.2203e-02,  5.0760e-03, -1.3395e-02,  8.1229e-04, -2.2352e-02,\n",
       "                       1.2651e-02, -1.2464e-02, -3.0403e-02, -3.1187e-02, -1.2662e-02,\n",
       "                      -1.4057e-02, -8.4176e-03,  5.0056e-04,  1.0326e-02,  1.3345e-02,\n",
       "                       6.5894e-03, -2.5299e-02, -2.8525e-02, -7.5190e-03, -1.2051e-02,\n",
       "                      -9.5629e-03, -3.0925e-02,  7.4643e-03, -1.4693e-02, -1.0582e-02,\n",
       "                      -3.9535e-02, -1.1747e-02,  4.5518e-03, -5.6916e-04, -4.0359e-03,\n",
       "                       9.9386e-03, -9.7656e-04, -7.3949e-03, -1.0990e-02, -5.9514e-03,\n",
       "                      -2.9144e-02,  2.6297e-02, -2.3229e-02, -5.6735e-03, -5.0059e-02,\n",
       "                      -1.3840e-03,  2.7542e-02, -4.5169e-02, -4.2887e-02,  1.0581e-02,\n",
       "                       3.2663e-03,  9.8354e-03, -7.1817e-03, -2.1566e-02,  1.5867e-02,\n",
       "                      -2.7057e-02, -1.2595e-02, -1.4093e-02,  7.5875e-03, -2.6344e-02,\n",
       "                      -6.7278e-03, -2.1022e-02, -1.6144e-02,  1.1588e-02,  1.0391e-02,\n",
       "                      -2.8566e-02, -9.2498e-03, -3.1232e-02, -1.7451e-02, -4.0817e-03,\n",
       "                      -1.0339e-02, -4.1294e-03, -9.2264e-03, -9.9310e-03, -1.4117e-02,\n",
       "                      -3.7199e-02, -2.6353e-02,  8.7215e-03, -8.3122e-03,  5.2003e-03,\n",
       "                       1.0516e-02, -6.2590e-03, -7.3765e-03, -3.9875e-02, -2.1029e-02,\n",
       "                      -2.5772e-02, -1.2603e-02,  5.2990e-03, -4.1275e-02, -6.5308e-03,\n",
       "                      -1.1155e-04,  1.2805e-02, -8.6912e-03, -2.0246e-02, -7.2987e-04,\n",
       "                       3.5864e-03,  8.6450e-03, -3.3016e-02, -6.7874e-03,  2.8551e-02,\n",
       "                       1.6399e-02, -1.3945e-02, -1.5707e-02,  1.6154e-02, -7.5863e-03,\n",
       "                      -1.1020e-02, -1.8696e-02,  3.1671e-02, -9.6012e-03,  2.3397e-02,\n",
       "                      -4.1191e-02,  2.2878e-02,  2.2753e-02, -2.3158e-02, -6.1135e-02,\n",
       "                      -1.6097e-02, -1.4729e-02,  7.1850e-03, -1.1230e-02, -2.4180e-03,\n",
       "                      -1.7433e-03, -1.1401e-02,  2.8564e-03,  2.8209e-02,  3.7401e-02,\n",
       "                       1.4812e-02, -3.4089e-02, -5.6528e-04, -1.6112e-03, -3.4402e-03,\n",
       "                       3.1376e-02, -2.0078e-02,  9.4834e-03, -1.7506e-02, -1.3204e-02,\n",
       "                      -6.0452e-03, -2.2948e-02, -1.5876e-02, -2.4080e-02,  3.1611e-02,\n",
       "                       5.4452e-05, -3.2200e-02, -1.5909e-02, -8.5028e-03, -1.2084e-02,\n",
       "                      -3.5350e-03,  2.8132e-03,  7.3841e-03,  4.7484e-03,  1.0069e-02,\n",
       "                      -1.6799e-02, -1.4549e-02, -2.3812e-03, -1.2070e-02, -1.6253e-02,\n",
       "                      -2.7459e-02, -1.3435e-02,  2.4539e-02, -1.2368e-03,  1.9910e-02,\n",
       "                      -1.6080e-02])),\n",
       "             ('layer3.1.bn2.running_var',\n",
       "              tensor([1.8557e-03, 1.4020e-03, 1.1419e-03, 6.6474e-04, 5.1903e-04, 1.4538e-03,\n",
       "                      9.3428e-04, 1.1387e-03, 9.4842e-10, 6.5190e-04, 7.0989e-04, 8.8632e-04,\n",
       "                      1.5241e-03, 6.1506e-06, 2.9976e-04, 1.6112e-03, 1.6519e-03, 4.6456e-04,\n",
       "                      6.8187e-04, 5.4288e-05, 1.1305e-03, 8.9006e-04, 1.1567e-03, 4.1727e-04,\n",
       "                      1.0303e-03, 5.8785e-04, 8.2407e-04, 1.6688e-03, 7.8180e-04, 5.5866e-04,\n",
       "                      3.2565e-04, 2.1519e-03, 1.6096e-03, 7.1301e-04, 8.9321e-04, 1.0362e-03,\n",
       "                      1.5091e-03, 5.2098e-04, 1.0640e-03, 4.5889e-04, 7.6251e-04, 1.1374e-03,\n",
       "                      9.5410e-04, 1.0914e-03, 2.5079e-04, 9.1362e-04, 6.3103e-04, 1.0617e-03,\n",
       "                      1.4785e-03, 1.0673e-03, 2.3270e-03, 9.4542e-04, 5.3411e-04, 4.8315e-04,\n",
       "                      7.5202e-04, 3.0640e-04, 1.4385e-03, 8.2941e-04, 5.2310e-04, 4.9875e-04,\n",
       "                      1.5635e-04, 3.2931e-03, 4.9432e-04, 9.9868e-04, 1.9116e-03, 7.8149e-04,\n",
       "                      1.5283e-03, 1.4047e-03, 1.2976e-03, 1.7416e-03, 8.5330e-04, 1.8672e-03,\n",
       "                      8.1146e-04, 7.5747e-04, 6.6975e-04, 1.8915e-03, 7.6763e-04, 4.3276e-04,\n",
       "                      1.5383e-03, 1.5035e-03, 1.3979e-03, 1.0341e-03, 8.5383e-04, 7.5001e-04,\n",
       "                      8.4139e-04, 1.1149e-03, 1.2531e-03, 1.3880e-03, 4.1359e-04, 8.2036e-04,\n",
       "                      2.0216e-03, 8.4729e-04, 1.4095e-04, 6.5776e-04, 4.5127e-04, 4.1836e-04,\n",
       "                      1.9017e-03, 1.0140e-03, 5.8376e-04, 1.4872e-03, 9.7609e-04, 3.7023e-04,\n",
       "                      2.4690e-04, 8.1096e-04, 8.5523e-04, 3.2535e-04, 1.3471e-03, 1.4563e-03,\n",
       "                      1.9287e-03, 1.0548e-03, 1.6398e-03, 4.4886e-04, 9.7186e-04, 5.3550e-04,\n",
       "                      4.3411e-04, 3.4798e-04, 1.4611e-03, 8.4692e-04, 7.1795e-04, 6.6593e-04,\n",
       "                      4.4139e-04, 1.7297e-03, 5.3431e-04, 5.3556e-04, 7.5456e-04, 1.5262e-03,\n",
       "                      7.1206e-04, 1.3069e-03, 2.5460e-03, 1.4718e-03, 3.8437e-04, 4.7198e-04,\n",
       "                      7.6432e-04, 4.6386e-04, 1.3476e-03, 7.6587e-04, 8.2263e-04, 1.4721e-03,\n",
       "                      1.0797e-03, 6.6103e-04, 7.2126e-04, 7.8883e-04, 1.7556e-03, 1.7621e-03,\n",
       "                      6.4310e-04, 1.2688e-03, 8.9508e-04, 4.2249e-04, 8.3140e-04, 2.8950e-04,\n",
       "                      1.1252e-03, 8.6708e-04, 4.7530e-04, 4.3715e-04, 7.9598e-04, 1.3017e-03,\n",
       "                      1.8531e-03, 9.0648e-04, 3.1499e-04, 1.0031e-03, 8.0114e-04, 5.3833e-04,\n",
       "                      3.7535e-04, 1.0323e-03, 1.6198e-04, 8.6668e-04, 2.7150e-03, 1.1201e-03,\n",
       "                      6.1823e-04, 7.1035e-04, 1.1727e-03, 3.0990e-03, 1.4087e-03, 1.3955e-04,\n",
       "                      5.3913e-04, 1.0494e-03, 1.1744e-03, 7.8256e-04, 2.0641e-03, 2.8134e-03,\n",
       "                      9.8089e-04, 3.6048e-04, 2.8996e-04, 1.9282e-03, 1.0982e-04, 5.3958e-04,\n",
       "                      1.3959e-03, 6.3235e-04, 1.7096e-03, 1.2168e-03, 6.9192e-04, 3.7838e-04,\n",
       "                      5.9517e-04, 9.8797e-04, 1.1418e-03, 9.8591e-04, 1.6231e-03, 4.4444e-04,\n",
       "                      7.0614e-04, 1.4188e-04, 3.5336e-04, 5.8683e-04, 1.6550e-03, 1.4603e-03,\n",
       "                      5.6128e-04, 2.2969e-03, 1.6505e-03, 5.7313e-04, 2.8671e-04, 3.3774e-03,\n",
       "                      9.3129e-04, 8.9028e-04, 1.6292e-03, 2.4003e-04, 7.2422e-04, 1.0730e-03,\n",
       "                      5.5415e-04, 8.0500e-04, 3.6828e-03, 1.2048e-03, 7.2384e-04, 3.2161e-03,\n",
       "                      4.1318e-04, 2.8608e-04, 5.8065e-04, 2.6266e-04, 6.1679e-04, 9.8269e-04,\n",
       "                      1.4504e-03, 5.3088e-04, 1.3956e-03, 4.6080e-04, 4.1841e-04, 9.3133e-04,\n",
       "                      7.1913e-04, 1.4752e-03, 2.5315e-03, 2.0124e-03, 1.0099e-03, 8.1002e-04,\n",
       "                      8.7065e-04, 2.4882e-04, 3.7015e-04, 9.9240e-05, 1.0179e-03, 1.3730e-03,\n",
       "                      2.8430e-04, 3.2644e-04, 1.0375e-03, 1.0092e-03, 8.3195e-04, 6.9052e-04,\n",
       "                      8.8744e-04, 1.2382e-03, 5.1491e-04, 1.4518e-03])),\n",
       "             ('layer3.1.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.1.conv3.weight',\n",
       "              tensor([[[[ 4.3907e-03]],\n",
       "              \n",
       "                       [[ 1.1699e-03]],\n",
       "              \n",
       "                       [[ 3.8132e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.2427e-04]],\n",
       "              \n",
       "                       [[-3.3816e-03]],\n",
       "              \n",
       "                       [[-4.1030e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8907e-03]],\n",
       "              \n",
       "                       [[-2.0224e-03]],\n",
       "              \n",
       "                       [[ 3.3513e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.8218e-03]],\n",
       "              \n",
       "                       [[-1.5408e-03]],\n",
       "              \n",
       "                       [[-4.7980e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9278e-03]],\n",
       "              \n",
       "                       [[ 1.2642e-03]],\n",
       "              \n",
       "                       [[ 1.8932e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4600e-04]],\n",
       "              \n",
       "                       [[-2.1688e-04]],\n",
       "              \n",
       "                       [[ 2.0514e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 3.5155e-03]],\n",
       "              \n",
       "                       [[-5.0328e-03]],\n",
       "              \n",
       "                       [[ 8.0713e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0150e-02]],\n",
       "              \n",
       "                       [[-3.3583e-03]],\n",
       "              \n",
       "                       [[ 1.4629e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.9460e-03]],\n",
       "              \n",
       "                       [[-3.1463e-03]],\n",
       "              \n",
       "                       [[-5.6566e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.2985e-03]],\n",
       "              \n",
       "                       [[-5.1277e-03]],\n",
       "              \n",
       "                       [[-1.0986e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4048e-06]],\n",
       "              \n",
       "                       [[ 2.4130e-06]],\n",
       "              \n",
       "                       [[-1.3296e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.2562e-07]],\n",
       "              \n",
       "                       [[ 4.2965e-06]],\n",
       "              \n",
       "                       [[-4.9893e-06]]]])),\n",
       "             ('layer3.1.bn3.weight',\n",
       "              tensor([ 2.2618e-02, -1.4795e-02, -3.9289e-03,  ...,  3.1214e-02,\n",
       "                       3.0916e-02,  2.3496e-05])),\n",
       "             ('layer3.1.bn3.bias',\n",
       "              tensor([-0.0197, -0.0122, -0.0149,  ..., -0.0505, -0.0158, -0.0013])),\n",
       "             ('layer3.1.bn3.running_mean',\n",
       "              tensor([-2.1883e-03, -3.4569e-03,  4.3692e-04,  ..., -4.5306e-04,\n",
       "                      -4.0936e-03,  5.5151e-06])),\n",
       "             ('layer3.1.bn3.running_var',\n",
       "              tensor([2.7350e-05, 1.6766e-05, 9.6906e-06,  ..., 5.4339e-05, 3.7614e-05,\n",
       "                      6.4039e-11])),\n",
       "             ('layer3.1.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.2.conv1.weight',\n",
       "              tensor([[[[ 1.7278e-03]],\n",
       "              \n",
       "                       [[ 1.0093e-02]],\n",
       "              \n",
       "                       [[-5.6467e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.8360e-03]],\n",
       "              \n",
       "                       [[-5.4138e-03]],\n",
       "              \n",
       "                       [[ 1.5325e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0651e-03]],\n",
       "              \n",
       "                       [[ 3.0776e-03]],\n",
       "              \n",
       "                       [[ 5.9281e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.9151e-03]],\n",
       "              \n",
       "                       [[ 3.3975e-03]],\n",
       "              \n",
       "                       [[ 1.7838e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2307e-03]],\n",
       "              \n",
       "                       [[-9.0824e-03]],\n",
       "              \n",
       "                       [[-1.8681e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3298e-03]],\n",
       "              \n",
       "                       [[ 8.6152e-03]],\n",
       "              \n",
       "                       [[-6.0270e-05]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-6.6540e-04]],\n",
       "              \n",
       "                       [[-4.0158e-03]],\n",
       "              \n",
       "                       [[-6.1478e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0173e-03]],\n",
       "              \n",
       "                       [[-3.9206e-03]],\n",
       "              \n",
       "                       [[ 5.5168e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5082e-03]],\n",
       "              \n",
       "                       [[-2.8497e-03]],\n",
       "              \n",
       "                       [[-3.1574e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.8510e-03]],\n",
       "              \n",
       "                       [[-3.6974e-03]],\n",
       "              \n",
       "                       [[ 2.4080e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.2481e-03]],\n",
       "              \n",
       "                       [[ 8.3205e-04]],\n",
       "              \n",
       "                       [[ 5.2142e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.4068e-03]],\n",
       "              \n",
       "                       [[-1.2544e-03]],\n",
       "              \n",
       "                       [[ 8.5207e-06]]]])),\n",
       "             ('layer3.2.bn1.weight',\n",
       "              tensor([0.0537, 0.0594, 0.0756, 0.0275, 0.0760, 0.0338, 0.0396, 0.0560, 0.1029,\n",
       "                      0.0464, 0.0990, 0.0633, 0.0527, 0.0793, 0.0501, 0.0628, 0.0635, 0.0745,\n",
       "                      0.0572, 0.0475, 0.0382, 0.0307, 0.0303, 0.0737, 0.0477, 0.0366, 0.0515,\n",
       "                      0.0722, 0.0509, 0.0313, 0.0524, 0.0528, 0.0939, 0.0393, 0.0333, 0.0984,\n",
       "                      0.0434, 0.0235, 0.0808, 0.0464, 0.0616, 0.0538, 0.0665, 0.0766, 0.0577,\n",
       "                      0.0513, 0.0464, 0.0683, 0.0317, 0.0745, 0.0365, 0.0780, 0.0397, 0.0441,\n",
       "                      0.0562, 0.0525, 0.0256, 0.0662, 0.0539, 0.0579, 0.0678, 0.0896, 0.0680,\n",
       "                      0.0595, 0.0617, 0.0543, 0.0194, 0.0793, 0.0774, 0.0389, 0.0593, 0.0757,\n",
       "                      0.0281, 0.0334, 0.0437, 0.0484, 0.0940, 0.0620, 0.0421, 0.0406, 0.0440,\n",
       "                      0.1100, 0.0553, 0.0219, 0.0775, 0.0888, 0.0624, 0.0481, 0.0434, 0.0333,\n",
       "                      0.0846, 0.0507, 0.0602, 0.0619, 0.0445, 0.0476, 0.0572, 0.0564, 0.0599,\n",
       "                      0.0544, 0.0774, 0.0533, 0.0509, 0.0694, 0.0416, 0.0691, 0.0467, 0.0548,\n",
       "                      0.0824, 0.0674, 0.0695, 0.1060, 0.0635, 0.0506, 0.0629, 0.0696, 0.0710,\n",
       "                      0.0536, 0.0698, 0.0390, 0.0889, 0.0585, 0.0593, 0.0331, 0.0307, 0.0501,\n",
       "                      0.0876, 0.0650, 0.0387, 0.0639, 0.0499, 0.1028, 0.0285, 0.0401, 0.0672,\n",
       "                      0.0788, 0.0460, 0.0968, 0.0751, 0.0561, 0.0847, 0.0422, 0.0881, 0.0491,\n",
       "                      0.0349, 0.0827, 0.0697, 0.0343, 0.0400, 0.0476, 0.0293, 0.0719, 0.0485,\n",
       "                      0.0942, 0.0331, 0.0340, 0.0722, 0.0416, 0.0897, 0.0676, 0.0601, 0.0423,\n",
       "                      0.0709, 0.1052, 0.0724, 0.0511, 0.0460, 0.0831, 0.0581, 0.0578, 0.0461,\n",
       "                      0.0562, 0.0616, 0.0592, 0.0542, 0.0456, 0.0343, 0.0510, 0.0795, 0.0612,\n",
       "                      0.0329, 0.0404, 0.0801, 0.0322, 0.0881, 0.0337, 0.0651, 0.0455, 0.0373,\n",
       "                      0.0434, 0.0590, 0.0981, 0.0422, 0.1161, 0.0783, 0.0380, 0.0585, 0.0693,\n",
       "                      0.0317, 0.0602, 0.0550, 0.0625, 0.0540, 0.0617, 0.0509, 0.0688, 0.0709,\n",
       "                      0.0647, 0.0371, 0.0926, 0.0544, 0.0670, 0.0422, 0.0654, 0.0538, 0.0918,\n",
       "                      0.0651, 0.0501, 0.0629, 0.0606, 0.0779, 0.0248, 0.0603, 0.0664, 0.0280,\n",
       "                      0.0496, 0.0629, 0.0471, 0.0876, 0.0518, 0.0810, 0.0774, 0.0492, 0.0905,\n",
       "                      0.0801, 0.0772, 0.0511, 0.0634, 0.0426, 0.0932, 0.0710, 0.0383, 0.0817,\n",
       "                      0.0801, 0.0485, 0.0792, 0.0362, 0.0707, 0.0539, 0.0328, 0.0344, 0.0394,\n",
       "                      0.0425, 0.0799, 0.0753, 0.0482])),\n",
       "             ('layer3.2.bn1.bias',\n",
       "              tensor([ 1.1968e-02, -1.4646e-02,  3.0348e-02, -7.6055e-03, -6.4058e-02,\n",
       "                      -5.5267e-03,  4.9974e-03, -3.0455e-03, -3.0762e-02, -3.4935e-03,\n",
       "                      -2.7121e-02, -8.7583e-03, -1.3160e-02, -2.8534e-02, -1.2383e-03,\n",
       "                      -1.5998e-02, -1.9123e-02, -3.4909e-02, -1.2190e-02, -1.9077e-02,\n",
       "                       2.2166e-02,  7.9951e-04,  3.8493e-03, -1.6693e-03, -6.4940e-03,\n",
       "                       1.3769e-02,  9.0136e-03, -1.4570e-02, -5.0477e-03, -7.7182e-03,\n",
       "                       3.5299e-02, -8.9279e-03, -3.5432e-02,  2.1485e-03, -6.4017e-03,\n",
       "                      -1.1506e-02,  1.5655e-02,  6.6935e-03, -9.3692e-03,  3.7342e-03,\n",
       "                      -1.3920e-02,  1.4641e-02, -4.2193e-03, -2.8441e-02, -1.1900e-02,\n",
       "                       1.0484e-02,  1.0936e-02,  2.2536e-02,  1.9217e-02, -9.3520e-03,\n",
       "                       1.0334e-02, -5.6094e-03,  3.9192e-03,  7.6939e-03,  2.1934e-03,\n",
       "                      -3.9453e-03,  7.2666e-03,  7.5765e-03, -4.1791e-03,  2.5524e-02,\n",
       "                      -1.6230e-02, -4.3791e-02, -1.4924e-02,  1.1248e-03,  5.5391e-03,\n",
       "                      -5.4701e-03,  3.5577e-03, -3.5060e-02,  1.8842e-04, -2.7745e-04,\n",
       "                       8.0424e-03, -2.4107e-02, -7.2624e-03,  1.2015e-02,  1.1440e-02,\n",
       "                      -5.8295e-03, -4.7626e-02, -1.5233e-02,  1.1752e-02, -1.2617e-02,\n",
       "                       2.0461e-03, -4.8411e-02, -1.7901e-02, -9.4843e-04, -1.4673e-02,\n",
       "                      -5.3734e-02,  4.7025e-03, -1.7228e-02,  1.0942e-02,  5.2650e-03,\n",
       "                      -2.5540e-02,  4.0198e-02, -2.1691e-02, -6.1978e-03, -9.2232e-03,\n",
       "                      -1.2239e-02,  3.2976e-03,  2.4369e-02, -2.0143e-02, -2.0502e-02,\n",
       "                      -2.6215e-02,  2.2842e-02,  2.3723e-02, -1.7138e-02, -4.8230e-03,\n",
       "                      -1.5631e-02, -7.8463e-03,  6.7585e-03, -3.9038e-02,  3.7496e-03,\n",
       "                      -5.5451e-03, -6.2485e-02, -1.7815e-02,  5.6502e-04,  2.8093e-03,\n",
       "                      -2.7878e-02, -2.2866e-02,  5.9032e-03, -3.5691e-02,  7.7726e-03,\n",
       "                      -3.8223e-03, -2.7365e-04,  6.1478e-03,  1.9034e-02, -2.5189e-03,\n",
       "                      -9.1918e-03,  1.8979e-02, -7.9722e-03,  5.4209e-03, -1.6088e-02,\n",
       "                       2.9959e-02, -2.7582e-02,  1.9579e-03, -4.2929e-05, -8.9548e-03,\n",
       "                      -6.9750e-03, -9.8378e-03, -1.4842e-02, -3.1094e-02, -2.7288e-02,\n",
       "                      -8.4960e-03,  1.2457e-02, -3.2926e-02, -6.1329e-03,  1.0907e-02,\n",
       "                      -2.5896e-02,  1.8187e-04,  5.9341e-03, -1.3804e-02,  4.2459e-03,\n",
       "                      -4.5394e-03,  8.9516e-03, -5.1115e-04, -1.8239e-02,  1.9563e-02,\n",
       "                       2.0202e-03, -3.3846e-02, -1.3933e-03, -4.6692e-02,  1.2288e-02,\n",
       "                       1.9817e-02,  1.8889e-02, -1.8928e-02, -5.2826e-02, -2.2932e-02,\n",
       "                       1.0385e-02, -1.3345e-02, -1.8191e-02,  7.2278e-04, -1.1005e-03,\n",
       "                       9.5367e-03, -1.6514e-03, -1.4867e-02, -1.4379e-02,  1.2635e-02,\n",
       "                       2.9394e-04, -1.0350e-02,  7.3948e-04, -8.8735e-03, -1.1192e-02,\n",
       "                      -1.0559e-02,  1.2554e-02, -3.4606e-02,  7.6328e-03, -4.2447e-02,\n",
       "                       4.8018e-03, -2.1350e-02,  2.0455e-03, -9.0649e-04, -9.9652e-03,\n",
       "                       1.1907e-02, -4.4206e-02,  4.2865e-03, -6.7089e-02, -2.4550e-02,\n",
       "                      -1.3635e-02,  3.2953e-02, -7.5762e-03,  1.5164e-03, -1.1298e-02,\n",
       "                       4.9287e-03, -9.3418e-03,  2.4749e-02,  2.5154e-03, -5.1356e-03,\n",
       "                      -2.1641e-02,  1.2965e-02,  1.9859e-02, -1.2135e-02, -1.9409e-02,\n",
       "                      -1.1470e-02,  2.0911e-02, -4.9994e-03,  4.3288e-03,  2.5613e-04,\n",
       "                       2.6217e-02,  5.7554e-03, -1.8397e-02, -1.2540e-02, -1.6240e-02,\n",
       "                      -1.4659e-02,  2.7304e-02, -6.3822e-03, -1.7006e-02,  1.6031e-02,\n",
       "                       2.8837e-02, -6.1853e-03,  6.3434e-03, -3.4357e-02,  1.5059e-03,\n",
       "                      -1.3448e-02, -1.1073e-02,  1.2966e-02,  4.2971e-02, -4.3843e-04,\n",
       "                       2.1198e-02,  2.5832e-03, -3.7111e-02, -2.0027e-03, -5.7658e-02,\n",
       "                       2.2046e-02, -5.4570e-03, -3.3323e-03, -2.0669e-02, -2.5695e-02,\n",
       "                      -3.8347e-02, -1.2489e-02, -2.4095e-02, -1.0180e-02,  1.3078e-02,\n",
       "                       2.9974e-03, -4.0499e-03,  1.1850e-02, -2.1387e-02, -6.6163e-03,\n",
       "                      -7.4995e-03])),\n",
       "             ('layer3.2.bn1.running_mean',\n",
       "              tensor([-0.0114, -0.0086,  0.0022, -0.0004,  0.0131, -0.0042, -0.0253, -0.0171,\n",
       "                       0.0085,  0.0020,  0.0089, -0.0024, -0.0015,  0.0020, -0.0216,  0.0011,\n",
       "                      -0.0268, -0.0101,  0.0014,  0.0146, -0.0208, -0.0174,  0.0116, -0.0163,\n",
       "                      -0.0089,  0.0165, -0.0340, -0.0202,  0.0022, -0.0056,  0.0072, -0.0090,\n",
       "                       0.0390, -0.0048,  0.0125, -0.0247,  0.0074, -0.0077, -0.0054, -0.0153,\n",
       "                       0.0050, -0.0141, -0.0122, -0.0134, -0.0036,  0.0083, -0.0357, -0.0297,\n",
       "                      -0.0096, -0.0071, -0.0137, -0.0015, -0.0025,  0.0040, -0.0364,  0.0165,\n",
       "                      -0.0099,  0.0189, -0.0195, -0.0447,  0.0174, -0.0343, -0.0002,  0.0005,\n",
       "                       0.0239, -0.0171, -0.0032, -0.0305,  0.0086, -0.0193, -0.0011,  0.0051,\n",
       "                      -0.0084,  0.0101, -0.0369, -0.0038, -0.0201, -0.0013,  0.0203, -0.0083,\n",
       "                       0.0022, -0.0250, -0.0166, -0.0131, -0.0028,  0.0110, -0.0429, -0.0033,\n",
       "                      -0.0217, -0.0122, -0.0300, -0.0205,  0.0245, -0.0076, -0.0088, -0.0038,\n",
       "                      -0.0018, -0.0151, -0.0101, -0.0269, -0.0056, -0.0154, -0.0188,  0.0247,\n",
       "                       0.0099, -0.0160,  0.0023, -0.0359, -0.0101, -0.0430, -0.0105,  0.0176,\n",
       "                       0.0069,  0.0040, -0.0104,  0.0046,  0.0029, -0.0122, -0.0016, -0.0283,\n",
       "                       0.0107, -0.0383, -0.0101, -0.0383, -0.0096, -0.0139, -0.0704, -0.0022,\n",
       "                      -0.0189, -0.0129, -0.0130, -0.0507,  0.0065, -0.0031,  0.0218,  0.0238,\n",
       "                       0.0018,  0.0100,  0.0054, -0.0201, -0.0269, -0.0030,  0.0003, -0.0068,\n",
       "                      -0.0120, -0.0232, -0.0082, -0.0058, -0.0154,  0.0010,  0.0024, -0.0521,\n",
       "                      -0.0081, -0.0143,  0.0032, -0.0162, -0.0027, -0.0050, -0.0318, -0.0212,\n",
       "                      -0.0422, -0.0238,  0.0021,  0.0010, -0.0044, -0.0239,  0.0076, -0.0208,\n",
       "                      -0.0132, -0.0471, -0.0120, -0.0181, -0.0071, -0.0135,  0.0192,  0.0003,\n",
       "                      -0.0140, -0.0274, -0.0263, -0.0159, -0.0049, -0.0033, -0.0006, -0.0121,\n",
       "                       0.0378,  0.0105, -0.0196, -0.0468, -0.0002, -0.0014,  0.0288, -0.0021,\n",
       "                      -0.0107, -0.0071, -0.0213, -0.0027, -0.0336,  0.0162, -0.0057, -0.0074,\n",
       "                      -0.0144, -0.0204,  0.0046, -0.0374, -0.0304,  0.0123, -0.0269, -0.0311,\n",
       "                      -0.0056, -0.0257, -0.0271,  0.0021, -0.0164, -0.0490, -0.0047, -0.0498,\n",
       "                       0.0077,  0.0171, -0.0107, -0.0159, -0.0341, -0.0085, -0.0396,  0.0016,\n",
       "                      -0.0184, -0.0142, -0.0395,  0.0043,  0.0071, -0.0141,  0.0120,  0.0028,\n",
       "                       0.0106, -0.0144, -0.0270, -0.0343,  0.0010,  0.0009, -0.0002, -0.0375,\n",
       "                      -0.0415, -0.0172, -0.0342,  0.0113, -0.0124,  0.0189, -0.0050,  0.0052,\n",
       "                      -0.0117,  0.0049, -0.0099, -0.0065, -0.0102,  0.0027, -0.0175,  0.0084])),\n",
       "             ('layer3.2.bn1.running_var',\n",
       "              tensor([0.0028, 0.0012, 0.0037, 0.0002, 0.0022, 0.0003, 0.0021, 0.0009, 0.0035,\n",
       "                      0.0008, 0.0027, 0.0018, 0.0008, 0.0022, 0.0015, 0.0011, 0.0009, 0.0014,\n",
       "                      0.0008, 0.0015, 0.0021, 0.0003, 0.0007, 0.0012, 0.0004, 0.0015, 0.0015,\n",
       "                      0.0021, 0.0012, 0.0004, 0.0031, 0.0007, 0.0037, 0.0007, 0.0009, 0.0033,\n",
       "                      0.0025, 0.0005, 0.0020, 0.0009, 0.0013, 0.0015, 0.0009, 0.0013, 0.0004,\n",
       "                      0.0024, 0.0010, 0.0048, 0.0012, 0.0018, 0.0009, 0.0013, 0.0016, 0.0011,\n",
       "                      0.0019, 0.0025, 0.0007, 0.0019, 0.0015, 0.0022, 0.0040, 0.0022, 0.0016,\n",
       "                      0.0025, 0.0045, 0.0012, 0.0001, 0.0015, 0.0018, 0.0011, 0.0021, 0.0016,\n",
       "                      0.0002, 0.0011, 0.0018, 0.0008, 0.0023, 0.0020, 0.0021, 0.0005, 0.0018,\n",
       "                      0.0020, 0.0012, 0.0003, 0.0015, 0.0032, 0.0012, 0.0010, 0.0020, 0.0008,\n",
       "                      0.0020, 0.0038, 0.0037, 0.0008, 0.0009, 0.0012, 0.0021, 0.0023, 0.0010,\n",
       "                      0.0015, 0.0020, 0.0039, 0.0027, 0.0027, 0.0021, 0.0014, 0.0004, 0.0013,\n",
       "                      0.0016, 0.0023, 0.0012, 0.0035, 0.0014, 0.0017, 0.0017, 0.0017, 0.0036,\n",
       "                      0.0020, 0.0018, 0.0013, 0.0032, 0.0019, 0.0016, 0.0011, 0.0006, 0.0019,\n",
       "                      0.0034, 0.0013, 0.0010, 0.0018, 0.0026, 0.0033, 0.0007, 0.0006, 0.0019,\n",
       "                      0.0032, 0.0004, 0.0031, 0.0013, 0.0008, 0.0020, 0.0015, 0.0021, 0.0009,\n",
       "                      0.0014, 0.0015, 0.0014, 0.0006, 0.0010, 0.0021, 0.0002, 0.0032, 0.0011,\n",
       "                      0.0021, 0.0015, 0.0004, 0.0022, 0.0005, 0.0021, 0.0018, 0.0024, 0.0015,\n",
       "                      0.0022, 0.0025, 0.0016, 0.0012, 0.0006, 0.0011, 0.0018, 0.0018, 0.0024,\n",
       "                      0.0009, 0.0015, 0.0013, 0.0021, 0.0008, 0.0011, 0.0009, 0.0014, 0.0013,\n",
       "                      0.0002, 0.0011, 0.0009, 0.0003, 0.0033, 0.0011, 0.0014, 0.0013, 0.0007,\n",
       "                      0.0005, 0.0038, 0.0034, 0.0016, 0.0029, 0.0024, 0.0004, 0.0045, 0.0011,\n",
       "                      0.0005, 0.0029, 0.0011, 0.0016, 0.0018, 0.0017, 0.0014, 0.0016, 0.0014,\n",
       "                      0.0018, 0.0006, 0.0031, 0.0017, 0.0026, 0.0009, 0.0025, 0.0012, 0.0050,\n",
       "                      0.0021, 0.0017, 0.0018, 0.0010, 0.0017, 0.0014, 0.0019, 0.0008, 0.0008,\n",
       "                      0.0015, 0.0018, 0.0016, 0.0018, 0.0010, 0.0018, 0.0016, 0.0016, 0.0098,\n",
       "                      0.0027, 0.0031, 0.0019, 0.0025, 0.0006, 0.0031, 0.0023, 0.0012, 0.0029,\n",
       "                      0.0021, 0.0013, 0.0024, 0.0010, 0.0012, 0.0013, 0.0012, 0.0005, 0.0006,\n",
       "                      0.0014, 0.0014, 0.0025, 0.0019])),\n",
       "             ('layer3.2.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.2.conv2.weight',\n",
       "              tensor([[[[-9.9109e-04,  3.7630e-03, -1.5205e-03],\n",
       "                        [-2.1172e-03,  4.6533e-03, -9.8754e-03],\n",
       "                        [ 1.7227e-03,  7.8080e-03, -2.6404e-03]],\n",
       "              \n",
       "                       [[-5.6804e-05, -1.8269e-03,  2.1235e-03],\n",
       "                        [ 3.8660e-03, -2.6831e-03,  6.2666e-03],\n",
       "                        [-8.4873e-05, -5.9217e-03,  4.3188e-03]],\n",
       "              \n",
       "                       [[-1.0408e-03,  6.1019e-04,  1.2181e-03],\n",
       "                        [ 8.1147e-05, -6.0342e-03,  2.6438e-03],\n",
       "                        [ 7.7621e-03, -3.1797e-03, -9.4945e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.8646e-04,  4.3493e-03,  4.0732e-03],\n",
       "                        [-2.5853e-05,  3.3051e-03,  7.3148e-04],\n",
       "                        [-5.1355e-03, -2.5223e-03, -3.6046e-03]],\n",
       "              \n",
       "                       [[ 4.5583e-04, -1.3380e-04, -2.2211e-03],\n",
       "                        [-1.3492e-04,  7.3442e-03, -6.1002e-03],\n",
       "                        [-2.0437e-03,  6.1012e-03, -1.3956e-03]],\n",
       "              \n",
       "                       [[-4.0446e-04, -1.9583e-04, -1.9832e-03],\n",
       "                        [ 3.5035e-03, -1.1624e-04,  7.0789e-03],\n",
       "                        [-1.2245e-04,  2.0162e-03, -8.5974e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2791e-02,  1.3564e-02,  1.0586e-03],\n",
       "                        [-9.5857e-03,  6.2442e-03, -6.7325e-03],\n",
       "                        [-2.1688e-02, -1.3868e-02, -1.1770e-02]],\n",
       "              \n",
       "                       [[ 1.0701e-02,  1.4134e-02,  4.4546e-03],\n",
       "                        [-1.0691e-02, -2.7310e-03, -3.6158e-03],\n",
       "                        [-1.1030e-02, -6.5474e-03, -8.1940e-03]],\n",
       "              \n",
       "                       [[-7.2400e-03, -2.1612e-02, -3.4218e-03],\n",
       "                        [-9.6558e-03, -1.2955e-02,  1.3953e-02],\n",
       "                        [ 1.1548e-02,  2.4458e-02,  4.5973e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1195e-02,  1.6950e-02,  1.6072e-02],\n",
       "                        [ 1.2922e-02,  3.2838e-03,  6.3013e-03],\n",
       "                        [ 9.0258e-03,  7.5313e-03,  1.4559e-03]],\n",
       "              \n",
       "                       [[ 1.5482e-02,  1.1969e-02,  1.2567e-02],\n",
       "                        [ 2.3503e-03, -2.2087e-03, -2.5392e-03],\n",
       "                        [-1.0426e-02, -6.9410e-03, -1.0491e-02]],\n",
       "              \n",
       "                       [[ 2.0176e-03,  6.2454e-03, -6.0452e-03],\n",
       "                        [-1.9374e-03,  5.3251e-03,  1.4281e-03],\n",
       "                        [-5.2920e-03, -1.1923e-02, -1.5041e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.7179e-03,  2.5452e-03, -1.1892e-03],\n",
       "                        [ 1.7371e-05, -3.5862e-03, -5.6297e-03],\n",
       "                        [ 5.7890e-03,  8.1339e-03, -1.7444e-03]],\n",
       "              \n",
       "                       [[-1.7386e-03, -1.2190e-03,  3.5924e-03],\n",
       "                        [-3.1482e-03, -2.2803e-03,  1.1435e-03],\n",
       "                        [ 5.5687e-03,  4.6351e-03,  6.7832e-03]],\n",
       "              \n",
       "                       [[ 2.8852e-03, -6.9292e-03, -9.1311e-03],\n",
       "                        [ 6.9611e-03, -3.4904e-03, -3.2135e-03],\n",
       "                        [ 5.3907e-03, -7.7495e-03, -6.1799e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.2499e-03, -4.2426e-03, -1.0022e-02],\n",
       "                        [ 4.2950e-04, -2.2439e-03, -7.6224e-03],\n",
       "                        [ 5.6127e-04, -6.0889e-03, -5.1531e-03]],\n",
       "              \n",
       "                       [[-3.8932e-03,  2.8900e-03,  2.0629e-03],\n",
       "                        [-1.4825e-03,  5.7276e-04, -1.1235e-03],\n",
       "                        [ 2.4652e-03,  2.0440e-03,  6.6979e-04]],\n",
       "              \n",
       "                       [[-7.1579e-03, -7.7027e-03, -4.8390e-03],\n",
       "                        [-2.3367e-03,  5.7826e-04,  5.1558e-03],\n",
       "                        [ 7.6587e-03,  5.0805e-03,  1.3968e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0465e-03, -1.7660e-03, -8.6615e-03],\n",
       "                        [ 1.3809e-02, -6.2365e-04, -1.4256e-02],\n",
       "                        [ 2.7170e-03,  4.6094e-03, -5.5361e-03]],\n",
       "              \n",
       "                       [[ 6.8488e-03,  2.1452e-03, -2.1521e-03],\n",
       "                        [ 1.0420e-03,  6.2328e-03, -7.6467e-03],\n",
       "                        [-1.5069e-02, -2.8413e-03,  5.1421e-03]],\n",
       "              \n",
       "                       [[ 1.0824e-02,  6.5101e-03,  2.4554e-03],\n",
       "                        [ 1.0281e-03, -7.4324e-03,  1.7407e-02],\n",
       "                        [-7.2100e-03, -4.6857e-03,  4.9552e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.1114e-03, -3.9318e-03,  6.9243e-03],\n",
       "                        [ 2.5818e-03, -1.9667e-03,  3.9211e-03],\n",
       "                        [-2.7681e-03, -3.2781e-03,  2.9135e-03]],\n",
       "              \n",
       "                       [[-4.5444e-04,  1.0629e-02, -3.2325e-03],\n",
       "                        [ 1.2936e-02, -5.1136e-03, -7.6592e-03],\n",
       "                        [ 1.2694e-02,  4.4966e-03, -1.0398e-02]],\n",
       "              \n",
       "                       [[ 9.5708e-03,  1.6976e-03, -1.6935e-02],\n",
       "                        [ 6.8235e-03,  6.1553e-03, -3.2755e-03],\n",
       "                        [-6.2051e-03, -3.9393e-03, -6.2791e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8550e-03, -4.1648e-03,  2.4448e-04],\n",
       "                        [-2.7475e-03,  5.1692e-03,  3.6235e-03],\n",
       "                        [ 3.5057e-03,  5.1895e-04,  5.0340e-03]],\n",
       "              \n",
       "                       [[ 3.4452e-03,  5.3590e-03, -3.4893e-03],\n",
       "                        [-5.2001e-03, -3.2538e-03,  2.1333e-03],\n",
       "                        [-1.4736e-03, -1.0068e-03, -3.4908e-03]],\n",
       "              \n",
       "                       [[-8.0820e-03, -7.0339e-03, -5.2665e-04],\n",
       "                        [-3.4730e-03,  8.3183e-03, -1.1320e-02],\n",
       "                        [-7.9557e-03,  3.2441e-03, -7.5056e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.8412e-03,  4.0005e-04,  2.1063e-04],\n",
       "                        [ 3.9917e-04, -1.0890e-03,  1.2815e-03],\n",
       "                        [ 6.2994e-03, -1.0383e-03,  5.4976e-07]],\n",
       "              \n",
       "                       [[ 1.0266e-02,  9.4857e-03,  3.3304e-03],\n",
       "                        [ 2.2704e-03, -2.1110e-04, -3.9940e-03],\n",
       "                        [-8.7472e-04, -2.4308e-03, -2.2200e-03]],\n",
       "              \n",
       "                       [[-7.2344e-03, -2.1352e-03, -5.0107e-04],\n",
       "                        [-8.4448e-03,  4.4511e-03,  1.2097e-02],\n",
       "                        [-5.5795e-03,  1.2427e-03,  7.2139e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2701e-03,  1.5190e-04,  6.6835e-03],\n",
       "                        [-3.2274e-03, -4.0306e-03,  2.6374e-03],\n",
       "                        [-4.7894e-04, -1.8671e-03,  4.9338e-03]],\n",
       "              \n",
       "                       [[ 1.0039e-03, -8.8773e-04,  8.9725e-04],\n",
       "                        [ 1.2839e-03,  3.5106e-03,  1.4756e-03],\n",
       "                        [ 2.1783e-03,  1.5552e-03, -2.2048e-03]],\n",
       "              \n",
       "                       [[ 1.3579e-03, -5.0321e-03, -3.2325e-03],\n",
       "                        [ 3.0246e-03, -3.8490e-03, -8.6559e-03],\n",
       "                        [ 1.8020e-03, -3.7599e-03, -3.7488e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.2803e-03, -2.4626e-03,  2.1405e-03],\n",
       "                        [ 3.6103e-05,  4.5781e-04,  4.1360e-03],\n",
       "                        [-2.4595e-03,  7.1847e-04,  2.8701e-03]],\n",
       "              \n",
       "                       [[-4.1293e-03,  3.2738e-04, -3.5532e-03],\n",
       "                        [-8.4563e-03, -4.0163e-03, -2.5079e-03],\n",
       "                        [-3.2299e-03, -1.7588e-03,  1.8311e-03]],\n",
       "              \n",
       "                       [[ 2.6646e-03,  7.2668e-04, -1.1400e-03],\n",
       "                        [-1.7641e-03, -3.9292e-03, -3.4701e-03],\n",
       "                        [ 7.1744e-04, -9.0885e-05, -3.8275e-03]]]])),\n",
       "             ('layer3.2.bn2.weight',\n",
       "              tensor([7.5650e-02, 1.1757e-01, 4.8700e-02, 9.9140e-02, 6.0312e-02, 8.2478e-02,\n",
       "                      3.5517e-02, 3.1882e-03, 7.5669e-02, 7.9664e-02, 3.8594e-02, 1.0596e-01,\n",
       "                      7.0470e-02, 3.5922e-02, 1.1294e-01, 1.0929e-01, 4.9603e-02, 7.5690e-02,\n",
       "                      7.5957e-02, 7.9651e-02, 6.9888e-02, 5.1460e-02, 8.7894e-02, 6.1043e-02,\n",
       "                      7.2622e-02, 6.8103e-02, 8.3413e-02, 1.0039e-01, 5.8497e-02, 9.7460e-02,\n",
       "                      9.5243e-02, 4.4213e-02, 8.0073e-02, 1.0065e-01, 9.2277e-02, 3.6041e-02,\n",
       "                      1.1928e-01, 6.3882e-02, 5.1227e-02, 3.5931e-02, 3.5335e-04, 6.4129e-02,\n",
       "                      7.6525e-02, 7.4975e-02, 1.1746e-01, 3.9949e-02, 7.2475e-02, 9.3052e-02,\n",
       "                      6.7532e-02, 1.0303e-01, 8.9869e-02, 4.9642e-02, 6.3779e-02, 9.4244e-02,\n",
       "                      5.5195e-02, 9.5999e-02, 7.0513e-02, 5.6306e-02, 9.9457e-02, 5.4902e-02,\n",
       "                      1.1060e-01, 4.9387e-02, 1.1229e-01, 7.6431e-02, 8.5352e-02, 8.2969e-02,\n",
       "                      7.0286e-02, 8.6634e-02, 9.9428e-02, 7.5066e-02, 6.2622e-02, 1.0149e-01,\n",
       "                      6.5725e-02, 4.8990e-02, 9.2612e-02, 4.9933e-02, 4.8346e-02, 7.6707e-02,\n",
       "                      5.6615e-02, 5.0122e-02, 4.0528e-02, 4.2885e-02, 6.3918e-02, 7.9090e-02,\n",
       "                      3.4983e-02, 4.5738e-02, 8.9708e-02, 6.7594e-02, 9.5898e-02, 1.5626e-02,\n",
       "                      6.3748e-02, 9.0871e-02, 7.0160e-02, 8.6160e-02, 8.5422e-02, 9.5196e-02,\n",
       "                      6.0727e-02, 4.0703e-02, 7.1880e-02, 4.8912e-02, 6.6709e-02, 7.2594e-02,\n",
       "                      4.9330e-02, 7.7647e-02, 6.3273e-02, 2.9764e-02, 4.6332e-02, 8.9967e-02,\n",
       "                      1.0089e-01, 7.4038e-02, 9.8843e-02, 4.7078e-02, 9.2904e-02, 7.1987e-02,\n",
       "                      1.0597e-01, 1.0166e-01, 7.8241e-02, 8.5385e-02, 6.2806e-02, 5.1934e-02,\n",
       "                      7.9442e-02, 8.9470e-02, 9.7284e-02, 4.4332e-02, 1.0684e-01, 6.3701e-02,\n",
       "                      7.1167e-02, 7.9595e-02, 6.9074e-02, 1.0483e-01, 6.8723e-02, 6.8621e-02,\n",
       "                      3.5003e-02, 6.5142e-02, 7.5179e-02, 1.2485e-01, 1.0933e-01, 7.3417e-02,\n",
       "                      1.4846e-01, 6.4586e-02, 1.2639e-04, 8.5993e-02, 1.0677e-01, 8.1603e-02,\n",
       "                      9.9878e-02, 5.9816e-02, 6.6652e-02, 6.2777e-02, 9.0600e-02, 9.4603e-02,\n",
       "                      9.4685e-02, 1.2395e-01, 7.5856e-02, 9.5557e-02, 6.9321e-02, 7.0621e-02,\n",
       "                      9.1231e-02, 4.6861e-02, 4.9726e-02, 9.8718e-02, 6.4013e-02, 9.7868e-02,\n",
       "                      1.0896e-01, 1.1164e-01, 1.1876e-01, 3.7785e-02, 4.4806e-02, 8.8368e-02,\n",
       "                      8.3756e-02, 6.1441e-02, 1.0136e-01, 8.9049e-02, 5.7672e-02, 6.6432e-02,\n",
       "                      1.0685e-01, 9.2806e-02, 1.3085e-01, 1.0987e-01, 8.5779e-02, 5.5290e-02,\n",
       "                      1.1105e-01, 8.0283e-02, 1.0004e-01, 8.3012e-02, 8.0827e-02, 9.1112e-02,\n",
       "                      4.9257e-02, 6.1119e-02, 7.1303e-02, 4.7648e-02, 1.1439e-01, 7.5728e-02,\n",
       "                      7.6855e-02, 5.0182e-02, 5.2188e-02, 9.0917e-02, 6.2487e-02, 6.4503e-02,\n",
       "                      2.0394e-05, 9.3135e-02, 9.3681e-02, 5.0918e-02, 6.7959e-02, 8.6243e-02,\n",
       "                      1.5219e-01, 6.7127e-02, 6.7115e-02, 8.2669e-02, 6.6422e-02, 7.5856e-02,\n",
       "                      8.5073e-02, 7.5920e-02, 4.8799e-02, 1.0717e-01, 8.3193e-02, 5.2995e-02,\n",
       "                      7.6848e-02, 5.7710e-02, 6.8221e-02, 9.0477e-02, 4.2359e-02, 6.4620e-02,\n",
       "                      5.0500e-02, 7.6287e-02, 1.0253e-01, 6.0338e-02, 9.3161e-02, 3.1303e-02,\n",
       "                      1.0789e-01, 5.7584e-02, 5.0828e-02, 1.1840e-01, 6.9121e-02, 1.0544e-01,\n",
       "                      8.7734e-02, 1.1090e-01, 6.5805e-02, 7.9359e-02, 9.9243e-02, 5.7928e-02,\n",
       "                      1.2754e-01, 9.6764e-02, 7.3327e-02, 4.6853e-02, 6.1359e-02, 9.7455e-02,\n",
       "                      1.0795e-01, 6.3006e-02, 5.9989e-02, 3.8203e-02, 1.1457e-01, 6.1271e-02,\n",
       "                      6.9519e-02, 1.0122e-01, 6.8205e-02, 5.8595e-02])),\n",
       "             ('layer3.2.bn2.bias',\n",
       "              tensor([-3.2299e-02, -5.2748e-02, -1.9666e-02, -4.0701e-02, -2.0016e-02,\n",
       "                      -8.1546e-02, -1.8514e-02, -1.7251e-02, -3.5017e-02, -3.0708e-02,\n",
       "                      -1.1784e-02, -4.6862e-02, -3.2725e-02, -3.1282e-02, -7.1121e-02,\n",
       "                      -5.5176e-02, -3.9858e-02, -3.6781e-02, -9.5610e-03, -2.7521e-02,\n",
       "                      -3.5861e-02, -1.6283e-02, -4.7249e-02, -3.1347e-02, -2.8789e-02,\n",
       "                      -6.1770e-02, -1.7890e-02, -6.4166e-02, -3.9760e-02, -7.8337e-02,\n",
       "                      -8.3004e-02, -1.7816e-02, -3.3305e-02, -2.4232e-02, -5.0705e-02,\n",
       "                      -1.4592e-02, -6.3885e-02, -2.6322e-02, -1.0045e-02, -1.8601e-02,\n",
       "                      -2.4189e-03, -2.5540e-02, -4.2460e-02, -4.5066e-02, -1.0470e-01,\n",
       "                      -2.0629e-02, -2.5057e-02, -5.5619e-02, -3.6457e-02, -6.1043e-02,\n",
       "                      -4.1497e-02, -1.3408e-02, -2.3297e-02, -3.7642e-02, -2.9837e-02,\n",
       "                      -3.5086e-02, -4.1241e-02, -8.5575e-03, -2.2494e-02, -2.2190e-02,\n",
       "                      -1.2042e-01, -5.8716e-02, -4.9444e-02, -3.7081e-02,  4.4785e-02,\n",
       "                      -1.2589e-02, -2.4502e-02, -6.2264e-02, -7.1191e-02, -1.5035e-02,\n",
       "                       3.9788e-03, -3.7266e-02, -3.0537e-02, -2.7606e-02, -5.0972e-02,\n",
       "                      -2.0296e-02,  2.0425e-02, -1.2877e-02, -2.3137e-02, -2.1413e-02,\n",
       "                      -2.2991e-02, -3.9798e-02, -2.7521e-02, -1.3146e-02, -1.9250e-02,\n",
       "                      -2.1361e-02, -4.0517e-02, -3.0498e-02, -5.3893e-02, -8.2860e-02,\n",
       "                      -1.3188e-02, -9.8315e-02, -1.9568e-02, -5.4649e-02, -3.4180e-02,\n",
       "                      -3.1789e-02, -2.2448e-02, -1.8272e-02, -1.7607e-02, -1.8578e-02,\n",
       "                      -1.7682e-02, -4.0866e-02, -1.2709e-02, -3.7557e-02, -2.9478e-02,\n",
       "                      -4.5361e-03, -3.2228e-02, -4.8067e-02, -6.1044e-03, -4.0871e-02,\n",
       "                      -4.3926e-02, -1.8642e-02, -5.8834e-02, -2.6950e-02, -6.0508e-02,\n",
       "                      -1.4177e-02, -3.5137e-02, -9.0911e-02,  2.8833e-03,  1.5496e-02,\n",
       "                      -3.2224e-02, -7.9216e-02, -4.5555e-02, -3.6078e-02, -6.7413e-02,\n",
       "                      -3.6038e-02, -4.0338e-02, -6.1680e-02, -3.3420e-03, -3.0206e-02,\n",
       "                      -4.1193e-02, -8.4466e-03, -7.3344e-03, -2.3022e-02, -2.5620e-02,\n",
       "                      -7.0118e-02, -4.7956e-02, -2.5402e-02, -1.2445e-01, -3.6065e-02,\n",
       "                      -8.0829e-04, -6.4287e-02, -3.9463e-02,  2.5002e-02, -5.5638e-02,\n",
       "                      -4.2162e-02, -5.6166e-02, -1.0871e-02, -6.3120e-02, -6.1169e-02,\n",
       "                      -6.5565e-02, -3.9462e-02, -3.8930e-02, -5.5163e-02, -5.1350e-02,\n",
       "                      -4.3451e-02, -2.0995e-02, -3.5025e-02, -3.9887e-03, -4.7896e-02,\n",
       "                      -2.8702e-02, -3.2397e-02, -5.9174e-02, -6.0794e-02, -4.0317e-02,\n",
       "                      -2.2101e-02, -9.8051e-03, -6.1940e-02, -6.6392e-02, -1.2701e-02,\n",
       "                      -4.5694e-02, -3.7070e-02, -2.6629e-02, -2.9787e-02, -4.7874e-02,\n",
       "                      -3.4229e-02, -6.2998e-02, -7.2693e-02, -3.6556e-02, -1.9488e-03,\n",
       "                      -7.4820e-02, -8.2175e-02, -5.9569e-02, -5.4220e-02, -1.9298e-02,\n",
       "                      -6.2270e-02, -5.6254e-02, -2.5806e-02, -1.8258e-02,  7.9146e-03,\n",
       "                      -8.7816e-02, -1.7487e-02, -2.9760e-02, -1.2268e-02, -2.1574e-02,\n",
       "                      -5.8080e-02, -1.2370e-02, -3.4068e-02, -1.1908e-04, -7.9957e-02,\n",
       "                      -8.1576e-02, -3.0751e-02, -4.2989e-02, -3.6859e-02, -6.6006e-02,\n",
       "                      -2.3576e-02, -3.1689e-02, -6.3210e-02, -5.1259e-02, -2.9462e-02,\n",
       "                       6.3464e-03,  4.6842e-03, -4.2066e-02, -6.8728e-02, -6.9754e-02,\n",
       "                      -4.5509e-02, -3.4134e-02, -2.5980e-02, -2.8791e-02, -5.0660e-02,\n",
       "                      -2.2778e-02, -3.5681e-02, -3.9764e-03, -6.1163e-02, -2.0783e-02,\n",
       "                      -2.3788e-02, -5.5656e-02,  7.9983e-03, -3.0155e-02, -2.6312e-02,\n",
       "                       1.5466e-03, -8.3316e-02, -5.1671e-02, -6.3912e-02, -3.7483e-02,\n",
       "                      -5.2391e-02, -4.6782e-02, -4.6663e-02, -4.8714e-02, -3.4238e-02,\n",
       "                      -6.1364e-02, -6.3236e-02, -3.0187e-02, -1.3500e-02, -3.1084e-02,\n",
       "                      -4.1375e-02, -8.8611e-02, -5.6989e-02, -3.6433e-02,  3.1417e-02,\n",
       "                      -5.3439e-02, -3.1243e-02, -5.1378e-02, -5.1429e-02, -3.1383e-02,\n",
       "                      -3.4313e-02])),\n",
       "             ('layer3.2.bn2.running_mean',\n",
       "              tensor([-8.8332e-03, -3.2785e-02, -1.2830e-02, -2.7246e-02, -1.2834e-02,\n",
       "                      -1.1468e-03,  2.9918e-03, -2.3438e-03,  1.0710e-03, -1.1503e-02,\n",
       "                      -1.5728e-02, -3.0076e-02, -1.3652e-02,  2.7467e-03, -2.2471e-02,\n",
       "                      -1.0452e-02,  9.4884e-03,  9.3932e-03,  3.7354e-03, -1.1767e-02,\n",
       "                       7.4073e-03,  8.3644e-03, -1.9974e-02, -2.2284e-02, -1.0164e-02,\n",
       "                      -6.2143e-03, -2.8035e-02, -7.7513e-03,  3.8341e-03,  1.7670e-03,\n",
       "                       4.4428e-03, -5.0950e-03, -2.7748e-02, -8.9836e-03, -1.4473e-02,\n",
       "                      -1.8692e-02, -4.1880e-02, -1.3035e-02, -1.5061e-02, -1.0024e-02,\n",
       "                       1.9910e-03, -2.1413e-02, -1.4842e-02, -2.0315e-02,  7.5983e-04,\n",
       "                      -3.1956e-02, -2.0592e-02, -4.4790e-03, -9.3705e-04, -1.7276e-02,\n",
       "                      -6.4824e-03,  3.2408e-03, -2.0528e-02, -2.6556e-02, -6.4251e-03,\n",
       "                      -2.2894e-02, -2.1540e-02, -2.2598e-02, -2.2121e-02, -6.3305e-03,\n",
       "                       9.6886e-03, -2.1752e-03, -3.4825e-02,  3.8407e-02, -5.4895e-03,\n",
       "                      -1.3229e-02, -8.6602e-03,  5.1215e-02,  5.3053e-02, -3.1543e-02,\n",
       "                       6.1024e-03, -2.0755e-02, -2.2023e-02, -1.9373e-02, -3.0753e-02,\n",
       "                      -8.1483e-03, -4.3936e-02, -3.3072e-03, -2.7464e-03, -3.7526e-03,\n",
       "                       9.8075e-04, -3.3780e-03, -1.6212e-02, -2.9630e-02, -2.1796e-02,\n",
       "                      -7.3055e-03, -1.0294e-02, -2.8221e-02, -1.6822e-02,  7.1317e-02,\n",
       "                      -3.4956e-02,  3.0892e-02, -1.4762e-02, -1.6779e-02, -6.2204e-03,\n",
       "                      -1.8004e-02, -3.1925e-02, -1.0844e-02, -2.3203e-02, -8.7588e-03,\n",
       "                      -2.2686e-02,  6.0200e-03, -1.3524e-02, -1.8483e-02, -2.1975e-02,\n",
       "                      -6.9844e-03, -1.6561e-02, -1.0400e-02, -2.4311e-02, -1.3043e-02,\n",
       "                      -1.7682e-02, -1.6606e-02, -7.6252e-03, -2.5895e-02, -1.8836e-03,\n",
       "                      -1.0108e-02,  1.6922e-03, -2.2133e-02,  2.1642e-02,  2.9907e-02,\n",
       "                      -1.9869e-02,  6.1816e-04, -1.3731e-02, -1.7904e-03, -1.1806e-02,\n",
       "                       2.1946e-03, -9.7669e-03, -1.1238e-02, -1.4001e-02, -4.9367e-02,\n",
       "                      -1.7372e-03,  2.2634e-03, -1.4058e-02, -1.2280e-02, -3.0822e-02,\n",
       "                      -5.4362e-02, -1.7790e-02,  5.0418e-02,  1.0749e-02, -1.7579e-02,\n",
       "                       8.1503e-04,  2.9354e-03, -8.2685e-03, -4.3296e-02, -2.6715e-02,\n",
       "                       8.2451e-03,  1.8546e-02, -3.7077e-03,  1.7108e-02,  8.6788e-02,\n",
       "                      -2.7662e-02, -3.6207e-02, -4.1021e-03, -1.5783e-02, -3.9434e-02,\n",
       "                      -1.5094e-02,  6.5609e-03,  1.7767e-02, -1.6461e-02, -1.2612e-02,\n",
       "                      -7.8685e-03, -1.9467e-02, -2.9590e-02, -3.6000e-02, -2.2901e-02,\n",
       "                      -8.9506e-03, -2.8620e-03, -4.0294e-02, -2.3590e-02, -8.3894e-03,\n",
       "                      -1.1863e-02, -6.8342e-03,  8.6364e-03, -1.2609e-02, -3.4565e-02,\n",
       "                       5.3456e-02, -6.3624e-02, -2.0933e-02, -1.0540e-02, -6.1603e-03,\n",
       "                      -2.3542e-02,  9.9192e-04, -3.6782e-02,  3.6837e-02, -8.3589e-03,\n",
       "                      -8.1287e-03,  7.1919e-04, -3.4844e-03, -3.9136e-03, -2.7855e-02,\n",
       "                      -3.7523e-02, -2.5678e-02, -2.4408e-02, -4.6379e-03, -3.4842e-02,\n",
       "                      -3.2227e-02, -6.9319e-03, -1.0466e-02, -2.4103e-06,  9.1314e-03,\n",
       "                       1.6842e-02, -1.9309e-03, -1.4149e-03, -2.5913e-02, -2.3491e-02,\n",
       "                      -1.0101e-02, -1.7173e-02,  7.3581e-03, -1.4286e-02, -3.4393e-03,\n",
       "                      -4.1695e-02, -4.5134e-02,  1.5972e-04, -2.3532e-02, -7.2561e-03,\n",
       "                       2.1912e-03, -3.5089e-03, -7.9948e-04, -2.9884e-02, -3.4820e-02,\n",
       "                      -1.9115e-02, -8.5028e-03, -8.4167e-03, -1.0873e-02, -1.7915e-02,\n",
       "                      -9.0831e-04, -2.7989e-02, -1.4590e-02, -7.4854e-03, -2.3297e-03,\n",
       "                      -1.3443e-02, -3.6845e-02,  1.6598e-02, -2.5907e-02, -6.1316e-03,\n",
       "                      -7.4878e-03, -2.4421e-02, -2.3155e-02, -9.0310e-03,  5.3787e-02,\n",
       "                      -2.8074e-02, -2.4430e-02, -2.6097e-02, -3.7253e-02, -2.9532e-02,\n",
       "                      -4.0976e-02,  6.0493e-02,  6.1920e-04, -1.7469e-05, -8.2157e-03,\n",
       "                      -2.2851e-02,  5.2802e-03,  2.5452e-02, -1.0179e-02, -7.5944e-03,\n",
       "                      -1.3874e-02])),\n",
       "             ('layer3.2.bn2.running_var',\n",
       "              tensor([7.2329e-04, 2.6937e-03, 6.6433e-04, 1.2672e-03, 6.0791e-04, 1.5891e-03,\n",
       "                      2.5381e-04, 4.8063e-05, 7.8693e-04, 8.4554e-04, 5.7443e-04, 5.0839e-03,\n",
       "                      1.3196e-03, 2.5436e-04, 2.0453e-03, 1.9532e-03, 3.2514e-04, 1.9440e-03,\n",
       "                      8.9533e-04, 1.0438e-03, 1.1270e-03, 6.8743e-04, 1.5698e-03, 4.5430e-04,\n",
       "                      1.1229e-03, 8.1898e-04, 8.9404e-04, 1.8670e-03, 4.5073e-04, 1.9533e-03,\n",
       "                      1.4660e-03, 4.4853e-04, 1.4858e-03, 2.5839e-03, 1.3485e-03, 2.1047e-04,\n",
       "                      3.3877e-03, 8.8060e-04, 7.7700e-04, 5.0059e-04, 5.4740e-06, 8.6262e-04,\n",
       "                      1.1523e-03, 1.6584e-03, 3.1041e-03, 6.2112e-04, 1.2332e-03, 1.7068e-03,\n",
       "                      9.4610e-04, 1.4218e-03, 1.1420e-03, 8.6967e-04, 7.1510e-04, 1.4532e-03,\n",
       "                      4.0469e-04, 2.3506e-03, 7.1169e-04, 6.8560e-04, 2.3980e-03, 8.4155e-04,\n",
       "                      1.8398e-03, 4.9053e-04, 3.8381e-03, 1.6274e-03, 2.2500e-03, 1.2507e-03,\n",
       "                      8.8542e-04, 8.0243e-04, 1.9909e-03, 9.3309e-04, 1.7853e-03, 2.3469e-03,\n",
       "                      6.5852e-04, 6.4703e-04, 1.3334e-03, 4.2727e-04, 1.0122e-03, 1.0581e-03,\n",
       "                      5.1543e-04, 3.9607e-04, 2.6017e-04, 2.4222e-04, 4.6976e-04, 9.2358e-04,\n",
       "                      3.8375e-04, 3.6037e-04, 8.1521e-04, 9.2132e-04, 2.0361e-03, 1.6102e-03,\n",
       "                      9.0997e-04, 2.0591e-03, 9.7986e-04, 1.8265e-03, 1.2064e-03, 1.8341e-03,\n",
       "                      1.1995e-03, 3.9474e-04, 6.4784e-04, 3.9407e-04, 1.3797e-03, 7.6449e-04,\n",
       "                      3.4660e-04, 1.5958e-03, 4.7932e-04, 1.1267e-04, 6.1020e-04, 2.0281e-03,\n",
       "                      2.7853e-03, 9.4576e-04, 2.3142e-03, 4.6044e-04, 1.0702e-03, 1.8899e-03,\n",
       "                      2.2750e-03, 1.9125e-03, 1.6930e-03, 1.3222e-03, 1.3258e-03, 7.7696e-04,\n",
       "                      1.1120e-03, 1.1610e-03, 1.7619e-03, 3.0179e-04, 2.8830e-03, 4.9916e-04,\n",
       "                      7.6466e-04, 1.1026e-03, 7.8655e-04, 2.3389e-03, 9.2490e-04, 9.9358e-04,\n",
       "                      4.9199e-04, 8.0149e-04, 1.3948e-03, 2.6427e-03, 3.7950e-03, 1.3360e-03,\n",
       "                      4.1498e-03, 9.6174e-04, 2.9577e-07, 1.0332e-03, 1.2967e-03, 2.4516e-03,\n",
       "                      1.4976e-03, 5.1619e-04, 8.8586e-04, 7.9571e-04, 1.3440e-03, 2.2219e-03,\n",
       "                      1.6755e-03, 2.3941e-03, 1.7542e-03, 1.5372e-03, 9.4927e-04, 5.8500e-04,\n",
       "                      1.0428e-03, 4.0461e-04, 6.1286e-04, 1.4202e-03, 6.0403e-04, 1.3678e-03,\n",
       "                      1.8983e-03, 1.3460e-03, 5.9584e-03, 1.9076e-04, 3.3795e-04, 1.4486e-03,\n",
       "                      1.3255e-03, 8.3179e-04, 1.6370e-03, 1.1671e-03, 5.4578e-04, 5.2453e-04,\n",
       "                      1.8845e-03, 2.1520e-03, 3.3544e-03, 3.2024e-03, 2.2976e-03, 8.5037e-04,\n",
       "                      2.9967e-03, 8.9131e-04, 1.4729e-03, 1.1212e-03, 1.6173e-03, 1.7662e-03,\n",
       "                      4.6825e-04, 5.4685e-04, 1.3013e-03, 1.0074e-03, 1.9417e-03, 1.0477e-03,\n",
       "                      9.7957e-04, 3.6656e-04, 1.2079e-03, 1.5934e-03, 1.0916e-03, 8.0055e-04,\n",
       "                      6.9002e-09, 1.3205e-03, 1.1931e-03, 5.2483e-04, 9.1407e-04, 2.0260e-03,\n",
       "                      4.8333e-03, 1.0568e-03, 9.7475e-04, 1.1494e-03, 1.1429e-03, 7.9390e-04,\n",
       "                      1.8751e-03, 1.8860e-03, 5.6096e-04, 2.8829e-03, 1.0277e-03, 5.1144e-04,\n",
       "                      9.4929e-04, 7.2629e-04, 1.0673e-03, 1.9463e-03, 5.5200e-04, 8.3785e-04,\n",
       "                      7.2199e-04, 8.6782e-04, 2.8866e-03, 4.5891e-04, 1.0656e-03, 3.8087e-04,\n",
       "                      1.8370e-03, 6.2588e-04, 7.2938e-04, 2.7767e-03, 1.1863e-03, 3.1220e-03,\n",
       "                      1.1436e-03, 1.2033e-03, 1.2845e-03, 1.0636e-03, 1.2741e-03, 1.3382e-03,\n",
       "                      4.3221e-03, 1.9185e-03, 1.6005e-03, 1.0101e-03, 8.7831e-04, 2.4026e-03,\n",
       "                      2.3380e-03, 8.7763e-04, 6.9598e-04, 1.1441e-03, 1.6209e-03, 4.4261e-04,\n",
       "                      1.0478e-03, 2.2249e-03, 1.2987e-03, 7.1805e-04])),\n",
       "             ('layer3.2.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.2.conv3.weight',\n",
       "              tensor([[[[-4.9634e-03]],\n",
       "              \n",
       "                       [[ 3.8259e-02]],\n",
       "              \n",
       "                       [[ 5.9100e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.0295e-03]],\n",
       "              \n",
       "                       [[-2.0313e-03]],\n",
       "              \n",
       "                       [[-8.6742e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7469e-03]],\n",
       "              \n",
       "                       [[-2.4526e-03]],\n",
       "              \n",
       "                       [[ 8.7618e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4944e-03]],\n",
       "              \n",
       "                       [[ 1.7956e-03]],\n",
       "              \n",
       "                       [[ 1.8134e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0953e-04]],\n",
       "              \n",
       "                       [[ 7.6813e-03]],\n",
       "              \n",
       "                       [[ 1.0685e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.0868e-03]],\n",
       "              \n",
       "                       [[ 3.5316e-03]],\n",
       "              \n",
       "                       [[-3.8761e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.0467e-02]],\n",
       "              \n",
       "                       [[-1.7398e-02]],\n",
       "              \n",
       "                       [[-7.5770e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1055e-02]],\n",
       "              \n",
       "                       [[ 5.0529e-03]],\n",
       "              \n",
       "                       [[ 2.7098e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5394e-03]],\n",
       "              \n",
       "                       [[-6.2233e-03]],\n",
       "              \n",
       "                       [[ 7.4523e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0634e-02]],\n",
       "              \n",
       "                       [[-6.9431e-03]],\n",
       "              \n",
       "                       [[-8.2640e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1369e-05]],\n",
       "              \n",
       "                       [[ 4.4146e-03]],\n",
       "              \n",
       "                       [[ 1.3725e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3905e-03]],\n",
       "              \n",
       "                       [[-1.3405e-03]],\n",
       "              \n",
       "                       [[ 2.1149e-03]]]])),\n",
       "             ('layer3.2.bn3.weight',\n",
       "              tensor([ 0.0435,  0.0030,  0.0262,  ...,  0.1104,  0.0346, -0.0164])),\n",
       "             ('layer3.2.bn3.bias',\n",
       "              tensor([-0.0227, -0.0033, -0.0332,  ...,  0.0039, -0.0190,  0.0014])),\n",
       "             ('layer3.2.bn3.running_mean',\n",
       "              tensor([ 0.0018,  0.0007, -0.0004,  ...,  0.0022, -0.0027,  0.0042])),\n",
       "             ('layer3.2.bn3.running_var',\n",
       "              tensor([8.1344e-05, 1.0603e-05, 3.8039e-05,  ..., 3.7400e-04, 5.0266e-05,\n",
       "                      8.5153e-06])),\n",
       "             ('layer3.2.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.3.conv1.weight',\n",
       "              tensor([[[[-0.0090]],\n",
       "              \n",
       "                       [[ 0.0076]],\n",
       "              \n",
       "                       [[ 0.0011]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0115]],\n",
       "              \n",
       "                       [[ 0.0094]],\n",
       "              \n",
       "                       [[ 0.0020]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0200]],\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       [[-0.0002]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0244]],\n",
       "              \n",
       "                       [[ 0.0001]],\n",
       "              \n",
       "                       [[ 0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0053]],\n",
       "              \n",
       "                       [[ 0.0058]],\n",
       "              \n",
       "                       [[ 0.0074]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0008]],\n",
       "              \n",
       "                       [[ 0.0018]],\n",
       "              \n",
       "                       [[ 0.0008]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0023]],\n",
       "              \n",
       "                       [[-0.0080]],\n",
       "              \n",
       "                       [[-0.0150]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0183]],\n",
       "              \n",
       "                       [[-0.0029]],\n",
       "              \n",
       "                       [[ 0.0001]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0018]],\n",
       "              \n",
       "                       [[-0.0103]],\n",
       "              \n",
       "                       [[-0.0098]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0071]],\n",
       "              \n",
       "                       [[ 0.0020]],\n",
       "              \n",
       "                       [[-0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0019]],\n",
       "              \n",
       "                       [[ 0.0060]],\n",
       "              \n",
       "                       [[ 0.0069]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0044]],\n",
       "              \n",
       "                       [[-0.0059]],\n",
       "              \n",
       "                       [[ 0.0005]]]])),\n",
       "             ('layer3.3.bn1.weight',\n",
       "              tensor([0.0776, 0.1195, 0.0525, 0.0491, 0.0584, 0.0429, 0.0269, 0.0341, 0.0601,\n",
       "                      0.0780, 0.0486, 0.0630, 0.0759, 0.0866, 0.0837, 0.0763, 0.0811, 0.0839,\n",
       "                      0.0572, 0.0947, 0.0644, 0.0434, 0.0664, 0.0891, 0.0681, 0.0473, 0.1087,\n",
       "                      0.0721, 0.0966, 0.0606, 0.0663, 0.0900, 0.0574, 0.0292, 0.0748, 0.0288,\n",
       "                      0.0539, 0.1004, 0.0601, 0.0818, 0.0716, 0.0841, 0.0223, 0.0665, 0.0705,\n",
       "                      0.0713, 0.1199, 0.0686, 0.0532, 0.0803, 0.0852, 0.0716, 0.0932, 0.0525,\n",
       "                      0.0481, 0.1044, 0.0828, 0.0595, 0.0964, 0.0972, 0.0336, 0.0855, 0.0429,\n",
       "                      0.0696, 0.0675, 0.1033, 0.0972, 0.0809, 0.0456, 0.0601, 0.0915, 0.0783,\n",
       "                      0.0567, 0.0798, 0.0297, 0.0732, 0.0739, 0.0968, 0.0562, 0.0887, 0.0852,\n",
       "                      0.0849, 0.0754, 0.0742, 0.0701, 0.0475, 0.1073, 0.0879, 0.0647, 0.0816,\n",
       "                      0.0465, 0.0536, 0.0708, 0.1203, 0.0752, 0.0741, 0.0858, 0.0429, 0.0580,\n",
       "                      0.0613, 0.0804, 0.0685, 0.0700, 0.0562, 0.0871, 0.0934, 0.0658, 0.1203,\n",
       "                      0.0999, 0.0574, 0.0820, 0.0461, 0.0733, 0.0680, 0.0487, 0.0662, 0.0999,\n",
       "                      0.0475, 0.0770, 0.1018, 0.0495, 0.1111, 0.0992, 0.0882, 0.0844, 0.0754,\n",
       "                      0.0938, 0.0542, 0.0883, 0.0509, 0.0643, 0.1096, 0.0532, 0.0604, 0.0400,\n",
       "                      0.0369, 0.0956, 0.0874, 0.0996, 0.0770, 0.0747, 0.0620, 0.0734, 0.1003,\n",
       "                      0.1008, 0.0685, 0.0605, 0.0572, 0.0995, 0.0694, 0.0811, 0.1025, 0.0603,\n",
       "                      0.0938, 0.0793, 0.0794, 0.0977, 0.0478, 0.0796, 0.0986, 0.1190, 0.0725,\n",
       "                      0.0506, 0.0987, 0.0726, 0.0542, 0.0700, 0.0753, 0.0538, 0.1202, 0.1003,\n",
       "                      0.0559, 0.0958, 0.0874, 0.0613, 0.0803, 0.0818, 0.0714, 0.0891, 0.0604,\n",
       "                      0.0914, 0.1221, 0.1078, 0.0639, 0.0606, 0.0744, 0.0669, 0.0828, 0.1186,\n",
       "                      0.0501, 0.0717, 0.0801, 0.0561, 0.0772, 0.0418, 0.0805, 0.0452, 0.0629,\n",
       "                      0.1100, 0.0515, 0.0679, 0.0637, 0.0876, 0.0914, 0.1258, 0.0287, 0.0583,\n",
       "                      0.0560, 0.1012, 0.0686, 0.0567, 0.0845, 0.1275, 0.0758, 0.0718, 0.1100,\n",
       "                      0.0659, 0.0997, 0.0817, 0.0702, 0.0959, 0.0937, 0.0932, 0.0569, 0.0740,\n",
       "                      0.0578, 0.0747, 0.0829, 0.0752, 0.0687, 0.1013, 0.0537, 0.0600, 0.0574,\n",
       "                      0.0653, 0.0670, 0.0278, 0.0814, 0.0723, 0.0827, 0.0633, 0.0635, 0.0620,\n",
       "                      0.0985, 0.0946, 0.0632, 0.0851, 0.1036, 0.0706, 0.0715, 0.0652, 0.0656,\n",
       "                      0.0816, 0.1168, 0.0626, 0.0472])),\n",
       "             ('layer3.3.bn1.bias',\n",
       "              tensor([-0.0129, -0.0350, -0.0253, -0.0506, -0.0059,  0.0055,  0.0084, -0.0249,\n",
       "                      -0.0026, -0.0711,  0.0001, -0.0431, -0.0209, -0.0314, -0.0202,  0.0045,\n",
       "                      -0.0177, -0.0361,  0.0131, -0.0207, -0.0256, -0.0201, -0.0321, -0.0621,\n",
       "                       0.0073, -0.0359, -0.0253, -0.0109, -0.0163, -0.0492, -0.0364, -0.0142,\n",
       "                       0.0057, -0.0022, -0.0393,  0.0030, -0.0042, -0.0616, -0.0297, -0.0539,\n",
       "                      -0.0528, -0.0069,  0.0010, -0.0308, -0.0115, -0.0199, -0.0884,  0.0054,\n",
       "                      -0.0046, -0.0172, -0.0478,  0.0155, -0.0221,  0.0107, -0.0150, -0.0646,\n",
       "                      -0.0059, -0.0180, -0.0028, -0.0350,  0.0183, -0.0082, -0.0065, -0.0301,\n",
       "                      -0.0363, -0.0153, -0.0474, -0.0345, -0.0031, -0.0026, -0.0202, -0.0173,\n",
       "                      -0.0224, -0.0368, -0.0046, -0.0195, -0.0392, -0.0156, -0.0261, -0.0590,\n",
       "                      -0.0266, -0.0157, -0.0338,  0.0149,  0.0012,  0.0267, -0.0487, -0.0546,\n",
       "                      -0.0282, -0.0325,  0.0218, -0.0136, -0.0474, -0.0683,  0.0135, -0.0092,\n",
       "                      -0.0282, -0.0195, -0.0202, -0.0103, -0.0193, -0.0278, -0.0396, -0.0152,\n",
       "                       0.0035, -0.0466,  0.0022, -0.0658, -0.0724, -0.0089, -0.0074,  0.0205,\n",
       "                      -0.0106, -0.0124, -0.0140, -0.0182, -0.0381, -0.0294, -0.0050, -0.0685,\n",
       "                       0.0066, -0.0475, -0.0563, -0.0722, -0.0322, -0.0312, -0.0374, -0.0195,\n",
       "                      -0.0344, -0.0006, -0.0326, -0.0207, -0.0333,  0.0021,  0.0282, -0.0124,\n",
       "                      -0.0733, -0.0443, -0.0239, -0.0144, -0.0077, -0.0136, -0.0307, -0.0492,\n",
       "                      -0.0468, -0.0144, -0.0112,  0.0076, -0.0344,  0.0067, -0.0211, -0.0523,\n",
       "                      -0.0127, -0.0470, -0.0409, -0.0107, -0.0482, -0.0099, -0.0170, -0.0697,\n",
       "                      -0.0877, -0.0421, -0.0187, -0.0386, -0.0251, -0.0093, -0.0237,  0.0001,\n",
       "                      -0.0265, -0.0359, -0.0479,  0.0070, -0.0401, -0.0087, -0.0255,  0.0009,\n",
       "                      -0.0355, -0.0201, -0.0002, -0.0278, -0.0357, -0.0178, -0.0046, -0.0275,\n",
       "                      -0.0252, -0.0159, -0.0085, -0.0234, -0.0596, -0.0139, -0.0067, -0.0211,\n",
       "                      -0.0130, -0.0557,  0.0021, -0.0269, -0.0037, -0.0300, -0.0477, -0.0228,\n",
       "                       0.0071, -0.0234, -0.0190, -0.0614, -0.0790, -0.0046,  0.0081, -0.0359,\n",
       "                      -0.0732, -0.0053, -0.0069, -0.0145, -0.0524, -0.0269, -0.0253, -0.0554,\n",
       "                      -0.0227, -0.0404, -0.0661, -0.0410, -0.0148, -0.0448, -0.0415, -0.0127,\n",
       "                      -0.0358,  0.0106, -0.0201, -0.0402, -0.0551, -0.0412, -0.0657,  0.0111,\n",
       "                      -0.0012,  0.0230, -0.0224, -0.0161, -0.0083, -0.0533, -0.0506, -0.0359,\n",
       "                      -0.0191, -0.0285,  0.0093, -0.0354, -0.0537,  0.0058, -0.0268, -0.0446,\n",
       "                      -0.0538, -0.0361, -0.0619, -0.0344, -0.0512, -0.0258, -0.0294,  0.0259])),\n",
       "             ('layer3.3.bn1.running_mean',\n",
       "              tensor([-0.0182, -0.0658, -0.0142,  0.0036, -0.0329, -0.0114, -0.0293, -0.0192,\n",
       "                      -0.0429, -0.0263, -0.0230, -0.0049, -0.0379, -0.0248, -0.0495, -0.0173,\n",
       "                      -0.0169,  0.0006, -0.0469, -0.0402, -0.0381,  0.0073,  0.0057,  0.0063,\n",
       "                      -0.0604,  0.0013, -0.0425,  0.0023, -0.0217,  0.0120, -0.0041, -0.0305,\n",
       "                      -0.0223, -0.0024, -0.0274, -0.0227, -0.0067,  0.0037, -0.0171,  0.0267,\n",
       "                      -0.0180, -0.0533, -0.0146, -0.0221, -0.0169, -0.0141,  0.0037, -0.0314,\n",
       "                      -0.0373,  0.0048, -0.0030, -0.0584, -0.0442, -0.0424, -0.0060, -0.0242,\n",
       "                      -0.0246,  0.0013, -0.0773, -0.0388, -0.0403, -0.0432, -0.0167, -0.0168,\n",
       "                       0.0057, -0.0235, -0.0319, -0.0375, -0.0217, -0.0354, -0.0300, -0.0219,\n",
       "                      -0.0170, -0.0156, -0.0048, -0.0075, -0.0065, -0.0124,  0.0028, -0.0073,\n",
       "                      -0.0149, -0.0297,  0.0169, -0.0462, -0.0427, -0.0005,  0.0150,  0.0039,\n",
       "                       0.0075, -0.0306, -0.0327, -0.0151, -0.0126, -0.0410, -0.0364, -0.0342,\n",
       "                      -0.0165, -0.0060, -0.0076, -0.0260, -0.0286, -0.0026, -0.0007, -0.0136,\n",
       "                      -0.0155,  0.0052, -0.0194, -0.0273,  0.0014, -0.0386, -0.0250, -0.0335,\n",
       "                      -0.0134, -0.0236, -0.0094, -0.0417, -0.0365, -0.0052, -0.0348, -0.0478,\n",
       "                      -0.0117, -0.0294, -0.0143, -0.0076, -0.0077, -0.0031, -0.0057, -0.0188,\n",
       "                      -0.0214, -0.0113, -0.0110, -0.0738, -0.0024, -0.0492, -0.0217, -0.0124,\n",
       "                       0.0114,  0.0026, -0.0816,  0.0013, -0.0190, -0.0101, -0.0311, -0.0228,\n",
       "                      -0.0264, -0.0109, -0.0208, -0.0195, -0.0312, -0.0430, -0.0268, -0.0213,\n",
       "                      -0.0167, -0.0030, -0.0069, -0.0204, -0.0514, -0.0074, -0.0119, -0.0098,\n",
       "                       0.0024, -0.0365, -0.0031, -0.0049, -0.0302, -0.0150, -0.0203, -0.0314,\n",
       "                      -0.0078, -0.0338, -0.0058, -0.0314, -0.0310, -0.0037, -0.0129, -0.0519,\n",
       "                      -0.0030, -0.0078, -0.0586, -0.0123, -0.0096, -0.0568, -0.0853, -0.0080,\n",
       "                      -0.0330, -0.0250, -0.0226, -0.0367, -0.0484, -0.0097, -0.0099, -0.0451,\n",
       "                      -0.0470, -0.0266, -0.0146, -0.0214, -0.0252, -0.0013, -0.0473,  0.0098,\n",
       "                      -0.0331, -0.0219, -0.0270, -0.0012,  0.0153,  0.0028, -0.0541, -0.0188,\n",
       "                      -0.0270, -0.0085, -0.0009, -0.0495, -0.0214, -0.0205, -0.0271, -0.0160,\n",
       "                      -0.0193, -0.0057, -0.0064, -0.0203, -0.0568, -0.0328, -0.0556, -0.0079,\n",
       "                      -0.0201, -0.0485, -0.0189, -0.0057, -0.0166, -0.0134, -0.0214, -0.0276,\n",
       "                      -0.0303, -0.0193, -0.0137, -0.0619, -0.0027,  0.0329, -0.0168, -0.0284,\n",
       "                      -0.0145, -0.0075, -0.0496, -0.0366,  0.0173, -0.0499, -0.0557, -0.0195,\n",
       "                       0.0026,  0.0018,  0.0089, -0.0019, -0.0189, -0.0641, -0.0249, -0.0364])),\n",
       "             ('layer3.3.bn1.running_var',\n",
       "              tensor([0.0020, 0.0047, 0.0008, 0.0013, 0.0009, 0.0015, 0.0007, 0.0004, 0.0011,\n",
       "                      0.0016, 0.0010, 0.0020, 0.0025, 0.0019, 0.0023, 0.0021, 0.0027, 0.0026,\n",
       "                      0.0024, 0.0019, 0.0013, 0.0005, 0.0009, 0.0019, 0.0033, 0.0012, 0.0032,\n",
       "                      0.0015, 0.0035, 0.0016, 0.0013, 0.0037, 0.0020, 0.0002, 0.0011, 0.0007,\n",
       "                      0.0011, 0.0023, 0.0008, 0.0019, 0.0019, 0.0028, 0.0002, 0.0016, 0.0026,\n",
       "                      0.0014, 0.0032, 0.0018, 0.0020, 0.0014, 0.0017, 0.0026, 0.0033, 0.0013,\n",
       "                      0.0010, 0.0028, 0.0017, 0.0008, 0.0045, 0.0021, 0.0017, 0.0027, 0.0009,\n",
       "                      0.0023, 0.0011, 0.0025, 0.0027, 0.0019, 0.0006, 0.0010, 0.0014, 0.0018,\n",
       "                      0.0012, 0.0017, 0.0005, 0.0008, 0.0010, 0.0035, 0.0004, 0.0021, 0.0013,\n",
       "                      0.0025, 0.0020, 0.0020, 0.0019, 0.0027, 0.0034, 0.0031, 0.0008, 0.0020,\n",
       "                      0.0034, 0.0011, 0.0015, 0.0030, 0.0047, 0.0023, 0.0021, 0.0004, 0.0014,\n",
       "                      0.0007, 0.0018, 0.0014, 0.0014, 0.0010, 0.0040, 0.0013, 0.0021, 0.0030,\n",
       "                      0.0022, 0.0010, 0.0018, 0.0015, 0.0016, 0.0012, 0.0008, 0.0018, 0.0016,\n",
       "                      0.0006, 0.0020, 0.0024, 0.0011, 0.0024, 0.0016, 0.0027, 0.0022, 0.0011,\n",
       "                      0.0014, 0.0004, 0.0017, 0.0007, 0.0010, 0.0040, 0.0008, 0.0029, 0.0015,\n",
       "                      0.0005, 0.0029, 0.0027, 0.0052, 0.0009, 0.0012, 0.0013, 0.0024, 0.0035,\n",
       "                      0.0025, 0.0012, 0.0015, 0.0017, 0.0030, 0.0025, 0.0021, 0.0030, 0.0009,\n",
       "                      0.0018, 0.0022, 0.0020, 0.0036, 0.0007, 0.0032, 0.0027, 0.0033, 0.0015,\n",
       "                      0.0008, 0.0029, 0.0017, 0.0012, 0.0013, 0.0018, 0.0009, 0.0038, 0.0020,\n",
       "                      0.0020, 0.0025, 0.0023, 0.0013, 0.0031, 0.0015, 0.0010, 0.0028, 0.0006,\n",
       "                      0.0015, 0.0040, 0.0039, 0.0008, 0.0010, 0.0021, 0.0016, 0.0019, 0.0054,\n",
       "                      0.0006, 0.0011, 0.0024, 0.0018, 0.0012, 0.0008, 0.0025, 0.0011, 0.0010,\n",
       "                      0.0036, 0.0017, 0.0031, 0.0011, 0.0022, 0.0016, 0.0038, 0.0003, 0.0017,\n",
       "                      0.0010, 0.0036, 0.0013, 0.0013, 0.0021, 0.0047, 0.0023, 0.0015, 0.0030,\n",
       "                      0.0014, 0.0020, 0.0016, 0.0022, 0.0027, 0.0025, 0.0025, 0.0012, 0.0013,\n",
       "                      0.0022, 0.0015, 0.0014, 0.0009, 0.0014, 0.0029, 0.0013, 0.0021, 0.0019,\n",
       "                      0.0017, 0.0020, 0.0002, 0.0028, 0.0016, 0.0015, 0.0007, 0.0011, 0.0018,\n",
       "                      0.0035, 0.0028, 0.0021, 0.0026, 0.0056, 0.0010, 0.0013, 0.0011, 0.0007,\n",
       "                      0.0025, 0.0057, 0.0010, 0.0021])),\n",
       "             ('layer3.3.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.3.conv2.weight',\n",
       "              tensor([[[[ 2.9294e-03,  4.2060e-03,  7.2905e-03],\n",
       "                        [-6.6725e-03, -3.1120e-03,  1.0023e-02],\n",
       "                        [-8.4679e-03, -1.0145e-03, -1.4573e-03]],\n",
       "              \n",
       "                       [[-4.4717e-03, -1.8318e-02, -1.5342e-02],\n",
       "                        [-2.5173e-03,  2.4962e-03, -9.2367e-03],\n",
       "                        [ 4.6107e-04,  3.4359e-02,  5.0917e-03]],\n",
       "              \n",
       "                       [[ 6.2746e-04,  1.8637e-03,  6.5960e-03],\n",
       "                        [-4.7870e-03, -7.4398e-04,  2.8445e-03],\n",
       "                        [-3.5595e-03, -1.8894e-03, -5.4522e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1363e-04,  3.1948e-03, -5.1440e-03],\n",
       "                        [ 8.8581e-03,  9.7601e-03,  7.0370e-03],\n",
       "                        [ 3.3771e-03,  6.7545e-03,  8.3134e-03]],\n",
       "              \n",
       "                       [[-1.2873e-03,  1.3849e-03,  4.7314e-03],\n",
       "                        [ 1.8028e-03, -1.8893e-04, -2.0968e-03],\n",
       "                        [ 3.6869e-03,  1.0302e-03,  3.1785e-03]],\n",
       "              \n",
       "                       [[-2.1088e-03,  4.6191e-03,  3.7097e-03],\n",
       "                        [ 3.8394e-03,  5.7542e-03,  8.0194e-03],\n",
       "                        [-8.0345e-04, -2.0407e-03,  1.4325e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3319e-03, -3.1537e-03,  6.5884e-03],\n",
       "                        [-1.9824e-03, -3.8839e-03,  1.0205e-02],\n",
       "                        [ 2.1749e-03,  6.5992e-03,  3.2211e-03]],\n",
       "              \n",
       "                       [[-8.1094e-03, -2.1039e-02,  8.2281e-03],\n",
       "                        [ 1.0786e-02,  1.0940e-02, -5.8013e-03],\n",
       "                        [ 1.6275e-02,  3.5158e-02,  1.5300e-02]],\n",
       "              \n",
       "                       [[-2.6066e-04, -4.3996e-04,  3.8509e-03],\n",
       "                        [-1.3964e-03, -3.1962e-03,  3.0553e-03],\n",
       "                        [ 7.1844e-04,  1.4317e-03, -2.6577e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.0553e-03,  1.8810e-02,  1.6770e-02],\n",
       "                        [ 1.2086e-02,  1.7991e-02,  5.9459e-03],\n",
       "                        [ 1.6891e-02, -4.2980e-03,  2.9991e-03]],\n",
       "              \n",
       "                       [[-4.2740e-03, -6.9085e-03, -6.9587e-03],\n",
       "                        [-5.6688e-03, -6.6700e-03, -6.1144e-03],\n",
       "                        [ 6.9681e-04, -1.0405e-02, -6.1075e-03]],\n",
       "              \n",
       "                       [[-1.0420e-02, -1.5296e-02,  1.2906e-03],\n",
       "                        [ 1.3016e-03,  1.7580e-03,  1.9574e-03],\n",
       "                        [-6.6442e-04,  1.2225e-03, -7.4348e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2658e-04, -6.6078e-04,  8.7015e-03],\n",
       "                        [ 2.2821e-03,  3.9546e-03,  6.3850e-03],\n",
       "                        [ 2.3447e-03,  9.2380e-03,  8.5669e-03]],\n",
       "              \n",
       "                       [[-1.6888e-03, -5.0920e-04,  1.0114e-03],\n",
       "                        [-1.6942e-02, -1.6734e-02,  2.5958e-03],\n",
       "                        [-2.3022e-02, -1.2592e-03, -1.6864e-02]],\n",
       "              \n",
       "                       [[-1.8602e-03, -3.3430e-03,  4.4847e-03],\n",
       "                        [-1.4699e-03,  3.0987e-03, -4.5626e-04],\n",
       "                        [-2.7966e-04,  1.2564e-03,  9.4235e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9512e-02, -1.5566e-02,  4.4726e-03],\n",
       "                        [-6.9441e-03, -1.1982e-02,  1.6245e-03],\n",
       "                        [-4.3035e-03, -5.1186e-03, -1.3783e-02]],\n",
       "              \n",
       "                       [[-1.6507e-03, -1.1845e-03, -7.5194e-03],\n",
       "                        [-5.6426e-03, -1.6711e-02, -1.4294e-02],\n",
       "                        [-7.1339e-03,  2.6833e-03, -9.4021e-03]],\n",
       "              \n",
       "                       [[-9.4086e-04, -5.8629e-03,  1.2625e-02],\n",
       "                        [-3.2604e-03, -5.3879e-03,  2.1199e-03],\n",
       "                        [ 2.6857e-03,  7.9693e-04,  4.8694e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-6.3302e-03, -1.7361e-03, -5.7777e-03],\n",
       "                        [-2.1258e-03, -7.1571e-03, -1.0340e-02],\n",
       "                        [ 5.9462e-03,  4.5527e-03, -4.7818e-04]],\n",
       "              \n",
       "                       [[-9.0673e-03, -1.1326e-02, -5.9696e-03],\n",
       "                        [-8.4513e-03, -1.1860e-02, -1.0046e-02],\n",
       "                        [ 7.9557e-03,  1.0455e-02,  2.0875e-02]],\n",
       "              \n",
       "                       [[-3.2368e-03,  1.0351e-03, -3.1572e-03],\n",
       "                        [-2.2593e-03, -5.1990e-04, -5.1066e-03],\n",
       "                        [ 4.2733e-05, -9.8569e-04, -2.4300e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1467e-03,  1.7041e-02,  1.0647e-02],\n",
       "                        [-9.7899e-03, -2.0100e-03,  1.4070e-02],\n",
       "                        [-1.1072e-02, -1.2872e-02, -6.3845e-03]],\n",
       "              \n",
       "                       [[-1.2156e-03, -2.8943e-03,  3.6619e-03],\n",
       "                        [-9.8433e-04, -3.2232e-03,  3.6410e-05],\n",
       "                        [ 5.8728e-03, -1.8108e-04,  5.7238e-03]],\n",
       "              \n",
       "                       [[-8.8534e-03, -7.3861e-03,  2.6582e-03],\n",
       "                        [ 7.9930e-03,  8.5915e-04,  1.2519e-03],\n",
       "                        [ 6.6732e-03,  7.2040e-03,  6.6631e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2147e-03, -1.3924e-02, -1.9559e-02],\n",
       "                        [ 8.4180e-03, -9.4204e-05, -1.4580e-02],\n",
       "                        [-2.4726e-03, -2.5246e-03, -2.8931e-03]],\n",
       "              \n",
       "                       [[-2.3326e-03,  8.4866e-03,  6.7547e-03],\n",
       "                        [-9.5382e-03, -6.2532e-03,  7.0804e-03],\n",
       "                        [-1.6818e-02,  1.3105e-03, -5.3589e-03]],\n",
       "              \n",
       "                       [[-3.3750e-03, -5.9413e-03, -1.0580e-02],\n",
       "                        [ 6.5517e-04, -4.4970e-03, -7.1103e-03],\n",
       "                        [-2.2116e-03, -9.3383e-03, -3.4734e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1031e-03,  1.8279e-03,  2.2017e-02],\n",
       "                        [ 7.7699e-04,  3.7818e-03,  2.1247e-02],\n",
       "                        [-7.7595e-03, -1.3343e-02,  4.8896e-03]],\n",
       "              \n",
       "                       [[-1.1732e-03,  1.0247e-03,  4.1442e-03],\n",
       "                        [ 2.7735e-03, -3.8514e-03, -2.7978e-03],\n",
       "                        [-4.4108e-03,  3.4331e-03, -1.5152e-03]],\n",
       "              \n",
       "                       [[ 4.4494e-03, -6.3627e-03, -7.8628e-03],\n",
       "                        [-2.1400e-03, -9.4414e-03, -3.8021e-03],\n",
       "                        [-7.2921e-03, -3.7619e-03, -1.8317e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0540e-03, -1.0123e-02, -2.9492e-03],\n",
       "                        [-6.4879e-03, -6.7552e-04, -1.1163e-03],\n",
       "                        [-5.9266e-03, -2.0689e-03, -1.9802e-03]],\n",
       "              \n",
       "                       [[-6.6588e-03, -1.3411e-02, -2.0013e-03],\n",
       "                        [-1.0327e-02,  5.7232e-03, -6.4979e-03],\n",
       "                        [-1.3088e-02,  1.5490e-02,  1.2136e-02]],\n",
       "              \n",
       "                       [[-1.0512e-03, -2.1006e-03, -1.2305e-03],\n",
       "                        [-1.5560e-03, -4.3158e-05, -4.3787e-03],\n",
       "                        [-5.8069e-04, -3.4763e-03, -1.0424e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4910e-02,  4.8390e-03, -4.5803e-03],\n",
       "                        [-1.5885e-03,  6.7768e-03, -2.0164e-02],\n",
       "                        [ 5.2053e-03,  2.1348e-03, -1.1752e-02]],\n",
       "              \n",
       "                       [[-5.4016e-03, -1.2957e-03, -3.6861e-03],\n",
       "                        [ 2.4814e-03,  6.2712e-03,  5.6363e-03],\n",
       "                        [-5.2111e-03, -2.0870e-03,  1.0012e-02]],\n",
       "              \n",
       "                       [[-8.7914e-03,  4.5600e-03,  9.2363e-03],\n",
       "                        [-7.4857e-03,  2.8436e-03,  9.6111e-03],\n",
       "                        [-1.0539e-02, -9.5393e-03,  1.0406e-02]]]])),\n",
       "             ('layer3.3.bn2.weight',\n",
       "              tensor([8.0049e-02, 9.9447e-02, 9.5496e-02, 6.5715e-02, 6.6231e-02, 8.9028e-02,\n",
       "                      8.9109e-02, 7.0907e-02, 1.0137e-01, 5.8502e-02, 5.2102e-02, 9.3031e-02,\n",
       "                      9.6137e-02, 8.4007e-02, 8.0481e-02, 8.5037e-02, 5.9642e-02, 1.1777e-01,\n",
       "                      1.0658e-01, 8.9691e-02, 8.3404e-02, 6.0809e-02, 8.4580e-02, 1.0542e-01,\n",
       "                      4.5213e-02, 1.0680e-01, 6.0289e-02, 5.6841e-02, 5.8219e-02, 4.9210e-02,\n",
       "                      5.7956e-02, 7.7414e-02, 9.0386e-02, 1.2964e-01, 8.2844e-02, 6.7707e-02,\n",
       "                      5.9484e-02, 1.0574e-01, 6.0465e-02, 1.0183e-01, 6.1222e-02, 1.0264e-01,\n",
       "                      9.8323e-02, 9.7427e-02, 7.6509e-02, 1.0670e-01, 1.0002e-01, 3.5099e-02,\n",
       "                      9.7201e-02, 1.0124e-01, 1.1151e-01, 4.6448e-02, 1.0983e-01, 7.7705e-02,\n",
       "                      8.9182e-02, 5.2720e-02, 9.0577e-02, 8.6298e-02, 6.9472e-02, 1.1729e-01,\n",
       "                      6.8279e-02, 1.2823e-01, 7.7258e-02, 9.0781e-02, 1.0142e-01, 8.3403e-02,\n",
       "                      6.0612e-02, 9.5665e-02, 1.1450e-01, 1.0666e-01, 1.0620e-01, 1.7274e-05,\n",
       "                      9.2136e-02, 9.4231e-02, 9.9304e-02, 9.2923e-02, 1.1717e-01, 7.9227e-02,\n",
       "                      9.5119e-02, 1.1402e-01, 8.6221e-02, 9.0257e-02, 8.1369e-02, 5.4705e-02,\n",
       "                      1.0207e-01, 6.2187e-02, 5.4464e-02, 8.6336e-02, 8.5776e-02, 7.8830e-02,\n",
       "                      2.6506e-02, 1.0641e-01, 5.7406e-02, 8.1986e-02, 1.2544e-01, 4.6400e-02,\n",
       "                      7.5149e-02, 3.2357e-02, 8.0236e-02, 1.2511e-01, 5.4266e-02, 9.5546e-02,\n",
       "                      6.2078e-02, 8.9341e-02, 1.3609e-01, 7.4894e-02, 1.1508e-01, 9.8059e-02,\n",
       "                      9.2713e-02, 8.4080e-02, 1.3216e-01, 9.2100e-02, 8.0394e-02, 8.7386e-02,\n",
       "                      8.2186e-02, 8.2148e-02, 9.0349e-02, 6.2849e-02, 1.2740e-01, 1.1421e-01,\n",
       "                      1.1016e-01, 8.3806e-02, 7.8158e-02, 9.0936e-02, 1.1572e-01, 8.8915e-02,\n",
       "                      1.0489e-01, 6.3519e-02, 9.5313e-02, 9.3661e-02, 8.8408e-02, 6.8231e-02,\n",
       "                      9.7050e-02, 6.9937e-02, 9.3576e-02, 9.6214e-02, 6.0765e-02, 5.8929e-02,\n",
       "                      4.3792e-02, 7.8270e-02, 1.0182e-01, 1.2382e-01, 1.2742e-01, 8.5172e-02,\n",
       "                      6.7639e-02, 8.9008e-02, 6.6180e-02, 1.1753e-01, 8.8881e-02, 6.6907e-02,\n",
       "                      7.6292e-02, 8.7792e-02, 9.9114e-02, 9.8808e-02, 8.7832e-02, 9.4024e-02,\n",
       "                      1.0533e-01, 6.8849e-02, 5.9439e-02, 8.0238e-02, 7.2300e-02, 1.0521e-01,\n",
       "                      2.4463e-05, 1.2252e-01, 1.0363e-01, 6.4521e-02, 1.3596e-01, 7.2302e-02,\n",
       "                      6.5405e-02, 6.5925e-02, 7.6785e-02, 8.5724e-02, 1.2142e-01, 9.3673e-02,\n",
       "                      7.1648e-02, 9.2240e-02, 6.3253e-02, 7.7039e-02, 5.6344e-02, 9.4951e-02,\n",
       "                      7.5640e-02, 7.3882e-02, 7.1998e-02, 7.3880e-02, 9.0805e-02, 4.3094e-02,\n",
       "                      9.7296e-02, 1.1143e-01, 8.5031e-02, 1.1413e-01, 9.7854e-02, 8.6790e-02,\n",
       "                      9.7175e-02, 1.2466e-01, 1.0123e-01, 7.8728e-02, 6.1672e-02, 9.3277e-02,\n",
       "                      1.4595e-01, 9.3265e-02, 1.0618e-01, 8.3594e-02, 6.9712e-02, 1.3325e-01,\n",
       "                      1.1146e-01, 1.1248e-01, 6.6196e-02, 1.0282e-01, 7.1241e-02, 6.0246e-02,\n",
       "                      1.0436e-01, 9.9564e-02, 4.2181e-02, 1.1297e-01, 6.8507e-02, 7.8378e-02,\n",
       "                      7.9500e-02, 7.1288e-02, 7.4894e-02, 9.6221e-02, 6.0231e-02, 8.7588e-02,\n",
       "                      9.3866e-02, 6.0656e-05, 7.2078e-02, 5.8289e-02, 5.4666e-02, 9.0037e-02,\n",
       "                      7.0321e-02, 7.2744e-02, 7.5731e-02, 8.3927e-02, 1.0199e-01, 9.0911e-02,\n",
       "                      9.4318e-02, 1.0166e-01, 1.1512e-01, 5.9041e-02, 8.8458e-02, 7.0792e-02,\n",
       "                      4.5240e-02, 7.6978e-02, 6.9272e-02, 7.8331e-02, 1.5304e-01, 1.1024e-01,\n",
       "                      1.0792e-01, 7.2717e-02, 7.8634e-02, 9.4390e-02, 7.2493e-02, 6.0964e-02,\n",
       "                      9.2521e-02, 7.6216e-02, 9.0887e-02, 9.9198e-02])),\n",
       "             ('layer3.3.bn2.bias',\n",
       "              tensor([-0.0615, -0.0793, -0.0615, -0.0184, -0.0310, -0.0529, -0.0633, -0.0282,\n",
       "                      -0.0560, -0.0361, -0.0291, -0.0449, -0.0689, -0.0260, -0.0905, -0.0345,\n",
       "                      -0.0182, -0.0566, -0.0810, -0.0590, -0.0751, -0.0536, -0.0356, -0.0615,\n",
       "                      -0.0131, -0.0904, -0.0069, -0.0175, -0.0279, -0.0154, -0.0487, -0.0063,\n",
       "                      -0.0466, -0.0635, -0.0600, -0.0424, -0.0290, -0.0879,  0.0063, -0.0635,\n",
       "                      -0.0307, -0.0646, -0.0731, -0.0284, -0.0880, -0.0734, -0.0655, -0.0302,\n",
       "                      -0.0194, -0.0864, -0.0954, -0.0558, -0.1028, -0.0091, -0.0408, -0.0379,\n",
       "                      -0.0331, -0.0277, -0.0325, -0.0757, -0.0182, -0.0679, -0.0516, -0.0496,\n",
       "                      -0.0508, -0.0506, -0.0160, -0.0435, -0.0917, -0.0864, -0.0876, -0.0002,\n",
       "                      -0.0581, -0.0492, -0.0632, -0.0694, -0.0748, -0.0387, -0.0622, -0.0619,\n",
       "                      -0.0716, -0.1136, -0.0366, -0.0278, -0.0638, -0.0487, -0.0277, -0.0683,\n",
       "                      -0.0729, -0.0459, -0.0076, -0.0745, -0.0201, -0.0631, -0.0855, -0.0375,\n",
       "                      -0.0393, -0.0132, -0.0779, -0.0986, -0.0175, -0.0660, -0.0277, -0.0997,\n",
       "                      -0.0863, -0.0303, -0.0892, -0.0538, -0.1017, -0.0512, -0.0807, -0.0593,\n",
       "                      -0.0807, -0.0163, -0.0663, -0.0477, -0.0446, -0.0511, -0.0705, -0.0953,\n",
       "                      -0.1013, -0.0849, -0.0330, -0.0667, -0.0476, -0.0775, -0.0926, -0.0307,\n",
       "                      -0.0385, -0.0620, -0.0659, -0.0387, -0.0515, -0.0473, -0.0454, -0.0657,\n",
       "                      -0.0155, -0.0182, -0.0213, -0.0604, -0.0596, -0.0862, -0.0852, -0.0227,\n",
       "                      -0.0337, -0.0619, -0.0403, -0.0756, -0.0537, -0.0492, -0.0437, -0.0697,\n",
       "                      -0.0804, -0.0346, -0.0737, -0.0535, -0.0879, -0.0605, -0.0563, -0.0638,\n",
       "                      -0.0555, -0.0827, -0.0002, -0.0767, -0.0917, -0.0370, -0.0868, -0.0282,\n",
       "                      -0.0469, -0.0384, -0.0590, -0.0483, -0.0694, -0.0464, -0.0549, -0.0612,\n",
       "                      -0.0830, -0.0561, -0.0289, -0.0587, -0.0711, -0.0387, -0.0462, -0.0238,\n",
       "                      -0.0183, -0.0010, -0.0773, -0.0491, -0.0635, -0.0990, -0.0542, -0.0491,\n",
       "                      -0.0766, -0.0548, -0.0207, -0.0402, -0.0412, -0.0431, -0.1106, -0.0361,\n",
       "                      -0.0762, -0.0489, -0.0922, -0.0891, -0.0859, -0.0820, -0.0233, -0.0606,\n",
       "                      -0.0520, -0.0319, -0.0584, -0.0699, -0.0221, -0.1254, -0.0549, -0.0416,\n",
       "                      -0.0467, -0.0446, -0.0204, -0.0569, -0.0441, -0.0475, -0.0481, -0.0004,\n",
       "                      -0.0292, -0.0654, -0.0699, -0.0430, -0.0603, -0.0377, -0.0374, -0.0567,\n",
       "                      -0.0810, -0.0674, -0.0473, -0.0743, -0.0817, -0.0344, -0.0889, -0.0586,\n",
       "                      -0.0259, -0.0336, -0.0515, -0.0635, -0.1311, -0.0658, -0.0720, -0.0449,\n",
       "                      -0.0303, -0.0441, -0.0351, -0.0350, -0.0634, -0.0492, -0.0235, -0.0532])),\n",
       "             ('layer3.3.bn2.running_mean',\n",
       "              tensor([ 2.2731e-02,  1.2143e-02, -9.2853e-03, -1.5071e-02, -1.6164e-02,\n",
       "                      -2.5774e-02, -1.3480e-02, -1.1669e-02, -1.2586e-02, -1.2411e-02,\n",
       "                      -2.0661e-02, -2.3203e-02, -2.0271e-02, -2.2565e-02,  2.0277e-03,\n",
       "                       6.9050e-03, -4.3426e-03, -1.0289e-02, -1.7511e-02, -2.1875e-03,\n",
       "                      -2.3141e-02, -3.4248e-02, -6.1765e-03, -3.3923e-02, -6.3135e-03,\n",
       "                      -2.0379e-02, -9.9977e-03, -1.0771e-03, -2.8260e-02,  8.8292e-03,\n",
       "                      -2.0484e-02, -6.5565e-03, -3.2056e-02, -5.7849e-02, -2.0591e-02,\n",
       "                      -2.9028e-02, -9.5485e-03, -2.8861e-02, -3.9529e-02, -1.0401e-02,\n",
       "                      -2.2761e-02,  8.0179e-03, -2.0812e-02, -1.4545e-02, -2.0708e-03,\n",
       "                      -2.9503e-02, -2.3927e-02, -1.3963e-02,  2.9603e-03, -2.1635e-02,\n",
       "                      -1.9477e-02, -1.9127e-03, -6.2982e-03, -1.7241e-02, -1.7871e-02,\n",
       "                      -6.5175e-04, -1.5613e-02, -7.1418e-03, -1.2472e-02, -2.4649e-02,\n",
       "                       4.8720e-04, -2.7943e-02, -1.4470e-02, -1.8314e-02, -4.1902e-03,\n",
       "                      -2.9830e-02,  1.7518e-03, -4.2657e-02, -5.7109e-02, -5.4889e-02,\n",
       "                      -3.2846e-02, -1.4431e-05, -3.6260e-02, -1.7117e-02,  1.6229e-02,\n",
       "                       5.6560e-02, -1.8092e-02, -2.1647e-02, -3.6666e-02, -6.6823e-02,\n",
       "                      -1.8676e-02,  8.6266e-03, -7.6903e-03,  1.4825e-02,  5.3552e-03,\n",
       "                      -6.2804e-03, -2.0144e-02,  2.8735e-03, -2.0274e-02, -4.0883e-02,\n",
       "                      -1.4210e-02, -1.4238e-02, -2.9616e-02, -1.1071e-02, -4.5532e-02,\n",
       "                      -4.3720e-03, -1.5914e-02, -1.6726e-02, -1.3729e-02, -4.7406e-02,\n",
       "                      -1.8222e-02, -3.0133e-02, -5.5380e-03, -9.5680e-03, -2.9958e-02,\n",
       "                      -1.2450e-02, -2.5028e-02, -2.5358e-02, -2.0648e-03, -2.7128e-02,\n",
       "                      -1.1080e-02, -1.1125e-02, -1.0896e-02, -4.6072e-02,  1.2380e-02,\n",
       "                      -1.3418e-02,  6.6165e-03,  1.0244e-02, -2.1508e-02,  5.2268e-02,\n",
       "                      -4.1714e-02,  2.0815e-02, -2.2991e-02, -2.1341e-02, -2.0494e-02,\n",
       "                      -2.3245e-02,  1.7415e-03, -4.8371e-03, -5.4047e-03, -1.1972e-02,\n",
       "                      -1.6333e-02, -1.1113e-03, -2.0951e-02, -1.0274e-02, -1.0924e-02,\n",
       "                      -2.3352e-02, -8.8621e-03, -3.4634e-02, -8.5770e-03, -1.0749e-02,\n",
       "                      -1.3712e-02, -1.5267e-02, -4.7799e-02, -2.3013e-02, -5.8312e-03,\n",
       "                      -3.5300e-02, -9.3762e-03, -7.4766e-02, -8.8160e-03, -1.9428e-02,\n",
       "                      -1.7663e-02, -4.9934e-03, -2.6736e-02, -2.2215e-02, -3.4318e-02,\n",
       "                      -5.5071e-03,  8.3153e-03, -1.5982e-02, -1.5283e-02, -2.7200e-02,\n",
       "                      -2.6760e-02, -2.5971e-02,  3.2974e-06, -1.6678e-02,  6.3933e-02,\n",
       "                       8.8258e-03, -4.3039e-02, -3.3432e-03,  5.5621e-03,  6.6994e-03,\n",
       "                       2.9583e-03, -1.6238e-02, -3.4034e-02, -2.4886e-02, -1.3555e-02,\n",
       "                      -6.8732e-03, -2.5529e-02,  4.9373e-03, -2.3816e-03,  5.6967e-04,\n",
       "                      -2.4928e-02, -1.0862e-02, -4.7115e-03, -2.3951e-02, -2.9956e-02,\n",
       "                      -6.0624e-03, -9.8582e-03, -4.3068e-02, -2.0476e-02,  1.2946e-02,\n",
       "                      -1.8405e-02, -1.6309e-02, -4.6254e-02, -2.2149e-02, -2.3899e-02,\n",
       "                      -1.6598e-03, -1.4966e-02, -3.1255e-02,  7.0454e-02, -7.4020e-03,\n",
       "                      -2.4689e-02, -1.2050e-02, -9.5637e-04, -2.5536e-02, -3.0810e-03,\n",
       "                       1.5202e-02,  1.0638e-02, -2.6948e-02,  5.0318e-03, -1.6542e-02,\n",
       "                      -2.8728e-02, -1.2470e-02, -1.6657e-03, -8.3792e-03, -6.9114e-03,\n",
       "                      -2.0814e-02, -5.1401e-03, -1.1897e-02, -1.6438e-02, -8.5239e-03,\n",
       "                      -2.6084e-02, -2.7020e-02,  1.5087e-02,  1.6812e-05, -2.3351e-02,\n",
       "                      -1.5683e-02, -1.0553e-02, -2.7970e-02,  2.6467e-02, -9.7545e-03,\n",
       "                      -3.9307e-02, -4.4324e-02,  2.4722e-03, -1.5371e-02, -1.2234e-02,\n",
       "                      -4.2810e-02, -5.5308e-02, -1.8101e-02, -3.6161e-02, -1.5192e-02,\n",
       "                      -1.2145e-03, -2.0883e-02, -2.0216e-02, -2.6651e-02, -2.5484e-02,\n",
       "                      -1.1617e-02, -3.3957e-02,  4.6538e-04, -4.5960e-03, -1.9990e-02,\n",
       "                      -1.5267e-02, -3.3616e-02, -2.3571e-02,  1.7058e-03, -2.9965e-02,\n",
       "                      -2.4691e-02])),\n",
       "             ('layer3.3.bn2.running_var',\n",
       "              tensor([7.7297e-04, 1.7222e-03, 1.3899e-03, 3.6112e-04, 4.8261e-04, 7.7988e-04,\n",
       "                      1.0634e-03, 1.6509e-03, 1.7964e-03, 4.9850e-04, 5.6523e-04, 1.4297e-03,\n",
       "                      1.4477e-03, 8.9706e-04, 8.4245e-04, 8.8639e-04, 7.3759e-04, 1.6728e-03,\n",
       "                      8.1139e-04, 1.4493e-03, 1.0102e-03, 7.0662e-04, 1.4777e-03, 1.4967e-03,\n",
       "                      3.3304e-04, 1.8941e-03, 1.0429e-03, 6.7374e-04, 6.1345e-04, 3.5634e-04,\n",
       "                      5.2435e-04, 1.6506e-03, 1.3309e-03, 4.9023e-03, 5.3463e-04, 7.7110e-04,\n",
       "                      5.8405e-04, 1.9023e-03, 2.0139e-03, 1.3658e-03, 1.7675e-03, 1.8695e-03,\n",
       "                      1.4702e-03, 2.0197e-03, 7.6814e-04, 1.6789e-03, 1.6338e-03, 3.6329e-04,\n",
       "                      1.2444e-03, 2.0525e-03, 1.2906e-03, 4.0500e-04, 1.5105e-03, 8.7404e-04,\n",
       "                      2.0265e-03, 4.1736e-04, 1.0861e-03, 8.7370e-04, 6.3551e-04, 2.1560e-03,\n",
       "                      1.0828e-03, 2.2783e-03, 9.6739e-04, 1.0408e-03, 1.1445e-03, 9.2913e-04,\n",
       "                      6.6490e-04, 1.2132e-03, 2.8599e-03, 2.1334e-03, 1.9684e-03, 1.5054e-09,\n",
       "                      1.3142e-03, 1.1416e-03, 1.3736e-03, 1.4960e-03, 1.9545e-03, 9.7330e-04,\n",
       "                      2.1265e-03, 2.8021e-03, 1.0811e-03, 1.1567e-03, 1.0803e-03, 7.0893e-04,\n",
       "                      2.0372e-03, 5.6566e-04, 3.8156e-04, 2.2867e-03, 7.3084e-04, 1.1113e-03,\n",
       "                      1.3496e-04, 1.5896e-03, 6.2603e-04, 1.1389e-03, 1.9003e-03, 3.0783e-04,\n",
       "                      9.7908e-04, 3.7875e-04, 8.3372e-04, 2.6610e-03, 4.8627e-04, 1.0560e-03,\n",
       "                      5.7914e-04, 1.3218e-03, 2.2683e-03, 1.4983e-03, 1.1976e-03, 1.1761e-03,\n",
       "                      1.4262e-03, 1.2610e-03, 1.9702e-03, 9.5132e-04, 1.1489e-03, 1.2736e-03,\n",
       "                      1.1806e-03, 1.2696e-03, 8.3683e-04, 7.0874e-04, 1.6265e-03, 1.1637e-03,\n",
       "                      2.2069e-03, 1.2310e-03, 1.5986e-03, 1.2218e-03, 2.0729e-03, 1.1351e-03,\n",
       "                      1.5128e-03, 3.6325e-04, 1.5935e-03, 7.5276e-04, 1.0566e-03, 4.8923e-04,\n",
       "                      1.2694e-03, 5.4836e-04, 1.3298e-03, 1.0785e-03, 6.4551e-04, 4.6236e-04,\n",
       "                      2.4697e-04, 8.6393e-04, 1.1949e-03, 1.6815e-03, 2.4101e-03, 7.6651e-04,\n",
       "                      6.6261e-04, 1.9278e-03, 6.2257e-04, 1.6885e-03, 7.8958e-04, 6.0708e-04,\n",
       "                      6.4617e-04, 1.0450e-03, 1.4913e-03, 1.2773e-03, 1.4811e-03, 1.5854e-03,\n",
       "                      1.7553e-03, 9.4146e-04, 3.8794e-04, 1.0232e-03, 7.1589e-04, 1.3065e-03,\n",
       "                      4.0330e-09, 1.6911e-03, 1.6196e-03, 6.3457e-04, 2.5367e-03, 7.5756e-04,\n",
       "                      6.6481e-04, 1.1234e-03, 6.5985e-04, 1.2681e-03, 2.0742e-03, 1.6159e-03,\n",
       "                      7.2334e-04, 1.2016e-03, 1.0813e-03, 8.7311e-04, 3.7606e-04, 1.4066e-03,\n",
       "                      1.3617e-03, 4.6734e-04, 6.6755e-04, 7.6554e-04, 1.7154e-03, 5.0004e-04,\n",
       "                      1.3356e-03, 2.5395e-03, 7.2692e-04, 1.6458e-03, 9.4054e-04, 8.6436e-04,\n",
       "                      1.4377e-03, 1.5866e-03, 1.6820e-03, 1.8141e-03, 3.5810e-04, 8.6391e-04,\n",
       "                      2.5562e-03, 7.1068e-04, 1.3913e-03, 7.1846e-04, 6.0190e-04, 2.1609e-03,\n",
       "                      1.1456e-03, 1.2753e-03, 8.8912e-04, 9.1371e-04, 5.3826e-04, 8.3149e-04,\n",
       "                      1.4869e-03, 1.2486e-03, 3.0662e-04, 2.1324e-03, 5.3043e-04, 8.4534e-04,\n",
       "                      8.7928e-04, 6.5129e-04, 7.7985e-04, 1.2454e-03, 1.2292e-03, 1.1468e-03,\n",
       "                      8.7796e-04, 2.4897e-08, 1.0463e-03, 4.6738e-04, 2.8268e-04, 2.0514e-03,\n",
       "                      8.1042e-04, 7.0367e-04, 1.3472e-03, 1.1510e-03, 1.4081e-03, 1.2136e-03,\n",
       "                      7.2848e-04, 1.3279e-03, 2.6990e-03, 3.5971e-04, 2.4105e-03, 6.9398e-04,\n",
       "                      2.4215e-04, 9.4639e-04, 7.3148e-04, 1.2379e-03, 3.5676e-03, 2.4480e-03,\n",
       "                      1.3431e-03, 5.7115e-04, 9.0454e-04, 2.4784e-03, 4.8126e-04, 8.5986e-04,\n",
       "                      1.0337e-03, 9.0821e-04, 1.2556e-03, 1.2364e-03])),\n",
       "             ('layer3.3.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.3.conv3.weight',\n",
       "              tensor([[[[-4.1022e-03]],\n",
       "              \n",
       "                       [[-4.5961e-03]],\n",
       "              \n",
       "                       [[ 2.0051e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.6472e-03]],\n",
       "              \n",
       "                       [[-9.2064e-03]],\n",
       "              \n",
       "                       [[-9.7044e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2724e-03]],\n",
       "              \n",
       "                       [[-1.1628e-02]],\n",
       "              \n",
       "                       [[ 2.8425e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1887e-02]],\n",
       "              \n",
       "                       [[ 7.8576e-03]],\n",
       "              \n",
       "                       [[ 1.0914e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7040e-03]],\n",
       "              \n",
       "                       [[-2.4105e-03]],\n",
       "              \n",
       "                       [[-9.2080e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.2829e-03]],\n",
       "              \n",
       "                       [[-9.7940e-03]],\n",
       "              \n",
       "                       [[-8.6843e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-5.2792e-03]],\n",
       "              \n",
       "                       [[-6.8278e-03]],\n",
       "              \n",
       "                       [[-3.2859e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2392e-02]],\n",
       "              \n",
       "                       [[ 3.1234e-02]],\n",
       "              \n",
       "                       [[ 1.5935e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5330e-03]],\n",
       "              \n",
       "                       [[-1.0111e-02]],\n",
       "              \n",
       "                       [[-1.1314e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0585e-02]],\n",
       "              \n",
       "                       [[ 1.8624e-02]],\n",
       "              \n",
       "                       [[-3.5136e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.0331e-03]],\n",
       "              \n",
       "                       [[ 1.4157e-03]],\n",
       "              \n",
       "                       [[ 8.3366e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.6541e-03]],\n",
       "              \n",
       "                       [[ 4.0442e-03]],\n",
       "              \n",
       "                       [[ 1.1178e-03]]]])),\n",
       "             ('layer3.3.bn3.weight',\n",
       "              tensor([ 0.0209, -0.0482,  0.0270,  ..., -0.0686,  0.0270, -0.0519])),\n",
       "             ('layer3.3.bn3.bias',\n",
       "              tensor([-0.0368, -0.0387, -0.0418,  ..., -0.0182, -0.0451,  0.0076])),\n",
       "             ('layer3.3.bn3.running_mean',\n",
       "              tensor([-0.0041,  0.0026, -0.0024,  ...,  0.0057, -0.0020,  0.0029])),\n",
       "             ('layer3.3.bn3.running_var',\n",
       "              tensor([3.0126e-05, 5.7818e-05, 3.0081e-05,  ..., 1.5474e-04, 3.1054e-05,\n",
       "                      4.7525e-05])),\n",
       "             ('layer3.3.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.4.conv1.weight',\n",
       "              tensor([[[[ 0.0039]],\n",
       "              \n",
       "                       [[-0.0069]],\n",
       "              \n",
       "                       [[-0.0006]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0005]],\n",
       "              \n",
       "                       [[-0.0020]],\n",
       "              \n",
       "                       [[-0.0005]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0012]],\n",
       "              \n",
       "                       [[-0.0014]],\n",
       "              \n",
       "                       [[ 0.0078]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0046]],\n",
       "              \n",
       "                       [[-0.0066]],\n",
       "              \n",
       "                       [[ 0.0007]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0039]],\n",
       "              \n",
       "                       [[-0.0066]],\n",
       "              \n",
       "                       [[-0.0039]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0107]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       [[ 0.0047]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0079]],\n",
       "              \n",
       "                       [[ 0.0188]],\n",
       "              \n",
       "                       [[ 0.0007]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0011]],\n",
       "              \n",
       "                       [[-0.0047]],\n",
       "              \n",
       "                       [[-0.0020]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0024]],\n",
       "              \n",
       "                       [[ 0.0061]],\n",
       "              \n",
       "                       [[-0.0075]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0181]],\n",
       "              \n",
       "                       [[-0.0095]],\n",
       "              \n",
       "                       [[ 0.0009]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0093]],\n",
       "              \n",
       "                       [[ 0.0147]],\n",
       "              \n",
       "                       [[ 0.0133]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0244]],\n",
       "              \n",
       "                       [[-0.0075]],\n",
       "              \n",
       "                       [[ 0.0041]]]])),\n",
       "             ('layer3.4.bn1.weight',\n",
       "              tensor([0.0603, 0.0722, 0.0392, 0.0982, 0.0661, 0.0726, 0.0898, 0.0886, 0.1089,\n",
       "                      0.1113, 0.0779, 0.0624, 0.0965, 0.1191, 0.1009, 0.0891, 0.1126, 0.0548,\n",
       "                      0.1068, 0.0661, 0.1107, 0.0919, 0.0744, 0.0872, 0.0964, 0.0846, 0.0595,\n",
       "                      0.0540, 0.0682, 0.1135, 0.1097, 0.1050, 0.1160, 0.0481, 0.1024, 0.0962,\n",
       "                      0.0994, 0.0565, 0.0722, 0.0728, 0.0878, 0.0860, 0.0704, 0.1195, 0.1188,\n",
       "                      0.1048, 0.0893, 0.0831, 0.0872, 0.0791, 0.0718, 0.0853, 0.0590, 0.0870,\n",
       "                      0.0572, 0.1116, 0.0878, 0.0506, 0.0758, 0.0021, 0.0762, 0.0977, 0.0732,\n",
       "                      0.1150, 0.0539, 0.0753, 0.0792, 0.0839, 0.0793, 0.0642, 0.1050, 0.0823,\n",
       "                      0.0805, 0.1373, 0.0923, 0.0803, 0.1033, 0.0983, 0.0879, 0.1040, 0.0807,\n",
       "                      0.0994, 0.0832, 0.0754, 0.0852, 0.1106, 0.0862, 0.0959, 0.1200, 0.0913,\n",
       "                      0.0858, 0.0006, 0.0892, 0.0824, 0.0802, 0.0765, 0.0950, 0.0828, 0.0621,\n",
       "                      0.0611, 0.0876, 0.0707, 0.0434, 0.0939, 0.1006, 0.0797, 0.1039, 0.0874,\n",
       "                      0.1083, 0.0698, 0.0688, 0.1262, 0.0682, 0.0958, 0.0891, 0.1008, 0.1090,\n",
       "                      0.0557, 0.0830, 0.1107, 0.0689, 0.0896, 0.1073, 0.0956, 0.0638, 0.0830,\n",
       "                      0.0930, 0.0809, 0.0659, 0.0803, 0.0747, 0.0698, 0.0751, 0.0825, 0.1013,\n",
       "                      0.0904, 0.0733, 0.1140, 0.1213, 0.0947, 0.0865, 0.0694, 0.1009, 0.0811,\n",
       "                      0.0812, 0.0782, 0.1004, 0.0840, 0.0794, 0.0551, 0.1106, 0.0970, 0.0810,\n",
       "                      0.0552, 0.1001, 0.1076, 0.0524, 0.0733, 0.0467, 0.0843, 0.1123, 0.0951,\n",
       "                      0.0879, 0.0908, 0.1079, 0.0669, 0.0671, 0.1001, 0.0537, 0.0514, 0.0780,\n",
       "                      0.0490, 0.1199, 0.0619, 0.0856, 0.1135, 0.0658, 0.0860, 0.0837, 0.0928,\n",
       "                      0.0762, 0.0843, 0.0853, 0.0820, 0.1017, 0.0873, 0.0945, 0.0831, 0.1038,\n",
       "                      0.1275, 0.1172, 0.1030, 0.0702, 0.1140, 0.1128, 0.0980, 0.0732, 0.0872,\n",
       "                      0.0765, 0.0918, 0.0415, 0.0666, 0.0984, 0.1129, 0.0763, 0.0932, 0.1269,\n",
       "                      0.1011, 0.0948, 0.0755, 0.0660, 0.0772, 0.0579, 0.0793, 0.0656, 0.0891,\n",
       "                      0.0999, 0.0881, 0.0705, 0.0716, 0.0405, 0.0867, 0.0726, 0.0804, 0.0704,\n",
       "                      0.1138, 0.0794, 0.0482, 0.1051, 0.0890, 0.0864, 0.0795, 0.1145, 0.0826,\n",
       "                      0.1110, 0.0918, 0.0839, 0.0860, 0.0705, 0.0795, 0.1154, 0.1237, 0.0836,\n",
       "                      0.0663, 0.1082, 0.1089, 0.0667, 0.1235, 0.0596, 0.0813, 0.0822, 0.1088,\n",
       "                      0.0729, 0.0944, 0.0846, 0.0980])),\n",
       "             ('layer3.4.bn1.bias',\n",
       "              tensor([-0.0198, -0.0523, -0.0154, -0.0210, -0.0343, -0.0705, -0.0591, -0.0240,\n",
       "                      -0.0604, -0.0573, -0.0084, -0.0093, -0.0711, -0.0584, -0.0642, -0.0431,\n",
       "                      -0.0344, -0.0274, -0.0253, -0.0152, -0.0493, -0.0873, -0.0429, -0.0341,\n",
       "                      -0.0415, -0.0230, -0.0062, -0.0081, -0.0152, -0.0632, -0.0442, -0.0448,\n",
       "                      -0.0394, -0.0199, -0.0220, -0.0280, -0.0488, -0.0142, -0.0175, -0.0327,\n",
       "                      -0.0080, -0.0309, -0.0489, -0.0463, -0.0636, -0.0427, -0.0422, -0.0338,\n",
       "                      -0.0285, -0.0239, -0.0320, -0.0235, -0.0135, -0.0491, -0.0101, -0.0217,\n",
       "                      -0.0501, -0.0152,  0.0061, -0.0159, -0.0359, -0.0489, -0.0221, -0.0974,\n",
       "                      -0.0152, -0.0642, -0.0456, -0.0291, -0.0200, -0.0174, -0.0432, -0.0243,\n",
       "                      -0.0654, -0.0729, -0.0521, -0.0321, -0.0719, -0.0591, -0.0181, -0.0890,\n",
       "                      -0.0087, -0.0541, -0.0144, -0.0596, -0.0375, -0.0558, -0.0318, -0.0658,\n",
       "                      -0.0449, -0.0430, -0.0265, -0.0102, -0.0488, -0.0209, -0.0296, -0.0382,\n",
       "                      -0.0086, -0.0406,  0.0087, -0.0068, -0.0372, -0.0077, -0.0154, -0.0593,\n",
       "                      -0.0480, -0.0522,  0.0024, -0.0503, -0.0695, -0.0377, -0.0153, -0.0531,\n",
       "                      -0.0245, -0.0539, -0.0536, -0.0227, -0.0963, -0.0216, -0.0170, -0.0480,\n",
       "                      -0.0342, -0.0517, -0.0229, -0.0787, -0.0071, -0.0379, -0.0259, -0.0018,\n",
       "                      -0.0275, -0.0263, -0.0607,  0.0014, -0.0172, -0.0226, -0.0458, -0.0239,\n",
       "                      -0.0348, -0.0508, -0.0634, -0.0397, -0.0464, -0.0136, -0.0314, -0.0383,\n",
       "                      -0.0214, -0.0219, -0.0529, -0.0783, -0.0465, -0.0002, -0.0533, -0.0594,\n",
       "                      -0.0256, -0.0348, -0.0639, -0.0313, -0.0282, -0.0152, -0.0047, -0.0397,\n",
       "                      -0.0341, -0.0104, -0.0417, -0.0376, -0.0324, -0.0352, -0.0296, -0.0518,\n",
       "                      -0.0265, -0.0102, -0.0326, -0.0414, -0.1138, -0.0303, -0.0504, -0.0413,\n",
       "                      -0.0390, -0.0478, -0.0650, -0.0343, -0.0431, -0.0204, -0.0570, -0.0461,\n",
       "                      -0.0660, -0.0765, -0.0465, -0.0282, -0.0425, -0.0699, -0.0411, -0.0444,\n",
       "                      -0.0228, -0.0709, -0.0495, -0.0191, -0.0343, -0.0365, -0.0450, -0.0608,\n",
       "                      -0.0272, -0.0164, -0.0836, -0.0519, -0.0192, -0.0670, -0.0379, -0.0829,\n",
       "                      -0.0625, -0.0522, -0.0186, -0.0286, -0.0047, -0.0231, -0.0244,  0.0023,\n",
       "                      -0.0322, -0.0390,  0.0049, -0.0272, -0.0134, -0.0369, -0.0578, -0.0283,\n",
       "                      -0.0354, -0.0890, -0.0143, -0.0131, -0.0245, -0.0122, -0.0486, -0.0399,\n",
       "                      -0.0409, -0.0377, -0.0316, -0.0555, -0.0232, -0.0401, -0.0221, -0.0160,\n",
       "                      -0.0852, -0.0799, -0.0570, -0.0253, -0.0086, -0.0226, -0.0157, -0.0828,\n",
       "                      -0.0127, -0.0236, -0.0157, -0.0336, -0.0205, -0.0350, -0.0101, -0.0382])),\n",
       "             ('layer3.4.bn1.running_mean',\n",
       "              tensor([-0.0314, -0.0111,  0.0029, -0.0520, -0.0238, -0.0032, -0.0092, -0.0129,\n",
       "                      -0.0382, -0.0436, -0.0303, -0.0234, -0.0070, -0.0040, -0.0048, -0.0104,\n",
       "                      -0.0528, -0.0094, -0.0539, -0.0208, -0.0313, -0.0184,  0.0001, -0.0098,\n",
       "                      -0.0608, -0.0066, -0.0160, -0.0186, -0.0319, -0.0157, -0.0545,  0.0069,\n",
       "                      -0.0148, -0.0129, -0.0227,  0.0163, -0.0424, -0.0245, -0.0430, -0.0305,\n",
       "                      -0.0417, -0.0244,  0.0037, -0.0215, -0.0008, -0.0205, -0.0186,  0.0055,\n",
       "                      -0.0289, -0.0223, -0.0027,  0.0008, -0.0135, -0.0021, -0.0132, -0.0368,\n",
       "                       0.0104, -0.0292, -0.0326, -0.0056, -0.0063,  0.0047, -0.0151, -0.0317,\n",
       "                      -0.0057, -0.0023,  0.0029, -0.0373, -0.0227,  0.0032, -0.0056,  0.0053,\n",
       "                      -0.0014,  0.0011, -0.0281, -0.0149,  0.0027, -0.0089, -0.0179,  0.0060,\n",
       "                      -0.0254, -0.0216, -0.0379, -0.0243, -0.0109, -0.0205, -0.0236, -0.0254,\n",
       "                      -0.0394,  0.0206, -0.0111, -0.0015, -0.0266, -0.0350, -0.0260, -0.0239,\n",
       "                      -0.0593,  0.0032, -0.0083, -0.0173, -0.0234, -0.0350, -0.0183, -0.0300,\n",
       "                      -0.0256,  0.0039, -0.0787,  0.0114,  0.0215, -0.0155, -0.0315, -0.0309,\n",
       "                      -0.0137, -0.0085, -0.0109, -0.0064, -0.0150, -0.0279, -0.0133,  0.0078,\n",
       "                      -0.0407,  0.0063, -0.0182, -0.0263, -0.0095, -0.0239, -0.0164, -0.0449,\n",
       "                      -0.0009, -0.0360, -0.0047, -0.0435, -0.0177, -0.0026, -0.0215, -0.0238,\n",
       "                      -0.0129, -0.0265, -0.0386,  0.0056, -0.0116, -0.0422, -0.0086, -0.0180,\n",
       "                      -0.0178, -0.0184, -0.0262,  0.0039, -0.0020, -0.0007, -0.0273, -0.0142,\n",
       "                      -0.0139, -0.0243, -0.0248, -0.0305, -0.0053, -0.0212,  0.0002, -0.0055,\n",
       "                      -0.0227, -0.0095, -0.0143,  0.0026, -0.0327, -0.0091, -0.0245, -0.0024,\n",
       "                      -0.0043, -0.0169,  0.0052,  0.0073,  0.0421,  0.0086, -0.0032, -0.0286,\n",
       "                       0.0094, -0.0219, -0.0013, -0.0282,  0.0149, -0.0329,  0.0107,  0.0048,\n",
       "                      -0.0183, -0.0310, -0.0035, -0.0293, -0.0235,  0.0251, -0.0468, -0.0528,\n",
       "                      -0.0221, -0.0281, -0.0056, -0.0333, -0.0252, -0.0162, -0.0210, -0.0323,\n",
       "                       0.0018, -0.0146,  0.0020, -0.0400, -0.0360, -0.0004, -0.0393, -0.0122,\n",
       "                      -0.0101, -0.0167, -0.0243, -0.0118, -0.0275, -0.0140, -0.0216, -0.0545,\n",
       "                      -0.0265,  0.0046, -0.0139,  0.0017, -0.0050, -0.0234, -0.0153, -0.0292,\n",
       "                      -0.0202, -0.0100, -0.0206, -0.0253, -0.0329, -0.0130, -0.0086, -0.0147,\n",
       "                      -0.0477,  0.0002, -0.0620, -0.0207, -0.0286, -0.0230, -0.0004, -0.0164,\n",
       "                      -0.0161, -0.0196, -0.0094, -0.0038, -0.0311, -0.0451, -0.0351, -0.0085,\n",
       "                      -0.0161,  0.0087, -0.0093, -0.0481,  0.0120, -0.0211, -0.0066, -0.0278])),\n",
       "             ('layer3.4.bn1.running_var',\n",
       "              tensor([1.0033e-03, 1.0306e-03, 7.7703e-04, 1.9312e-03, 1.4353e-03, 8.3742e-04,\n",
       "                      1.5428e-03, 1.9609e-03, 2.8304e-03, 2.0320e-03, 1.7028e-03, 1.2530e-03,\n",
       "                      2.0178e-03, 3.2211e-03, 1.9027e-03, 1.4949e-03, 2.4248e-03, 5.3207e-04,\n",
       "                      3.5384e-03, 1.2587e-03, 2.7738e-03, 2.0519e-03, 1.0004e-03, 1.3701e-03,\n",
       "                      3.1151e-03, 2.1015e-03, 4.0926e-04, 9.4530e-04, 1.5437e-03, 2.3591e-03,\n",
       "                      3.1391e-03, 1.9750e-03, 2.3978e-03, 5.2618e-04, 2.0129e-03, 1.5367e-03,\n",
       "                      1.4484e-03, 8.1307e-04, 1.9073e-03, 1.1830e-03, 3.2957e-03, 2.2539e-03,\n",
       "                      1.4671e-03, 2.4714e-03, 2.2772e-03, 1.7236e-03, 1.4590e-03, 1.4405e-03,\n",
       "                      1.7102e-03, 1.8279e-03, 1.0440e-03, 2.0959e-03, 6.6593e-04, 1.6200e-03,\n",
       "                      7.8740e-04, 3.4767e-03, 1.4401e-03, 5.3004e-04, 1.3329e-03, 3.4827e-04,\n",
       "                      1.6916e-03, 9.8311e-04, 1.2799e-03, 2.5544e-03, 5.8062e-04, 1.1613e-03,\n",
       "                      1.4369e-03, 2.1749e-03, 8.7915e-04, 1.1629e-03, 1.4503e-03, 1.6241e-03,\n",
       "                      1.1220e-03, 1.7172e-03, 1.9400e-03, 1.6778e-03, 1.6625e-03, 2.3305e-03,\n",
       "                      2.4845e-03, 2.0803e-03, 1.4375e-03, 1.8747e-03, 1.4168e-03, 1.4020e-03,\n",
       "                      1.1201e-03, 2.1372e-03, 1.6811e-03, 1.8911e-03, 2.5925e-03, 2.3445e-03,\n",
       "                      1.5483e-03, 9.5729e-06, 1.5274e-03, 2.4202e-03, 1.6749e-03, 1.6043e-03,\n",
       "                      3.5331e-03, 1.9262e-03, 8.3216e-04, 8.4603e-04, 1.9796e-03, 1.1558e-03,\n",
       "                      1.2912e-03, 1.8131e-03, 2.3547e-03, 1.7523e-03, 2.3896e-03, 1.5842e-03,\n",
       "                      2.1032e-03, 1.4066e-03, 1.0879e-03, 4.4114e-03, 5.7905e-04, 2.5029e-03,\n",
       "                      1.7628e-03, 3.0383e-03, 1.6292e-03, 8.7994e-04, 1.0890e-03, 3.8819e-03,\n",
       "                      1.5487e-03, 1.3805e-03, 2.5458e-03, 1.7214e-03, 2.9694e-03, 1.9341e-03,\n",
       "                      1.5556e-03, 2.8015e-03, 7.9694e-04, 1.3001e-03, 9.1675e-04, 1.9163e-03,\n",
       "                      1.5802e-03, 1.4241e-03, 2.2368e-03, 1.1907e-03, 1.1721e-03, 2.1161e-03,\n",
       "                      3.0827e-03, 1.5271e-03, 1.3522e-03, 1.1856e-03, 1.8052e-03, 1.8248e-03,\n",
       "                      1.5534e-03, 1.1789e-03, 1.6496e-03, 1.7617e-03, 7.8118e-04, 1.7622e-03,\n",
       "                      2.2967e-03, 2.1377e-03, 1.2352e-03, 1.1007e-03, 2.2463e-03, 2.6439e-03,\n",
       "                      8.3055e-04, 1.3058e-03, 5.3716e-04, 1.2169e-03, 3.1546e-03, 2.0172e-03,\n",
       "                      1.4842e-03, 9.2183e-04, 2.1270e-03, 6.7502e-04, 1.1597e-03, 2.1920e-03,\n",
       "                      3.9506e-04, 4.5555e-04, 1.0756e-03, 5.7179e-04, 2.6134e-03, 1.4023e-03,\n",
       "                      1.5236e-03, 2.9975e-03, 6.7807e-04, 1.1807e-03, 1.8592e-03, 1.6805e-03,\n",
       "                      8.2744e-04, 2.0469e-03, 1.7879e-03, 1.5594e-03, 2.0548e-03, 2.3991e-03,\n",
       "                      2.2397e-03, 1.1592e-03, 1.4019e-03, 3.3625e-03, 3.4296e-03, 2.3362e-03,\n",
       "                      1.4733e-03, 2.0680e-03, 2.4625e-03, 1.5233e-03, 1.8550e-03, 1.6877e-03,\n",
       "                      1.6484e-03, 2.7355e-03, 3.5808e-04, 9.1218e-04, 1.2652e-03, 4.5359e-03,\n",
       "                      1.5470e-03, 2.5477e-03, 2.8136e-03, 1.5053e-03, 2.1872e-03, 1.5471e-03,\n",
       "                      1.5046e-03, 1.0155e-03, 1.7046e-03, 1.9653e-03, 9.6144e-04, 2.2701e-03,\n",
       "                      1.3603e-03, 1.5593e-03, 1.7330e-03, 7.7079e-04, 4.0291e-04, 1.9144e-03,\n",
       "                      1.1368e-03, 8.2288e-04, 1.1754e-03, 3.1680e-03, 1.1584e-03, 6.7981e-04,\n",
       "                      2.3538e-03, 2.6342e-03, 1.7650e-03, 1.1500e-03, 3.2013e-03, 1.0963e-03,\n",
       "                      3.0741e-03, 2.2452e-03, 1.5866e-03, 1.8135e-03, 8.7238e-04, 2.4578e-03,\n",
       "                      3.5100e-03, 2.2454e-03, 1.8858e-03, 8.6939e-04, 2.6434e-03, 2.9836e-03,\n",
       "                      1.2545e-03, 2.6668e-03, 1.4242e-03, 1.5875e-03, 2.6865e-03, 2.1684e-03,\n",
       "                      9.2949e-04, 1.8911e-03, 2.6270e-03, 1.6447e-03])),\n",
       "             ('layer3.4.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.4.conv2.weight',\n",
       "              tensor([[[[ 6.6312e-03,  8.7402e-04, -9.2299e-03],\n",
       "                        [ 2.1471e-03,  1.2544e-03, -3.7032e-03],\n",
       "                        [ 1.9820e-04, -4.7284e-03, -7.3731e-03]],\n",
       "              \n",
       "                       [[-5.0908e-03, -5.0824e-04,  2.1801e-05],\n",
       "                        [-1.5964e-03, -4.1113e-03, -6.7233e-03],\n",
       "                        [ 7.9506e-03,  1.0281e-02, -7.9181e-04]],\n",
       "              \n",
       "                       [[ 6.9810e-03,  1.1890e-02,  3.4489e-03],\n",
       "                        [-8.0479e-03,  1.5470e-03, -3.1353e-03],\n",
       "                        [-7.5890e-03,  4.0780e-03,  1.6792e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.8865e-03,  1.5176e-02,  9.3762e-03],\n",
       "                        [-3.5413e-03, -6.6392e-03,  4.3605e-03],\n",
       "                        [ 7.1644e-03,  6.4162e-03,  1.2849e-02]],\n",
       "              \n",
       "                       [[ 1.1596e-02,  1.9576e-02,  1.8550e-02],\n",
       "                        [-1.6640e-03,  5.7979e-03,  5.0456e-03],\n",
       "                        [ 5.1158e-03,  1.0310e-02,  3.9509e-03]],\n",
       "              \n",
       "                       [[-1.5507e-02,  6.0720e-03,  4.4031e-03],\n",
       "                        [ 4.6463e-03,  1.2962e-02, -4.5340e-03],\n",
       "                        [ 1.3907e-02,  6.8936e-03, -1.4178e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.4871e-03,  1.1812e-03, -3.9864e-03],\n",
       "                        [ 6.1908e-03,  4.1131e-03, -3.9257e-03],\n",
       "                        [ 2.4163e-03,  3.3188e-03, -2.1072e-03]],\n",
       "              \n",
       "                       [[-1.3993e-03,  7.0618e-04,  1.0996e-03],\n",
       "                        [ 1.3229e-03,  2.8913e-03, -1.1027e-03],\n",
       "                        [ 1.3404e-03, -2.0827e-03,  1.3700e-03]],\n",
       "              \n",
       "                       [[-1.8141e-03, -1.3115e-03,  2.8828e-04],\n",
       "                        [-2.2612e-03, -1.1718e-03, -1.9323e-03],\n",
       "                        [-6.1866e-03,  1.9995e-03, -5.4664e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2493e-03, -1.6003e-03, -4.2592e-04],\n",
       "                        [-1.7418e-03, -2.1454e-03,  3.7999e-03],\n",
       "                        [-1.6196e-04,  1.2758e-03,  4.4156e-03]],\n",
       "              \n",
       "                       [[-2.8758e-04,  2.3626e-03,  3.8112e-03],\n",
       "                        [-1.1388e-03, -2.7848e-03, -1.2255e-03],\n",
       "                        [-1.9540e-03, -6.8936e-03, -1.0940e-03]],\n",
       "              \n",
       "                       [[ 7.6607e-04,  3.1258e-03, -3.7381e-03],\n",
       "                        [ 5.3237e-04,  5.2054e-03, -3.7932e-03],\n",
       "                        [-5.7646e-03,  2.0564e-03, -3.9538e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.5421e-03,  6.9860e-03,  1.8629e-02],\n",
       "                        [ 9.2924e-06, -6.8840e-03,  1.3853e-02],\n",
       "                        [-3.2116e-03, -5.0172e-03,  4.6233e-03]],\n",
       "              \n",
       "                       [[-3.1879e-03, -4.8324e-03,  1.6428e-02],\n",
       "                        [-3.5149e-03,  5.2253e-03,  2.7931e-03],\n",
       "                        [ 3.3086e-04,  7.8351e-03, -5.1446e-03]],\n",
       "              \n",
       "                       [[-6.5017e-03,  4.1304e-04, -3.3133e-03],\n",
       "                        [ 5.7889e-03, -3.5841e-03, -5.2805e-03],\n",
       "                        [-1.0912e-02, -6.3956e-03,  4.0157e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3385e-02,  1.6469e-02,  2.0118e-03],\n",
       "                        [ 1.3596e-02, -2.7459e-03, -5.3150e-03],\n",
       "                        [ 1.1239e-03,  3.4623e-03,  1.0270e-02]],\n",
       "              \n",
       "                       [[ 2.8657e-02, -2.2163e-02, -2.2293e-02],\n",
       "                        [ 1.5024e-02, -2.0264e-02, -2.0417e-02],\n",
       "                        [-1.3439e-02, -1.6755e-02, -8.1157e-03]],\n",
       "              \n",
       "                       [[-6.4554e-03, -2.9897e-03,  1.3476e-02],\n",
       "                        [-1.6444e-02, -3.9970e-03,  6.6601e-03],\n",
       "                        [-1.5089e-02, -1.1615e-02, -6.6545e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.3292e-03,  1.2857e-03,  3.5140e-03],\n",
       "                        [-8.5027e-03, -1.0713e-03,  2.8116e-03],\n",
       "                        [-4.1398e-03, -4.0412e-04,  1.2303e-02]],\n",
       "              \n",
       "                       [[-4.8225e-03, -8.5100e-03, -6.5973e-03],\n",
       "                        [-3.2457e-03, -5.4422e-03,  3.5226e-03],\n",
       "                        [ 3.2731e-03, -7.1500e-03,  3.1513e-03]],\n",
       "              \n",
       "                       [[ 5.5200e-03,  3.9697e-03,  4.1432e-04],\n",
       "                        [-2.4832e-04,  2.6919e-03, -2.7915e-03],\n",
       "                        [-5.1180e-03, -5.5352e-03, -1.0046e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.3555e-03, -2.0953e-03, -4.9571e-03],\n",
       "                        [-1.1617e-02, -7.7155e-03, -7.8359e-03],\n",
       "                        [-1.2945e-02, -6.7538e-03, -9.6061e-03]],\n",
       "              \n",
       "                       [[-3.2054e-03, -8.9805e-03, -3.8876e-03],\n",
       "                        [-4.3283e-03, -6.8814e-03, -1.7051e-03],\n",
       "                        [-3.8269e-03, -6.1511e-03,  3.2561e-03]],\n",
       "              \n",
       "                       [[ 2.5150e-04,  1.2130e-03,  2.3671e-03],\n",
       "                        [ 5.2506e-04,  7.3957e-03,  8.8633e-03],\n",
       "                        [ 6.1350e-03,  9.0859e-03,  7.1383e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5280e-05, -3.4017e-04, -3.1101e-04],\n",
       "                        [-3.8937e-03, -5.7256e-03,  1.1165e-03],\n",
       "                        [-1.1690e-03, -5.3596e-03, -4.8331e-03]],\n",
       "              \n",
       "                       [[-8.2525e-03, -1.0435e-02, -1.6948e-03],\n",
       "                        [-4.1391e-03,  2.6877e-04, -1.6648e-03],\n",
       "                        [ 3.6110e-03,  6.1394e-03, -1.6682e-03]],\n",
       "              \n",
       "                       [[-2.8562e-04, -2.5530e-03, -2.5274e-03],\n",
       "                        [ 6.7021e-04,  1.7912e-03,  1.6535e-03],\n",
       "                        [-1.6524e-03,  7.1637e-04,  2.2429e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.6277e-03,  4.0695e-03, -7.6448e-03],\n",
       "                        [-2.0813e-03, -4.6948e-04, -3.6877e-03],\n",
       "                        [ 2.2722e-03, -1.9694e-04,  4.4372e-03]],\n",
       "              \n",
       "                       [[ 3.4022e-03,  8.2664e-03,  1.3298e-03],\n",
       "                        [-5.5817e-03, -2.9223e-03,  4.1660e-03],\n",
       "                        [-6.3529e-04, -4.4321e-03, -2.6645e-03]],\n",
       "              \n",
       "                       [[-2.7201e-03, -2.5345e-03,  9.1730e-06],\n",
       "                        [-1.3490e-03, -2.5006e-03, -2.4646e-04],\n",
       "                        [ 9.1450e-03,  2.6441e-03, -1.0085e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7775e-03, -3.0037e-03, -2.4508e-03],\n",
       "                        [ 8.1210e-03,  2.9405e-03,  1.0664e-02],\n",
       "                        [ 4.3054e-03,  6.3242e-03,  1.0215e-02]],\n",
       "              \n",
       "                       [[ 1.0118e-02,  1.1268e-03, -1.7389e-02],\n",
       "                        [ 2.3447e-03, -5.1443e-03,  1.5743e-03],\n",
       "                        [-6.3486e-03, -5.3476e-03,  5.4499e-03]],\n",
       "              \n",
       "                       [[-1.0023e-02, -2.4717e-04,  1.3975e-02],\n",
       "                        [-1.2463e-02, -4.7307e-03,  1.2053e-02],\n",
       "                        [-6.5474e-03, -2.8671e-03,  2.0827e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.2485e-03, -1.0836e-02,  7.8543e-03],\n",
       "                        [-3.5097e-03,  3.4338e-03, -5.1824e-04],\n",
       "                        [ 2.4607e-03, -1.1414e-02, -1.2170e-02]],\n",
       "              \n",
       "                       [[-1.5152e-02,  5.0266e-03,  1.8637e-02],\n",
       "                        [-3.1204e-03, -2.2164e-03, -1.1807e-02],\n",
       "                        [ 8.0354e-03,  5.5603e-03, -1.3138e-02]],\n",
       "              \n",
       "                       [[ 6.2100e-03,  2.5457e-03, -6.8121e-03],\n",
       "                        [ 5.1127e-03,  2.6938e-03, -4.4245e-03],\n",
       "                        [-7.9925e-03,  6.5940e-03, -8.3433e-04]]]])),\n",
       "             ('layer3.4.bn2.weight',\n",
       "              tensor([ 1.2694e-01,  2.9070e-02,  1.1941e-01,  8.1868e-02,  4.1079e-02,\n",
       "                       9.4870e-02,  5.6510e-02,  6.7449e-02,  1.2023e-01,  8.3007e-02,\n",
       "                       8.1117e-02,  9.7701e-02,  5.7207e-02,  7.1964e-02,  8.0143e-02,\n",
       "                       1.1707e-01,  9.2151e-02,  1.2011e-01,  8.1078e-02,  8.4724e-02,\n",
       "                       4.7895e-02,  6.5819e-02,  1.0903e-01,  7.3794e-02,  1.0311e-01,\n",
       "                       5.4439e-02,  1.0625e-03,  8.6051e-04,  7.0587e-02,  6.0743e-02,\n",
       "                       5.1180e-02,  9.6180e-02,  7.3738e-02,  4.8449e-02,  8.4653e-02,\n",
       "                       8.2475e-02,  9.0994e-02,  1.0397e-01,  7.7953e-02,  9.2667e-02,\n",
       "                       7.2621e-02,  8.4112e-02,  7.1418e-02,  1.2176e-01,  9.7236e-02,\n",
       "                       9.2851e-02,  9.4896e-02,  9.4035e-02,  9.4417e-02,  6.4962e-02,\n",
       "                       9.8946e-02,  9.2824e-02,  1.1046e-01,  8.4446e-02,  1.2602e-01,\n",
       "                       4.9078e-02,  8.0478e-02,  8.9983e-02,  1.0361e-01,  5.7519e-02,\n",
       "                       9.3742e-02,  1.2114e-01,  8.8800e-02,  7.5936e-02,  6.7874e-02,\n",
       "                       1.1229e-01,  1.2851e-01,  7.4028e-02,  4.5742e-02,  1.2315e-01,\n",
       "                       9.7189e-02,  5.3212e-02,  7.5857e-02,  8.8676e-02,  1.0762e-01,\n",
       "                       7.4455e-02,  1.0338e-01,  1.1295e-01,  1.0540e-01,  8.9597e-02,\n",
       "                       1.0981e-01,  1.1043e-01,  7.3583e-02,  9.8326e-02,  1.1685e-01,\n",
       "                       7.8340e-02,  1.0151e-01,  8.5427e-02,  6.3128e-02,  7.8476e-02,\n",
       "                       7.2672e-02,  1.0882e-01,  7.4854e-02,  1.0074e-01,  9.4615e-02,\n",
       "                       6.3024e-02,  1.0054e-01,  8.0624e-02,  1.0173e-01,  8.6052e-02,\n",
       "                       1.0241e-01,  9.6743e-02,  7.1833e-02,  1.2998e-01,  9.5821e-02,\n",
       "                       8.6202e-02,  9.3370e-02,  1.0488e-01,  1.0451e-01,  1.0046e-01,\n",
       "                       9.4409e-02,  7.3180e-02,  5.7808e-02,  7.8035e-02,  1.3017e-01,\n",
       "                       7.6251e-02,  7.3065e-02,  7.6708e-02,  4.9350e-02,  1.1338e-01,\n",
       "                       5.2739e-02,  1.2900e-01,  8.7235e-02,  7.2630e-02,  1.1336e-01,\n",
       "                       8.5856e-02,  6.8211e-02,  8.8288e-02,  9.1277e-02,  1.1341e-01,\n",
       "                       6.2520e-02,  8.7185e-02,  3.5655e-02,  8.1199e-02,  6.8371e-02,\n",
       "                       9.3866e-02,  9.0834e-02,  3.9026e-02,  1.0135e-01,  6.8129e-02,\n",
       "                       1.0597e-01,  6.2812e-02,  6.2828e-02,  7.7376e-02, -4.1027e-06,\n",
       "                       8.1299e-02,  9.5384e-02,  9.3152e-02,  6.4053e-02,  1.0875e-01,\n",
       "                       1.2131e-01,  6.9648e-02,  7.5185e-02,  8.3945e-02,  8.8896e-02,\n",
       "                       1.1853e-01,  1.2431e-01,  1.0717e-01,  9.3237e-02,  8.6035e-02,\n",
       "                       1.0146e-01,  6.5037e-02,  7.2925e-02,  3.4311e-02,  3.2722e-02,\n",
       "                       7.7869e-02,  9.4757e-02,  1.0742e-01,  1.3737e-01,  8.0130e-02,\n",
       "                       9.5686e-02,  1.0663e-01,  7.7148e-02,  1.2844e-01,  1.0475e-01,\n",
       "                       3.8554e-02,  8.4014e-02,  8.2912e-02,  7.0467e-02,  1.1187e-01,\n",
       "                       8.3563e-02,  6.9191e-02,  1.0834e-01,  9.0776e-02,  9.9224e-02,\n",
       "                       7.8921e-02,  1.0851e-01,  9.3507e-02,  6.4497e-02,  1.0177e-01,\n",
       "                       7.0652e-02,  1.0245e-01,  1.0764e-01,  7.5383e-02,  5.9132e-02,\n",
       "                       1.0772e-01,  8.9342e-02,  7.9533e-02,  6.8005e-02,  1.1815e-04,\n",
       "                       8.1040e-02,  1.0782e-01,  6.9496e-02,  5.6651e-02,  9.8843e-02,\n",
       "                       8.0277e-02,  9.8814e-02,  6.8517e-02,  9.7996e-02,  1.1119e-01,\n",
       "                       8.5643e-02,  8.0577e-02,  7.5336e-02,  5.9425e-02,  7.6083e-02,\n",
       "                       8.3981e-02,  9.7648e-02,  9.5991e-02,  5.4396e-03,  1.2704e-01,\n",
       "                       6.0335e-02,  1.2897e-01,  1.1573e-01,  7.7359e-02,  7.0929e-02,\n",
       "                       1.0237e-01,  6.7552e-02,  1.0148e-01,  1.0319e-01,  8.6852e-02,\n",
       "                       9.0744e-02,  6.6547e-02,  4.8620e-02,  1.1532e-01,  7.1698e-03,\n",
       "                       9.7996e-02,  1.1462e-01,  9.2761e-02,  1.0325e-01,  8.8403e-02,\n",
       "                       8.1276e-02,  1.0546e-01,  5.0304e-03,  8.9671e-02,  8.9332e-02,\n",
       "                       1.0255e-01,  8.9851e-02,  8.2526e-02,  1.0198e-01,  7.1623e-02,\n",
       "                       8.5765e-02,  7.7883e-02,  1.0560e-01,  8.1604e-02,  7.3136e-02,\n",
       "                       1.2687e-01])),\n",
       "             ('layer3.4.bn2.bias',\n",
       "              tensor([-1.1455e-01,  1.8293e-03, -9.6076e-02, -4.5633e-02, -5.5230e-02,\n",
       "                      -9.2613e-02, -2.5221e-02, -5.9012e-02, -9.9159e-02, -6.9604e-02,\n",
       "                      -6.5718e-02, -7.0968e-02, -2.3784e-02, -4.6835e-02, -3.3279e-02,\n",
       "                      -1.0754e-01, -5.0642e-02, -7.5042e-02, -1.7110e-02, -6.3421e-02,\n",
       "                      -2.1140e-02, -5.4855e-02, -8.1559e-02, -4.2248e-02, -9.1367e-02,\n",
       "                      -4.0779e-02, -1.3410e-02, -9.4261e-03, -4.1635e-02, -3.2971e-02,\n",
       "                      -4.5974e-02, -7.8734e-02, -7.2940e-02, -4.8643e-02, -4.4670e-02,\n",
       "                      -8.7057e-02, -7.3516e-02, -8.3509e-02, -5.9190e-02, -5.2176e-02,\n",
       "                      -2.5754e-02, -6.8297e-02, -6.7675e-02, -8.3740e-02, -1.0091e-01,\n",
       "                      -7.9014e-02, -5.8069e-02, -4.4958e-02, -6.7736e-02, -7.0937e-02,\n",
       "                      -9.1302e-02, -5.2353e-02, -8.6006e-02, -7.2020e-02, -1.0977e-01,\n",
       "                      -2.6148e-02, -8.5207e-02, -5.1705e-02, -7.2420e-02, -4.6744e-02,\n",
       "                      -9.0142e-02, -7.2621e-02, -7.0439e-02, -5.3542e-02, -4.2599e-02,\n",
       "                      -1.2235e-01, -7.6272e-02, -2.0820e-02, -2.0871e-02, -1.0731e-01,\n",
       "                      -8.8472e-02, -6.3510e-02, -6.6487e-02, -2.7250e-02, -6.5146e-02,\n",
       "                      -3.1665e-02, -3.6687e-02, -8.1061e-02, -9.4426e-02, -4.4099e-02,\n",
       "                      -1.0412e-01, -3.0821e-02, -1.6619e-04, -6.3634e-02, -8.7291e-02,\n",
       "                      -4.3542e-02, -7.1361e-02, -5.6848e-02, -3.8396e-02, -6.2369e-02,\n",
       "                      -5.2955e-02, -7.7228e-02, -4.1909e-02, -5.5470e-02, -1.2026e-01,\n",
       "                      -4.1640e-02, -5.3245e-02, -7.4726e-02, -1.0932e-01, -6.7656e-02,\n",
       "                      -7.5504e-02, -8.4807e-02, -5.9894e-02, -1.1853e-01, -6.4240e-02,\n",
       "                      -7.0289e-02, -3.8163e-02, -9.4083e-02, -5.1529e-02, -7.5903e-02,\n",
       "                      -6.1068e-02, -5.0540e-02, -2.6226e-02, -1.6169e-02, -2.0686e-01,\n",
       "                      -5.6720e-02, -2.3477e-02, -7.1027e-02,  9.9922e-03, -1.0468e-01,\n",
       "                      -1.6855e-02, -8.1813e-02, -5.0578e-02, -3.5497e-02, -9.1750e-02,\n",
       "                      -7.1587e-02, -1.4966e-02, -7.7310e-02, -9.2429e-02, -1.3546e-01,\n",
       "                      -3.4877e-02, -1.1502e-01, -1.7780e-02, -8.2976e-02, -5.2367e-02,\n",
       "                      -7.6191e-02, -4.1553e-02, -7.9299e-03, -7.9562e-02, -7.0033e-02,\n",
       "                      -1.0201e-01, -6.2264e-02, -5.9939e-02, -4.0655e-02, -1.8636e-05,\n",
       "                      -8.3404e-02, -5.7756e-02, -6.9118e-02, -5.7405e-02, -1.0229e-01,\n",
       "                      -1.1565e-01, -5.4345e-02, -8.1424e-02, -5.9170e-02, -6.4213e-02,\n",
       "                      -7.8159e-02, -1.0424e-01, -8.3076e-02, -7.9115e-02, -6.2290e-02,\n",
       "                      -1.2803e-01, -3.8668e-02, -4.0278e-02, -2.3669e-02, -1.9956e-02,\n",
       "                      -4.7222e-02, -8.1997e-02, -8.1572e-02, -7.0008e-02, -6.3615e-02,\n",
       "                      -5.9065e-02, -1.0246e-01, -5.1903e-02, -9.3251e-02, -9.1729e-02,\n",
       "                      -2.5029e-02, -8.0617e-02, -6.6326e-02, -4.0949e-02, -1.1062e-01,\n",
       "                      -7.6099e-02, -4.7062e-02, -9.5979e-02, -5.3352e-02, -9.2492e-02,\n",
       "                      -6.1939e-02, -7.3458e-02, -6.6089e-02, -2.5140e-03, -7.9519e-02,\n",
       "                      -7.6527e-02, -8.0842e-02, -8.4869e-02, -5.1581e-02, -4.1605e-02,\n",
       "                      -5.4311e-02, -6.1045e-02, -5.8090e-02, -5.7563e-02, -8.0352e-04,\n",
       "                      -4.1114e-02, -8.0017e-02, -6.0374e-02, -5.9887e-02, -3.1753e-02,\n",
       "                      -7.0743e-02, -7.3248e-02, -3.8514e-02, -9.3118e-02, -1.3940e-01,\n",
       "                      -4.6190e-02, -6.1973e-02, -5.9993e-02, -3.5347e-02, -5.1908e-02,\n",
       "                      -2.9533e-02, -5.7273e-02, -6.9247e-02, -6.1883e-02, -1.0111e-01,\n",
       "                      -3.1392e-02, -1.4038e-01, -8.1244e-02, -7.7826e-02, -2.2362e-02,\n",
       "                      -1.1191e-01, -1.1836e-02, -1.0116e-01, -8.3790e-02, -5.9595e-02,\n",
       "                      -6.0568e-02, -5.7831e-02, -6.0601e-02, -8.0519e-02, -4.7967e-02,\n",
       "                      -7.7059e-02, -1.2731e-01, -9.6666e-02, -5.6871e-02, -6.4831e-02,\n",
       "                      -4.2877e-02, -7.1614e-02, -4.8196e-02, -5.3137e-02, -5.8510e-02,\n",
       "                      -5.5091e-02, -6.2151e-02, -2.8645e-02, -8.6625e-02, -5.1409e-02,\n",
       "                      -8.4554e-02, -8.1307e-02, -1.4137e-01, -3.6087e-02, -5.2427e-02,\n",
       "                      -9.1406e-02])),\n",
       "             ('layer3.4.bn2.running_mean',\n",
       "              tensor([ 3.4243e-02, -1.4592e-02, -4.7984e-02, -2.1141e-02, -8.4378e-03,\n",
       "                      -2.3697e-02, -1.6736e-02,  4.3820e-03, -5.1132e-02, -1.4130e-02,\n",
       "                      -2.3668e-02, -2.7488e-02, -4.8414e-03,  4.2051e-04, -1.8927e-02,\n",
       "                      -4.1107e-02, -6.5100e-03, -6.9770e-03, -1.2312e-02, -1.7269e-02,\n",
       "                       7.9072e-03, -3.6618e-02, -1.4385e-02, -1.6069e-02, -2.2014e-02,\n",
       "                      -8.0841e-03, -2.7215e-03, -3.0561e-06, -3.1228e-02, -4.4337e-03,\n",
       "                      -2.0547e-02, -3.5930e-02, -2.4449e-02, -8.6375e-03, -1.8039e-02,\n",
       "                      -5.0705e-03, -4.8483e-02, -3.5471e-02, -1.3905e-02, -5.1397e-02,\n",
       "                      -1.8147e-02, -2.3986e-02, -4.9834e-03, -4.5023e-02, -2.7037e-02,\n",
       "                      -4.6328e-04, -1.1810e-02, -4.1675e-02, -2.7550e-02, -3.4684e-02,\n",
       "                      -2.9719e-02, -3.7902e-02, -1.9902e-02, -1.7706e-02, -4.9803e-02,\n",
       "                      -1.7674e-02, -1.0731e-02, -3.6059e-02, -2.3568e-02, -1.1095e-02,\n",
       "                      -4.2469e-02, -5.9330e-02, -2.8556e-02, -3.3881e-02, -1.5938e-02,\n",
       "                      -1.1914e-02, -5.7777e-02, -1.5445e-02, -1.7311e-02, -6.5726e-02,\n",
       "                      -5.3551e-02, -6.4324e-03,  1.7363e-02, -2.9872e-02, -2.6585e-02,\n",
       "                      -4.5799e-02, -2.0238e-02, -2.3390e-03, -4.4216e-02, -2.6964e-03,\n",
       "                      -3.2196e-02, -2.9929e-02, -3.0395e-02, -1.0472e-02, -2.6904e-02,\n",
       "                      -2.9303e-02, -5.3298e-02, -3.8037e-03, -1.6990e-02, -1.6414e-02,\n",
       "                      -2.7738e-02, -2.7287e-02,  1.1014e-02, -3.7268e-02,  5.4164e-02,\n",
       "                      -2.5663e-02, -3.1924e-02, -2.4132e-02, -1.7135e-02, -4.2704e-02,\n",
       "                      -2.3148e-02,  1.6121e-02, -2.0517e-02, -5.7686e-02,  3.3023e-02,\n",
       "                      -1.9160e-02, -2.6976e-02, -4.0690e-02, -2.7059e-02, -1.5973e-02,\n",
       "                      -5.0736e-02, -1.8595e-02,  1.6811e-02, -6.8619e-03,  2.6197e-02,\n",
       "                      -3.3649e-03, -1.5244e-02, -9.5015e-03, -1.8802e-02, -1.6822e-02,\n",
       "                      -2.1455e-02, -3.4423e-02, -1.3009e-02, -2.5068e-02, -2.7942e-02,\n",
       "                       5.9927e-03, -3.1455e-02,  7.8567e-03, -3.0361e-02, -5.4240e-02,\n",
       "                      -2.3285e-02, -2.7011e-02, -6.0236e-03, -6.7553e-03,  5.5428e-03,\n",
       "                      -2.5544e-02, -3.5095e-02,  2.7091e-03, -3.3853e-02, -9.0658e-04,\n",
       "                      -2.5914e-02, -3.5149e-02, -1.4211e-02, -1.0007e-02, -1.2268e-05,\n",
       "                      -5.4151e-02, -2.6648e-02, -3.1837e-02, -4.5779e-03, -2.2861e-02,\n",
       "                      -5.3461e-02, -2.6959e-03, -1.6187e-02, -1.4721e-02,  1.0035e-03,\n",
       "                      -4.4736e-02, -8.4687e-03, -1.4437e-02, -2.5654e-02, -1.2620e-02,\n",
       "                      -1.2814e-02, -1.7263e-02, -2.4736e-03,  3.2244e-03,  4.3489e-03,\n",
       "                      -1.5335e-02, -2.2011e-02, -2.0233e-02, -4.8481e-02, -5.1276e-02,\n",
       "                      -3.3206e-02, -3.2060e-02, -1.1280e-02, -2.6777e-02, -1.2323e-02,\n",
       "                      -1.1868e-02, -1.2126e-02, -2.5187e-02, -2.4449e-02, -2.0850e-02,\n",
       "                       8.8930e-03,  3.2884e-03, -4.9212e-02, -2.2784e-02, -1.8163e-02,\n",
       "                      -1.3191e-02, -4.3650e-02, -2.8393e-02, -2.3171e-02, -1.1790e-02,\n",
       "                      -3.6755e-02, -4.5381e-02, -3.5764e-02,  3.9523e-03, -1.5073e-02,\n",
       "                      -1.9630e-02, -1.1191e-02, -4.2299e-02, -2.1785e-02, -9.8322e-04,\n",
       "                      -2.3188e-02,  8.8214e-03, -2.7595e-02,  2.7750e-03, -3.0374e-03,\n",
       "                      -3.2747e-02, -2.2028e-02, -6.0134e-03, -9.2994e-03, -2.6668e-02,\n",
       "                      -2.4282e-02, -1.3612e-02, -3.2339e-02, -2.0731e-02,  6.5022e-03,\n",
       "                      -1.4098e-02, -2.0438e-02,  5.5362e-02, -1.9705e-02, -4.4099e-02,\n",
       "                      -1.0816e-02, -6.6891e-02, -3.9706e-02, -7.1391e-03, -1.4058e-02,\n",
       "                       3.6076e-02, -2.4583e-02, -1.0677e-02, -1.6567e-02, -4.1326e-02,\n",
       "                      -2.4864e-02, -1.8731e-02,  4.6846e-03, -3.1195e-02, -2.9188e-02,\n",
       "                      -2.5920e-02, -3.6685e-02,  1.8107e-02, -4.5957e-03, -3.2142e-02,\n",
       "                      -1.2073e-02, -2.3967e-02,  2.0457e-04, -3.0794e-02, -2.6430e-02,\n",
       "                      -2.4028e-02, -2.2313e-02, -7.8822e-03, -5.6399e-02, -3.4538e-02,\n",
       "                      -9.6972e-03, -7.4081e-03, -2.3544e-02, -2.0323e-02,  1.0632e-02,\n",
       "                      -1.5281e-02])),\n",
       "             ('layer3.4.bn2.running_var',\n",
       "              tensor([1.6609e-03, 2.9726e-04, 2.4131e-03, 8.6878e-04, 4.4196e-04, 1.0601e-03,\n",
       "                      6.2419e-04, 4.1454e-04, 2.6220e-03, 2.0725e-03, 7.0828e-04, 1.4361e-03,\n",
       "                      3.6489e-04, 4.8766e-04, 8.9064e-04, 1.6577e-03, 1.1024e-03, 1.4144e-03,\n",
       "                      8.4178e-04, 1.1174e-03, 3.3719e-04, 1.0730e-03, 3.2375e-03, 7.7016e-04,\n",
       "                      1.2910e-03, 3.3563e-04, 1.5148e-05, 7.2763e-06, 6.2786e-04, 4.2941e-04,\n",
       "                      5.5553e-04, 1.1382e-03, 7.3311e-04, 2.8180e-04, 9.8849e-04, 8.9193e-04,\n",
       "                      1.1872e-03, 1.6584e-03, 8.0077e-04, 1.7915e-03, 5.9960e-04, 1.1014e-03,\n",
       "                      7.1405e-04, 2.1470e-03, 1.1661e-03, 1.0478e-03, 1.2337e-03, 1.1952e-03,\n",
       "                      9.9952e-04, 8.0250e-04, 1.3399e-03, 1.1859e-03, 1.6807e-03, 1.3792e-03,\n",
       "                      1.7533e-03, 2.8957e-04, 6.9794e-04, 1.0136e-03, 1.7532e-03, 4.6932e-04,\n",
       "                      1.4613e-03, 3.0956e-03, 9.4697e-04, 1.7541e-03, 6.3620e-04, 1.7095e-03,\n",
       "                      2.5751e-03, 6.6513e-04, 3.4571e-04, 2.6480e-03, 1.7040e-03, 3.8115e-04,\n",
       "                      6.3305e-04, 1.5963e-03, 2.3311e-03, 7.9465e-04, 1.3420e-03, 1.6752e-03,\n",
       "                      1.8123e-03, 1.0816e-03, 1.8000e-03, 9.5940e-04, 7.6663e-04, 1.2257e-03,\n",
       "                      2.1912e-03, 7.6147e-04, 2.1141e-03, 7.4812e-04, 6.2143e-04, 5.0681e-04,\n",
       "                      9.1874e-04, 1.2658e-03, 6.0689e-04, 1.7480e-03, 2.2004e-03, 3.5734e-04,\n",
       "                      1.3209e-03, 1.0384e-03, 1.3421e-03, 1.0064e-03, 1.7118e-03, 1.1379e-03,\n",
       "                      1.0026e-03, 2.7231e-03, 1.2237e-03, 1.0673e-03, 2.4903e-03, 1.8759e-03,\n",
       "                      1.2314e-03, 1.4805e-03, 1.2040e-03, 1.2134e-03, 8.7114e-04, 8.9756e-04,\n",
       "                      3.7603e-03, 4.5360e-04, 7.5021e-04, 7.2077e-04, 6.8254e-04, 1.9585e-03,\n",
       "                      1.2029e-03, 1.4148e-03, 8.4918e-04, 6.7384e-04, 1.9014e-03, 8.7731e-04,\n",
       "                      1.1594e-03, 1.3370e-03, 1.3696e-03, 2.2515e-03, 6.4028e-04, 1.3546e-03,\n",
       "                      1.1877e-04, 1.0880e-03, 5.4106e-04, 1.1901e-03, 8.6913e-04, 1.7613e-04,\n",
       "                      1.5100e-03, 8.1157e-04, 2.1397e-03, 8.3495e-04, 5.3481e-04, 7.9576e-04,\n",
       "                      5.8945e-10, 1.6324e-03, 1.2455e-03, 9.8187e-04, 5.2201e-04, 2.1771e-03,\n",
       "                      2.6578e-03, 6.3665e-04, 6.7824e-04, 7.3553e-04, 8.6801e-04, 2.5438e-03,\n",
       "                      1.6865e-03, 1.2411e-03, 1.0897e-03, 1.0624e-03, 1.6792e-03, 6.7717e-04,\n",
       "                      8.1721e-04, 2.4687e-04, 2.8109e-04, 6.3285e-04, 8.1284e-04, 1.8989e-03,\n",
       "                      2.0439e-03, 1.3193e-03, 1.6009e-03, 2.0803e-03, 7.5305e-04, 1.2678e-03,\n",
       "                      1.1277e-03, 1.6655e-04, 1.4452e-03, 8.0046e-04, 6.7824e-04, 1.5751e-03,\n",
       "                      9.7408e-04, 7.1293e-04, 1.7529e-03, 1.1533e-03, 1.3136e-03, 7.2281e-04,\n",
       "                      1.3677e-03, 1.3527e-03, 5.0443e-04, 1.0683e-03, 8.8531e-04, 2.7354e-03,\n",
       "                      1.7222e-03, 5.0288e-04, 4.8081e-04, 1.2694e-03, 8.6774e-04, 1.1750e-03,\n",
       "                      8.6267e-04, 4.4286e-07, 1.0147e-03, 1.5411e-03, 1.0931e-03, 5.4664e-04,\n",
       "                      1.0227e-03, 1.0230e-03, 1.4867e-03, 4.3060e-04, 1.1968e-03, 2.3033e-03,\n",
       "                      7.8751e-04, 7.4378e-04, 9.8994e-04, 6.6066e-04, 5.8710e-04, 7.3379e-04,\n",
       "                      1.5882e-03, 1.9522e-03, 3.1003e-04, 2.4439e-03, 4.2159e-04, 4.1984e-03,\n",
       "                      1.1171e-03, 7.0944e-04, 1.8438e-03, 1.6512e-03, 6.3905e-04, 1.1412e-03,\n",
       "                      1.0381e-03, 1.7206e-03, 1.1958e-03, 5.6617e-04, 3.5525e-04, 1.7579e-03,\n",
       "                      4.0286e-04, 1.1541e-03, 1.8030e-03, 1.3497e-03, 1.8125e-03, 1.7052e-03,\n",
       "                      7.1834e-04, 2.2549e-03, 2.2256e-04, 9.4094e-04, 8.2860e-04, 1.1169e-03,\n",
       "                      1.4445e-03, 6.6868e-04, 1.8351e-03, 7.2658e-04, 1.1928e-03, 8.2201e-04,\n",
       "                      1.8874e-03, 1.2207e-03, 7.8073e-04, 1.6771e-03])),\n",
       "             ('layer3.4.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.4.conv3.weight',\n",
       "              tensor([[[[ 0.0008]],\n",
       "              \n",
       "                       [[-0.0013]],\n",
       "              \n",
       "                       [[-0.0032]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0031]],\n",
       "              \n",
       "                       [[ 0.0019]],\n",
       "              \n",
       "                       [[-0.0011]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0059]],\n",
       "              \n",
       "                       [[-0.0011]],\n",
       "              \n",
       "                       [[-0.0135]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0013]],\n",
       "              \n",
       "                       [[-0.0060]],\n",
       "              \n",
       "                       [[-0.0038]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0051]],\n",
       "              \n",
       "                       [[-0.0049]],\n",
       "              \n",
       "                       [[ 0.0070]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0034]],\n",
       "              \n",
       "                       [[-0.0049]],\n",
       "              \n",
       "                       [[ 0.0122]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0062]],\n",
       "              \n",
       "                       [[-0.0057]],\n",
       "              \n",
       "                       [[ 0.0027]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0055]],\n",
       "              \n",
       "                       [[-0.0112]],\n",
       "              \n",
       "                       [[-0.0028]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0042]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0023]],\n",
       "              \n",
       "                       [[ 0.0021]],\n",
       "              \n",
       "                       [[-0.0017]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0038]],\n",
       "              \n",
       "                       [[-0.0031]],\n",
       "              \n",
       "                       [[-0.0067]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0059]],\n",
       "              \n",
       "                       [[-0.0018]],\n",
       "              \n",
       "                       [[-0.0008]]]])),\n",
       "             ('layer3.4.bn3.weight',\n",
       "              tensor([ 0.0026,  0.0446, -0.0260,  ...,  0.0284, -0.0058,  0.0248])),\n",
       "             ('layer3.4.bn3.bias',\n",
       "              tensor([-0.0366, -0.0340, -0.0200,  ..., -0.0231, -0.0420,  0.0030])),\n",
       "             ('layer3.4.bn3.running_mean',\n",
       "              tensor([-0.0004, -0.0067,  0.0024,  ..., -0.0052,  0.0017, -0.0037])),\n",
       "             ('layer3.4.bn3.running_var',\n",
       "              tensor([5.6101e-06, 6.1093e-05, 1.1898e-05,  ..., 2.9264e-05, 7.7494e-06,\n",
       "                      1.3262e-05])),\n",
       "             ('layer3.4.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.5.conv1.weight',\n",
       "              tensor([[[[-1.1438e-05]],\n",
       "              \n",
       "                       [[ 4.0470e-06]],\n",
       "              \n",
       "                       [[ 6.3770e-06]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.4496e-06]],\n",
       "              \n",
       "                       [[-1.8386e-06]],\n",
       "              \n",
       "                       [[-3.9068e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.1820e-03]],\n",
       "              \n",
       "                       [[-4.3988e-04]],\n",
       "              \n",
       "                       [[ 1.3835e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.0654e-02]],\n",
       "              \n",
       "                       [[ 5.7707e-03]],\n",
       "              \n",
       "                       [[ 6.1470e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6076e-03]],\n",
       "              \n",
       "                       [[-1.4258e-02]],\n",
       "              \n",
       "                       [[-6.9198e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3067e-04]],\n",
       "              \n",
       "                       [[ 7.4786e-03]],\n",
       "              \n",
       "                       [[-4.8700e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.7360e-03]],\n",
       "              \n",
       "                       [[ 1.1107e-02]],\n",
       "              \n",
       "                       [[ 8.4366e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.7181e-03]],\n",
       "              \n",
       "                       [[ 6.6159e-03]],\n",
       "              \n",
       "                       [[ 2.9362e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0081e-03]],\n",
       "              \n",
       "                       [[ 5.8337e-03]],\n",
       "              \n",
       "                       [[ 1.4346e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1747e-03]],\n",
       "              \n",
       "                       [[-4.4132e-03]],\n",
       "              \n",
       "                       [[ 1.3859e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.9150e-03]],\n",
       "              \n",
       "                       [[-1.5143e-02]],\n",
       "              \n",
       "                       [[ 7.1986e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.8468e-02]],\n",
       "              \n",
       "                       [[-8.9391e-03]],\n",
       "              \n",
       "                       [[-7.6149e-04]]]])),\n",
       "             ('layer3.5.bn1.weight',\n",
       "              tensor([2.2770e-06, 7.2545e-02, 1.0169e-01, 1.2463e-01, 9.6872e-02, 8.8420e-02,\n",
       "                      5.8153e-02, 7.9561e-02, 9.8377e-02, 6.5545e-02, 9.2361e-02, 1.0990e-01,\n",
       "                      1.0633e-01, 1.0285e-01, 9.1504e-02, 9.8120e-02, 9.4053e-02, 9.7299e-02,\n",
       "                      6.8425e-02, 8.4229e-02, 8.2946e-02, 7.3499e-02, 1.0462e-01, 7.5082e-02,\n",
       "                      1.1452e-01, 7.5487e-02, 9.7327e-02, 9.0796e-02, 8.7489e-02, 7.3139e-02,\n",
       "                      1.0981e-01, 6.7625e-02, 8.1213e-02, 6.0957e-02, 9.0588e-02, 9.9659e-02,\n",
       "                      4.6614e-02, 7.5878e-02, 9.7058e-02, 7.7137e-02, 1.4492e-01, 8.2315e-02,\n",
       "                      1.2486e-01, 7.1125e-02, 8.2256e-02, 8.3331e-02, 6.9606e-02, 1.0516e-01,\n",
       "                      7.5415e-02, 5.7582e-02, 9.8737e-02, 4.7104e-02, 7.3513e-02, 9.6429e-02,\n",
       "                      4.8030e-02, 9.3088e-02, 9.6360e-02, 7.0015e-02, 1.0052e-01, 1.2930e-01,\n",
       "                      1.0723e-01, 5.1198e-02, 8.6373e-02, 8.5364e-02, 7.3419e-02, 9.5867e-02,\n",
       "                      1.0687e-01, 8.3186e-02, 9.1120e-02, 5.2754e-02, 7.7098e-02, 6.9662e-02,\n",
       "                      8.6788e-02, 7.2853e-02, 7.7292e-02, 8.4503e-02, 9.3620e-02, 8.1267e-02,\n",
       "                      1.3086e-01, 8.1050e-02, 6.7484e-02, 7.1849e-02, 9.0145e-02, 7.6168e-02,\n",
       "                      8.1444e-02, 4.5533e-02, 8.5021e-02, 6.2035e-02, 7.6716e-02, 4.9332e-02,\n",
       "                      6.2805e-02, 8.5967e-02, 6.3680e-02, 7.0831e-02, 1.0026e-01, 6.9186e-02,\n",
       "                      5.9680e-02, 7.4418e-02, 1.1391e-01, 7.2574e-02, 7.2561e-02, 9.8674e-02,\n",
       "                      9.4608e-02, 7.1890e-02, 6.3892e-02, 7.6358e-02, 9.8576e-02, 6.0638e-02,\n",
       "                      9.1197e-02, 7.6790e-02, 9.2867e-02, 9.7207e-02, 9.8197e-02, 8.9736e-02,\n",
       "                      5.2203e-02, 1.1582e-01, 8.9624e-02, 1.0712e-01, 1.2331e-01, 6.5850e-02,\n",
       "                      8.0102e-02, 9.0859e-02, 7.5790e-02, 1.0037e-01, 6.1814e-02, 7.2229e-02,\n",
       "                      1.0411e-01, 7.4781e-02, 8.9507e-02, 8.0825e-02, 1.1045e-01, 1.1550e-01,\n",
       "                      8.9385e-02, 1.1830e-01, 7.4617e-02, 5.2251e-02, 6.5341e-02, 1.0061e-01,\n",
       "                      1.0995e-01, 7.9926e-02, 6.8791e-02, 7.0396e-02, 1.3499e-01, 1.0323e-01,\n",
       "                      9.9146e-02, 8.1408e-02, 9.8674e-02, 8.4075e-02, 8.4095e-02, 1.2675e-01,\n",
       "                      9.0098e-02, 1.0887e-01, 6.6786e-02, 7.7059e-02, 7.5315e-02, 5.4575e-02,\n",
       "                      6.7310e-02, 1.1098e-01, 8.2535e-02, 8.2925e-02, 6.1693e-02, 8.5237e-02,\n",
       "                      9.0944e-02, 9.6260e-02, 7.7416e-02, 9.7239e-02, 9.8541e-02, 8.6490e-02,\n",
       "                      1.1175e-01, 7.5582e-02, 9.1895e-02, 1.0237e-01, 6.1313e-02, 8.1964e-02,\n",
       "                      1.0163e-01, 5.4149e-02, 4.8686e-02, 5.5412e-02, 9.4022e-02, 7.3340e-02,\n",
       "                      5.4631e-02, 9.3671e-02, 5.0660e-02, 7.2798e-02, 7.0373e-02, 6.6679e-02,\n",
       "                      9.0220e-02, 4.8591e-02, 9.0427e-02, 1.0210e-01, 9.6573e-02, 8.1305e-02,\n",
       "                      7.9001e-02, 1.0087e-01, 7.3759e-02, 1.0981e-01, 4.0730e-02, 1.0923e-01,\n",
       "                      8.1415e-02, 1.1269e-01, 9.5404e-02, 8.6658e-02, 7.7785e-02, 9.3414e-02,\n",
       "                      5.4707e-02, 4.7041e-02, 1.0543e-01, 7.5453e-02, 1.0462e-01, 9.0287e-02,\n",
       "                      7.0502e-02, 8.1952e-02, 8.7698e-02, 8.9091e-02, 7.2407e-02, 5.8174e-02,\n",
       "                      8.5312e-02, 7.2966e-02, 9.8355e-02, 8.8475e-02, 9.9666e-02, 7.7149e-02,\n",
       "                      5.2752e-02, 7.3434e-02, 6.5303e-02, 7.6376e-02, 6.9875e-02, 9.0339e-02,\n",
       "                      7.6478e-02, 9.7137e-02, 7.2236e-02, 9.1744e-02, 1.2585e-01, 7.6138e-02,\n",
       "                      8.3352e-02, 7.4511e-02, 8.1600e-02, 1.1579e-01, 1.0330e-01, 9.9795e-02,\n",
       "                      7.7694e-02, 1.1873e-01, 8.7472e-02, 1.0394e-01, 8.7218e-02, 9.1780e-02,\n",
       "                      6.1173e-02, 7.4323e-02, 9.8506e-02, 6.1340e-02, 8.7886e-02, 1.1453e-01,\n",
       "                      8.8696e-02, 5.4980e-02, 9.7476e-02, 7.1956e-02])),\n",
       "             ('layer3.5.bn1.bias',\n",
       "              tensor([-2.4774e-05, -4.1446e-02, -3.7593e-02, -7.8896e-02, -6.4619e-02,\n",
       "                      -4.0012e-02, -3.2563e-02, -3.6653e-03, -4.6150e-02, -3.7574e-02,\n",
       "                      -2.8661e-03, -8.1970e-02, -9.0641e-02, -4.9976e-02, -3.1293e-02,\n",
       "                      -4.6706e-02, -6.8980e-02, -4.0714e-02, -2.1763e-02, -4.1037e-02,\n",
       "                      -3.7172e-02, -4.0196e-02, -3.2330e-02, -5.6724e-03, -5.6060e-02,\n",
       "                      -5.4038e-02, -5.1936e-02, -2.9333e-02, -5.5057e-02, -4.8410e-02,\n",
       "                      -3.7288e-02, -5.3984e-03, -3.8763e-02, -5.6636e-03, -5.2939e-02,\n",
       "                      -7.5845e-02, -1.9130e-02, -5.3545e-02,  1.9686e-03, -3.1074e-02,\n",
       "                      -6.0565e-02, -2.2667e-02, -7.1403e-02, -2.9518e-02,  1.2903e-03,\n",
       "                      -1.9474e-02, -3.3246e-02, -4.2812e-02,  1.1016e-02, -3.4898e-02,\n",
       "                      -1.9806e-02, -3.0694e-02, -1.6119e-03, -3.0186e-02, -1.6438e-02,\n",
       "                      -5.8162e-02, -2.1096e-02, -5.0557e-02, -7.4961e-02, -4.8425e-02,\n",
       "                      -8.5194e-02, -2.4719e-02, -4.9360e-02, -2.7306e-02, -1.6800e-02,\n",
       "                      -3.3068e-02, -3.3439e-02, -3.4303e-02, -4.6416e-02, -1.7247e-02,\n",
       "                      -1.3064e-02, -3.7344e-02, -2.7998e-02, -2.1036e-02, -5.5253e-03,\n",
       "                      -4.9355e-02, -4.9996e-02, -4.2477e-02, -9.5431e-02, -6.1848e-02,\n",
       "                      -3.3533e-02, -3.7516e-02, -5.1949e-02, -5.0358e-02, -3.1760e-02,\n",
       "                      -1.4375e-02, -4.0657e-02, -1.1728e-02, -3.8508e-02, -1.6899e-02,\n",
       "                      -2.6288e-02, -4.0749e-02, -1.5108e-02, -9.8864e-03, -6.5065e-02,\n",
       "                      -3.4500e-02, -2.7233e-02, -2.1821e-02, -7.3090e-02, -1.7004e-03,\n",
       "                      -2.6552e-02, -6.8125e-02, -6.3564e-02, -4.1945e-02, -8.4904e-03,\n",
       "                      -1.6759e-02, -5.3538e-02, -2.3069e-02, -2.6660e-02, -4.2562e-02,\n",
       "                      -4.5366e-02, -5.8037e-02, -1.1440e-02, -4.0460e-02, -3.0146e-02,\n",
       "                      -6.2893e-02, -6.3091e-02, -8.9825e-02, -7.0503e-02, -4.7263e-02,\n",
       "                      -3.0073e-02,  2.4964e-03, -3.4634e-02, -4.3630e-02, -4.1786e-02,\n",
       "                       1.2015e-02, -4.6046e-02, -4.5552e-02, -7.5362e-02, -2.0723e-02,\n",
       "                      -7.5396e-02, -9.3431e-02, -2.5542e-02, -4.6229e-02, -3.2083e-02,\n",
       "                      -3.2104e-02, -3.5782e-02, -2.0931e-02, -4.2057e-02, -3.3828e-02,\n",
       "                      -4.1629e-02, -6.5437e-02, -4.6024e-02, -5.4627e-02, -5.0956e-02,\n",
       "                      -3.8462e-02, -4.4087e-02, -3.6651e-02, -2.8791e-02, -8.3898e-02,\n",
       "                      -9.0921e-03, -7.3802e-02, -1.5109e-02, -4.4873e-02, -2.6421e-02,\n",
       "                      -2.1330e-02, -1.3135e-02, -9.1392e-02, -2.6874e-02, -7.3396e-02,\n",
       "                      -3.9346e-02, -4.3049e-02, -7.0417e-02, -3.4780e-02, -3.0450e-02,\n",
       "                      -5.0191e-02,  1.2989e-04, -5.8413e-02, -6.4717e-04, -1.6003e-02,\n",
       "                      -7.5213e-02, -4.6144e-02, -1.6041e-02, -4.2278e-02, -5.4106e-02,\n",
       "                      -1.4460e-02,  3.8592e-03, -8.2250e-03, -7.7290e-02, -3.1811e-02,\n",
       "                      -4.4641e-03, -3.1183e-02, -5.5522e-03, -3.4325e-02, -3.8586e-02,\n",
       "                      -3.4683e-02, -5.6261e-02, -1.0509e-02, -4.3457e-02, -4.8180e-02,\n",
       "                      -4.0461e-02, -2.0320e-02, -1.2178e-02, -1.4399e-02, -3.6245e-02,\n",
       "                      -6.6898e-02, -2.0580e-02, -9.9505e-02, -3.7551e-02, -3.0202e-02,\n",
       "                      -3.3650e-02, -5.0954e-02, -7.9120e-03, -6.9562e-02, -1.1333e-02,\n",
       "                      -2.8649e-02, -5.7900e-02, -5.7077e-02, -4.6432e-02, -4.2111e-02,\n",
       "                      -3.2177e-02, -4.2258e-02, -3.9928e-02,  9.8492e-04,  1.7332e-02,\n",
       "                      -1.5074e-02, -7.8123e-02, -2.6812e-02, -7.9827e-02, -3.0789e-02,\n",
       "                      -6.3071e-02, -3.5181e-02, -3.0335e-02, -2.3350e-02, -4.5163e-02,\n",
       "                      -1.3212e-02, -2.5680e-02, -3.2472e-02, -4.9435e-02, -2.4848e-02,\n",
       "                      -2.1720e-02, -2.5265e-02, -5.4146e-02, -6.9473e-02, -6.2072e-02,\n",
       "                       1.6818e-02, -2.3958e-02, -4.4164e-02, -3.1280e-02, -8.1101e-02,\n",
       "                      -1.2703e-02, -3.4052e-02, -2.5319e-02, -2.6441e-02, -4.6890e-02,\n",
       "                      -1.5660e-02, -1.4905e-02, -3.9514e-02, -5.1598e-02, -4.2134e-02,\n",
       "                      -5.4803e-02, -2.7356e-02, -1.3259e-02, -1.9839e-02, -3.6123e-02,\n",
       "                      -1.9485e-02])),\n",
       "             ('layer3.5.bn1.running_mean',\n",
       "              tensor([-2.5200e-05, -1.7767e-02, -1.6084e-02, -3.7765e-02, -6.8413e-03,\n",
       "                      -5.4575e-03, -2.3377e-02, -3.4992e-02, -6.6628e-03, -9.2963e-03,\n",
       "                      -4.1360e-02,  1.1219e-02, -5.4958e-03, -7.4934e-03, -3.2694e-02,\n",
       "                      -4.8817e-02, -1.8373e-02, -2.2938e-02, -3.4044e-02, -1.0634e-02,\n",
       "                      -1.1918e-02, -1.5450e-02, -1.0988e-02, -5.0708e-02, -8.3423e-03,\n",
       "                      -5.3154e-03, -3.4030e-02, -4.7208e-02, -2.2030e-02, -6.0640e-03,\n",
       "                      -1.4170e-02, -2.9504e-02, -1.4719e-02, -2.1711e-02, -3.3049e-02,\n",
       "                      -2.5590e-03, -1.8464e-02, -9.6641e-03, -2.2198e-02, -1.6746e-02,\n",
       "                      -2.2795e-02, -1.2460e-02, -2.3604e-02, -1.7879e-02, -2.3700e-02,\n",
       "                      -1.7613e-02,  3.7116e-03, -3.2094e-02, -2.4474e-02,  2.3686e-03,\n",
       "                      -3.7949e-02, -1.0661e-02, -1.0021e-02, -2.0984e-02, -9.9546e-03,\n",
       "                      -1.3234e-02, -4.8264e-02, -9.0528e-04, -2.6671e-02, -4.2562e-02,\n",
       "                       4.6069e-03, -8.8169e-03, -3.2744e-02, -2.5605e-02, -2.1619e-02,\n",
       "                      -2.6045e-02, -2.4855e-02, -2.9959e-02, -1.9826e-03, -6.3860e-03,\n",
       "                      -3.4267e-02, -1.4616e-02, -1.8226e-02, -2.0145e-02, -3.5161e-02,\n",
       "                      -3.6145e-02, -2.3466e-02, -1.2700e-02,  1.1017e-02, -1.2299e-02,\n",
       "                      -2.1680e-02, -7.9587e-03, -3.3265e-02, -7.3400e-03, -3.0214e-02,\n",
       "                      -1.2860e-02, -3.2571e-02, -7.3709e-03, -4.5995e-03, -7.5855e-03,\n",
       "                      -1.0087e-02,  3.7335e-03, -1.3469e-02, -8.9836e-03, -2.0210e-02,\n",
       "                      -1.3713e-02, -9.4145e-03, -7.5151e-04, -2.5991e-02, -4.2912e-02,\n",
       "                      -6.6516e-03, -7.8532e-03, -5.6934e-03, -1.2439e-02, -1.0348e-02,\n",
       "                      -3.3257e-02,  1.4892e-03, -2.6482e-02, -1.4517e-02, -1.3142e-02,\n",
       "                      -3.2377e-03, -2.4713e-02, -5.4901e-02, -2.4495e-02,  3.9119e-03,\n",
       "                      -3.2802e-02,  1.3022e-02, -2.3936e-02, -1.4803e-02, -7.2757e-03,\n",
       "                      -2.9951e-02, -2.1959e-02, -1.2205e-02, -3.2928e-02,  6.4759e-03,\n",
       "                      -3.4526e-02, -2.4931e-02,  1.0411e-02,  1.1839e-02, -1.1413e-02,\n",
       "                      -2.1838e-02,  2.2514e-02,  4.7004e-03, -3.4724e-03, -2.6341e-03,\n",
       "                      -1.9357e-03, -1.7680e-02, -4.0141e-02,  5.8826e-03, -1.5630e-02,\n",
       "                      -3.9070e-02, -1.0762e-02, -1.1334e-02, -3.6152e-02, -3.0524e-02,\n",
       "                      -1.2324e-02, -5.2542e-02, -3.0863e-02, -2.3690e-02, -8.0839e-03,\n",
       "                      -1.4140e-02, -1.4404e-02, -1.3807e-02, -1.1575e-02, -8.4504e-03,\n",
       "                      -1.0017e-02, -2.3522e-02, -1.9901e-02, -3.4560e-02, -2.1468e-02,\n",
       "                      -1.6794e-02, -4.2003e-02, -1.5027e-02, -4.2930e-02, -2.0759e-02,\n",
       "                      -1.6764e-02, -5.7816e-02, -2.9975e-02, -1.7395e-02, -2.7737e-02,\n",
       "                      -2.5431e-02, -3.2627e-02, -2.4248e-02, -5.2725e-03, -1.9672e-02,\n",
       "                       9.3719e-05, -2.8898e-02,  3.7381e-03, -4.0978e-03, -1.9751e-03,\n",
       "                      -2.0491e-02, -1.2021e-02, -6.3184e-03, -5.2147e-03, -6.5074e-03,\n",
       "                       2.3684e-03,  1.4679e-02, -1.5386e-02, -2.5923e-02, -5.6676e-03,\n",
       "                       4.9936e-03, -2.0056e-02, -1.7743e-02, -2.2772e-02,  1.9637e-03,\n",
       "                      -1.7723e-02,  3.0215e-03,  3.1316e-03, -2.4525e-02, -1.4869e-02,\n",
       "                      -2.3459e-02, -4.8454e-03,  4.8165e-04, -2.1639e-02, -5.6869e-03,\n",
       "                      -2.8684e-02, -2.3653e-02, -7.3180e-03, -8.8253e-03, -3.4299e-02,\n",
       "                      -1.4313e-02, -3.2554e-02, -9.7880e-03, -1.6741e-02, -1.7023e-02,\n",
       "                      -2.8155e-02,  4.0193e-03, -4.1219e-02, -1.2012e-02, -2.0444e-02,\n",
       "                       5.6515e-04,  2.6717e-03, -1.7534e-02, -2.3589e-02, -3.6392e-03,\n",
       "                      -1.4859e-02, -1.2718e-02, -1.6089e-02, -2.1610e-02, -1.2710e-02,\n",
       "                       7.9540e-04, -4.5913e-02, -3.6512e-02, -5.6187e-03, -3.0117e-04,\n",
       "                      -4.7041e-02, -1.3838e-02, -1.9129e-02, -2.0392e-02, -1.3791e-02,\n",
       "                      -1.0987e-02, -1.4158e-03, -1.3014e-02, -1.5486e-02, -1.5893e-02,\n",
       "                      -5.0858e-02,  2.9192e-03, -1.1166e-02, -2.1728e-02,  2.8114e-03,\n",
       "                      -1.4008e-02, -3.6610e-02, -2.1981e-02, -8.2781e-03, -2.7585e-02,\n",
       "                      -6.8663e-03])),\n",
       "             ('layer3.5.bn1.running_var',\n",
       "              tensor([5.4429e-10, 1.2829e-03, 1.2855e-03, 2.3946e-03, 2.2191e-03, 1.4714e-03,\n",
       "                      8.2970e-04, 1.0186e-03, 1.5180e-03, 6.4548e-04, 1.5743e-03, 2.6412e-03,\n",
       "                      1.2445e-03, 1.1750e-03, 1.3462e-03, 2.3474e-03, 1.1060e-03, 1.7771e-03,\n",
       "                      6.8286e-04, 1.0923e-03, 1.5192e-03, 9.5068e-04, 2.2142e-03, 1.2074e-03,\n",
       "                      2.0647e-03, 1.1000e-03, 1.5784e-03, 1.7223e-03, 1.2474e-03, 8.1361e-04,\n",
       "                      2.1805e-03, 1.2515e-03, 9.4894e-04, 9.2683e-04, 1.1997e-03, 1.5059e-03,\n",
       "                      3.4022e-04, 6.2800e-04, 3.0186e-03, 1.5094e-03, 3.3181e-03, 8.8218e-04,\n",
       "                      2.5604e-03, 7.8875e-04, 1.5578e-03, 1.2151e-03, 6.2644e-04, 1.1492e-03,\n",
       "                      1.3808e-03, 7.4677e-04, 2.4052e-03, 3.1336e-04, 8.4288e-04, 1.7046e-03,\n",
       "                      7.8649e-04, 2.1468e-03, 2.4327e-03, 8.8689e-04, 1.5106e-03, 3.1723e-03,\n",
       "                      1.2326e-03, 5.9988e-04, 1.6932e-03, 9.2999e-04, 8.6171e-04, 1.2407e-03,\n",
       "                      1.4718e-03, 1.4377e-03, 1.4612e-03, 2.2424e-04, 1.6962e-03, 8.3021e-04,\n",
       "                      1.2335e-03, 8.1184e-04, 9.6831e-04, 1.1037e-03, 1.7737e-03, 1.1708e-03,\n",
       "                      2.4895e-03, 1.1261e-03, 9.8450e-04, 9.1792e-04, 1.3856e-03, 1.3540e-03,\n",
       "                      1.0960e-03, 3.3941e-04, 1.4247e-03, 2.0930e-03, 9.8004e-04, 4.3802e-04,\n",
       "                      6.9373e-04, 1.2849e-03, 1.0541e-03, 1.1046e-03, 1.2857e-03, 8.0898e-04,\n",
       "                      3.7235e-04, 5.9648e-04, 1.7111e-03, 1.1144e-03, 1.0458e-03, 1.1278e-03,\n",
       "                      1.5388e-03, 1.2852e-03, 1.0884e-03, 1.6315e-03, 1.1975e-03, 7.2205e-04,\n",
       "                      1.0180e-03, 1.4115e-03, 1.8499e-03, 1.5144e-03, 1.4930e-03, 1.6367e-03,\n",
       "                      3.8271e-04, 2.4162e-03, 1.4759e-03, 1.5431e-03, 2.2423e-03, 6.8038e-04,\n",
       "                      1.1234e-03, 1.4276e-03, 9.1205e-04, 1.1621e-03, 4.9977e-04, 7.9562e-04,\n",
       "                      1.8798e-03, 1.1503e-03, 2.5657e-03, 1.1210e-03, 1.8473e-03, 2.9125e-03,\n",
       "                      1.2207e-03, 1.7983e-03, 8.3613e-04, 7.4504e-04, 6.7537e-04, 1.0903e-03,\n",
       "                      1.5541e-03, 9.6727e-04, 1.4209e-03, 1.0538e-03, 2.8761e-03, 1.4437e-03,\n",
       "                      1.6076e-03, 6.6094e-04, 2.5104e-03, 1.1968e-03, 1.2899e-03, 2.2798e-03,\n",
       "                      1.8091e-03, 1.3154e-03, 6.6929e-04, 1.3587e-03, 8.4009e-04, 6.4732e-04,\n",
       "                      1.2776e-03, 1.5569e-03, 1.5147e-03, 1.3729e-03, 1.1139e-03, 8.3447e-04,\n",
       "                      9.4343e-04, 1.6574e-03, 1.2434e-03, 1.4108e-03, 3.4354e-03, 1.4476e-03,\n",
       "                      1.5152e-03, 8.0422e-04, 1.7203e-03, 1.0857e-03, 8.0073e-04, 6.4846e-04,\n",
       "                      1.6718e-03, 5.5199e-04, 5.4047e-04, 3.8024e-04, 1.5055e-03, 8.2882e-04,\n",
       "                      8.1007e-04, 1.1070e-03, 2.7556e-04, 1.3949e-03, 7.3724e-04, 1.1587e-03,\n",
       "                      8.6598e-04, 4.1084e-04, 1.1559e-03, 1.6351e-03, 1.0085e-03, 9.4304e-04,\n",
       "                      2.4551e-03, 1.5037e-03, 7.3417e-04, 2.0075e-03, 4.0097e-04, 2.4862e-03,\n",
       "                      1.2237e-03, 2.1171e-03, 2.0127e-03, 1.2490e-03, 9.6505e-04, 9.6664e-04,\n",
       "                      8.3079e-04, 4.9886e-04, 2.1350e-03, 9.3896e-04, 1.8901e-03, 1.7832e-03,\n",
       "                      6.4387e-04, 1.3145e-03, 1.3726e-03, 9.7578e-04, 2.1368e-03, 8.6401e-04,\n",
       "                      2.1464e-03, 1.2155e-03, 1.0748e-03, 1.0756e-03, 1.0963e-03, 7.7853e-04,\n",
       "                      9.4133e-04, 1.1068e-03, 3.5741e-04, 9.9950e-04, 4.3886e-04, 1.8441e-03,\n",
       "                      1.7141e-03, 1.3897e-03, 1.0001e-03, 2.2064e-03, 2.2277e-03, 8.2532e-04,\n",
       "                      1.1468e-03, 1.1799e-03, 8.3154e-04, 2.4122e-03, 2.0762e-03, 1.8969e-03,\n",
       "                      1.1757e-03, 1.9209e-03, 2.1202e-03, 2.2586e-03, 1.0185e-03, 1.5518e-03,\n",
       "                      4.7984e-04, 9.0895e-04, 2.0243e-03, 3.0011e-04, 8.3790e-04, 1.7379e-03,\n",
       "                      1.1274e-03, 6.3371e-04, 1.2714e-03, 9.7150e-04])),\n",
       "             ('layer3.5.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.5.conv2.weight',\n",
       "              tensor([[[[-7.1583e-06, -2.4448e-07,  6.4524e-06],\n",
       "                        [-5.4757e-06,  2.1760e-06,  8.1779e-06],\n",
       "                        [-3.0145e-06,  3.3961e-07,  6.5107e-06]],\n",
       "              \n",
       "                       [[ 2.4113e-03,  1.0293e-03,  1.0923e-03],\n",
       "                        [-3.3415e-03,  4.1170e-04, -7.2678e-03],\n",
       "                        [ 7.3027e-05, -3.3030e-04, -9.6418e-04]],\n",
       "              \n",
       "                       [[ 5.2466e-04, -2.3834e-03, -3.8596e-03],\n",
       "                        [ 7.4047e-04,  3.0894e-03,  3.7976e-03],\n",
       "                        [ 2.1275e-03, -8.3531e-04,  7.4542e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.3267e-04, -3.0873e-03, -2.5762e-03],\n",
       "                        [-1.6353e-03,  2.1974e-04,  8.8863e-04],\n",
       "                        [-3.1468e-03,  1.8999e-03,  3.2797e-03]],\n",
       "              \n",
       "                       [[ 2.6953e-03,  5.5278e-04,  4.3302e-03],\n",
       "                        [-1.4629e-03, -3.3370e-03, -3.0892e-03],\n",
       "                        [-6.6001e-04, -3.6113e-03, -4.2272e-03]],\n",
       "              \n",
       "                       [[ 1.7803e-03,  4.0831e-03,  8.4069e-03],\n",
       "                        [-1.3030e-03,  2.7352e-03,  2.6988e-03],\n",
       "                        [-1.8195e-03,  8.2851e-04, -1.9955e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.7394e-06,  4.9600e-06,  1.5241e-06],\n",
       "                        [ 2.7031e-06,  5.4470e-06,  2.3126e-06],\n",
       "                        [ 3.6030e-06,  6.5953e-07, -9.1017e-08]],\n",
       "              \n",
       "                       [[-7.7617e-03,  3.5949e-03,  8.2940e-04],\n",
       "                        [-1.0709e-02,  5.8474e-03, -1.1761e-03],\n",
       "                        [-2.9488e-03,  4.2231e-04,  5.3786e-05]],\n",
       "              \n",
       "                       [[ 3.3495e-03,  3.1172e-03, -4.7360e-03],\n",
       "                        [ 8.8987e-03,  6.0167e-03, -3.9434e-03],\n",
       "                        [ 1.2621e-02,  5.0585e-03, -5.6234e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.5892e-04,  2.7487e-03, -3.5011e-03],\n",
       "                        [-5.5840e-04, -2.9111e-04, -5.6083e-03],\n",
       "                        [-4.5416e-03, -6.2413e-03, -4.8737e-03]],\n",
       "              \n",
       "                       [[ 5.8382e-03,  8.7343e-03,  7.6146e-04],\n",
       "                        [-3.1807e-03,  3.0122e-03, -1.0178e-02],\n",
       "                        [-8.2111e-03, -7.0696e-03, -9.4763e-03]],\n",
       "              \n",
       "                       [[ 1.1478e-03,  2.2860e-03,  4.2828e-04],\n",
       "                        [ 3.9256e-03,  4.5366e-03, -4.3243e-03],\n",
       "                        [-7.3709e-03, -6.8825e-03, -1.1366e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7709e-06,  9.4877e-06,  1.1658e-05],\n",
       "                        [-3.0681e-06,  9.0477e-07, -5.6660e-07],\n",
       "                        [-4.5962e-06, -1.6845e-06, -3.0191e-07]],\n",
       "              \n",
       "                       [[ 2.7220e-03, -7.0964e-03,  1.1673e-03],\n",
       "                        [-6.7599e-04, -2.0851e-03,  7.5989e-04],\n",
       "                        [ 1.1270e-02, -4.6937e-04,  1.0258e-03]],\n",
       "              \n",
       "                       [[-3.2471e-03, -4.6526e-03,  2.6331e-03],\n",
       "                        [ 3.2901e-03, -5.3370e-03,  8.8046e-03],\n",
       "                        [ 5.3099e-03,  5.5144e-03,  6.6090e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.4255e-03,  2.7921e-03,  1.7212e-03],\n",
       "                        [-3.5931e-05,  3.0959e-03, -2.5931e-03],\n",
       "                        [-2.5438e-03, -6.9094e-04, -6.3102e-03]],\n",
       "              \n",
       "                       [[-3.1675e-03, -6.0842e-03,  6.0517e-04],\n",
       "                        [ 1.7998e-03,  1.0620e-03,  6.3914e-03],\n",
       "                        [ 5.5991e-03, -2.5712e-03,  2.6431e-03]],\n",
       "              \n",
       "                       [[-8.2833e-03, -3.9734e-03, -1.3428e-03],\n",
       "                        [ 4.6031e-03, -4.3575e-03, -1.5196e-03],\n",
       "                        [-1.8811e-03, -8.1700e-03, -7.5267e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.4192e-06, -1.6021e-05, -9.3403e-06],\n",
       "                        [ 1.3859e-06,  5.3413e-06,  3.0263e-06],\n",
       "                        [-1.3711e-05, -1.6853e-05, -1.2788e-05]],\n",
       "              \n",
       "                       [[-1.3475e-03, -2.3099e-03,  3.0638e-03],\n",
       "                        [-6.8942e-03, -3.6306e-03, -3.8393e-03],\n",
       "                        [-1.5403e-02, -1.6663e-02, -1.0985e-02]],\n",
       "              \n",
       "                       [[ 1.7133e-02,  3.2744e-02,  8.8583e-03],\n",
       "                        [-2.9311e-03,  1.8337e-02,  1.4433e-03],\n",
       "                        [-1.3391e-02, -4.7002e-03, -2.3873e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4944e-03, -4.1771e-03,  1.6481e-03],\n",
       "                        [ 2.0886e-03,  2.2361e-03,  5.8158e-03],\n",
       "                        [-1.4778e-02, -7.9164e-03, -1.3662e-02]],\n",
       "              \n",
       "                       [[-7.3526e-03, -3.6593e-03,  2.1984e-03],\n",
       "                        [-7.6651e-03,  1.9371e-03,  4.0348e-03],\n",
       "                        [ 3.6302e-04, -4.7282e-03,  2.0391e-03]],\n",
       "              \n",
       "                       [[-1.5933e-02, -1.6438e-02,  6.6836e-04],\n",
       "                        [-1.1503e-02, -2.0008e-02, -8.7308e-03],\n",
       "                        [-4.8562e-03, -5.3345e-03, -2.9904e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2244e-06, -2.5589e-06, -4.7063e-06],\n",
       "                        [-1.6834e-06, -2.3156e-06, -2.7104e-06],\n",
       "                        [-9.7831e-07,  4.8427e-07, -3.2786e-06]],\n",
       "              \n",
       "                       [[-7.2049e-03, -2.6946e-03, -6.3618e-03],\n",
       "                        [-3.8144e-03,  5.4392e-04, -2.6953e-03],\n",
       "                        [-6.8994e-03, -9.0202e-03, -2.7036e-04]],\n",
       "              \n",
       "                       [[ 2.7784e-03,  9.4529e-03,  2.9983e-03],\n",
       "                        [-1.6098e-03,  8.8716e-03,  2.9676e-03],\n",
       "                        [-1.0372e-02, -5.8914e-03,  7.2094e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.5317e-03,  4.5171e-04, -5.3385e-03],\n",
       "                        [ 2.0620e-04,  2.2463e-03, -1.0204e-04],\n",
       "                        [ 4.6369e-03,  8.9158e-03,  1.5880e-02]],\n",
       "              \n",
       "                       [[-3.9131e-04, -5.3258e-04, -2.2332e-05],\n",
       "                        [-4.7897e-03, -5.5488e-03, -3.2595e-03],\n",
       "                        [-2.8974e-03, -7.2283e-03, -9.4298e-03]],\n",
       "              \n",
       "                       [[-1.2127e-02, -8.9920e-03, -2.7007e-03],\n",
       "                        [ 6.7707e-03,  2.9997e-03, -2.9581e-03],\n",
       "                        [-2.8351e-03, -1.4401e-02, -2.4183e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3364e-06,  6.7602e-07, -8.5209e-07],\n",
       "                        [-6.3843e-06,  1.1189e-06, -4.7274e-07],\n",
       "                        [-1.7646e-06, -1.5176e-07,  6.1956e-07]],\n",
       "              \n",
       "                       [[-5.8221e-03, -2.3108e-03,  2.4144e-03],\n",
       "                        [-4.7056e-03,  4.7968e-03,  7.8457e-03],\n",
       "                        [-2.2285e-04,  1.0055e-03,  2.0020e-03]],\n",
       "              \n",
       "                       [[ 3.3266e-03,  4.0101e-03,  7.6729e-03],\n",
       "                        [-1.7243e-03,  1.5374e-03, -2.4397e-03],\n",
       "                        [-3.0014e-05,  5.4263e-03,  4.4829e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.5444e-03, -3.9424e-03,  2.0209e-03],\n",
       "                        [ 1.2183e-03, -2.3665e-03,  4.0726e-03],\n",
       "                        [ 4.0403e-03, -1.7731e-03,  3.6768e-03]],\n",
       "              \n",
       "                       [[-2.6472e-04, -2.2223e-03,  1.0055e-03],\n",
       "                        [-3.1419e-03,  2.8735e-03,  7.2594e-03],\n",
       "                        [-5.6184e-03,  1.3471e-03,  7.7242e-04]],\n",
       "              \n",
       "                       [[-1.9013e-03,  9.0948e-03, -1.8109e-03],\n",
       "                        [-7.7915e-03,  4.1307e-04, -2.8227e-03],\n",
       "                        [-4.7778e-03,  9.3678e-04, -1.7848e-05]]]])),\n",
       "             ('layer3.5.bn2.weight',\n",
       "              tensor([4.3041e-02, 6.3688e-02, 6.2681e-02, 1.0109e-01, 6.7942e-02, 6.3414e-05,\n",
       "                      6.4441e-02, 8.4697e-02, 7.6493e-02, 8.2043e-02, 7.4876e-02, 7.7224e-02,\n",
       "                      1.0103e-01, 6.6266e-02, 9.0425e-02, 8.7569e-02, 9.1231e-02, 8.9972e-02,\n",
       "                      9.6094e-02, 9.5012e-02, 6.3896e-02, 9.4978e-02, 2.5433e-02, 8.3701e-02,\n",
       "                      9.2412e-02, 5.5570e-02, 9.8686e-02, 1.0263e-01, 9.6309e-02, 1.2243e-01,\n",
       "                      4.7980e-02, 1.1446e-01, 1.0217e-01, 6.7794e-02, 6.1944e-02, 7.6159e-02,\n",
       "                      3.9302e-02, 7.8032e-02, 1.0299e-01, 9.2181e-02, 7.5701e-02, 8.6092e-02,\n",
       "                      8.2946e-02, 8.7167e-02, 1.2660e-01, 4.3987e-02, 7.9667e-02, 1.0705e-01,\n",
       "                      3.9374e-02, 4.6430e-02, 9.4001e-02, 9.0885e-02, 5.3782e-02, 7.0771e-02,\n",
       "                      1.0253e-01, 9.3256e-02, 1.0247e-01, 5.4557e-02, 7.7575e-02, 1.0349e-01,\n",
       "                      9.6762e-02, 1.0918e-01, 1.0564e-01, 9.4028e-02, 6.2472e-02, 6.2141e-02,\n",
       "                      7.0025e-02, 7.1967e-02, 1.1131e-01, 9.7777e-02, 1.2779e-01, 1.1195e-01,\n",
       "                      8.4509e-02, 8.1254e-02, 4.2061e-02, 6.1814e-02, 1.0862e-01, 9.3630e-02,\n",
       "                      7.3713e-02, 4.4770e-02, 9.0859e-02, 4.5879e-02, 1.1270e-01, 7.9985e-02,\n",
       "                      6.1263e-02, 4.9379e-02, 1.2371e-01, 5.5119e-02, 9.8051e-02, 7.6163e-02,\n",
       "                      8.6761e-02, 8.4123e-02, 8.9589e-02, 7.8452e-02, 8.5527e-02, 8.1482e-02,\n",
       "                      6.5007e-02, 4.2985e-03, 6.2146e-02, 6.5923e-02, 7.7754e-02, 8.5498e-05,\n",
       "                      5.2672e-02, 9.9919e-02, 6.9468e-02, 8.6706e-02, 1.1229e-01, 1.0325e-01,\n",
       "                      7.8996e-02, 9.8239e-02, 7.0623e-02, 7.4079e-02, 7.5166e-02, 5.5594e-02,\n",
       "                      1.2566e-01, 7.0984e-02, 9.4959e-02, 9.7440e-02, 1.1239e-01, 5.7645e-02,\n",
       "                      7.9493e-02, 4.1814e-02, 9.3641e-02, 6.8686e-02, 8.8243e-02, 1.0312e-01,\n",
       "                      6.5961e-02, 7.5894e-02, 8.8309e-02, 1.0001e-01, 9.9147e-02, 1.1020e-01,\n",
       "                      9.9753e-02, 1.0352e-01, 9.5112e-02, 8.1594e-02, 1.1487e-01, 5.2675e-02,\n",
       "                      8.6017e-02, 9.5480e-02, 7.3123e-02, 1.1346e-01, 7.4122e-02, 4.8578e-02,\n",
       "                      7.6481e-02, 9.2720e-02, 8.2160e-02, 1.0441e-01, 6.0377e-02, 5.5231e-02,\n",
       "                      1.3586e-01, 1.3345e-01, 1.0172e-01, 6.9308e-02, 5.6578e-02, 4.3292e-02,\n",
       "                      8.7941e-02, 8.6977e-02, 6.7835e-02, 1.0772e-01, 1.1434e-01, 6.3183e-02,\n",
       "                      1.3729e-01, 1.1171e-01, 8.2726e-02, 6.0495e-02, 7.6241e-02, 8.1782e-02,\n",
       "                      7.5899e-02, 7.1918e-02, 6.9025e-02, 1.1429e-01, 8.6896e-02, 5.0709e-02,\n",
       "                      7.9258e-02, 1.3658e-05, 6.0624e-02, 5.3989e-02, 8.6695e-02, 7.7870e-02,\n",
       "                      8.6827e-02, 4.3413e-02, 7.5860e-02, 1.0767e-01, 7.5785e-02, 6.8849e-02,\n",
       "                      5.8879e-02, 8.8027e-02, 8.9037e-02, 3.3709e-02, 9.1618e-02, 1.0483e-01,\n",
       "                      9.8246e-02, 1.1435e-01, 1.2389e-01, 9.1878e-02, 1.0993e-01, 8.3715e-02,\n",
       "                      8.0032e-02, 1.1878e-02, 7.4410e-04, 8.2406e-02, 1.0118e-01, 7.6322e-02,\n",
       "                      6.8707e-02, 5.4781e-02, 7.1684e-02, 1.0268e-01, 9.2976e-02, 5.6395e-02,\n",
       "                      4.4137e-02, 7.7572e-02, 6.5769e-02, 6.6591e-02, 1.1180e-01, 1.1216e-01,\n",
       "                      6.1873e-02, 5.0812e-02, 8.2679e-02, 8.4416e-02, 9.7881e-02, 9.6787e-02,\n",
       "                      8.4967e-02, 1.2724e-01, 9.2864e-02, 1.1565e-01, 6.1473e-02, 7.7382e-02,\n",
       "                      8.6012e-02, 8.2907e-02, 9.7135e-02, 9.0949e-02, 1.0085e-01, 9.5993e-02,\n",
       "                      1.0453e-01, 1.0711e-01, 8.3428e-02, 7.5014e-02, 9.8467e-02, 1.0050e-01,\n",
       "                      5.1784e-02, 1.0863e-01, 1.2502e-01, 9.1447e-02, 9.9939e-02, 1.3602e-01,\n",
       "                      9.9841e-02, 9.7134e-02, 1.2780e-01, 1.1310e-01, 8.9783e-02, 1.0512e-01,\n",
       "                      1.0299e-01, 1.2867e-01, 8.1321e-02, 5.5593e-02])),\n",
       "             ('layer3.5.bn2.bias',\n",
       "              tensor([-0.0433, -0.0512, -0.0414, -0.0465, -0.0298, -0.0004, -0.0660, -0.0628,\n",
       "                      -0.0664, -0.0433, -0.0648, -0.0548, -0.0391, -0.0372, -0.0734, -0.0431,\n",
       "                      -0.0623, -0.0719, -0.0727, -0.0615, -0.0385, -0.0422,  0.0057, -0.0517,\n",
       "                      -0.0916, -0.0284, -0.1007, -0.1045, -0.0659, -0.0459, -0.0335, -0.0700,\n",
       "                      -0.0810, -0.0444, -0.0241, -0.0590, -0.0087, -0.0415, -0.0601, -0.0652,\n",
       "                      -0.0429, -0.0557, -0.0558, -0.0782, -0.0816, -0.0230, -0.0640, -0.0730,\n",
       "                      -0.0294, -0.0047, -0.0874, -0.0410, -0.0327, -0.0223, -0.0921, -0.0664,\n",
       "                      -0.1049, -0.0393, -0.0622, -0.0390, -0.0331, -0.0782, -0.0621, -0.0851,\n",
       "                      -0.0525, -0.0402, -0.0442, -0.0573, -0.1289, -0.0577, -0.0911, -0.0996,\n",
       "                      -0.0340, -0.0642, -0.0399, -0.0377, -0.1201, -0.0406, -0.0715, -0.0325,\n",
       "                      -0.0892, -0.0275, -0.0668, -0.0666, -0.0449, -0.0205, -0.1018, -0.0363,\n",
       "                      -0.0547, -0.0336, -0.0803, -0.0803, -0.0468, -0.0547, -0.0650, -0.0669,\n",
       "                      -0.0361, -0.0506, -0.0514, -0.0613, -0.0176, -0.0008, -0.0350, -0.0631,\n",
       "                      -0.0331, -0.0427, -0.0745, -0.0728, -0.0625, -0.0820, -0.0429, -0.0317,\n",
       "                      -0.0725, -0.0312, -0.0725, -0.0633, -0.0815, -0.0815, -0.0657, -0.0393,\n",
       "                      -0.1186, -0.0280, -0.0584, -0.0545, -0.0591, -0.0972,  0.0112, -0.0479,\n",
       "                      -0.0809, -0.1091, -0.0843, -0.0984, -0.1113, -0.0614, -0.0647,  0.0165,\n",
       "                      -0.0818, -0.0324, -0.0738, -0.0450, -0.0444, -0.0879, -0.0319, -0.0235,\n",
       "                      -0.0571, -0.0678, -0.0501, -0.0819, -0.0367, -0.0032, -0.1359, -0.1288,\n",
       "                      -0.0846, -0.0195, -0.0288, -0.0157, -0.0405, -0.0787, -0.0617, -0.0365,\n",
       "                      -0.0934, -0.0331, -0.0934, -0.0845, -0.0582, -0.0381, -0.0047, -0.0675,\n",
       "                      -0.0505, -0.0604, -0.0516, -0.0736, -0.0735, -0.0417, -0.0415, -0.0002,\n",
       "                      -0.0795, -0.0015, -0.0679, -0.0602, -0.0295, -0.0020, -0.0545, -0.0870,\n",
       "                      -0.0644, -0.0322, -0.0374, -0.0817, -0.0868,  0.0124, -0.0602, -0.0612,\n",
       "                      -0.1023, -0.0677, -0.0684, -0.0424, -0.0973, -0.0433, -0.0505, -0.1011,\n",
       "                      -0.0068, -0.0547, -0.0669, -0.0493, -0.0778, -0.0048, -0.0430, -0.0913,\n",
       "                      -0.0809, -0.0712, -0.0144, -0.0753, -0.0045, -0.0603, -0.1094, -0.0712,\n",
       "                      -0.0459, -0.0478, -0.0470, -0.0816, -0.0647, -0.0761, -0.0489, -0.1013,\n",
       "                      -0.0805, -0.0650, -0.0409, -0.0629, -0.0433, -0.0919, -0.0594, -0.0715,\n",
       "                      -0.0643, -0.0834, -0.0877, -0.0729, -0.0649, -0.0772, -0.0557, -0.0664,\n",
       "                      -0.0061, -0.0583, -0.1114, -0.0960, -0.0649, -0.1044, -0.0267, -0.0470,\n",
       "                      -0.0812, -0.0705, -0.0061, -0.0455, -0.0546, -0.0655, -0.0695, -0.0432])),\n",
       "             ('layer3.5.bn2.running_mean',\n",
       "              tensor([-6.7306e-05, -4.6389e-03, -1.4898e-03, -2.3426e-02, -2.1651e-02,\n",
       "                      -3.4818e-05, -1.7046e-02, -1.9618e-02, -2.3536e-02, -4.4255e-02,\n",
       "                      -4.8791e-02,  2.9593e-03, -1.5629e-02, -1.6900e-02, -1.8973e-02,\n",
       "                      -5.1169e-02, -4.1119e-02, -4.6707e-02, -2.4893e-03, -2.0866e-02,\n",
       "                      -1.7499e-02, -1.4906e-02, -1.5729e-02, -2.6510e-02, -3.8256e-02,\n",
       "                      -5.2724e-03,  2.8706e-02,  5.2165e-03, -5.0275e-02, -3.7110e-02,\n",
       "                       1.2977e-03, -4.9373e-02, -2.4333e-02, -9.5369e-03, -2.1440e-02,\n",
       "                      -1.7406e-02, -1.1976e-02, -2.9663e-02, -6.6698e-03, -4.9344e-02,\n",
       "                      -7.8765e-03, -7.6830e-03, -8.3508e-03, -2.4356e-02, -4.4572e-02,\n",
       "                       4.4456e-03, -2.4756e-02, -2.3770e-02, -1.3913e-02, -4.7573e-05,\n",
       "                      -7.9384e-03, -2.7792e-02, -1.9420e-02, -2.7312e-02, -7.4773e-02,\n",
       "                      -2.6300e-02,  1.1610e-03, -1.8705e-02, -2.3703e-02, -9.2053e-03,\n",
       "                      -2.0824e-02, -5.4474e-02, -4.5411e-02, -7.8125e-03, -1.7302e-02,\n",
       "                      -1.3881e-02, -1.7878e-02, -1.9645e-02, -4.6171e-02, -8.9492e-03,\n",
       "                      -3.9388e-02, -3.7956e-02, -2.7314e-02, -2.2848e-02, -2.2399e-04,\n",
       "                      -6.5657e-04, -3.5773e-02, -2.3560e-02, -9.8086e-03, -5.9448e-03,\n",
       "                      -3.5724e-02, -2.3097e-02, -1.7132e-02, -3.8080e-02, -2.1377e-02,\n",
       "                      -1.9253e-02, -4.0677e-02, -9.4192e-03, -5.2746e-02, -2.8232e-03,\n",
       "                      -1.0216e-02, -2.4989e-02, -1.3044e-03, -1.7656e-02, -1.8364e-02,\n",
       "                       4.9403e-03, -2.1696e-02, -1.9472e-02, -3.1452e-02, -1.6931e-02,\n",
       "                      -1.0611e-02, -1.9541e-05,  5.8689e-03,  6.1532e-03, -4.0275e-02,\n",
       "                      -2.3381e-02, -3.7112e-02, -3.5427e-02, -3.5109e-02, -1.2062e-02,\n",
       "                      -6.7000e-03, -1.1425e-02, -6.6462e-03, -1.8019e-03, -5.0128e-02,\n",
       "                      -1.9792e-02,  4.6791e-03, -3.0044e-02, -4.8310e-02, -8.6902e-03,\n",
       "                      -2.4560e-02, -1.7164e-02, -1.8158e-02,  5.9301e-03, -3.7415e-02,\n",
       "                       2.2587e-02, -2.9522e-02, -1.3019e-02, -1.8319e-02, -5.9518e-02,\n",
       "                      -2.4354e-02, -2.6537e-02, -2.7516e-02, -1.2649e-02, -3.6452e-02,\n",
       "                      -3.9044e-02, -2.8169e-02, -2.2624e-02, -5.3041e-02, -1.3344e-02,\n",
       "                      -2.8918e-02, -3.5315e-02, -1.4188e-02, -4.7931e-03, -2.2953e-02,\n",
       "                       6.6935e-03, -3.0532e-02, -4.4174e-03, -3.2950e-02, -1.8843e-02,\n",
       "                       1.5647e-02, -3.7998e-02, -2.3210e-02, -1.9638e-02, -1.5970e-02,\n",
       "                      -1.4619e-03, -2.5119e-02, -3.4916e-02, -8.9250e-03, -2.6109e-02,\n",
       "                      -3.9898e-02, -1.2552e-02, -4.1766e-02, -3.8804e-02, -2.8937e-02,\n",
       "                      -7.9677e-03, -4.3330e-02, -3.2506e-02, -1.2814e-02, -1.1990e-02,\n",
       "                      -1.6170e-02, -3.6611e-02, -1.9434e-02, -1.5103e-02, -3.5011e-02,\n",
       "                      -1.4438e-05, -9.7353e-03, -2.0086e-02, -1.0495e-02, -2.1374e-02,\n",
       "                      -2.2844e-02, -2.1201e-02, -2.9935e-02,  1.8274e-03, -1.7771e-02,\n",
       "                      -2.0013e-02, -1.4819e-02, -2.1981e-02, -3.0616e-02, -1.9148e-02,\n",
       "                      -3.7126e-02, -3.4469e-03,  1.0842e-02, -4.4971e-02, -2.3449e-02,\n",
       "                       4.9632e-03, -2.5099e-02, -2.4715e-02, -2.1175e-02, -1.4885e-02,\n",
       "                      -8.8951e-04, -1.7253e-02, -4.3846e-02, -3.4839e-02, -3.4833e-02,\n",
       "                      -4.3923e-02, -2.9864e-03, -2.5267e-02,  9.9020e-03, -2.0556e-02,\n",
       "                      -2.2735e-02,  1.6336e-03, -2.4272e-02,  1.6284e-02, -5.4326e-02,\n",
       "                      -9.2121e-03, -2.1698e-02, -1.4032e-02, -6.9135e-03, -5.8928e-03,\n",
       "                      -2.1587e-02, -1.9666e-02, -2.4508e-02, -2.3468e-03, -2.4426e-02,\n",
       "                      -3.2977e-02, -6.6864e-03, -1.4956e-02, -1.5879e-02, -2.3607e-02,\n",
       "                      -2.4122e-02, -2.8387e-02, -1.2824e-02, -1.8646e-02, -3.9804e-02,\n",
       "                      -2.8030e-02, -3.3338e-02, -2.9601e-02, -2.3838e-02, -1.5131e-02,\n",
       "                      -4.0865e-02, -2.3683e-02, -5.1194e-02,  4.3513e-04, -7.4755e-03,\n",
       "                      -5.0029e-02, -3.5040e-02, -1.9915e-02, -2.3188e-02, -4.5115e-02,\n",
       "                      -7.2623e-02, -1.8461e-02, -3.8669e-02, -3.0853e-02,  4.7027e-04,\n",
       "                      -1.6915e-02])),\n",
       "             ('layer3.5.bn2.running_var',\n",
       "              tensor([2.8458e-04, 8.0004e-04, 5.9516e-04, 1.0049e-03, 6.5606e-04, 5.6634e-08,\n",
       "                      5.2636e-04, 1.0849e-03, 9.4397e-04, 2.1321e-03, 1.1552e-03, 8.1512e-04,\n",
       "                      1.2512e-03, 6.4069e-04, 1.3997e-03, 1.5454e-03, 1.3897e-03, 1.2298e-03,\n",
       "                      1.3435e-03, 1.0657e-03, 5.8217e-04, 1.0332e-03, 3.0717e-04, 9.0906e-04,\n",
       "                      1.1883e-03, 4.0730e-04, 1.5328e-03, 1.4340e-03, 1.6050e-03, 2.8116e-03,\n",
       "                      3.0408e-04, 2.3651e-03, 1.1910e-03, 5.9236e-04, 5.4284e-04, 8.4817e-04,\n",
       "                      2.4901e-04, 5.5083e-04, 1.1030e-03, 1.0593e-03, 5.0974e-04, 7.5570e-04,\n",
       "                      8.3760e-04, 1.2371e-03, 2.3540e-03, 1.9615e-04, 8.3122e-04, 1.4259e-03,\n",
       "                      2.6333e-04, 2.6173e-04, 1.6004e-03, 1.6957e-03, 3.2772e-04, 4.9563e-04,\n",
       "                      2.7660e-03, 1.0403e-03, 1.5449e-03, 4.1301e-04, 5.9770e-04, 1.6512e-03,\n",
       "                      9.2287e-04, 1.8250e-03, 1.6961e-03, 1.2077e-03, 4.9566e-04, 5.6997e-04,\n",
       "                      1.1396e-03, 7.2169e-04, 1.8806e-03, 1.2279e-03, 2.1764e-03, 1.9327e-03,\n",
       "                      7.3115e-04, 8.7757e-04, 2.4707e-04, 6.7772e-04, 3.1525e-03, 1.1379e-03,\n",
       "                      7.0787e-04, 2.2269e-04, 1.2617e-03, 3.0275e-04, 1.5725e-03, 1.0283e-03,\n",
       "                      6.1032e-04, 3.4615e-04, 1.8952e-03, 2.7074e-04, 1.7317e-03, 6.2992e-04,\n",
       "                      1.9174e-03, 1.4068e-03, 9.4404e-04, 8.5125e-04, 1.1016e-03, 1.0829e-03,\n",
       "                      7.8901e-04, 2.3257e-04, 7.2686e-04, 6.8428e-04, 6.5014e-04, 7.4817e-08,\n",
       "                      2.5306e-04, 1.0424e-03, 9.7082e-04, 8.5154e-04, 2.3070e-03, 1.7258e-03,\n",
       "                      1.0067e-03, 1.0049e-03, 6.0656e-04, 7.5131e-04, 7.3920e-04, 5.0758e-04,\n",
       "                      1.4851e-03, 9.5787e-04, 1.5910e-03, 1.3062e-03, 2.4252e-03, 4.9039e-04,\n",
       "                      9.8846e-04, 3.0351e-04, 1.0419e-03, 8.5782e-04, 1.1339e-03, 2.1714e-03,\n",
       "                      1.1548e-03, 4.4684e-04, 1.4011e-03, 2.0745e-03, 1.2457e-03, 1.3360e-03,\n",
       "                      1.4443e-03, 1.2561e-03, 1.3581e-03, 1.4104e-03, 1.5664e-03, 4.9398e-04,\n",
       "                      1.3143e-03, 1.0620e-03, 9.2728e-04, 1.7776e-03, 4.6269e-04, 2.5636e-04,\n",
       "                      6.2766e-04, 1.1809e-03, 7.6192e-04, 1.5352e-03, 7.3764e-04, 6.5162e-04,\n",
       "                      2.4897e-03, 2.4638e-03, 1.1196e-03, 5.9815e-04, 7.5913e-04, 1.4669e-04,\n",
       "                      9.1795e-04, 1.0217e-03, 7.1641e-04, 1.8260e-03, 1.5505e-03, 4.7775e-04,\n",
       "                      3.0142e-03, 1.5743e-03, 8.0877e-04, 4.6262e-04, 1.7938e-03, 6.2470e-04,\n",
       "                      8.3098e-04, 8.6840e-04, 3.9369e-04, 2.9695e-03, 9.8185e-04, 3.7368e-04,\n",
       "                      7.2379e-04, 7.8495e-09, 6.0216e-04, 8.1095e-04, 9.4290e-04, 6.6556e-04,\n",
       "                      1.3978e-03, 7.9159e-04, 7.6158e-04, 1.7052e-03, 4.5458e-04, 5.3977e-04,\n",
       "                      3.1005e-04, 1.4487e-03, 1.2367e-03, 4.4051e-04, 1.4025e-03, 1.3604e-03,\n",
       "                      1.4818e-03, 1.7808e-03, 1.5196e-03, 2.1482e-03, 1.4049e-03, 9.4287e-04,\n",
       "                      1.2839e-03, 1.1971e-03, 4.4296e-06, 9.8344e-04, 1.5395e-03, 1.0959e-03,\n",
       "                      1.0240e-03, 6.5682e-04, 4.6194e-04, 1.0217e-03, 1.2042e-03, 6.0937e-04,\n",
       "                      3.1772e-04, 6.1284e-04, 4.6360e-04, 5.2711e-04, 1.9796e-03, 1.4355e-03,\n",
       "                      4.7750e-04, 3.2814e-04, 5.4594e-04, 1.7776e-03, 1.5369e-03, 9.0012e-04,\n",
       "                      1.1050e-03, 2.3696e-03, 1.5957e-03, 1.1045e-03, 3.8924e-04, 6.5055e-04,\n",
       "                      8.2216e-04, 1.2102e-03, 1.1123e-03, 7.8243e-04, 1.5282e-03, 1.4412e-03,\n",
       "                      1.7915e-03, 1.5223e-03, 1.3962e-03, 7.3330e-04, 8.8969e-04, 8.7395e-04,\n",
       "                      9.7532e-04, 1.8068e-03, 2.7897e-03, 1.4719e-03, 1.1559e-03, 3.9647e-03,\n",
       "                      1.7355e-03, 1.0596e-03, 1.5474e-03, 2.2463e-03, 2.8773e-03, 1.3475e-03,\n",
       "                      1.0409e-03, 3.2738e-03, 1.1460e-03, 3.9523e-04])),\n",
       "             ('layer3.5.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer3.5.conv3.weight',\n",
       "              tensor([[[[-0.0001]],\n",
       "              \n",
       "                       [[ 0.0008]],\n",
       "              \n",
       "                       [[-0.0031]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0018]],\n",
       "              \n",
       "                       [[-0.0014]],\n",
       "              \n",
       "                       [[-0.0015]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0015]],\n",
       "              \n",
       "                       [[ 0.0045]],\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0070]],\n",
       "              \n",
       "                       [[-0.0010]],\n",
       "              \n",
       "                       [[-0.0033]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0004]],\n",
       "              \n",
       "                       [[ 0.0028]],\n",
       "              \n",
       "                       [[-0.0033]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0094]],\n",
       "              \n",
       "                       [[-0.0006]],\n",
       "              \n",
       "                       [[-0.0008]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0026]],\n",
       "              \n",
       "                       [[-0.0106]],\n",
       "              \n",
       "                       [[-0.0018]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0097]],\n",
       "              \n",
       "                       [[ 0.0039]],\n",
       "              \n",
       "                       [[-0.0038]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0018]],\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       [[ 0.0029]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0036]],\n",
       "              \n",
       "                       [[ 0.0069]],\n",
       "              \n",
       "                       [[-0.0018]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0029]],\n",
       "              \n",
       "                       [[-0.0029]],\n",
       "              \n",
       "                       [[-0.0006]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0022]],\n",
       "              \n",
       "                       [[ 0.0021]],\n",
       "              \n",
       "                       [[-0.0025]]]])),\n",
       "             ('layer3.5.bn3.weight',\n",
       "              tensor([ 0.0163, -0.0236, -0.0242,  ..., -0.0358,  0.0482,  0.0322])),\n",
       "             ('layer3.5.bn3.bias',\n",
       "              tensor([-0.0313, -0.0441, -0.0017,  ..., -0.0438, -0.0141, -0.0025])),\n",
       "             ('layer3.5.bn3.running_mean',\n",
       "              tensor([-0.0022,  0.0003,  0.0019,  ...,  0.0010, -0.0042, -0.0003])),\n",
       "             ('layer3.5.bn3.running_var',\n",
       "              tensor([6.8586e-06, 1.1361e-05, 1.4705e-05,  ..., 2.9837e-05, 3.3419e-05,\n",
       "                      1.3480e-05])),\n",
       "             ('layer3.5.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.0.conv1.weight',\n",
       "              tensor([[[[-6.9856e-03]],\n",
       "              \n",
       "                       [[ 1.2538e-02]],\n",
       "              \n",
       "                       [[-7.6803e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.8158e-03]],\n",
       "              \n",
       "                       [[-6.2107e-03]],\n",
       "              \n",
       "                       [[-5.8479e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5930e-03]],\n",
       "              \n",
       "                       [[ 7.2212e-03]],\n",
       "              \n",
       "                       [[-8.9762e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.2024e-03]],\n",
       "              \n",
       "                       [[-1.2456e-03]],\n",
       "              \n",
       "                       [[-8.1232e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3796e-02]],\n",
       "              \n",
       "                       [[ 1.3785e-03]],\n",
       "              \n",
       "                       [[-3.8501e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6287e-03]],\n",
       "              \n",
       "                       [[ 5.4032e-03]],\n",
       "              \n",
       "                       [[ 2.6048e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9725e-03]],\n",
       "              \n",
       "                       [[-1.4351e-02]],\n",
       "              \n",
       "                       [[ 9.2312e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4006e-02]],\n",
       "              \n",
       "                       [[-4.7990e-03]],\n",
       "              \n",
       "                       [[-3.7941e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8286e-03]],\n",
       "              \n",
       "                       [[-7.9022e-03]],\n",
       "              \n",
       "                       [[-2.1834e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2494e-03]],\n",
       "              \n",
       "                       [[-1.2550e-03]],\n",
       "              \n",
       "                       [[-2.8974e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1112e-03]],\n",
       "              \n",
       "                       [[ 1.4910e-02]],\n",
       "              \n",
       "                       [[ 5.1598e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.9757e-03]],\n",
       "              \n",
       "                       [[-5.0159e-03]],\n",
       "              \n",
       "                       [[ 8.1972e-05]]]])),\n",
       "             ('layer4.0.bn1.weight',\n",
       "              tensor([0.0842, 0.0840, 0.0878, 0.0714, 0.0658, 0.1029, 0.0815, 0.0931, 0.1069,\n",
       "                      0.0376, 0.0841, 0.0791, 0.0841, 0.1177, 0.0623, 0.0823, 0.0577, 0.0742,\n",
       "                      0.1062, 0.0879, 0.1218, 0.1084, 0.0542, 0.0714, 0.0353, 0.0575, 0.0592,\n",
       "                      0.0571, 0.0788, 0.0941, 0.0985, 0.0887, 0.0586, 0.0642, 0.0694, 0.1053,\n",
       "                      0.0385, 0.0995, 0.0548, 0.0900, 0.0855, 0.0648, 0.0789, 0.0664, 0.0812,\n",
       "                      0.1006, 0.1165, 0.0455, 0.0945, 0.0677, 0.0845, 0.0589, 0.0809, 0.0728,\n",
       "                      0.0613, 0.1050, 0.1055, 0.0638, 0.1062, 0.0661, 0.0756, 0.0939, 0.0898,\n",
       "                      0.0869, 0.0642, 0.0727, 0.1006, 0.0872, 0.1020, 0.0745, 0.1003, 0.1090,\n",
       "                      0.0883, 0.0886, 0.0877, 0.0783, 0.1043, 0.0291, 0.0688, 0.0546, 0.0716,\n",
       "                      0.0903, 0.0777, 0.0552, 0.1245, 0.1026, 0.1029, 0.0824, 0.1050, 0.0570,\n",
       "                      0.0810, 0.0712, 0.0496, 0.0615, 0.0406, 0.0901, 0.0571, 0.0590, 0.0847,\n",
       "                      0.0662, 0.0783, 0.0485, 0.0625, 0.0545, 0.0635, 0.1230, 0.1138, 0.0859,\n",
       "                      0.0647, 0.0915, 0.0874, 0.0670, 0.0787, 0.1081, 0.0901, 0.0892, 0.0443,\n",
       "                      0.0664, 0.0716, 0.0705, 0.1016, 0.1101, 0.0981, 0.0841, 0.0773, 0.0938,\n",
       "                      0.0917, 0.0449, 0.0926, 0.0853, 0.0907, 0.0787, 0.0864, 0.0842, 0.1055,\n",
       "                      0.0978, 0.0407, 0.0837, 0.0787, 0.0730, 0.1169, 0.1020, 0.0715, 0.0818,\n",
       "                      0.0923, 0.0702, 0.1084, 0.0733, 0.0435, 0.0585, 0.0847, 0.0618, 0.0800,\n",
       "                      0.1073, 0.0547, 0.0991, 0.0839, 0.0657, 0.0753, 0.0438, 0.0803, 0.1156,\n",
       "                      0.1077, 0.0957, 0.0571, 0.0845, 0.0806, 0.0316, 0.0783, 0.0677, 0.1172,\n",
       "                      0.0622, 0.0360, 0.0871, 0.1113, 0.0822, 0.0639, 0.0883, 0.0426, 0.0860,\n",
       "                      0.0579, 0.1928, 0.0605, 0.0692, 0.0564, 0.0713, 0.0521, 0.1012, 0.0730,\n",
       "                      0.0955, 0.1047, 0.0839, 0.0387, 0.0704, 0.0363, 0.0522, 0.0801, 0.0440,\n",
       "                      0.0446, 0.0612, 0.0704, 0.0938, 0.0431, 0.0637, 0.0568, 0.0968, 0.0654,\n",
       "                      0.0532, 0.0440, 0.0686, 0.1063, 0.0974, 0.0610, 0.0840, 0.0881, 0.1006,\n",
       "                      0.0421, 0.0703, 0.0776, 0.0586, 0.0718, 0.0633, 0.0630, 0.0705, 0.0817,\n",
       "                      0.0792, 0.1093, 0.0867, 0.0744, 0.1309, 0.0996, 0.0637, 0.0821, 0.0905,\n",
       "                      0.0821, 0.0777, 0.0455, 0.0826, 0.1044, 0.0494, 0.0835, 0.0710, 0.0492,\n",
       "                      0.0682, 0.0715, 0.0620, 0.0747, 0.1089, 0.1204, 0.0897, 0.1009, 0.1364,\n",
       "                      0.1101, 0.1012, 0.0799, 0.0844, 0.0873, 0.0743, 0.1184, 0.0555, 0.0965,\n",
       "                      0.0391, 0.0955, 0.0533, 0.0502, 0.0982, 0.0887, 0.0657, 0.0577, 0.0809,\n",
       "                      0.0629, 0.1131, 0.0508, 0.0787, 0.0933, 0.1138, 0.0638, 0.0844, 0.0642,\n",
       "                      0.1286, 0.1003, 0.0827, 0.0995, 0.0997, 0.0778, 0.0774, 0.0367, 0.0862,\n",
       "                      0.0940, 0.0964, 0.0737, 0.0955, 0.0510, 0.1137, 0.0888, 0.0855, 0.0567,\n",
       "                      0.1013, 0.0827, 0.0713, 0.0675, 0.0725, 0.0629, 0.1257, 0.0670, 0.0655,\n",
       "                      0.0946, 0.0422, 0.0730, 0.0723, 0.0506, 0.0703, 0.1064, 0.1130, 0.0560,\n",
       "                      0.0480, 0.0857, 0.0419, 0.1208, 0.0810, 0.0827, 0.0887, 0.0958, 0.0788,\n",
       "                      0.1231, 0.0698, 0.0698, 0.0441, 0.0720, 0.0178, 0.1296, 0.0865, 0.0673,\n",
       "                      0.0595, 0.0735, 0.0704, 0.0768, 0.0662, 0.0660, 0.0774, 0.0560, 0.0850,\n",
       "                      0.1036, 0.0716, 0.0468, 0.0851, 0.0600, 0.1095, 0.1423, 0.0845, 0.0694,\n",
       "                      0.0839, 0.0885, 0.0914, 0.0614, 0.0901, 0.0797, 0.0801, 0.0869, 0.0383,\n",
       "                      0.0906, 0.1130, 0.0673, 0.0758, 0.0715, 0.0666, 0.1026, 0.0593, 0.0895,\n",
       "                      0.0969, 0.1476, 0.0647, 0.0644, 0.0643, 0.0867, 0.0674, 0.0656, 0.0809,\n",
       "                      0.1149, 0.0638, 0.0826, 0.1238, 0.0890, 0.0747, 0.0443, 0.0384, 0.0696,\n",
       "                      0.0591, 0.0575, 0.1233, 0.1042, 0.0823, 0.0486, 0.1022, 0.1061, 0.0729,\n",
       "                      0.0764, 0.0708, 0.0348, 0.0909, 0.0527, 0.0834, 0.0721, 0.0970, 0.0994,\n",
       "                      0.1113, 0.0904, 0.0459, 0.0586, 0.0592, 0.0798, 0.0680, 0.1074, 0.0890,\n",
       "                      0.0857, 0.0707, 0.0409, 0.0736, 0.0537, 0.0956, 0.0791, 0.0863, 0.1091,\n",
       "                      0.0730, 0.1175, 0.0760, 0.0518, 0.1123, 0.0967, 0.0880, 0.0643, 0.0706,\n",
       "                      0.0915, 0.0346, 0.0703, 0.0948, 0.0716, 0.0917, 0.0648, 0.0885, 0.0730,\n",
       "                      0.0660, 0.1056, 0.0822, 0.0944, 0.1462, 0.0939, 0.0722, 0.0812, 0.0599,\n",
       "                      0.1052, 0.0547, 0.0794, 0.0999, 0.0691, 0.0768, 0.0740, 0.0734, 0.0424,\n",
       "                      0.0566, 0.0629, 0.0812, 0.0768, 0.0910, 0.0753, 0.0530, 0.0473, 0.0544,\n",
       "                      0.0610, 0.0670, 0.1009, 0.0801, 0.0979, 0.0768, 0.0993, 0.0822, 0.0942,\n",
       "                      0.0921, 0.1064, 0.1138, 0.1014, 0.0626, 0.0747, 0.0673, 0.1032, 0.0444,\n",
       "                      0.0698, 0.0603, 0.0471, 0.0853, 0.1091, 0.0667, 0.0658, 0.0462, 0.0546,\n",
       "                      0.1165, 0.1487, 0.0689, 0.0650, 0.0504, 0.0940, 0.0644, 0.1266, 0.0826,\n",
       "                      0.0672, 0.0425, 0.0717, 0.0889, 0.0421, 0.0732, 0.0480, 0.1138])),\n",
       "             ('layer4.0.bn1.bias',\n",
       "              tensor([-0.0467, -0.0636, -0.0593, -0.0330, -0.0084, -0.0955, -0.0480, -0.0133,\n",
       "                      -0.0585, -0.0002, -0.0296, -0.0311, -0.0048, -0.1104, -0.0241, -0.0427,\n",
       "                      -0.0327, -0.0403, -0.0472, -0.0696, -0.0548, -0.0355, -0.0358, -0.0574,\n",
       "                      -0.0285, -0.0339, -0.0310, -0.0293, -0.0366, -0.0085, -0.0723, -0.0608,\n",
       "                      -0.0210, -0.0337, -0.0343, -0.0666, -0.0161, -0.0466, -0.0042, -0.0470,\n",
       "                      -0.0387, -0.0359, -0.0107, -0.0410, -0.0415, -0.0608, -0.0480, -0.0213,\n",
       "                      -0.0416, -0.0210, -0.0614, -0.0072, -0.0692, -0.0547,  0.0089, -0.0580,\n",
       "                      -0.0902, -0.0057, -0.0346, -0.0179, -0.0163, -0.0583, -0.0496, -0.0253,\n",
       "                      -0.0424, -0.0381, -0.0457, -0.0334, -0.0119, -0.0419, -0.0794, -0.0246,\n",
       "                      -0.0497, -0.0693, -0.0239, -0.0096, -0.0610, -0.0081, -0.0298, -0.0111,\n",
       "                      -0.0327, -0.0417, -0.0142, -0.0071, -0.1157, -0.0842, -0.0783, -0.0399,\n",
       "                      -0.0790, -0.0146, -0.0275, -0.0457, -0.0128, -0.0187, -0.0092, -0.0566,\n",
       "                      -0.0318, -0.0330, -0.0495, -0.0264, -0.0593, -0.0008, -0.0397, -0.0469,\n",
       "                       0.0027, -0.0284, -0.0668, -0.0431, -0.0432, -0.0520, -0.0540, -0.0406,\n",
       "                      -0.0210, -0.0775, -0.0464, -0.0151, -0.0113, -0.0396, -0.0326, -0.0328,\n",
       "                      -0.0840, -0.0501, -0.0293, -0.0602, -0.0384, -0.0622, -0.0160, -0.0225,\n",
       "                      -0.0465,  0.0111, -0.0607, -0.0501, -0.0691, -0.0326, -0.0636, -0.0358,\n",
       "                      -0.0114, -0.0527, -0.0646, -0.0441, -0.0589, -0.0328, -0.0364, -0.0082,\n",
       "                      -0.0251, -0.0213, -0.0320, -0.0607, -0.0115, -0.0037, -0.0197, -0.0228,\n",
       "                      -0.0368, -0.0709, -0.0414, -0.0702, -0.0365, -0.0396, -0.0117, -0.0240,\n",
       "                      -0.0456, -0.1177, -0.0640, -0.0690, -0.0173, -0.0582, -0.0439, -0.0222,\n",
       "                      -0.0229, -0.0435, -0.0893, -0.0401, -0.0059, -0.0590, -0.0440, -0.0472,\n",
       "                      -0.0298, -0.0592, -0.0249, -0.0501, -0.0190, -0.0054, -0.0247, -0.0051,\n",
       "                       0.0063, -0.0393, -0.0231, -0.0743, -0.0410, -0.0585, -0.0686, -0.0394,\n",
       "                       0.0017, -0.0193, -0.0214, -0.0257, -0.0492, -0.0166, -0.0240, -0.0223,\n",
       "                      -0.0466, -0.0355, -0.0157, -0.0306, -0.0107, -0.0424, -0.0418, -0.0127,\n",
       "                      -0.0341, -0.0369, -0.1117, -0.0378, -0.0020, -0.0557, -0.0118, -0.0731,\n",
       "                      -0.0183, -0.0228, -0.0349, -0.0027, -0.0199, -0.0160, -0.0120, -0.0307,\n",
       "                      -0.0249, -0.0204, -0.0773, -0.0471, -0.0321, -0.0689, -0.0309, -0.0288,\n",
       "                      -0.0529, -0.0477, -0.0554, -0.0560, -0.0019, -0.0025, -0.0860, -0.0201,\n",
       "                      -0.0377, -0.0569, -0.0301, -0.0430, -0.0508, -0.0068, -0.0184, -0.0744,\n",
       "                      -0.0900, -0.0412, -0.0189, -0.0688, -0.0567, -0.0625, -0.0268, -0.0359,\n",
       "                      -0.0485, -0.0378, -0.0730, -0.0042, -0.0629, -0.0051,  0.0102, -0.0207,\n",
       "                      -0.0199, -0.0613,  0.0024, -0.0712, -0.0243, -0.0397, -0.0318, -0.0716,\n",
       "                      -0.0383, -0.0189, -0.0670, -0.0634, -0.0380, -0.0649, -0.0516, -0.1414,\n",
       "                      -0.0686, -0.0501, -0.0721, -0.0740, -0.0253, -0.0614, -0.0278, -0.0548,\n",
       "                      -0.0502, -0.0293, -0.0312, -0.0266, -0.0305, -0.0480, -0.0201, -0.0483,\n",
       "                      -0.0514, -0.0270, -0.0382, -0.0346, -0.0511, -0.0380, -0.0262, -0.0640,\n",
       "                      -0.0568, -0.0481, -0.0192, -0.0280, -0.0520, -0.0416, -0.0234, -0.0250,\n",
       "                      -0.0539, -0.0475, -0.0150, -0.0123, -0.0590, -0.0252, -0.1040, -0.0880,\n",
       "                      -0.0587, -0.0639, -0.0695, -0.0110, -0.0707, -0.0457, -0.0416, -0.0216,\n",
       "                      -0.0583, -0.0012, -0.0011, -0.0562, -0.0394, -0.0295, -0.0699, -0.0166,\n",
       "                      -0.0049, -0.0376, -0.0247, -0.0615, -0.0303, -0.0751, -0.0556,  0.0055,\n",
       "                      -0.0302, -0.0595, -0.0226, -0.0667, -0.0828, -0.0405, -0.0515, -0.0452,\n",
       "                      -0.0679, -0.0361, -0.0310, -0.0305, -0.0707, -0.0564, -0.0408, -0.0231,\n",
       "                      -0.0792, -0.0704, -0.0605, -0.0118, -0.0435, -0.0328, -0.0670, -0.0278,\n",
       "                      -0.0891, -0.0228, -0.0306, -0.0358, -0.0305, -0.0295, -0.0437, -0.0349,\n",
       "                      -0.0602, -0.0152, -0.0866, -0.0547, -0.0338, -0.1068, -0.0623, -0.0370,\n",
       "                      -0.0391, -0.0151, -0.0679, -0.0126, -0.0183, -0.0589, -0.0574, -0.0639,\n",
       "                      -0.0275, -0.0510, -0.0649, -0.0367, -0.0486, -0.0079, -0.0115, -0.0609,\n",
       "                      -0.0236, -0.0444, -0.0175, -0.0620, -0.0779, -0.0992, -0.0667, -0.0006,\n",
       "                      -0.0090, -0.0271, -0.0630, -0.0348, -0.0321, -0.0716, -0.0843, -0.0476,\n",
       "                       0.0155, -0.0480, -0.0265, -0.0631, -0.0398, -0.0340, -0.0893, -0.0811,\n",
       "                      -0.0808, -0.0378, -0.0193, -0.0464, -0.0498, -0.0357, -0.0175, -0.0332,\n",
       "                      -0.0642, -0.0019, -0.0313, -0.0576, -0.0046, -0.0103, -0.0382, -0.0412,\n",
       "                      -0.0472, -0.0247, -0.0406, -0.0501, -0.0373, -0.0914, -0.0547, -0.0317,\n",
       "                      -0.0528, -0.0169, -0.0418, -0.0434, -0.0615, -0.0688, -0.0270, -0.0514,\n",
       "                      -0.0406, -0.0438, -0.0125, -0.0392, -0.0504, -0.0401, -0.0369, -0.0367,\n",
       "                      -0.0171, -0.0285, -0.0160, -0.0212, -0.0317, -0.0247, -0.0494, -0.0556,\n",
       "                      -0.0664, -0.0413, -0.0756, -0.0113, -0.0512, -0.0783, -0.0514, -0.0491,\n",
       "                      -0.0764, -0.0284, -0.0242, -0.0569, -0.0751, -0.0045, -0.0210, -0.0317,\n",
       "                      -0.0233, -0.0503, -0.0656, -0.0416, -0.0264, -0.0203, -0.0077, -0.0296,\n",
       "                      -0.0906, -0.0130, -0.0443, -0.0151, -0.0573, -0.0589, -0.1114, -0.0558,\n",
       "                      -0.0434, -0.0062, -0.0407, -0.0618, -0.0348, -0.0330, -0.0317, -0.0670])),\n",
       "             ('layer4.0.bn1.running_mean',\n",
       "              tensor([-3.0252e-02, -3.9552e-02, -5.5229e-03, -2.9678e-02, -1.8901e-02,\n",
       "                      -2.0034e-02, -2.7625e-02, -2.7302e-02, -5.0055e-03, -2.7170e-02,\n",
       "                      -1.2932e-03, -8.5174e-03, -1.7275e-02, -1.9451e-02, -2.3695e-02,\n",
       "                      -2.9412e-02, -1.7967e-02, -2.9026e-02, -5.8284e-02, -3.8557e-02,\n",
       "                      -2.2923e-02, -9.2862e-03, -1.3225e-02, -2.1091e-02, -1.6034e-02,\n",
       "                      -1.4824e-02, -8.8255e-03, -3.1285e-02, -6.5268e-03,  8.7191e-03,\n",
       "                      -6.1060e-03, -3.3640e-02, -3.2943e-02, -1.8644e-02, -1.3997e-02,\n",
       "                      -8.3894e-03, -1.7054e-02, -2.4889e-02, -9.2005e-03, -1.9620e-02,\n",
       "                      -4.5619e-02, -2.7886e-02, -2.9823e-02, -1.5585e-02, -6.6257e-03,\n",
       "                      -2.7814e-02, -1.8699e-02, -1.5255e-02, -3.2837e-02, -3.3798e-03,\n",
       "                      -2.5370e-02, -2.0149e-02, -8.0114e-03, -5.2693e-03, -1.3682e-02,\n",
       "                      -3.5444e-02, -5.3606e-02, -2.5864e-02, -3.7521e-02, -2.5559e-02,\n",
       "                      -4.0529e-02,  6.9951e-03, -1.2090e-02, -2.8326e-02, -2.8779e-02,\n",
       "                      -2.3985e-02, -1.7972e-02, -3.6025e-02, -2.9414e-02, -1.7624e-02,\n",
       "                      -2.1906e-02, -1.9982e-02, -2.3272e-02, -7.4346e-03, -2.5240e-02,\n",
       "                      -3.0127e-02, -1.1206e-02, -1.0860e-02, -1.6182e-02, -2.6168e-02,\n",
       "                      -2.4954e-02, -1.5868e-02, -1.0815e-02, -3.5011e-02, -1.2986e-02,\n",
       "                       1.0396e-02, -2.3766e-02, -1.7283e-02, -1.9035e-02, -2.1043e-02,\n",
       "                      -3.7267e-02, -1.2485e-02, -1.8298e-02, -2.7199e-02, -1.1678e-02,\n",
       "                      -7.1471e-03, -1.4733e-02, -2.2778e-02,  8.1991e-04, -2.0726e-02,\n",
       "                      -8.7620e-03, -2.2432e-02, -1.6920e-02, -1.1477e-04, -2.9557e-02,\n",
       "                      -1.4842e-02, -4.4247e-02, -6.5030e-03, -3.3561e-02, -7.9637e-03,\n",
       "                      -1.4307e-02, -2.1258e-02, -2.2617e-02, -8.5162e-03, -2.8422e-02,\n",
       "                      -4.1823e-02, -1.6493e-02, -1.9236e-02, -3.5220e-02, -1.4374e-02,\n",
       "                      -4.2240e-02, -2.5615e-02, -2.1743e-02, -3.2957e-02, -2.1233e-02,\n",
       "                      -2.3429e-02, -7.7345e-03, -7.9011e-03,  4.5319e-03, -8.2923e-03,\n",
       "                      -8.9462e-03, -4.5999e-03, -1.5585e-02, -4.2866e-02, -2.0346e-02,\n",
       "                      -2.2949e-02, -1.2528e-02, -3.4812e-02, -2.2212e-02, -2.4459e-02,\n",
       "                       3.7403e-03, -1.6256e-02, -1.8118e-02, -2.4814e-02, -1.8720e-02,\n",
       "                      -3.3476e-02, -2.9565e-02, -2.7727e-03, -1.5799e-02, -1.9898e-02,\n",
       "                      -3.8584e-02, -3.0485e-02, -1.1689e-02, -1.4720e-02, -1.3767e-03,\n",
       "                      -2.1419e-02, -3.5139e-02, -1.9114e-02, -2.9042e-02, -6.5607e-03,\n",
       "                      -4.9849e-02,  4.2610e-03, -1.6494e-02, -1.2399e-02, -1.4860e-02,\n",
       "                       7.6050e-04, -1.4845e-02, -5.8724e-03, -7.5723e-03, -2.5697e-02,\n",
       "                      -1.6484e-02, -7.3585e-03, -1.7777e-02, -1.6810e-02, -3.7968e-02,\n",
       "                      -1.9955e-02, -2.6941e-03, -2.8337e-02, -1.7108e-02, -4.1965e-02,\n",
       "                      -2.5427e-02,  9.8648e-03, -3.0657e-02, -1.7219e-02, -2.6623e-02,\n",
       "                      -1.6635e-02, -2.2973e-02, -8.6048e-03,  3.3702e-04, -2.0185e-02,\n",
       "                      -2.5088e-02, -3.5141e-02, -1.3856e-02, -3.8586e-02, -1.0549e-02,\n",
       "                      -1.5560e-02, -1.8665e-02,  6.0424e-04, -1.9630e-02, -3.2021e-02,\n",
       "                      -1.7599e-02, -9.1306e-03, -2.3877e-02, -7.1492e-03, -2.6495e-02,\n",
       "                      -2.0783e-02, -1.3251e-02, -2.0158e-02, -1.1544e-02, -2.9714e-02,\n",
       "                       1.0406e-02, -2.9052e-02, -2.1877e-02, -1.4954e-02, -2.0590e-02,\n",
       "                      -3.1309e-02, -1.6383e-02, -2.2125e-02, -3.7968e-02, -3.0366e-02,\n",
       "                      -1.6814e-02, -1.8045e-02, -2.0841e-02, -1.8449e-02, -2.3466e-02,\n",
       "                      -2.0571e-02,  3.1702e-02, -2.5330e-02, -1.1699e-02, -1.4577e-02,\n",
       "                       3.6411e-03, -1.3398e-02, -1.6773e-02, -9.2423e-03, -1.5529e-02,\n",
       "                      -2.2898e-02, -2.7819e-03, -3.2830e-02, -2.1632e-02, -1.4728e-02,\n",
       "                      -3.8558e-02, -2.6106e-02, -1.0559e-02, -1.8161e-02, -8.0347e-03,\n",
       "                      -1.5203e-02, -1.5705e-02, -3.7787e-02,  2.6704e-02, -2.0141e-02,\n",
       "                      -8.4931e-03, -3.1189e-02, -3.1524e-03, -2.1642e-02, -4.0635e-02,\n",
       "                      -3.7278e-02, -9.9546e-03, -1.8408e-03, -4.0139e-02, -3.4362e-02,\n",
       "                      -1.8239e-02, -1.7463e-02, -4.1922e-02, -2.0131e-02, -8.5734e-03,\n",
       "                      -2.7580e-02, -2.7241e-02, -9.5056e-03, -1.2172e-02, -2.7275e-02,\n",
       "                      -2.1120e-02, -3.8046e-02, -7.1067e-03, -2.3226e-02, -4.4777e-03,\n",
       "                      -4.7883e-02, -7.0610e-03, -4.3477e-03, -2.8340e-02, -3.0330e-02,\n",
       "                      -1.9940e-02, -3.6552e-02, -3.1953e-02, -1.9420e-02, -2.7699e-02,\n",
       "                      -1.4587e-02, -2.7500e-02, -1.6761e-02, -2.3741e-02, -4.3393e-03,\n",
       "                      -2.6788e-02, -1.4183e-02, -3.0355e-02, -9.7050e-03, -3.8276e-02,\n",
       "                      -3.2750e-02, -2.0803e-02, -3.7626e-02, -5.5750e-03, -1.1004e-02,\n",
       "                      -2.1437e-02, -2.9176e-02, -1.7099e-02, -3.6356e-02, -3.4772e-03,\n",
       "                      -2.7771e-02, -3.1059e-02, -9.6952e-03, -2.3028e-02, -4.0112e-02,\n",
       "                      -1.6442e-02, -2.5000e-02, -2.6978e-02, -3.9607e-02,  4.7224e-03,\n",
       "                      -2.0329e-02, -4.7022e-03, -1.9721e-02,  2.4633e-02,  8.4847e-04,\n",
       "                      -1.8790e-02, -2.8019e-02, -5.8940e-03, -2.3428e-02,  2.0028e-02,\n",
       "                      -3.6336e-02,  2.4536e-03, -9.4251e-03, -2.5907e-02, -7.2180e-03,\n",
       "                      -5.9481e-02, -6.4316e-03, -3.3754e-02, -4.0097e-02, -1.3850e-02,\n",
       "                      -2.6609e-02, -2.9068e-02, -1.6830e-02, -2.9859e-02, -1.4635e-02,\n",
       "                      -2.2340e-02, -1.8613e-02, -4.1803e-02, -2.0284e-02, -1.0542e-02,\n",
       "                      -2.2248e-02, -3.0661e-02, -1.8131e-02, -1.7278e-02, -2.1827e-02,\n",
       "                      -4.0521e-03, -2.4284e-02, -2.6483e-02, -3.8637e-02, -1.9436e-02,\n",
       "                      -1.2269e-02, -5.3629e-02, -1.7919e-02, -2.8991e-02, -1.3188e-02,\n",
       "                      -2.5093e-02, -2.3713e-02,  1.3297e-02, -3.0571e-02, -1.2733e-02,\n",
       "                      -1.0600e-02,  9.6828e-03, -2.0457e-02, -1.9100e-02,  2.2862e-02,\n",
       "                       2.9910e-02, -1.7221e-02, -4.3264e-03, -1.4114e-02, -2.0886e-02,\n",
       "                      -1.6702e-02,  4.2203e-03, -1.3392e-02, -4.2631e-02, -1.3846e-02,\n",
       "                      -2.1888e-02,  4.7175e-03, -3.3885e-02, -3.4308e-02,  1.0786e-03,\n",
       "                      -8.9867e-03, -2.3082e-02, -3.3949e-02, -4.1775e-03, -4.2834e-02,\n",
       "                      -2.9761e-02, -2.8797e-03, -7.0854e-03, -1.8991e-02, -1.9236e-02,\n",
       "                      -1.8596e-02, -2.6332e-02, -1.3896e-02, -8.1039e-03, -6.5421e-03,\n",
       "                      -2.3093e-02, -2.6466e-02, -2.2600e-02, -2.6061e-02, -2.3974e-02,\n",
       "                      -9.4220e-03, -1.8156e-02,  2.5150e-03, -2.3149e-02, -3.1183e-02,\n",
       "                      -4.4018e-03, -3.7703e-03, -3.0746e-02, -2.9790e-02, -3.8666e-02,\n",
       "                      -2.9637e-02, -1.4307e-02, -1.2356e-02, -3.7990e-02, -2.0509e-02,\n",
       "                      -3.9456e-02, -3.9617e-02, -5.9867e-02, -4.7909e-03,  7.9042e-05,\n",
       "                      -2.8440e-02, -2.6620e-02, -1.7450e-02, -2.2627e-02, -1.7104e-02,\n",
       "                      -3.3165e-02, -2.1596e-02, -2.8413e-02, -2.2532e-02, -7.5342e-03,\n",
       "                      -2.0566e-02, -4.7492e-02, -3.4692e-02, -1.9547e-02, -2.9450e-02,\n",
       "                      -3.1470e-02, -3.4075e-02,  1.0271e-04, -3.5456e-02, -5.1923e-03,\n",
       "                      -1.9087e-02, -2.5290e-02, -1.8644e-02, -1.6968e-02, -3.8457e-02,\n",
       "                      -3.5323e-02, -9.6769e-03, -1.5305e-02, -2.6846e-02, -2.7526e-02,\n",
       "                       2.0426e-03, -1.5623e-02, -1.1553e-02, -1.6272e-02,  3.1198e-04,\n",
       "                      -7.8634e-03, -1.8772e-02, -1.3423e-02, -2.9724e-02, -2.3317e-02,\n",
       "                      -7.8872e-03, -1.5527e-02, -6.9166e-03, -4.0562e-03, -1.8020e-02,\n",
       "                      -2.6273e-02, -4.7266e-02, -2.4758e-02, -3.8254e-02, -4.8264e-02,\n",
       "                      -3.1646e-02, -1.1612e-02, -1.9124e-02, -8.4838e-03, -1.9179e-02,\n",
       "                      -1.1425e-02, -3.1999e-02, -1.7875e-02, -9.4093e-03, -4.3224e-03,\n",
       "                      -2.4307e-02, -6.6540e-03, -3.0922e-02, -1.8278e-02, -2.0351e-02,\n",
       "                      -3.4771e-02, -3.4196e-02, -1.2188e-02, -1.2142e-02, -1.2070e-02,\n",
       "                      -4.2664e-02, -3.6551e-02, -3.2022e-03, -1.1403e-02, -3.6501e-02,\n",
       "                      -2.2857e-02, -2.2483e-02, -1.5517e-03, -1.2028e-02, -2.2912e-02,\n",
       "                      -2.1187e-02,  1.8935e-03, -1.2325e-02, -1.7239e-02, -2.3548e-02,\n",
       "                      -1.0487e-02, -1.7840e-02])),\n",
       "             ('layer4.0.bn1.running_var',\n",
       "              tensor([8.3496e-04, 1.1353e-03, 1.2192e-03, 5.2477e-04, 7.2506e-04, 1.3058e-03,\n",
       "                      1.2681e-03, 1.0002e-03, 1.4040e-03, 3.9845e-04, 6.7887e-04, 9.2334e-04,\n",
       "                      1.1386e-03, 1.7932e-03, 9.0529e-04, 1.0676e-03, 6.8296e-04, 8.6640e-04,\n",
       "                      2.0197e-03, 1.9865e-03, 1.3337e-03, 1.5762e-03, 6.2575e-04, 8.8881e-04,\n",
       "                      3.7572e-04, 6.1068e-04, 5.9511e-04, 7.5324e-04, 5.7571e-04, 5.8370e-04,\n",
       "                      1.2041e-03, 1.0268e-03, 7.0523e-04, 6.1725e-04, 8.5726e-04, 1.4993e-03,\n",
       "                      4.5498e-04, 1.0985e-03, 4.5070e-04, 1.0401e-03, 2.0399e-03, 1.3517e-03,\n",
       "                      8.8758e-04, 9.2286e-04, 1.3343e-03, 1.0754e-03, 1.8872e-03, 5.2913e-04,\n",
       "                      1.0959e-03, 4.8519e-04, 9.7068e-04, 6.2788e-04, 9.0406e-04, 4.7847e-04,\n",
       "                      1.0162e-03, 1.0981e-03, 1.5885e-03, 5.9145e-04, 1.6460e-03, 6.2388e-04,\n",
       "                      9.4532e-04, 1.7039e-03, 1.3022e-03, 1.1642e-03, 1.5223e-03, 8.7488e-04,\n",
       "                      1.2082e-03, 1.0777e-03, 1.6146e-03, 9.9699e-04, 1.9719e-03, 1.2208e-03,\n",
       "                      1.1567e-03, 1.6746e-03, 9.7326e-04, 6.8088e-04, 1.2604e-03, 1.7946e-04,\n",
       "                      6.7144e-04, 1.1583e-03, 7.9516e-04, 6.1986e-04, 8.3503e-04, 6.8285e-04,\n",
       "                      1.9035e-03, 1.3956e-03, 2.2154e-03, 9.5530e-04, 1.1221e-03, 8.0691e-04,\n",
       "                      8.6063e-04, 6.1849e-04, 4.3889e-04, 9.9375e-04, 3.3667e-04, 1.5191e-03,\n",
       "                      3.6603e-04, 1.1493e-03, 8.3751e-04, 4.7431e-04, 9.3351e-04, 4.4237e-04,\n",
       "                      5.9648e-04, 5.3179e-04, 9.2469e-04, 1.2905e-03, 1.8231e-03, 7.6158e-04,\n",
       "                      8.2581e-04, 7.6060e-04, 6.8542e-04, 6.9141e-04, 7.6203e-04, 1.2872e-03,\n",
       "                      1.5990e-03, 9.2384e-04, 5.5598e-04, 1.2013e-03, 1.0939e-03, 6.3661e-04,\n",
       "                      2.2580e-03, 1.9143e-03, 1.3030e-03, 1.0696e-03, 9.6999e-04, 1.4975e-03,\n",
       "                      1.6123e-03, 2.8765e-04, 5.9583e-04, 7.3204e-04, 2.2529e-03, 8.5130e-04,\n",
       "                      1.7766e-03, 8.9980e-04, 1.2642e-03, 9.4075e-04, 2.6052e-04, 1.2326e-03,\n",
       "                      1.4245e-03, 8.7890e-04, 2.6015e-03, 1.2944e-03, 6.0380e-04, 1.2260e-03,\n",
       "                      1.3504e-03, 9.1368e-04, 1.3788e-03, 1.2017e-03, 2.0844e-04, 1.0718e-03,\n",
       "                      9.8237e-04, 1.1693e-03, 8.2605e-04, 1.3022e-03, 6.1817e-04, 1.7152e-03,\n",
       "                      1.5271e-03, 7.0770e-04, 1.0366e-03, 3.1897e-04, 1.1595e-03, 1.5017e-03,\n",
       "                      1.1761e-03, 1.8652e-03, 5.6067e-04, 9.0325e-04, 8.0063e-04, 2.4504e-04,\n",
       "                      7.1131e-04, 8.3398e-04, 1.8815e-03, 4.4996e-04, 2.3680e-04, 1.5515e-03,\n",
       "                      1.6796e-03, 1.4130e-03, 4.5850e-04, 1.6183e-03, 3.0279e-04, 1.4864e-03,\n",
       "                      5.9064e-04, 2.0660e-03, 6.1805e-04, 4.6111e-04, 5.2993e-04, 6.5561e-04,\n",
       "                      8.5855e-04, 1.5158e-03, 8.3432e-04, 1.2282e-03, 1.2997e-03, 1.3687e-03,\n",
       "                      2.3915e-04, 1.3528e-03, 1.7686e-04, 4.0162e-04, 7.2865e-04, 4.1385e-04,\n",
       "                      6.9263e-04, 1.1489e-03, 5.9894e-04, 1.4473e-03, 2.8136e-04, 7.5269e-04,\n",
       "                      5.0517e-04, 1.1387e-03, 7.5891e-04, 5.9290e-04, 3.3000e-04, 1.7014e-03,\n",
       "                      1.5930e-03, 1.1610e-03, 9.2661e-04, 7.8142e-04, 9.8464e-04, 1.5074e-03,\n",
       "                      3.7281e-04, 6.8510e-04, 1.3195e-03, 1.6006e-03, 8.4892e-04, 1.2003e-03,\n",
       "                      5.7240e-04, 9.7038e-04, 1.3695e-03, 1.0674e-03, 1.5889e-03, 1.0216e-03,\n",
       "                      6.8233e-04, 1.9802e-03, 9.5297e-04, 5.8205e-04, 8.8054e-04, 1.1370e-03,\n",
       "                      8.0420e-04, 9.4203e-04, 1.7095e-04, 9.6735e-04, 1.5037e-03, 3.5342e-04,\n",
       "                      7.8790e-04, 1.1525e-03, 5.8256e-04, 5.7566e-04, 7.1574e-04, 3.9590e-04,\n",
       "                      5.6387e-04, 2.2256e-03, 1.7196e-03, 8.7703e-04, 1.0329e-03, 2.5587e-03,\n",
       "                      1.1110e-03, 1.0893e-03, 1.2243e-03, 9.6195e-04, 1.2673e-03, 7.3947e-04,\n",
       "                      1.9270e-03, 5.3458e-04, 1.6330e-03, 4.7098e-04, 2.0329e-03, 1.1782e-03,\n",
       "                      4.8183e-04, 1.5605e-03, 1.0673e-03, 8.1919e-04, 5.0285e-04, 9.1769e-04,\n",
       "                      7.9418e-04, 1.9691e-03, 6.4528e-04, 8.0834e-04, 1.7712e-03, 1.7121e-03,\n",
       "                      8.1596e-04, 9.4714e-04, 8.4106e-04, 2.2472e-03, 1.7955e-03, 1.5245e-03,\n",
       "                      1.5323e-03, 9.7001e-04, 6.9019e-04, 9.8742e-04, 3.5086e-04, 8.8644e-04,\n",
       "                      1.3798e-03, 1.4541e-03, 7.3896e-04, 1.1617e-03, 7.0650e-04, 1.4406e-03,\n",
       "                      1.7961e-03, 1.3978e-03, 6.4515e-04, 1.2215e-03, 6.0317e-04, 7.1898e-04,\n",
       "                      7.1461e-04, 1.0407e-03, 8.4329e-04, 2.5148e-03, 8.5975e-04, 7.2542e-04,\n",
       "                      1.8421e-03, 3.6076e-04, 1.0342e-03, 1.0710e-03, 4.6899e-04, 6.4379e-04,\n",
       "                      1.7906e-03, 1.4168e-03, 4.6392e-04, 5.4073e-04, 7.6723e-04, 3.5883e-04,\n",
       "                      1.8856e-03, 8.5317e-04, 8.1013e-04, 1.2469e-03, 9.4394e-04, 9.1445e-04,\n",
       "                      2.5810e-03, 2.2917e-03, 5.2661e-04, 4.9876e-04, 9.6252e-04, 5.5489e-05,\n",
       "                      1.9258e-03, 8.3207e-04, 1.0096e-03, 8.5588e-04, 8.0687e-04, 9.4214e-04,\n",
       "                      9.8351e-04, 1.0094e-03, 8.8803e-04, 8.2649e-04, 6.1437e-04, 1.2189e-03,\n",
       "                      2.2750e-03, 6.7173e-04, 6.5958e-04, 1.1206e-03, 6.2674e-04, 2.0807e-03,\n",
       "                      3.3292e-03, 9.3242e-04, 4.1973e-04, 9.1577e-04, 1.0162e-03, 1.0345e-03,\n",
       "                      7.0331e-04, 1.1154e-03, 1.5778e-03, 8.8266e-04, 1.0365e-03, 1.6412e-04,\n",
       "                      1.0468e-03, 1.3070e-03, 8.7534e-04, 1.0246e-03, 5.8562e-04, 6.1146e-04,\n",
       "                      1.0263e-03, 7.4649e-04, 1.0885e-03, 1.3111e-03, 1.8301e-03, 4.8074e-04,\n",
       "                      4.3558e-04, 6.7286e-04, 1.0482e-03, 6.0385e-04, 7.8445e-04, 7.1906e-04,\n",
       "                      1.4828e-03, 6.0799e-04, 1.2810e-03, 1.5090e-03, 1.4726e-03, 8.1444e-04,\n",
       "                      2.3807e-04, 1.5055e-04, 1.1043e-03, 6.8851e-04, 4.4142e-04, 1.4568e-03,\n",
       "                      1.0184e-03, 5.9787e-04, 1.9844e-04, 6.7464e-04, 1.6236e-03, 8.1277e-04,\n",
       "                      1.1038e-03, 1.1413e-03, 2.3455e-04, 1.1351e-03, 4.2710e-04, 1.2276e-03,\n",
       "                      4.1380e-04, 8.7867e-04, 1.6847e-03, 1.5590e-03, 1.1842e-03, 2.0001e-04,\n",
       "                      5.2842e-04, 8.4916e-04, 6.5934e-04, 5.2941e-04, 2.0989e-03, 1.2965e-03,\n",
       "                      1.3726e-03, 7.4240e-04, 4.7022e-04, 1.1173e-03, 9.7360e-04, 9.4304e-04,\n",
       "                      1.0077e-03, 1.0759e-03, 2.0826e-03, 1.1101e-03, 1.1407e-03, 1.0206e-03,\n",
       "                      5.7722e-04, 1.3578e-03, 1.0804e-03, 1.0820e-03, 1.6046e-03, 7.4398e-04,\n",
       "                      8.2051e-04, 2.1136e-04, 7.1836e-04, 1.2314e-03, 6.8476e-04, 1.6716e-03,\n",
       "                      6.1405e-04, 1.0529e-03, 1.8261e-03, 7.0964e-04, 1.4234e-03, 1.2172e-03,\n",
       "                      1.1999e-03, 2.4688e-03, 1.0044e-03, 7.6952e-04, 9.4126e-04, 7.4806e-04,\n",
       "                      1.9006e-03, 4.1236e-04, 7.0434e-04, 1.4625e-03, 7.0488e-04, 1.0889e-03,\n",
       "                      8.0794e-04, 8.0688e-04, 2.9952e-04, 4.9976e-04, 5.7870e-04, 1.6971e-03,\n",
       "                      6.8911e-04, 1.5659e-03, 1.5066e-03, 4.0434e-04, 3.5006e-04, 3.3253e-04,\n",
       "                      7.9089e-04, 6.8339e-04, 2.7953e-03, 9.6504e-04, 1.3008e-03, 9.2265e-04,\n",
       "                      1.7758e-03, 8.3475e-04, 1.1432e-03, 1.4236e-03, 1.4103e-03, 1.9977e-03,\n",
       "                      1.2211e-03, 6.7133e-04, 9.0292e-04, 6.7195e-04, 2.0727e-03, 4.1406e-04,\n",
       "                      9.9728e-04, 8.1131e-04, 4.9516e-04, 1.2372e-03, 1.7349e-03, 1.0552e-03,\n",
       "                      6.2586e-04, 4.6671e-04, 5.3233e-04, 1.3073e-03, 2.8495e-03, 8.4583e-04,\n",
       "                      5.7084e-04, 6.6660e-04, 1.0160e-03, 8.2428e-04, 1.0076e-03, 1.5522e-03,\n",
       "                      5.6006e-04, 5.2708e-04, 7.7055e-04, 1.8562e-03, 6.0865e-04, 9.3431e-04,\n",
       "                      5.0291e-04, 1.3996e-03])),\n",
       "             ('layer4.0.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.0.conv2.weight',\n",
       "              tensor([[[[ 4.5232e-03,  6.0165e-03,  4.3043e-03],\n",
       "                        [ 1.7185e-03, -1.5987e-04, -3.4663e-03],\n",
       "                        [-1.3381e-04, -7.6113e-04, -1.0373e-03]],\n",
       "              \n",
       "                       [[-4.2191e-03, -5.3148e-03,  6.2430e-04],\n",
       "                        [-2.2487e-03, -4.3730e-03,  1.0407e-03],\n",
       "                        [ 8.8806e-04, -4.7979e-03,  4.1124e-03]],\n",
       "              \n",
       "                       [[-7.3192e-04,  3.1020e-03,  2.0724e-03],\n",
       "                        [ 1.7290e-03, -1.0696e-03,  5.0863e-03],\n",
       "                        [-1.6549e-03, -2.8221e-03, -6.1322e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.8995e-04,  1.4413e-03,  1.3639e-03],\n",
       "                        [-1.8730e-03,  2.8344e-03, -2.8465e-03],\n",
       "                        [-1.3081e-03, -1.9864e-03, -4.8596e-03]],\n",
       "              \n",
       "                       [[ 2.8092e-03,  6.0238e-03,  3.2985e-03],\n",
       "                        [ 5.1465e-03,  4.2486e-03,  5.9831e-03],\n",
       "                        [-3.6718e-03, -1.3614e-03, -6.5065e-03]],\n",
       "              \n",
       "                       [[ 1.1103e-03, -2.2513e-03,  6.2078e-03],\n",
       "                        [-7.6308e-04, -4.1153e-03,  2.3337e-03],\n",
       "                        [-4.4303e-03, -8.6017e-03,  3.1386e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7691e-03, -4.7142e-03, -6.2676e-03],\n",
       "                        [-1.1119e-03, -5.0000e-03, -6.8074e-04],\n",
       "                        [ 3.9826e-03,  3.9808e-03,  5.6929e-03]],\n",
       "              \n",
       "                       [[-1.8487e-04,  1.9814e-03,  6.6392e-03],\n",
       "                        [ 1.0304e-04,  1.9771e-03,  2.1894e-03],\n",
       "                        [-6.4124e-04,  4.5136e-03,  2.5782e-03]],\n",
       "              \n",
       "                       [[ 1.5205e-03, -6.9434e-03,  1.0953e-03],\n",
       "                        [-2.3411e-03, -1.6496e-03,  1.7046e-04],\n",
       "                        [ 2.6364e-03, -2.5061e-03, -1.9038e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.4578e-03, -4.6273e-04, -1.1755e-04],\n",
       "                        [ 1.8922e-03,  7.6357e-04, -3.7915e-03],\n",
       "                        [ 1.3013e-03, -4.2958e-04, -3.5216e-03]],\n",
       "              \n",
       "                       [[-8.2689e-04, -3.9638e-03, -1.2944e-03],\n",
       "                        [-1.4530e-03, -2.4377e-03, -3.6445e-03],\n",
       "                        [-1.2504e-04,  1.5132e-03,  1.2087e-03]],\n",
       "              \n",
       "                       [[-3.9880e-03, -3.8590e-03, -2.6633e-03],\n",
       "                        [-2.2361e-03, -3.1965e-03, -2.5352e-03],\n",
       "                        [-2.4305e-03, -3.0616e-03, -3.2944e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4998e-03, -2.0512e-03, -8.4175e-04],\n",
       "                        [-2.5671e-03, -2.2345e-03, -2.2587e-03],\n",
       "                        [-3.7029e-03, -4.2517e-03, -4.6514e-03]],\n",
       "              \n",
       "                       [[-3.5349e-03, -1.1368e-03, -1.0158e-03],\n",
       "                        [-3.5437e-04,  8.3457e-04,  2.1724e-03],\n",
       "                        [ 3.6958e-04,  6.2832e-03,  4.3005e-03]],\n",
       "              \n",
       "                       [[-6.1940e-03, -1.3786e-03, -4.1020e-03],\n",
       "                        [-2.5251e-03, -4.0349e-03,  9.1895e-04],\n",
       "                        [-8.3701e-04, -1.4960e-03, -3.0466e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.8546e-03, -6.0326e-04,  2.6601e-03],\n",
       "                        [ 1.7937e-03,  1.0868e-03,  1.9647e-03],\n",
       "                        [-8.6092e-04,  9.8585e-04,  1.3698e-03]],\n",
       "              \n",
       "                       [[ 1.6646e-03,  9.5514e-04,  7.2103e-04],\n",
       "                        [ 9.5802e-04, -1.0330e-03,  1.7961e-03],\n",
       "                        [ 1.5987e-03, -3.7358e-04,  5.0390e-04]],\n",
       "              \n",
       "                       [[-1.7796e-03, -5.1454e-03, -2.8047e-03],\n",
       "                        [-2.3537e-03, -1.1163e-03, -3.1599e-03],\n",
       "                        [-1.8035e-04, -7.7223e-04, -1.3682e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4779e-04,  6.1039e-03,  9.3224e-03],\n",
       "                        [ 4.9976e-03,  1.5597e-03,  5.4842e-03],\n",
       "                        [ 7.5782e-05,  2.6536e-04,  2.4893e-03]],\n",
       "              \n",
       "                       [[ 7.5516e-04, -3.8347e-04, -3.0068e-03],\n",
       "                        [-1.1971e-03, -3.9758e-03, -4.9615e-03],\n",
       "                        [ 2.7296e-04, -7.4721e-03, -6.5600e-03]],\n",
       "              \n",
       "                       [[-5.3697e-03, -4.8193e-03, -2.1619e-03],\n",
       "                        [ 7.4661e-04,  5.5800e-04,  9.3213e-04],\n",
       "                        [-2.3847e-03,  4.5889e-03, -3.1576e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.2047e-03,  1.0823e-04, -3.9873e-03],\n",
       "                        [-7.2701e-03, -5.1704e-04, -2.4765e-03],\n",
       "                        [-4.5531e-05, -3.1363e-04,  6.0182e-04]],\n",
       "              \n",
       "                       [[-5.6137e-04,  7.5438e-04, -3.5134e-04],\n",
       "                        [ 7.1609e-04, -3.6659e-04, -1.2591e-03],\n",
       "                        [ 3.6641e-06, -3.2232e-03, -1.9710e-03]],\n",
       "              \n",
       "                       [[-8.2406e-04,  5.3508e-03,  1.8909e-03],\n",
       "                        [ 2.4287e-03,  9.2845e-03,  4.5195e-03],\n",
       "                        [ 1.4048e-02,  1.6114e-02,  1.5693e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8381e-04,  6.4507e-04,  4.7279e-04],\n",
       "                        [-2.9485e-04, -5.0488e-04, -1.4258e-03],\n",
       "                        [-1.3792e-03, -3.5694e-03, -4.3303e-03]],\n",
       "              \n",
       "                       [[-2.7913e-03, -3.5110e-03, -3.9893e-03],\n",
       "                        [ 7.6628e-04, -4.9453e-04,  1.0168e-03],\n",
       "                        [ 8.1796e-04,  2.3454e-03,  1.2158e-04]],\n",
       "              \n",
       "                       [[ 1.7510e-03, -2.5204e-03, -1.2639e-03],\n",
       "                        [-2.6495e-03, -3.1387e-03,  1.7239e-04],\n",
       "                        [-4.5393e-04, -2.4568e-03, -5.1997e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.4519e-03,  5.2862e-03,  4.8473e-03],\n",
       "                        [ 5.4513e-03,  1.0662e-03,  7.3669e-03],\n",
       "                        [ 1.7950e-03,  1.3068e-03,  2.2020e-03]],\n",
       "              \n",
       "                       [[ 1.7111e-03,  2.8202e-03,  1.1929e-03],\n",
       "                        [ 1.7949e-03, -3.8884e-04,  3.4379e-03],\n",
       "                        [ 1.6329e-03,  1.4790e-03,  1.3107e-03]],\n",
       "              \n",
       "                       [[-6.1011e-04, -1.6709e-03, -4.7024e-04],\n",
       "                        [-4.0380e-03,  1.5868e-03,  5.1091e-04],\n",
       "                        [-1.8660e-03, -1.4748e-03, -2.6810e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7908e-03,  3.2046e-03,  4.3172e-03],\n",
       "                        [ 3.6805e-03,  5.8947e-03,  3.6417e-03],\n",
       "                        [ 1.8724e-03,  3.4577e-03,  3.1776e-03]],\n",
       "              \n",
       "                       [[ 3.6339e-04, -2.5427e-04,  1.1674e-03],\n",
       "                        [-6.1819e-04, -3.8073e-03, -5.0341e-03],\n",
       "                        [-3.9163e-04, -4.5595e-03, -1.4577e-03]],\n",
       "              \n",
       "                       [[ 6.6057e-03,  8.9495e-03,  9.6925e-03],\n",
       "                        [ 7.2509e-03,  9.7090e-03,  1.8319e-02],\n",
       "                        [ 4.9937e-03,  1.2632e-02,  1.2054e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.0158e-03, -2.9963e-03, -1.9673e-03],\n",
       "                        [-2.7382e-03, -7.1462e-04, -2.3872e-04],\n",
       "                        [ 1.9531e-03,  2.5072e-03,  2.3910e-03]],\n",
       "              \n",
       "                       [[-5.1604e-04, -6.8086e-04, -1.2846e-03],\n",
       "                        [ 1.0512e-03,  8.9464e-04,  4.9873e-03],\n",
       "                        [ 1.0822e-03,  4.8784e-03,  2.0002e-03]],\n",
       "              \n",
       "                       [[ 8.2391e-04,  4.1638e-04, -1.7910e-04],\n",
       "                        [-1.7896e-03, -7.1676e-03, -1.5798e-03],\n",
       "                        [-7.3224e-04, -3.3912e-03, -1.7071e-03]]]])),\n",
       "             ('layer4.0.bn2.weight',\n",
       "              tensor([0.0754, 0.0677, 0.0531, 0.0406, 0.0503, 0.0666, 0.0609, 0.0573, 0.0481,\n",
       "                      0.0599, 0.0795, 0.0656, 0.0783, 0.0394, 0.0457, 0.0794, 0.0877, 0.0765,\n",
       "                      0.0958, 0.0607, 0.0984, 0.0838, 0.0719, 0.0529, 0.0775, 0.0786, 0.0420,\n",
       "                      0.0783, 0.0811, 0.0659, 0.0879, 0.0630, 0.0391, 0.0581, 0.0347, 0.0467,\n",
       "                      0.0626, 0.0718, 0.0979, 0.0756, 0.0513, 0.0748, 0.0448, 0.0477, 0.0880,\n",
       "                      0.0771, 0.0593, 0.0630, 0.0811, 0.1036, 0.0791, 0.0950, 0.0741, 0.0831,\n",
       "                      0.0733, 0.0757, 0.1048, 0.0719, 0.0697, 0.1059, 0.0461, 0.0530, 0.0628,\n",
       "                      0.0621, 0.0707, 0.0513, 0.0454, 0.0626, 0.0627, 0.0452, 0.0377, 0.0633,\n",
       "                      0.0906, 0.0976, 0.0983, 0.0725, 0.0799, 0.1049, 0.0517, 0.0566, 0.0779,\n",
       "                      0.0732, 0.0432, 0.0746, 0.0741, 0.0441, 0.0639, 0.0694, 0.0639, 0.0422,\n",
       "                      0.0836, 0.0531, 0.0711, 0.0910, 0.0520, 0.1021, 0.0646, 0.1036, 0.1433,\n",
       "                      0.0785, 0.0636, 0.0861, 0.0932, 0.0855, 0.0545, 0.0629, 0.0553, 0.0548,\n",
       "                      0.0782, 0.0479, 0.0813, 0.0603, 0.0547, 0.0753, 0.1218, 0.0703, 0.0410,\n",
       "                      0.0717, 0.0473, 0.0669, 0.0455, 0.0781, 0.0437, 0.0765, 0.0558, 0.0911,\n",
       "                      0.0778, 0.0874, 0.0688, 0.0891, 0.0567, 0.0654, 0.0513, 0.0856, 0.0470,\n",
       "                      0.0622, 0.0782, 0.0690, 0.1165, 0.0893, 0.0606, 0.0901, 0.0897, 0.0839,\n",
       "                      0.0507, 0.0959, 0.0747, 0.0787, 0.1038, 0.0747, 0.0683, 0.0503, 0.0638,\n",
       "                      0.0817, 0.0842, 0.0954, 0.0314, 0.1071, 0.0911, 0.0545, 0.0760, 0.0646,\n",
       "                      0.0682, 0.0810, 0.0663, 0.0601, 0.0417, 0.0920, 0.0913, 0.0896, 0.0432,\n",
       "                      0.0747, 0.0723, 0.0610, 0.0592, 0.0766, 0.0531, 0.0891, 0.0759, 0.0506,\n",
       "                      0.1133, 0.0848, 0.0579, 0.0790, 0.0374, 0.0501, 0.0722, 0.1129, 0.0643,\n",
       "                      0.0585, 0.0473, 0.0746, 0.0745, 0.0501, 0.0840, 0.0499, 0.0401, 0.0928,\n",
       "                      0.0643, 0.0634, 0.0724, 0.0330, 0.0819, 0.0494, 0.0775, 0.0728, 0.0704,\n",
       "                      0.0652, 0.0606, 0.0290, 0.0665, 0.0742, 0.1122, 0.0820, 0.0765, 0.0466,\n",
       "                      0.0675, 0.0877, 0.1139, 0.0719, 0.0877, 0.1360, 0.0770, 0.0641, 0.0711,\n",
       "                      0.0775, 0.0764, 0.0940, 0.0621, 0.0843, 0.0535, 0.0671, 0.0939, 0.0879,\n",
       "                      0.0830, 0.0825, 0.0315, 0.0661, 0.0864, 0.0753, 0.0445, 0.0632, 0.0497,\n",
       "                      0.0990, 0.0712, 0.0740, 0.0709, 0.0400, 0.1143, 0.1243, 0.0324, 0.0618,\n",
       "                      0.0914, 0.0740, 0.0755, 0.0527, 0.0667, 0.0601, 0.0405, 0.0863, 0.0267,\n",
       "                      0.1065, 0.0642, 0.0994, 0.0623, 0.0900, 0.0894, 0.0979, 0.0518, 0.0874,\n",
       "                      0.0726, 0.0650, 0.0726, 0.0775, 0.0859, 0.0778, 0.0592, 0.0593, 0.0713,\n",
       "                      0.0807, 0.0680, 0.0659, 0.0810, 0.0617, 0.0415, 0.0726, 0.0820, 0.1056,\n",
       "                      0.1226, 0.0868, 0.0882, 0.0723, 0.0878, 0.1182, 0.0693, 0.0844, 0.0938,\n",
       "                      0.0625, 0.0310, 0.0642, 0.0811, 0.0902, 0.0588, 0.0824, 0.0828, 0.0710,\n",
       "                      0.0925, 0.0730, 0.0559, 0.0739, 0.0375, 0.0856, 0.0414, 0.0506, 0.0704,\n",
       "                      0.0526, 0.0380, 0.0529, 0.0798, 0.0405, 0.1005, 0.0433, 0.0943, 0.0711,\n",
       "                      0.0626, 0.1146, 0.0490, 0.0759, 0.0690, 0.0542, 0.0590, 0.0628, 0.0521,\n",
       "                      0.0568, 0.1386, 0.0570, 0.0822, 0.0629, 0.0991, 0.0547, 0.0478, 0.0627,\n",
       "                      0.0716, 0.0649, 0.0647, 0.0700, 0.0686, 0.0784, 0.0563, 0.0910, 0.0514,\n",
       "                      0.0694, 0.0620, 0.0404, 0.0755, 0.0969, 0.0795, 0.0498, 0.1045, 0.0838,\n",
       "                      0.0408, 0.0537, 0.0741, 0.0673, 0.0912, 0.0836, 0.0505, 0.0684, 0.0648,\n",
       "                      0.0774, 0.0453, 0.1252, 0.0423, 0.1051, 0.0613, 0.0816, 0.0442, 0.0954,\n",
       "                      0.0683, 0.0759, 0.0500, 0.0427, 0.1039, 0.1055, 0.0755, 0.0810, 0.0720,\n",
       "                      0.0477, 0.0632, 0.0584, 0.0622, 0.1198, 0.0773, 0.0416, 0.0557, 0.0620,\n",
       "                      0.0844, 0.0425, 0.0552, 0.0747, 0.0946, 0.0639, 0.0574, 0.0922, 0.0538,\n",
       "                      0.0824, 0.0628, 0.0352, 0.0780, 0.0954, 0.0641, 0.0519, 0.0700, 0.1000,\n",
       "                      0.0612, 0.1275, 0.0479, 0.0831, 0.0880, 0.0838, 0.0462, 0.1064, 0.0880,\n",
       "                      0.0415, 0.1098, 0.0604, 0.0801, 0.0609, 0.0514, 0.0676, 0.1024, 0.0629,\n",
       "                      0.0513, 0.0726, 0.0970, 0.0417, 0.0794, 0.0882, 0.0493, 0.0590, 0.0475,\n",
       "                      0.0668, 0.0743, 0.0749, 0.0686, 0.0740, 0.0557, 0.0792, 0.0775, 0.0688,\n",
       "                      0.0842, 0.0352, 0.0811, 0.0714, 0.0442, 0.0666, 0.0343, 0.0342, 0.0515,\n",
       "                      0.0408, 0.0860, 0.0528, 0.0755, 0.0662, 0.0655, 0.0956, 0.0945, 0.0583,\n",
       "                      0.0664, 0.0903, 0.0559, 0.0522, 0.1172, 0.1101, 0.0564, 0.0573, 0.0386,\n",
       "                      0.0734, 0.0508, 0.0746, 0.0385, 0.0455, 0.0797, 0.0866, 0.0779, 0.0851,\n",
       "                      0.0996, 0.0979, 0.0557, 0.0444, 0.0702, 0.0803, 0.0731, 0.0726, 0.0658,\n",
       "                      0.0467, 0.1049, 0.0371, 0.0603, 0.0437, 0.0767, 0.0550, 0.0927, 0.0510,\n",
       "                      0.0887, 0.0650, 0.0579, 0.0704, 0.0527, 0.0881, 0.0374, 0.0463])),\n",
       "             ('layer4.0.bn2.bias',\n",
       "              tensor([-9.0804e-03, -2.1169e-02,  7.8212e-03, -1.0613e-02, -1.9557e-02,\n",
       "                      -1.0981e-02, -6.1697e-02, -2.5561e-02, -1.8766e-02,  4.0938e-03,\n",
       "                      -9.2235e-03, -1.9836e-02, -1.5485e-02, -1.4284e-03, -2.3095e-02,\n",
       "                       2.1343e-02, -3.9903e-02, -1.2674e-02, -1.7364e-02,  1.1209e-02,\n",
       "                      -2.5163e-02, -1.3354e-02, -4.8145e-02,  3.8327e-03,  1.7173e-02,\n",
       "                      -2.2390e-02, -1.1599e-02, -1.7810e-02, -5.5810e-05, -6.4959e-03,\n",
       "                      -3.9812e-02, -1.4409e-02, -7.0938e-03, -1.3673e-02,  1.1660e-02,\n",
       "                      -1.0153e-02,  1.3092e-02,  3.4623e-02, -4.7149e-02, -2.9228e-02,\n",
       "                      -3.4903e-04, -4.0024e-02, -2.4281e-02, -2.1868e-02, -6.1489e-03,\n",
       "                      -1.1319e-02, -1.6350e-03, -1.2406e-02, -1.6566e-02, -4.3289e-02,\n",
       "                      -1.7846e-02, -2.0020e-02, -2.0984e-02, -6.0721e-02, -2.1760e-02,\n",
       "                      -1.1039e-02, -2.2855e-02, -1.5424e-02, -3.4556e-02, -3.5387e-02,\n",
       "                      -1.5654e-02, -5.4813e-03, -9.1591e-03, -1.2694e-02,  3.1430e-02,\n",
       "                       2.6331e-04, -1.0382e-02,  5.9856e-03,  7.8033e-03, -2.5165e-02,\n",
       "                      -8.4923e-03, -1.0156e-02, -2.6826e-02, -5.0583e-02, -4.0217e-02,\n",
       "                      -3.6593e-02, -2.8412e-02, -4.3314e-02, -2.0705e-02, -1.7772e-02,\n",
       "                       5.1264e-02,  2.2165e-02, -3.7431e-02, -2.4117e-02, -3.8629e-02,\n",
       "                      -1.3913e-02,  5.4702e-03,  1.1024e-03,  1.1359e-02, -3.5900e-02,\n",
       "                      -2.2625e-02, -1.6621e-02, -2.7933e-02, -5.3913e-03, -1.0692e-02,\n",
       "                      -4.5420e-02, -2.1518e-02, -4.1493e-03, -3.7870e-02, -3.6389e-03,\n",
       "                      -2.3258e-02,  1.8664e-02, -1.4381e-02, -5.4720e-02, -1.5629e-02,\n",
       "                      -1.9330e-02,  1.4200e-02, -6.0210e-03, -3.7006e-02, -1.6128e-02,\n",
       "                       1.6893e-03, -8.4299e-03, -1.0291e-02, -2.2440e-02, -2.3991e-02,\n",
       "                      -2.8508e-02, -6.2981e-03, -1.5480e-02, -1.6604e-02, -5.8427e-03,\n",
       "                      -1.5891e-03, -1.7114e-03,  1.1094e-02, -2.4925e-02, -1.4960e-02,\n",
       "                       7.9295e-03, -8.5014e-03, -3.3550e-02,  5.6794e-02, -3.4558e-02,\n",
       "                       1.6092e-02, -3.0985e-02,  8.3695e-03, -2.6428e-02,  5.3995e-03,\n",
       "                      -5.6516e-03, -7.2725e-03, -7.7458e-03,  4.3305e-02, -3.4914e-02,\n",
       "                       6.1801e-03, -2.5574e-02,  1.2455e-02, -4.6217e-03,  4.5700e-03,\n",
       "                      -8.1098e-03, -3.3515e-02, -1.0782e-02,  2.6967e-02,  8.5002e-03,\n",
       "                      -3.1247e-02, -6.7412e-03, -1.6965e-02, -5.7852e-02, -8.6529e-03,\n",
       "                      -8.4625e-03, -4.7456e-03, -1.3830e-02, -2.6203e-02, -6.8756e-03,\n",
       "                       2.1336e-02, -1.2997e-02, -1.4962e-02, -1.6930e-02, -2.2854e-02,\n",
       "                       3.0594e-03,  8.2865e-03, -9.2398e-03,  5.7511e-03, -2.4659e-02,\n",
       "                       3.6818e-02, -2.2782e-02, -1.6447e-02, -2.1962e-02,  1.3022e-02,\n",
       "                      -1.4694e-02,  4.5747e-03, -1.8691e-02,  1.6127e-02, -1.5620e-02,\n",
       "                      -5.8640e-02, -3.0657e-02, -2.0571e-02, -2.2301e-02, -2.1969e-02,\n",
       "                      -2.3003e-02, -2.1839e-02, -4.1833e-02, -4.2656e-02, -1.5091e-02,\n",
       "                      -1.2279e-02,  3.3607e-02, -2.1125e-02, -4.0137e-02, -1.5950e-02,\n",
       "                      -1.5387e-02, -1.0773e-02, -2.2448e-02, -1.6291e-02, -8.7063e-03,\n",
       "                       7.4424e-03, -1.5133e-02, -8.1506e-03, -3.4149e-02, -3.2338e-02,\n",
       "                      -3.8361e-02,  9.2341e-03, -2.4299e-02, -1.8205e-03, -5.8629e-03,\n",
       "                       8.5717e-03, -1.1773e-02, -1.7339e-02, -6.9324e-03, -4.2675e-03,\n",
       "                      -5.0544e-03,  3.0545e-02, -4.3805e-02, -3.6499e-02,  4.4341e-02,\n",
       "                      -1.6345e-02, -2.2513e-02, -5.1267e-02,  3.5436e-02, -2.3964e-02,\n",
       "                      -1.7905e-02, -2.0779e-02, -1.2512e-02, -1.0411e-02,  3.3907e-02,\n",
       "                      -7.0140e-03,  1.0609e-03, -1.7262e-02, -3.4630e-02, -2.3087e-02,\n",
       "                      -2.3819e-02, -1.5341e-03, -3.7826e-02,  2.0376e-02,  3.4051e-02,\n",
       "                      -1.1057e-02, -9.0428e-03, -3.6687e-03, -2.1223e-02, -2.2799e-02,\n",
       "                      -4.7291e-02, -3.0414e-02, -1.0746e-02, -3.1016e-03, -8.9814e-04,\n",
       "                       5.4332e-03, -9.9209e-03, -2.2565e-02,  1.1731e-04,  1.3255e-02,\n",
       "                      -8.1601e-03,  6.9156e-03, -2.6353e-02, -2.0975e-02,  1.6708e-02,\n",
       "                      -2.5430e-03, -1.7712e-02, -2.8976e-02, -2.6151e-03, -8.6926e-03,\n",
       "                      -3.5229e-02, -3.9244e-02, -2.8943e-02,  1.0719e-02, -2.8049e-02,\n",
       "                      -1.7120e-02, -1.4558e-02,  6.5863e-03, -6.9497e-03, -2.0747e-02,\n",
       "                      -2.5666e-02, -4.8290e-03,  1.2269e-02, -3.2927e-02, -1.8756e-02,\n",
       "                      -4.1570e-02, -7.2713e-03, -4.8958e-02, -3.1765e-02,  8.6886e-03,\n",
       "                      -2.3220e-02, -4.6991e-03, -3.6787e-02, -3.0797e-02,  7.2662e-04,\n",
       "                      -2.6158e-02, -9.8088e-03, -1.1887e-02, -4.2632e-02, -2.7269e-02,\n",
       "                      -3.9435e-02, -3.3531e-02, -9.4296e-03, -1.0161e-02,  7.3978e-03,\n",
       "                      -9.9337e-03,  8.1503e-03, -5.1171e-03,  4.1285e-03, -3.0502e-02,\n",
       "                      -1.7479e-02, -7.7523e-03, -1.7817e-02, -2.9565e-02, -7.8104e-03,\n",
       "                       7.1285e-03, -1.6618e-02,  3.8077e-03, -6.9168e-03, -1.7191e-02,\n",
       "                      -1.7678e-02, -6.6988e-03, -1.7572e-02, -3.0917e-02, -1.3938e-02,\n",
       "                      -4.9863e-02, -9.2956e-03, -3.4788e-02,  6.4787e-03, -9.9443e-03,\n",
       "                      -4.0085e-02, -9.2037e-03, -7.0844e-02, -1.8830e-02, -2.7465e-02,\n",
       "                      -1.4870e-02, -1.2389e-02,  1.5783e-03, -2.1527e-03, -7.0943e-02,\n",
       "                      -2.8978e-02, -2.7272e-02, -1.1860e-02, -2.6091e-02, -9.1093e-03,\n",
       "                      -2.6311e-02, -4.5509e-02, -2.2889e-02, -6.9571e-03,  9.6360e-03,\n",
       "                      -1.6633e-02, -1.0519e-02, -5.1571e-02,  3.9105e-03, -2.1318e-02,\n",
       "                       1.0933e-02, -2.0392e-02, -8.3374e-04, -2.1887e-02,  1.6509e-02,\n",
       "                      -3.9046e-02, -2.6758e-02, -7.6980e-03, -6.4582e-03, -4.4716e-02,\n",
       "                       2.0045e-03, -1.8874e-02, -2.5978e-02, -7.7095e-03, -2.8965e-02,\n",
       "                      -1.8319e-02, -6.9766e-03, -1.0183e-02,  3.5294e-02, -1.2495e-02,\n",
       "                      -2.0195e-02, -7.8168e-02, -2.0919e-02, -4.4210e-02, -1.0571e-02,\n",
       "                      -3.2772e-02,  4.6885e-03, -1.5751e-02, -1.5416e-02, -4.0401e-02,\n",
       "                      -1.4089e-02, -1.7666e-02, -6.7140e-03, -3.6456e-02,  1.0936e-02,\n",
       "                      -4.9394e-02,  9.5296e-05, -9.7754e-03, -2.3880e-02, -2.1375e-02,\n",
       "                      -1.4723e-02, -1.7452e-02,  6.0110e-04,  6.1544e-03, -1.3885e-02,\n",
       "                      -2.8268e-02, -2.2662e-03, -1.4566e-02,  7.7150e-03,  1.5647e-02,\n",
       "                      -1.0110e-02, -6.9683e-03,  2.6526e-03, -1.2140e-02, -1.1472e-02,\n",
       "                       1.7316e-02,  2.1197e-03, -1.1830e-02, -3.6264e-02, -2.6374e-02,\n",
       "                      -3.4132e-02, -2.0466e-02, -2.0527e-02, -6.8175e-02,  2.6650e-04,\n",
       "                      -3.3307e-02,  1.3569e-02, -3.9339e-02, -3.3937e-02, -8.0986e-03,\n",
       "                      -2.4827e-02, -1.2085e-02, -2.4601e-02, -2.7034e-05, -2.7154e-02,\n",
       "                      -6.0309e-03,  1.8519e-02, -7.7317e-03, -2.7295e-02, -1.0300e-02,\n",
       "                      -6.3577e-03, -2.8496e-02,  3.7475e-03,  1.1668e-04, -6.8431e-02,\n",
       "                       1.9485e-03, -1.4061e-02,  2.3077e-02, -2.3122e-02, -1.7524e-02,\n",
       "                      -1.7715e-02, -1.5475e-02, -1.4995e-02,  1.3935e-02, -3.1973e-02,\n",
       "                      -2.4600e-02, -2.4703e-02, -2.3946e-02,  8.9571e-03, -3.3892e-02,\n",
       "                      -3.2047e-02, -7.6111e-03, -4.0310e-02,  1.4035e-02, -1.8978e-02,\n",
       "                      -3.3290e-02, -1.9437e-02, -2.1387e-04, -5.4475e-03, -1.4266e-02,\n",
       "                      -3.7500e-02,  3.3936e-03,  2.6282e-02, -1.6960e-02, -5.5135e-03,\n",
       "                       1.6846e-02, -1.8029e-02, -3.2547e-02, -3.9491e-02, -2.7079e-02,\n",
       "                      -3.3706e-02,  3.0618e-03, -2.9761e-02,  9.7804e-03,  2.6924e-03,\n",
       "                       3.1515e-02,  1.6561e-02, -1.1708e-02, -1.9553e-02, -1.6705e-02,\n",
       "                      -3.6141e-03, -4.7138e-03,  2.5207e-03, -6.8234e-03, -8.9712e-03,\n",
       "                      -4.4918e-03,  4.1023e-02, -1.0902e-02, -1.4131e-02, -5.6220e-03,\n",
       "                      -2.3566e-02,  1.2883e-02,  9.6553e-03, -3.5196e-02, -3.6278e-02,\n",
       "                      -3.4683e-02,  6.0942e-02, -9.2750e-03,  3.8835e-02,  2.2122e-03,\n",
       "                      -8.4268e-03, -7.1348e-03, -1.6639e-02,  1.3983e-03, -1.5875e-02,\n",
       "                      -1.3548e-02, -2.1760e-02, -3.1335e-02, -2.3514e-02, -1.5058e-02,\n",
       "                      -2.8992e-03, -1.2767e-02])),\n",
       "             ('layer4.0.bn2.running_mean',\n",
       "              tensor([-2.3880e-02, -1.1976e-02, -1.1295e-02, -1.7394e-02, -6.5994e-03,\n",
       "                      -1.1205e-02, -7.7933e-03, -1.4089e-02,  2.1668e-03, -1.8592e-02,\n",
       "                      -2.7964e-02, -9.8866e-03, -2.3052e-02, -1.1817e-02, -8.8526e-03,\n",
       "                      -4.5145e-02,  2.0284e-03, -2.3460e-02,  2.4881e-04, -1.9591e-02,\n",
       "                      -2.6578e-02, -1.3283e-02, -9.9836e-03, -4.6684e-03, -2.3653e-02,\n",
       "                      -1.1038e-02, -2.3554e-02, -6.9253e-03, -4.0095e-03, -1.6229e-02,\n",
       "                      -8.5797e-03,  4.4565e-03, -8.1102e-03,  9.4316e-04, -1.3748e-02,\n",
       "                      -7.2972e-03, -1.2582e-03, -3.4697e-02, -2.1731e-02, -6.2559e-04,\n",
       "                      -4.4416e-04, -1.4310e-02, -3.0446e-03, -4.2226e-04, -4.5905e-02,\n",
       "                      -2.3600e-02, -2.6074e-02, -1.5444e-02, -2.3740e-02, -8.4982e-03,\n",
       "                      -1.9346e-02, -1.3644e-02,  2.0980e-04, -1.2686e-03, -7.2203e-03,\n",
       "                      -1.6668e-02, -2.6433e-02, -2.2095e-02, -1.6857e-02, -2.5771e-02,\n",
       "                      -1.2296e-02, -2.8006e-02, -1.8302e-02, -2.8348e-02, -4.0251e-02,\n",
       "                      -1.2538e-02, -2.4398e-02, -1.4485e-02, -1.0530e-02,  7.3848e-04,\n",
       "                      -1.6821e-03, -6.7558e-03, -8.3693e-03, -3.0711e-02, -1.1599e-02,\n",
       "                      -2.1018e-02, -1.3029e-02, -4.3667e-04, -3.1054e-03,  1.5676e-03,\n",
       "                      -2.9812e-02, -2.0386e-02, -9.9954e-03, -1.1039e-02, -2.2134e-02,\n",
       "                      -9.4064e-03, -1.6337e-02,  4.1648e-03, -1.6945e-02, -6.0486e-03,\n",
       "                      -6.9814e-03, -2.4690e-03, -2.2617e-02, -3.7895e-02, -1.3943e-02,\n",
       "                      -1.1178e-02, -2.1609e-02, -2.6455e-02, -4.8303e-02, -2.1439e-02,\n",
       "                      -4.8283e-03, -3.7515e-02, -5.1598e-03,  3.1095e-04, -7.3726e-03,\n",
       "                      -2.7482e-02, -1.4861e-02, -2.5329e-02, -1.9280e-03,  1.8611e-02,\n",
       "                      -1.6182e-02, -5.4273e-03, -1.1640e-02, -1.0350e-02,  3.9883e-03,\n",
       "                      -8.4901e-03, -2.4981e-03, -1.4077e-02, -1.4378e-03, -1.9822e-02,\n",
       "                      -1.1285e-02, -3.1357e-02, -2.6331e-02, -1.2587e-02, -3.9378e-03,\n",
       "                      -3.9777e-02, -3.2562e-02, -4.5595e-03, -2.5650e-02, -2.1290e-02,\n",
       "                      -2.4968e-03,  5.8186e-04, -1.1062e-02, -4.3627e-03, -7.4784e-03,\n",
       "                      -6.7427e-03, -2.6027e-02,  1.7449e-03, -4.8758e-04, -8.1887e-03,\n",
       "                      -2.6339e-02, -1.2456e-02, -1.7107e-02, -3.7377e-02, -7.5772e-03,\n",
       "                      -2.4916e-02, -2.2453e-02, -9.9964e-03, -4.5298e-02, -3.2634e-02,\n",
       "                      -2.6354e-02, -7.9307e-03, -2.4754e-02,  5.4852e-03, -3.6956e-02,\n",
       "                      -3.9428e-02, -9.7789e-03, -3.8267e-02,  1.9975e-02, -1.9591e-02,\n",
       "                      -3.1050e-02, -1.3644e-02, -3.8032e-03, -3.2737e-02, -1.3978e-02,\n",
       "                      -3.5880e-02, -1.5893e-02, -1.9852e-02, -3.3986e-02, -7.1957e-03,\n",
       "                      -1.7774e-02, -1.6719e-02, -1.3970e-02, -1.6997e-02, -3.3961e-02,\n",
       "                      -3.0041e-02, -4.0363e-03, -1.1179e-02, -3.0723e-02, -2.3049e-02,\n",
       "                       5.9885e-03, -5.3341e-03, -2.0038e-02, -2.2104e-02, -1.5709e-02,\n",
       "                      -2.0365e-03, -4.1777e-03,  4.7496e-02, -9.7016e-04, -1.3700e-02,\n",
       "                      -1.2618e-02, -1.7559e-02, -3.1779e-02, -9.6106e-03, -1.3218e-02,\n",
       "                      -7.2452e-03, -4.6270e-03, -2.5586e-02, -1.0032e-02, -6.8090e-03,\n",
       "                      -1.9262e-02, -2.8924e-03, -4.5976e-02, -2.2979e-02, -1.5568e-02,\n",
       "                      -1.0448e-02, -2.6062e-02, -1.7543e-02, -1.6940e-02, -3.1792e-03,\n",
       "                      -2.6706e-02,  5.3214e-03, -1.5373e-02, -2.0667e-02, -2.6038e-02,\n",
       "                      -2.1153e-02, -3.1225e-02, -2.4463e-02,  9.2949e-03, -2.4489e-02,\n",
       "                      -1.4415e-02, -1.2742e-02,  1.4715e-03, -1.1087e-02, -2.3302e-02,\n",
       "                      -1.8912e-02, -1.5512e-03, -4.1436e-03, -1.3207e-02, -5.1733e-02,\n",
       "                      -7.8852e-03, -3.4770e-02, -2.7789e-02, -1.0626e-02, -1.9314e-02,\n",
       "                      -3.2523e-02, -6.9305e-03, -1.6175e-02, -3.1892e-02, -2.0766e-02,\n",
       "                      -1.4273e-02, -3.2075e-02,  1.0470e-02, -1.3213e-02,  1.1976e-02,\n",
       "                      -2.2184e-02, -7.2392e-03,  2.5990e-03, -4.9784e-02, -4.1122e-02,\n",
       "                      -1.0243e-02, -1.8183e-02, -3.0659e-02, -1.5481e-02, -2.2460e-02,\n",
       "                      -1.7032e-03, -2.5772e-02,  4.2913e-03, -3.3853e-03, -1.0432e-02,\n",
       "                       3.9393e-04, -3.8782e-02, -9.7704e-03, -3.9990e-02, -1.1683e-02,\n",
       "                      -1.7230e-02, -1.8188e-02, -3.0874e-02, -1.1726e-02, -1.8215e-02,\n",
       "                      -1.7605e-02, -9.8856e-03, -2.6838e-02, -2.8147e-02, -1.2535e-02,\n",
       "                      -3.3480e-04, -1.2445e-02, -3.0133e-02, -9.8925e-03, -1.7974e-02,\n",
       "                      -3.4445e-02, -1.5738e-02, -1.9476e-03, -5.4204e-04,  4.7049e-03,\n",
       "                      -9.5136e-06, -1.8860e-02, -3.8789e-02, -1.2264e-02, -3.4120e-02,\n",
       "                      -2.8630e-02, -2.2047e-02, -1.5880e-02, -1.5553e-02, -1.3007e-02,\n",
       "                      -5.7689e-03,  2.0784e-02,  8.7753e-03, -5.3366e-03, -4.4255e-03,\n",
       "                      -2.2294e-02, -4.4837e-02, -6.0492e-03, -2.8766e-02, -2.1917e-02,\n",
       "                      -1.4100e-02,  2.6417e-03, -2.1588e-02, -8.6649e-04, -3.6035e-02,\n",
       "                       4.8150e-04, -1.4861e-02, -9.0379e-03, -5.3668e-03, -7.4867e-03,\n",
       "                       5.2029e-03, -6.0073e-03, -3.3306e-02,  3.5755e-03, -9.5811e-03,\n",
       "                      -2.9717e-02, -1.8677e-02, -1.6825e-02, -1.8280e-02, -1.2621e-02,\n",
       "                      -5.9976e-03, -1.1738e-02, -3.0240e-02, -1.6894e-02, -1.0907e-02,\n",
       "                       2.0463e-02,  9.4281e-03, -2.5962e-03, -1.7863e-02, -4.0012e-02,\n",
       "                      -7.0825e-03, -2.2001e-02, -1.2906e-02, -3.6025e-02,  3.5812e-03,\n",
       "                      -8.6236e-03,  1.0559e-02, -3.4738e-02, -2.2923e-02, -9.3437e-04,\n",
       "                      -1.4095e-02, -1.9627e-02, -1.4390e-02, -2.2495e-02, -2.8626e-02,\n",
       "                      -3.0486e-02, -3.6551e-03, -2.6972e-02,  5.0956e-03, -3.9756e-02,\n",
       "                       4.6476e-03,  2.1356e-03, -2.2478e-02, -2.7439e-02, -2.5096e-02,\n",
       "                      -1.0987e-02, -2.5298e-02, -8.4319e-03, -1.2359e-02, -1.2519e-02,\n",
       "                      -1.8512e-02, -1.3406e-02, -1.5063e-02, -2.7185e-02, -2.7553e-02,\n",
       "                      -1.0485e-02, -2.4401e-02, -4.4304e-04, -1.7408e-02, -8.5028e-03,\n",
       "                      -1.5906e-02, -1.0073e-02, -2.1156e-02, -7.7332e-03, -2.4245e-02,\n",
       "                      -1.1255e-02, -9.1965e-03, -2.6707e-02, -1.6542e-02, -1.4584e-02,\n",
       "                      -6.8920e-03, -3.0654e-02, -1.7301e-02, -1.6873e-02, -2.2301e-02,\n",
       "                      -5.5746e-03, -2.4522e-02, -2.3908e-02, -3.5933e-03, -1.5878e-02,\n",
       "                      -2.9226e-02, -2.6278e-02, -9.7882e-03, -1.5359e-02, -1.1867e-02,\n",
       "                      -3.4124e-02, -2.0060e-02, -2.1792e-02, -7.9789e-03, -3.8728e-03,\n",
       "                      -1.7064e-02, -1.7443e-02, -4.1124e-03,  3.7656e-03, -2.6558e-02,\n",
       "                      -2.4844e-02, -9.4929e-03, -2.1691e-02, -2.0754e-02, -7.6661e-03,\n",
       "                      -5.5874e-02, -3.2113e-02, -2.0459e-02, -1.5324e-02, -2.9739e-02,\n",
       "                      -9.2145e-03, -3.9800e-02, -7.4851e-03, -1.6176e-02, -1.4793e-02,\n",
       "                      -5.8560e-03, -1.5491e-02, -5.8457e-03, -1.1004e-02, -3.6212e-03,\n",
       "                      -5.0363e-02,  1.1279e-02, -1.9263e-02, -1.7344e-02, -4.0186e-03,\n",
       "                      -1.1467e-02,  4.3011e-03, -2.3168e-02, -5.2546e-04,  1.8480e-03,\n",
       "                      -1.3988e-03, -1.6498e-03, -1.0905e-02, -1.9866e-02, -8.7554e-03,\n",
       "                       8.0119e-03, -1.2264e-02, -1.7799e-02, -2.3834e-02, -8.9530e-03,\n",
       "                      -2.5000e-02,  1.1938e-03, -2.0577e-02, -1.1117e-03, -6.3406e-03,\n",
       "                      -1.9857e-02, -4.8963e-03, -1.3356e-02, -5.6387e-03, -1.6486e-02,\n",
       "                      -4.0456e-02,  1.0166e-02, -7.5147e-03,  9.8016e-04, -8.4905e-03,\n",
       "                      -3.0679e-02, -2.2993e-02, -1.9066e-02, -1.1769e-02, -4.8453e-02,\n",
       "                       4.4626e-03, -2.2215e-02, -1.9338e-02, -4.4195e-02, -7.3016e-03,\n",
       "                      -1.0844e-02, -1.9223e-02, -1.1855e-02, -5.2131e-03, -1.8631e-02,\n",
       "                      -1.2965e-02, -5.9997e-04, -2.0508e-02, -4.2037e-02, -3.3424e-02,\n",
       "                      -2.7223e-02, -2.6412e-02, -2.5404e-02,  3.9153e-03, -1.9291e-02,\n",
       "                      -1.2413e-02, -2.9439e-02, -2.7614e-02, -1.3968e-02,  1.3824e-02,\n",
       "                      -9.8569e-03, -3.1996e-02,  4.6818e-03, -2.9569e-02, -4.9692e-03,\n",
       "                      -1.2606e-02, -5.7355e-03, -3.3812e-02, -2.2380e-02, -3.3282e-02,\n",
       "                      -3.2456e-02, -3.8594e-03, -9.2509e-03,  3.9019e-03,  8.0279e-04,\n",
       "                      -1.1466e-02, -7.1932e-03])),\n",
       "             ('layer4.0.bn2.running_var',\n",
       "              tensor([0.0015, 0.0012, 0.0011, 0.0007, 0.0007, 0.0011, 0.0012, 0.0007, 0.0014,\n",
       "                      0.0009, 0.0016, 0.0008, 0.0013, 0.0008, 0.0003, 0.0018, 0.0022, 0.0020,\n",
       "                      0.0020, 0.0011, 0.0024, 0.0015, 0.0012, 0.0017, 0.0030, 0.0017, 0.0010,\n",
       "                      0.0015, 0.0014, 0.0015, 0.0014, 0.0015, 0.0003, 0.0010, 0.0008, 0.0010,\n",
       "                      0.0008, 0.0017, 0.0025, 0.0019, 0.0009, 0.0011, 0.0011, 0.0014, 0.0016,\n",
       "                      0.0013, 0.0020, 0.0006, 0.0016, 0.0020, 0.0009, 0.0024, 0.0013, 0.0018,\n",
       "                      0.0011, 0.0016, 0.0033, 0.0011, 0.0009, 0.0027, 0.0010, 0.0008, 0.0012,\n",
       "                      0.0012, 0.0020, 0.0015, 0.0006, 0.0008, 0.0016, 0.0006, 0.0002, 0.0012,\n",
       "                      0.0017, 0.0027, 0.0014, 0.0015, 0.0012, 0.0021, 0.0012, 0.0009, 0.0030,\n",
       "                      0.0019, 0.0007, 0.0026, 0.0015, 0.0007, 0.0010, 0.0022, 0.0022, 0.0004,\n",
       "                      0.0031, 0.0007, 0.0011, 0.0026, 0.0006, 0.0017, 0.0016, 0.0032, 0.0040,\n",
       "                      0.0027, 0.0008, 0.0020, 0.0026, 0.0019, 0.0014, 0.0013, 0.0010, 0.0025,\n",
       "                      0.0016, 0.0014, 0.0014, 0.0007, 0.0011, 0.0013, 0.0028, 0.0011, 0.0010,\n",
       "                      0.0008, 0.0013, 0.0015, 0.0007, 0.0023, 0.0006, 0.0015, 0.0011, 0.0023,\n",
       "                      0.0021, 0.0015, 0.0021, 0.0016, 0.0015, 0.0010, 0.0010, 0.0022, 0.0009,\n",
       "                      0.0013, 0.0015, 0.0008, 0.0034, 0.0013, 0.0010, 0.0030, 0.0024, 0.0022,\n",
       "                      0.0008, 0.0026, 0.0024, 0.0012, 0.0033, 0.0026, 0.0012, 0.0009, 0.0012,\n",
       "                      0.0020, 0.0019, 0.0023, 0.0003, 0.0033, 0.0038, 0.0012, 0.0022, 0.0015,\n",
       "                      0.0023, 0.0030, 0.0012, 0.0013, 0.0005, 0.0020, 0.0017, 0.0020, 0.0015,\n",
       "                      0.0017, 0.0013, 0.0009, 0.0011, 0.0020, 0.0011, 0.0019, 0.0013, 0.0009,\n",
       "                      0.0027, 0.0011, 0.0013, 0.0015, 0.0004, 0.0006, 0.0015, 0.0032, 0.0021,\n",
       "                      0.0018, 0.0007, 0.0028, 0.0018, 0.0009, 0.0020, 0.0006, 0.0005, 0.0025,\n",
       "                      0.0015, 0.0021, 0.0019, 0.0004, 0.0026, 0.0012, 0.0022, 0.0012, 0.0012,\n",
       "                      0.0011, 0.0011, 0.0004, 0.0031, 0.0017, 0.0037, 0.0023, 0.0016, 0.0011,\n",
       "                      0.0014, 0.0021, 0.0022, 0.0023, 0.0020, 0.0052, 0.0010, 0.0010, 0.0018,\n",
       "                      0.0027, 0.0014, 0.0011, 0.0016, 0.0034, 0.0006, 0.0016, 0.0029, 0.0021,\n",
       "                      0.0012, 0.0014, 0.0007, 0.0015, 0.0018, 0.0033, 0.0006, 0.0010, 0.0008,\n",
       "                      0.0022, 0.0023, 0.0019, 0.0014, 0.0006, 0.0024, 0.0025, 0.0004, 0.0020,\n",
       "                      0.0026, 0.0011, 0.0021, 0.0010, 0.0012, 0.0010, 0.0006, 0.0029, 0.0003,\n",
       "                      0.0028, 0.0016, 0.0025, 0.0018, 0.0019, 0.0021, 0.0024, 0.0008, 0.0019,\n",
       "                      0.0012, 0.0010, 0.0021, 0.0017, 0.0013, 0.0009, 0.0011, 0.0010, 0.0016,\n",
       "                      0.0012, 0.0010, 0.0010, 0.0011, 0.0008, 0.0009, 0.0013, 0.0027, 0.0021,\n",
       "                      0.0026, 0.0015, 0.0022, 0.0018, 0.0016, 0.0023, 0.0018, 0.0027, 0.0015,\n",
       "                      0.0027, 0.0006, 0.0018, 0.0020, 0.0028, 0.0016, 0.0009, 0.0013, 0.0014,\n",
       "                      0.0020, 0.0013, 0.0012, 0.0024, 0.0006, 0.0023, 0.0013, 0.0009, 0.0014,\n",
       "                      0.0014, 0.0014, 0.0011, 0.0018, 0.0006, 0.0026, 0.0008, 0.0017, 0.0015,\n",
       "                      0.0010, 0.0029, 0.0004, 0.0021, 0.0016, 0.0010, 0.0013, 0.0014, 0.0010,\n",
       "                      0.0010, 0.0039, 0.0012, 0.0015, 0.0010, 0.0032, 0.0007, 0.0007, 0.0010,\n",
       "                      0.0019, 0.0023, 0.0016, 0.0014, 0.0016, 0.0021, 0.0020, 0.0023, 0.0014,\n",
       "                      0.0016, 0.0009, 0.0004, 0.0018, 0.0028, 0.0010, 0.0009, 0.0023, 0.0023,\n",
       "                      0.0007, 0.0009, 0.0012, 0.0013, 0.0030, 0.0011, 0.0005, 0.0007, 0.0014,\n",
       "                      0.0020, 0.0005, 0.0060, 0.0003, 0.0024, 0.0014, 0.0018, 0.0006, 0.0023,\n",
       "                      0.0009, 0.0015, 0.0015, 0.0010, 0.0047, 0.0025, 0.0015, 0.0016, 0.0023,\n",
       "                      0.0007, 0.0016, 0.0018, 0.0011, 0.0029, 0.0017, 0.0012, 0.0015, 0.0013,\n",
       "                      0.0017, 0.0009, 0.0005, 0.0015, 0.0019, 0.0009, 0.0008, 0.0026, 0.0004,\n",
       "                      0.0023, 0.0014, 0.0009, 0.0009, 0.0018, 0.0013, 0.0015, 0.0008, 0.0013,\n",
       "                      0.0008, 0.0050, 0.0018, 0.0016, 0.0027, 0.0015, 0.0006, 0.0044, 0.0019,\n",
       "                      0.0008, 0.0027, 0.0028, 0.0017, 0.0007, 0.0008, 0.0021, 0.0040, 0.0007,\n",
       "                      0.0023, 0.0014, 0.0017, 0.0008, 0.0013, 0.0021, 0.0010, 0.0012, 0.0005,\n",
       "                      0.0014, 0.0018, 0.0012, 0.0012, 0.0026, 0.0007, 0.0012, 0.0012, 0.0012,\n",
       "                      0.0020, 0.0008, 0.0019, 0.0011, 0.0006, 0.0009, 0.0004, 0.0008, 0.0010,\n",
       "                      0.0006, 0.0020, 0.0017, 0.0021, 0.0009, 0.0017, 0.0023, 0.0019, 0.0017,\n",
       "                      0.0009, 0.0041, 0.0013, 0.0014, 0.0039, 0.0027, 0.0011, 0.0015, 0.0006,\n",
       "                      0.0017, 0.0006, 0.0016, 0.0006, 0.0007, 0.0025, 0.0033, 0.0015, 0.0018,\n",
       "                      0.0031, 0.0034, 0.0006, 0.0009, 0.0015, 0.0016, 0.0018, 0.0009, 0.0009,\n",
       "                      0.0007, 0.0028, 0.0004, 0.0016, 0.0005, 0.0013, 0.0008, 0.0025, 0.0010,\n",
       "                      0.0026, 0.0013, 0.0015, 0.0016, 0.0012, 0.0022, 0.0006, 0.0009])),\n",
       "             ('layer4.0.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.0.conv3.weight',\n",
       "              tensor([[[[-4.4592e-04]],\n",
       "              \n",
       "                       [[ 6.5004e-04]],\n",
       "              \n",
       "                       [[ 3.2418e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.9370e-03]],\n",
       "              \n",
       "                       [[-3.7403e-04]],\n",
       "              \n",
       "                       [[-1.7175e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9850e-03]],\n",
       "              \n",
       "                       [[-4.3480e-03]],\n",
       "              \n",
       "                       [[ 1.2555e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.1976e-03]],\n",
       "              \n",
       "                       [[-2.5454e-03]],\n",
       "              \n",
       "                       [[ 5.0122e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.4131e-04]],\n",
       "              \n",
       "                       [[ 1.4629e-05]],\n",
       "              \n",
       "                       [[-4.4719e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5440e-03]],\n",
       "              \n",
       "                       [[ 2.1833e-03]],\n",
       "              \n",
       "                       [[-1.7176e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-8.1973e-04]],\n",
       "              \n",
       "                       [[ 1.2192e-03]],\n",
       "              \n",
       "                       [[ 2.6966e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.8914e-03]],\n",
       "              \n",
       "                       [[ 2.9095e-04]],\n",
       "              \n",
       "                       [[-1.2260e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7340e-03]],\n",
       "              \n",
       "                       [[-8.4344e-04]],\n",
       "              \n",
       "                       [[-3.9852e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6261e-03]],\n",
       "              \n",
       "                       [[ 6.3274e-04]],\n",
       "              \n",
       "                       [[ 2.2464e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3639e-02]],\n",
       "              \n",
       "                       [[ 6.6534e-04]],\n",
       "              \n",
       "                       [[ 6.0864e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4575e-02]],\n",
       "              \n",
       "                       [[ 9.4663e-04]],\n",
       "              \n",
       "                       [[-6.4453e-04]]]])),\n",
       "             ('layer4.0.bn3.weight',\n",
       "              tensor([ 0.0243,  0.1295, -0.0954,  ...,  0.0583,  0.0979,  0.1570])),\n",
       "             ('layer4.0.bn3.bias',\n",
       "              tensor([0.0077, 0.0374, 0.0304,  ..., 0.0170, 0.0345, 0.0278])),\n",
       "             ('layer4.0.bn3.running_mean',\n",
       "              tensor([-3.0339e-04, -1.0202e-02,  2.3971e-03,  ..., -2.3449e-03,\n",
       "                      -8.3636e-05,  1.3803e-03])),\n",
       "             ('layer4.0.bn3.running_var',\n",
       "              tensor([7.5722e-06, 2.1395e-04, 1.1022e-04,  ..., 4.2849e-05, 1.0766e-04,\n",
       "                      2.2981e-04])),\n",
       "             ('layer4.0.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.0.downsample.0.weight',\n",
       "              tensor([[[[ 0.0003]],\n",
       "              \n",
       "                       [[ 0.0017]],\n",
       "              \n",
       "                       [[ 0.0007]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0008]],\n",
       "              \n",
       "                       [[ 0.0001]],\n",
       "              \n",
       "                       [[ 0.0002]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0031]],\n",
       "              \n",
       "                       [[-0.0024]],\n",
       "              \n",
       "                       [[-0.0003]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0036]],\n",
       "              \n",
       "                       [[ 0.0022]],\n",
       "              \n",
       "                       [[ 0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0016]],\n",
       "              \n",
       "                       [[-0.0027]],\n",
       "              \n",
       "                       [[ 0.0017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0024]],\n",
       "              \n",
       "                       [[ 0.0039]],\n",
       "              \n",
       "                       [[-0.0002]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0011]],\n",
       "              \n",
       "                       [[ 0.0025]],\n",
       "              \n",
       "                       [[ 0.0015]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0021]],\n",
       "              \n",
       "                       [[-0.0017]],\n",
       "              \n",
       "                       [[-0.0009]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0006]],\n",
       "              \n",
       "                       [[ 0.0068]],\n",
       "              \n",
       "                       [[-0.0011]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0026]],\n",
       "              \n",
       "                       [[-0.0019]],\n",
       "              \n",
       "                       [[ 0.0004]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0023]],\n",
       "              \n",
       "                       [[-0.0020]],\n",
       "              \n",
       "                       [[-0.0017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0005]],\n",
       "              \n",
       "                       [[-0.0005]],\n",
       "              \n",
       "                       [[-0.0014]]]])),\n",
       "             ('layer4.0.downsample.1.weight',\n",
       "              tensor([0.0162, 0.0551, 0.0646,  ..., 0.0347, 0.0412, 0.0642])),\n",
       "             ('layer4.0.downsample.1.bias',\n",
       "              tensor([0.0077, 0.0374, 0.0304,  ..., 0.0170, 0.0345, 0.0278])),\n",
       "             ('layer4.0.downsample.1.running_mean',\n",
       "              tensor([ 0.0009,  0.0020,  0.0039,  ...,  0.0020, -0.0035, -0.0003])),\n",
       "             ('layer4.0.downsample.1.running_var',\n",
       "              tensor([9.3220e-06, 1.8734e-04, 1.4064e-04,  ..., 6.2208e-05, 7.8835e-05,\n",
       "                      1.2569e-04])),\n",
       "             ('layer4.0.downsample.1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.1.conv1.weight',\n",
       "              tensor([[[[-2.8394e-04]],\n",
       "              \n",
       "                       [[-2.3541e-03]],\n",
       "              \n",
       "                       [[ 3.2544e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.5503e-04]],\n",
       "              \n",
       "                       [[-4.0722e-03]],\n",
       "              \n",
       "                       [[ 2.2733e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5310e-04]],\n",
       "              \n",
       "                       [[ 8.9300e-04]],\n",
       "              \n",
       "                       [[-9.8506e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3722e-03]],\n",
       "              \n",
       "                       [[-2.0569e-03]],\n",
       "              \n",
       "                       [[-4.4467e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7095e-04]],\n",
       "              \n",
       "                       [[ 2.9974e-04]],\n",
       "              \n",
       "                       [[-1.7250e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2613e-03]],\n",
       "              \n",
       "                       [[ 7.3903e-04]],\n",
       "              \n",
       "                       [[-3.5018e-04]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.7955e-04]],\n",
       "              \n",
       "                       [[-3.3504e-03]],\n",
       "              \n",
       "                       [[-1.5869e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3809e-03]],\n",
       "              \n",
       "                       [[-2.3704e-03]],\n",
       "              \n",
       "                       [[ 6.2455e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.7440e-04]],\n",
       "              \n",
       "                       [[ 2.3665e-04]],\n",
       "              \n",
       "                       [[ 1.1914e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.7807e-03]],\n",
       "              \n",
       "                       [[ 4.2179e-04]],\n",
       "              \n",
       "                       [[ 8.3774e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1917e-04]],\n",
       "              \n",
       "                       [[ 4.5361e-03]],\n",
       "              \n",
       "                       [[-3.9458e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1454e-03]],\n",
       "              \n",
       "                       [[ 7.1853e-05]],\n",
       "              \n",
       "                       [[ 3.2174e-04]]]])),\n",
       "             ('layer4.1.bn1.weight',\n",
       "              tensor([0.0303, 0.0330, 0.0226, 0.0498, 0.0366, 0.0384, 0.0512, 0.0324, 0.0619,\n",
       "                      0.0190, 0.0345, 0.0252, 0.0373, 0.0336, 0.0598, 0.0391, 0.0405, 0.0265,\n",
       "                      0.0384, 0.0423, 0.0370, 0.0731, 0.0296, 0.0365, 0.0212, 0.0345, 0.0406,\n",
       "                      0.0367, 0.0229, 0.0429, 0.0671, 0.0286, 0.0484, 0.0429, 0.0397, 0.0387,\n",
       "                      0.0262, 0.0501, 0.0369, 0.0428, 0.0524, 0.0436, 0.0314, 0.0294, 0.0431,\n",
       "                      0.0360, 0.0338, 0.0455, 0.0312, 0.0495, 0.0491, 0.0423, 0.0529, 0.0238,\n",
       "                      0.0399, 0.0281, 0.0287, 0.0515, 0.0217, 0.0638, 0.0410, 0.0554, 0.0511,\n",
       "                      0.0679, 0.0182, 0.0421, 0.0600, 0.0286, 0.0344, 0.0402, 0.0587, 0.0235,\n",
       "                      0.0249, 0.0378, 0.0523, 0.0315, 0.0314, 0.0593, 0.0506, 0.0431, 0.0263,\n",
       "                      0.0435, 0.0259, 0.0252, 0.0139, 0.0293, 0.0329, 0.0421, 0.0178, 0.0377,\n",
       "                      0.0172, 0.0368, 0.0333, 0.0412, 0.0286, 0.0269, 0.0446, 0.0307, 0.0257,\n",
       "                      0.0547, 0.0349, 0.0401, 0.0467, 0.0524, 0.0595, 0.0317, 0.0579, 0.0346,\n",
       "                      0.0284, 0.0775, 0.0299, 0.0304, 0.0437, 0.0328, 0.0369, 0.0386, 0.0479,\n",
       "                      0.0406, 0.0507, 0.0485, 0.0226, 0.0525, 0.0244, 0.0307, 0.0576, 0.0277,\n",
       "                      0.0341, 0.0326, 0.0701, 0.0353, 0.0445, 0.0355, 0.0417, 0.0242, 0.0420,\n",
       "                      0.0409, 0.0330, 0.0420, 0.0419, 0.0328, 0.0381, 0.0385, 0.0263, 0.0526,\n",
       "                      0.0441, 0.0393, 0.0312, 0.0242, 0.0442, 0.0794, 0.0231, 0.0459, 0.0258,\n",
       "                      0.0296, 0.0390, 0.0675, 0.0465, 0.0513, 0.0276, 0.0709, 0.0369, 0.0332,\n",
       "                      0.0275, 0.0304, 0.0372, 0.0389, 0.0317, 0.0203, 0.0235, 0.0555, 0.0436,\n",
       "                      0.0259, 0.0414, 0.0332, 0.0671, 0.0207, 0.0627, 0.0348, 0.0336, 0.0348,\n",
       "                      0.0347, 0.0586, 0.0640, 0.0475, 0.0364, 0.0445, 0.0192, 0.0343, 0.0367,\n",
       "                      0.0325, 0.0681, 0.0320, 0.0601, 0.0419, 0.0280, 0.0401, 0.0423, 0.0401,\n",
       "                      0.0314, 0.0392, 0.0466, 0.0451, 0.0451, 0.0511, 0.0338, 0.0432, 0.0541,\n",
       "                      0.0167, 0.0371, 0.0372, 0.0492, 0.0286, 0.0452, 0.0287, 0.0526, 0.0296,\n",
       "                      0.0478, 0.0478, 0.0347, 0.0559, 0.0359, 0.0378, 0.0395, 0.0349, 0.0248,\n",
       "                      0.0368, 0.0193, 0.0601, 0.0253, 0.0520, 0.0324, 0.0209, 0.0415, 0.0226,\n",
       "                      0.0306, 0.0220, 0.0272, 0.0367, 0.0531, 0.0419, 0.0246, 0.0584, 0.0400,\n",
       "                      0.0225, 0.0442, 0.0497, 0.0232, 0.0302, 0.0307, 0.0176, 0.0570, 0.0346,\n",
       "                      0.0835, 0.0542, 0.0408, 0.0294, 0.0236, 0.0594, 0.0307, 0.0340, 0.0347,\n",
       "                      0.0276, 0.0405, 0.0418, 0.0534, 0.0313, 0.0413, 0.0303, 0.0514, 0.0622,\n",
       "                      0.0322, 0.0279, 0.0300, 0.0273, 0.0438, 0.0393, 0.0434, 0.0457, 0.0282,\n",
       "                      0.0263, 0.0236, 0.0431, 0.0293, 0.0290, 0.0624, 0.0580, 0.0331, 0.0467,\n",
       "                      0.0291, 0.0512, 0.0268, 0.0389, 0.0219, 0.0527, 0.0406, 0.0320, 0.0294,\n",
       "                      0.0480, 0.0338, 0.0424, 0.0478, 0.0249, 0.0327, 0.0398, 0.0295, 0.0475,\n",
       "                      0.0391, 0.0414, 0.0439, 0.0488, 0.0319, 0.0435, 0.0358, 0.0341, 0.0519,\n",
       "                      0.0331, 0.0263, 0.0416, 0.0394, 0.0526, 0.0533, 0.0338, 0.0286, 0.0430,\n",
       "                      0.0320, 0.0562, 0.0360, 0.0567, 0.0533, 0.0307, 0.0174, 0.0532, 0.0517,\n",
       "                      0.0330, 0.0421, 0.0368, 0.0417, 0.0343, 0.0493, 0.0342, 0.0524, 0.0389,\n",
       "                      0.0361, 0.0486, 0.0850, 0.0386, 0.0187, 0.0331, 0.0406, 0.0322, 0.0632,\n",
       "                      0.0314, 0.0387, 0.0228, 0.0364, 0.0316, 0.0322, 0.0491, 0.0491, 0.0309,\n",
       "                      0.0263, 0.0438, 0.0461, 0.0198, 0.0356, 0.0376, 0.0338, 0.0547, 0.0267,\n",
       "                      0.0122, 0.0294, 0.0309, 0.0459, 0.0259, 0.0531, 0.0475, 0.0327, 0.0670,\n",
       "                      0.0442, 0.0283, 0.0404, 0.0592, 0.0647, 0.0425, 0.0278, 0.0371, 0.0355,\n",
       "                      0.0433, 0.0443, 0.0442, 0.0301, 0.0299, 0.0282, 0.0402, 0.0505, 0.0272,\n",
       "                      0.0704, 0.0298, 0.0472, 0.0430, 0.0751, 0.0408, 0.0425, 0.0296, 0.0254,\n",
       "                      0.0464, 0.0417, 0.0421, 0.0399, 0.0383, 0.0330, 0.0261, 0.0430, 0.0241,\n",
       "                      0.0371, 0.0251, 0.0377, 0.0412, 0.0497, 0.0340, 0.0231, 0.0408, 0.0643,\n",
       "                      0.0278, 0.0346, 0.0292, 0.0430, 0.0198, 0.0524, 0.0516, 0.0260, 0.0467,\n",
       "                      0.0410, 0.0396, 0.0281, 0.0291, 0.0643, 0.0316, 0.0314, 0.0321, 0.0298,\n",
       "                      0.0382, 0.0452, 0.0432, 0.0320, 0.0499, 0.0284, 0.0241, 0.0244, 0.0400,\n",
       "                      0.0325, 0.0554, 0.0262, 0.0433, 0.0688, 0.0345, 0.0836, 0.0312, 0.0288,\n",
       "                      0.0280, 0.0403, 0.0393, 0.0447, 0.0307, 0.0463, 0.0338, 0.0282, 0.0498,\n",
       "                      0.0329, 0.0451, 0.0351, 0.0528, 0.0456, 0.0258, 0.0276, 0.0621, 0.0248,\n",
       "                      0.0275, 0.0239, 0.0349, 0.0308, 0.0276, 0.0289, 0.0319, 0.0495, 0.0385,\n",
       "                      0.0447, 0.0299, 0.0538, 0.0288, 0.0356, 0.0630, 0.0718, 0.0375, 0.0313,\n",
       "                      0.0494, 0.0428, 0.0700, 0.0563, 0.0294, 0.0266, 0.0574, 0.0338, 0.0310,\n",
       "                      0.0624, 0.0311, 0.0313, 0.0290, 0.0403, 0.0370, 0.0533, 0.0273])),\n",
       "             ('layer4.1.bn1.bias',\n",
       "              tensor([ 9.7559e-03,  7.2624e-03,  1.3258e-03, -8.7912e-03,  2.6111e-02,\n",
       "                      -1.1139e-02, -1.4229e-02, -8.8683e-04, -1.6775e-02, -1.0091e-03,\n",
       "                      -2.9708e-03,  8.7013e-03, -7.6068e-04,  1.4098e-02, -2.8441e-02,\n",
       "                      -5.8483e-03,  8.4036e-03,  9.5600e-03,  1.0756e-02,  2.2443e-03,\n",
       "                      -1.5505e-02, -2.4489e-02,  2.8533e-03,  1.3459e-03, -2.4378e-03,\n",
       "                       1.6695e-03,  1.0891e-02, -3.5092e-03,  1.2424e-02, -4.1753e-03,\n",
       "                      -1.0682e-02, -4.6904e-03, -1.5284e-02, -2.4731e-02,  1.6845e-03,\n",
       "                      -1.1553e-03,  7.5947e-03,  3.3327e-04, -9.4884e-04, -1.1279e-02,\n",
       "                       3.4777e-03, -2.4770e-02,  3.8559e-03,  1.3969e-02, -1.2318e-02,\n",
       "                       5.4829e-03,  6.7044e-03, -5.1308e-03,  1.5737e-02,  2.0844e-03,\n",
       "                      -6.5091e-03, -5.6877e-03, -2.8536e-03,  5.8351e-03,  3.9480e-03,\n",
       "                       4.6175e-04,  1.2984e-02, -1.0244e-02,  1.0337e-02, -3.0140e-02,\n",
       "                      -4.6603e-04, -5.6385e-03,  8.5441e-04, -2.6039e-03,  6.5850e-03,\n",
       "                      -1.1759e-02, -2.8269e-03,  8.8304e-03,  2.7946e-03,  1.0105e-02,\n",
       "                      -7.5478e-03,  9.3383e-03,  1.1473e-03,  1.0406e-03,  3.1504e-03,\n",
       "                      -1.4552e-03,  1.4832e-03, -6.3342e-03, -8.0453e-03, -1.1234e-02,\n",
       "                       1.6806e-03,  1.0548e-02,  3.5818e-03,  2.4835e-04,  4.5738e-03,\n",
       "                      -1.0425e-04,  3.5519e-03,  3.6322e-03,  4.8394e-03,  4.8262e-03,\n",
       "                       1.0330e-02,  1.2318e-02,  6.2933e-03, -1.4836e-02, -2.5597e-03,\n",
       "                      -2.4009e-03, -1.2003e-02, -2.8254e-03,  3.3695e-03, -1.2736e-03,\n",
       "                      -1.2906e-02,  2.3168e-03,  4.2093e-03, -1.3925e-02,  9.9798e-05,\n",
       "                       7.0279e-03, -8.2115e-03,  7.5501e-03,  9.2685e-04, -1.2660e-02,\n",
       "                      -8.3508e-03,  7.6438e-03,  4.5019e-03, -4.1862e-03, -7.4623e-03,\n",
       "                       8.2827e-03, -9.3662e-03,  7.9049e-03, -1.0574e-02, -1.2956e-02,\n",
       "                       2.7913e-03, -4.9477e-03, -3.5342e-03, -3.4217e-03, -2.6946e-03,\n",
       "                       6.5537e-04,  9.0275e-03,  1.1976e-03, -1.9467e-02,  3.3157e-04,\n",
       "                       6.3510e-03, -4.8405e-03,  5.3732e-03, -3.6605e-03, -1.4254e-02,\n",
       "                      -3.1036e-03, -3.3827e-04, -5.0978e-03,  8.2643e-04,  4.4735e-03,\n",
       "                      -1.3198e-02,  5.8308e-03,  6.8349e-03,  2.1218e-02, -8.3264e-03,\n",
       "                      -8.4973e-03, -9.1587e-03,  6.9652e-03, -2.2179e-02, -2.2498e-02,\n",
       "                      -6.5987e-03, -1.4884e-02, -1.0121e-02,  2.4713e-03, -4.2064e-03,\n",
       "                      -1.4679e-02, -1.5208e-03, -2.3459e-02,  5.8293e-03, -2.7475e-03,\n",
       "                       8.0036e-03, -1.7374e-02, -1.7275e-03,  8.7279e-03,  3.8695e-03,\n",
       "                      -1.2747e-02, -5.4164e-03,  5.6043e-03,  3.7584e-03, -1.4317e-02,\n",
       "                      -1.1648e-02,  1.1979e-02, -2.4526e-03, -5.4450e-03, -9.8786e-03,\n",
       "                       5.0413e-03, -1.6907e-02, -6.1575e-03,  2.6941e-03,  1.0100e-02,\n",
       "                       5.3989e-03, -1.3306e-02,  9.7124e-03, -8.5002e-04,  1.0703e-02,\n",
       "                       1.3173e-02,  4.1806e-03,  1.2130e-02, -9.8823e-03, -3.6224e-04,\n",
       "                      -1.5853e-02,  9.8812e-05, -9.2209e-03, -1.6973e-02,  1.3289e-02,\n",
       "                       8.8765e-04, -1.5757e-03,  9.6212e-03, -2.9405e-03,  1.4042e-02,\n",
       "                      -7.5386e-03, -1.3016e-02,  3.5468e-03, -5.8128e-03,  9.7669e-03,\n",
       "                       4.3328e-03, -1.8636e-02,  7.7041e-03, -1.1123e-02,  1.1840e-02,\n",
       "                      -3.1733e-03,  2.4103e-03,  7.6446e-05,  4.2135e-03,  1.3015e-02,\n",
       "                       2.8141e-04,  1.1352e-02, -3.1577e-03,  4.5227e-03, -9.1754e-03,\n",
       "                       5.1166e-03, -1.3055e-02, -4.7668e-03, -4.2402e-03,  4.4063e-03,\n",
       "                      -7.4780e-03,  4.9883e-03, -2.2806e-02, -7.2448e-03, -6.9263e-03,\n",
       "                       5.8077e-03,  1.2072e-02, -1.7987e-03, -3.7224e-03, -3.0992e-03,\n",
       "                      -1.9555e-03,  6.2584e-03,  1.2774e-02, -2.4666e-02,  1.1709e-02,\n",
       "                       8.1446e-04,  9.3837e-03,  9.6639e-04,  6.9970e-03, -2.6383e-03,\n",
       "                      -8.0656e-03, -1.4605e-04,  3.9447e-03,  5.5781e-03,  5.9379e-03,\n",
       "                       1.2324e-03, -1.2048e-02, -2.8532e-02, -4.3485e-03,  5.2113e-03,\n",
       "                       6.0107e-03,  5.6038e-03, -5.6671e-03,  1.2305e-02,  4.8274e-03,\n",
       "                      -3.1140e-03, -7.5209e-04, -2.0834e-03, -1.5135e-02, -2.4602e-02,\n",
       "                       3.4589e-03,  8.7870e-03,  3.3006e-03, -5.6143e-04,  6.3174e-03,\n",
       "                      -1.8161e-02, -3.9774e-03, -6.1764e-03, -1.3268e-03, -1.7103e-02,\n",
       "                      -1.1053e-03,  1.9326e-03, -1.6423e-02,  4.7560e-03, -1.6323e-04,\n",
       "                       5.2677e-03,  5.8383e-04, -6.4314e-03,  5.4142e-03, -1.0330e-02,\n",
       "                      -1.1285e-02, -1.8775e-03, -1.0080e-02,  7.5016e-03, -1.3429e-02,\n",
       "                       3.8720e-03,  9.2526e-03,  2.6706e-03, -6.4492e-03,  2.5095e-02,\n",
       "                      -1.1511e-03,  9.0779e-03, -1.0894e-02,  4.5457e-03, -5.0946e-03,\n",
       "                      -6.4656e-03, -6.7924e-03, -8.6982e-03,  1.5474e-04,  1.1550e-02,\n",
       "                      -1.0459e-02,  4.9103e-03,  3.3058e-03,  1.2067e-04,  2.2143e-04,\n",
       "                       2.8813e-03,  1.6259e-02,  1.2281e-02,  7.9145e-03,  1.0785e-02,\n",
       "                      -5.5995e-03,  5.4649e-03, -1.6297e-03, -1.0895e-02, -7.1785e-03,\n",
       "                      -1.4342e-02,  6.7400e-03,  2.6156e-03, -4.9297e-03, -4.7043e-03,\n",
       "                      -2.1946e-03, -7.3523e-05,  1.8477e-02, -7.8208e-03,  8.9180e-05,\n",
       "                      -3.4589e-04, -1.8482e-02, -2.0913e-03, -1.2513e-02, -2.6805e-03,\n",
       "                       5.0826e-03,  2.2813e-02,  3.4243e-03, -8.9802e-03, -6.1666e-03,\n",
       "                      -8.4047e-03,  1.0026e-02,  8.5966e-03,  1.6034e-03,  1.8504e-02,\n",
       "                       3.1901e-03,  3.9216e-03,  4.1336e-03, -6.7538e-03,  2.7453e-03,\n",
       "                      -6.3088e-03,  1.5199e-02,  2.8527e-04,  1.1162e-02, -2.4550e-04,\n",
       "                      -4.8164e-03,  1.6718e-02,  6.3968e-03,  9.9611e-04,  2.1002e-03,\n",
       "                       5.0046e-03, -9.1985e-03, -5.1406e-03,  7.1964e-03,  1.7215e-02,\n",
       "                       8.9557e-03, -2.6566e-03, -1.0032e-02,  4.2455e-03,  1.8220e-03,\n",
       "                       4.6353e-03,  5.4583e-03, -5.9476e-03,  1.6384e-03,  8.6784e-03,\n",
       "                      -1.2682e-02, -5.0627e-03,  5.4667e-03, -4.9092e-03,  4.8594e-03,\n",
       "                       4.9078e-03, -1.2478e-04,  3.9433e-03, -2.8156e-04, -1.1942e-02,\n",
       "                      -5.3697e-03,  1.4028e-03,  2.6221e-03,  9.0265e-03, -2.0758e-03,\n",
       "                      -4.8015e-03, -8.1541e-03, -7.4557e-03, -4.6935e-03, -1.3180e-02,\n",
       "                       4.2969e-03, -1.2856e-02,  3.6300e-03, -1.3800e-03, -1.8840e-02,\n",
       "                       2.1051e-03, -1.2159e-02, -1.7022e-02, -4.4830e-05, -5.2566e-03,\n",
       "                       1.1748e-02, -3.1018e-03,  1.7993e-03, -2.4913e-03, -5.0386e-03,\n",
       "                      -2.4489e-03,  5.9325e-03, -1.1870e-02,  1.4740e-03,  6.4216e-03,\n",
       "                       1.3319e-03, -3.8373e-03,  1.2215e-02,  7.3691e-03,  8.0032e-03,\n",
       "                       5.0864e-03, -4.0045e-03, -1.7641e-02, -5.0516e-03, -8.7361e-04,\n",
       "                       7.0211e-03, -2.4748e-03,  3.4636e-03,  1.1921e-02,  8.3687e-03,\n",
       "                       1.1242e-02, -1.1567e-02,  7.5810e-03, -1.0503e-02, -3.1253e-04,\n",
       "                      -5.8115e-03, -1.8238e-02,  7.7508e-05,  2.0506e-02, -6.3403e-03,\n",
       "                       1.0113e-02, -7.9821e-03, -1.1847e-02,  1.8885e-04, -8.6752e-03,\n",
       "                      -7.8166e-03,  1.2408e-02,  8.8940e-03, -1.0719e-03, -1.7045e-02,\n",
       "                      -5.0077e-03, -8.0831e-04, -3.7836e-04, -9.4417e-03, -1.8269e-02,\n",
       "                      -1.1810e-03, -3.4601e-02,  7.6963e-03,  1.3623e-03,  5.2936e-03,\n",
       "                      -1.0508e-02, -8.5964e-03,  1.6915e-03, -3.0489e-03, -1.2901e-02,\n",
       "                       2.0468e-02,  6.9101e-03, -1.0315e-02, -9.1850e-03, -6.0277e-03,\n",
       "                       1.1082e-02,  5.7306e-03, -7.7605e-03,  5.1053e-03, -1.2371e-03,\n",
       "                      -5.2178e-03, -3.5475e-03,  1.0704e-02,  1.9620e-03, -1.9152e-04,\n",
       "                       7.3558e-03,  1.1794e-02,  7.4016e-03, -1.7412e-03, -1.2210e-02,\n",
       "                      -3.4239e-03, -2.0630e-02, -7.1408e-03, -3.7539e-03,  5.1836e-03,\n",
       "                       5.1723e-03, -4.3594e-03, -2.2593e-02, -5.0884e-03,  1.0088e-02,\n",
       "                      -4.0486e-03, -1.4608e-02, -1.3052e-02,  1.4246e-02, -2.9901e-03,\n",
       "                       7.9591e-03,  1.5187e-03, -2.1781e-03, -1.1199e-02, -1.5374e-02,\n",
       "                      -1.2733e-02,  9.3145e-03, -1.7544e-03,  5.9093e-03,  1.7303e-02,\n",
       "                      -2.4525e-02,  5.8515e-03])),\n",
       "             ('layer4.1.bn1.running_mean',\n",
       "              tensor([-2.4723e-02, -1.9634e-03, -2.2652e-02, -3.8632e-02, -1.8009e-02,\n",
       "                       2.2129e-02,  4.3789e-02,  2.1142e-03,  4.4591e-02,  2.7059e-02,\n",
       "                      -2.8191e-02, -1.9850e-02,  2.5403e-02,  3.0703e-02,  4.0475e-02,\n",
       "                      -3.4184e-02, -4.3465e-02, -4.1064e-03, -1.2198e-02,  1.3141e-02,\n",
       "                       7.3117e-03,  6.0507e-02, -2.2281e-02,  5.4727e-03, -2.5945e-03,\n",
       "                      -2.3281e-02,  4.3429e-03, -2.0542e-03, -8.2997e-03,  2.9394e-04,\n",
       "                       2.0545e-02, -4.7093e-03, -1.2372e-02,  2.0034e-02, -2.4120e-02,\n",
       "                      -5.0415e-02, -3.9277e-02, -2.3849e-02,  2.5123e-02,  2.7096e-05,\n",
       "                      -4.7874e-02,  1.5016e-02, -1.6455e-02, -3.9075e-03,  9.9861e-04,\n",
       "                      -1.4361e-02, -1.0820e-02,  7.4838e-03, -1.4045e-02,  1.7118e-02,\n",
       "                      -3.8196e-02, -2.0137e-03,  1.0888e-02, -1.5582e-02,  3.2173e-02,\n",
       "                       2.0592e-02, -3.3903e-02, -2.2188e-02, -1.2652e-02,  3.3316e-02,\n",
       "                       5.4950e-02, -5.4164e-02,  3.3834e-02, -5.1587e-02, -1.5873e-02,\n",
       "                       4.8408e-02,  2.6564e-02, -2.6780e-02, -2.4118e-02, -2.7059e-02,\n",
       "                      -8.5030e-03,  1.0183e-03, -2.7096e-02, -1.9913e-02, -4.6203e-02,\n",
       "                      -1.5685e-02, -4.4021e-02,  4.8756e-02, -2.8576e-02,  1.5045e-02,\n",
       "                      -1.6562e-02,  1.1973e-02, -1.4097e-02,  1.9289e-02, -2.3157e-03,\n",
       "                       3.3249e-02, -3.0372e-02, -3.7301e-02, -5.8140e-03, -1.9720e-02,\n",
       "                      -6.8605e-03, -2.8604e-02, -2.4406e-02,  1.9712e-02,  6.8261e-03,\n",
       "                      -2.7769e-02,  1.5916e-03,  1.4006e-03, -1.3336e-02, -6.3347e-03,\n",
       "                       1.0976e-02,  4.4422e-02,  4.7784e-02,  2.0487e-02,  2.3840e-03,\n",
       "                      -3.2422e-02,  4.6837e-03, -3.3279e-02,  1.1742e-02,  2.8458e-02,\n",
       "                       2.2745e-04, -1.9214e-02, -1.4315e-03,  4.0863e-02, -5.1007e-02,\n",
       "                       1.0529e-02,  4.9671e-02, -7.8900e-02,  2.9048e-02,  7.8737e-03,\n",
       "                      -2.8035e-02, -9.2913e-03,  2.9070e-02,  6.9924e-04, -4.8027e-02,\n",
       "                      -7.1719e-03, -3.2586e-02, -5.4081e-03,  1.7191e-02, -5.0732e-03,\n",
       "                       1.7598e-02,  3.9679e-02, -4.4411e-02,  2.2615e-02,  8.0228e-02,\n",
       "                      -3.3845e-02,  1.6148e-02,  1.6518e-02, -4.7911e-03,  1.4870e-03,\n",
       "                       3.0503e-02, -8.1328e-02, -8.7822e-03, -4.4982e-02, -2.3710e-02,\n",
       "                       5.8714e-04, -1.0692e-02,  5.0268e-03, -5.0830e-04,  1.3031e-01,\n",
       "                       1.5545e-02, -1.7240e-02,  8.6431e-03, -2.0692e-02,  8.4982e-03,\n",
       "                       3.6235e-02, -4.5512e-02,  5.6701e-02, -1.6988e-02,  5.8890e-03,\n",
       "                       2.1215e-02,  2.3071e-02, -5.0032e-03, -2.5998e-02, -4.3961e-02,\n",
       "                      -1.3757e-02,  2.0311e-02,  8.2248e-03, -1.8036e-02,  1.2278e-01,\n",
       "                       6.4502e-02, -2.6362e-02,  2.5299e-02,  2.1207e-02, -3.3265e-02,\n",
       "                      -2.7476e-02,  9.4429e-03,  7.3046e-03, -6.0909e-03,  1.3158e-02,\n",
       "                       1.9407e-02,  1.0556e-01, -5.6847e-02,  1.1972e-03, -1.3594e-02,\n",
       "                      -3.4144e-02, -9.1770e-03, -1.2155e-02,  2.9860e-02,  8.8523e-04,\n",
       "                       7.4642e-02, -2.1827e-02,  2.7113e-03, -1.7278e-02, -1.3488e-02,\n",
       "                      -1.5067e-02,  3.6985e-02, -8.5746e-02,  2.6886e-03, -4.2943e-02,\n",
       "                       8.9484e-03,  1.5032e-02,  3.9375e-02,  2.3458e-02, -1.5006e-03,\n",
       "                      -3.5126e-02,  1.5453e-02, -1.5846e-02,  1.0075e-03, -3.2610e-02,\n",
       "                      -4.5483e-02,  2.2159e-03, -3.6521e-02, -1.8692e-02, -1.5255e-02,\n",
       "                       7.0742e-03, -4.2116e-02,  4.3140e-02, -1.3786e-02,  1.2676e-02,\n",
       "                      -2.7994e-02, -3.7344e-03, -9.5176e-04,  2.1869e-02,  3.1608e-02,\n",
       "                       1.4721e-03, -9.1232e-03, -8.5612e-03,  2.3143e-02,  4.1456e-03,\n",
       "                      -2.2904e-02, -2.4165e-02, -1.7394e-02, -7.9979e-05,  2.9257e-02,\n",
       "                       5.3669e-03, -1.9165e-02, -4.3603e-02, -6.0278e-03,  7.7830e-03,\n",
       "                      -6.0616e-03, -4.2360e-02, -3.4538e-02,  8.0884e-03,  1.7510e-02,\n",
       "                       4.5586e-03, -1.9431e-02, -3.0566e-03, -2.0157e-02, -8.5464e-03,\n",
       "                      -5.3590e-02,  2.5706e-02,  8.0280e-02,  5.7544e-04, -9.3058e-03,\n",
       "                      -3.3866e-02,  1.4254e-03, -5.2402e-02,  4.0210e-03, -1.7722e-02,\n",
       "                      -1.1509e-02,  2.1341e-02, -1.5891e-02, -4.1271e-02,  2.5836e-02,\n",
       "                      -5.7580e-03, -1.5662e-02,  1.2994e-02, -5.8283e-02,  4.9368e-02,\n",
       "                       2.2774e-02,  1.7319e-03,  1.5904e-02, -3.8020e-02,  3.1284e-02,\n",
       "                       5.1335e-02, -1.2691e-02, -1.9588e-03, -1.0176e-02, -1.0311e-02,\n",
       "                      -2.6244e-02,  1.2898e-02,  6.3227e-03, -8.4066e-03,  2.8960e-03,\n",
       "                       1.3555e-02, -1.3720e-02,  4.8206e-02, -2.3779e-02, -4.1369e-03,\n",
       "                      -1.0860e-02, -1.7841e-02,  7.0417e-03, -8.2938e-03, -3.9606e-03,\n",
       "                       3.4814e-02,  2.4026e-02, -7.2836e-03,  7.5743e-03,  4.5877e-02,\n",
       "                       3.3474e-02,  7.8076e-03,  1.5449e-02,  3.0500e-03,  5.3704e-03,\n",
       "                       2.6599e-02,  4.4351e-02,  8.4500e-03, -2.6188e-02, -9.5058e-03,\n",
       "                      -1.6657e-02, -1.3163e-02, -3.1273e-02, -3.6285e-02, -1.5428e-02,\n",
       "                       7.5485e-03, -5.8917e-03, -1.9530e-02, -7.5647e-03,  1.6795e-02,\n",
       "                       4.1369e-02, -2.7559e-02, -8.2007e-03, -3.4613e-02,  1.1097e-02,\n",
       "                      -5.3103e-02, -7.1317e-03, -4.9510e-02,  4.3493e-02, -1.4890e-02,\n",
       "                       3.1096e-02,  5.7721e-02, -1.4674e-02, -4.6272e-03, -2.2958e-02,\n",
       "                       3.8671e-02, -4.3147e-02, -1.2005e-02,  6.6506e-03, -1.1900e-02,\n",
       "                      -1.7345e-02, -3.9557e-02, -1.1101e-02, -6.2088e-03, -1.5011e-01,\n",
       "                      -2.5051e-02, -3.0723e-03,  2.6349e-02,  2.6806e-02, -2.4055e-02,\n",
       "                      -1.6786e-03, -4.9239e-02, -2.9126e-03, -3.7568e-03, -7.7346e-05,\n",
       "                       1.5768e-02, -2.9944e-02, -2.3584e-02, -1.7941e-02, -9.1261e-03,\n",
       "                       1.0051e-03,  5.3339e-02, -1.3285e-02, -3.8295e-02, -5.5699e-02,\n",
       "                      -3.4481e-02, -2.3272e-02,  4.3062e-02, -5.2779e-04,  1.8991e-02,\n",
       "                      -3.7115e-02,  1.8842e-02, -2.8254e-02, -2.8660e-02, -1.0154e-01,\n",
       "                       6.7085e-02,  5.8485e-02, -4.9838e-02,  6.1987e-02, -1.7426e-02,\n",
       "                      -4.0240e-02,  5.0702e-02,  3.2567e-02,  3.1046e-04,  8.5769e-03,\n",
       "                       1.5203e-02,  3.5473e-02, -1.9105e-02, -1.6402e-02, -4.8889e-03,\n",
       "                       2.8589e-02, -4.9805e-03,  1.8621e-02,  4.5595e-02,  1.3123e-02,\n",
       "                      -8.4223e-03,  1.7408e-02, -2.4858e-02, -5.5969e-02,  4.9308e-02,\n",
       "                      -4.8059e-02, -1.6855e-03, -1.2727e-03, -4.8071e-02,  2.8412e-02,\n",
       "                      -3.2485e-02,  1.8067e-04, -1.5382e-02,  3.3124e-02,  1.0341e-02,\n",
       "                       4.2183e-02, -2.6772e-02, -2.2851e-02, -1.3377e-02, -1.1002e-02,\n",
       "                       1.2463e-02,  1.7808e-02,  1.7208e-03, -2.0770e-02, -1.2327e-02,\n",
       "                      -2.6231e-02,  3.3400e-02,  2.7754e-02, -1.0451e-02, -2.0007e-02,\n",
       "                       1.5458e-02,  4.3924e-04, -1.6636e-03, -4.5285e-02, -5.8561e-02,\n",
       "                       1.0469e-02,  2.6012e-02, -5.2669e-02,  1.3572e-02, -3.7258e-02,\n",
       "                       3.2689e-02,  6.4939e-02,  1.2605e-03, -1.7879e-02,  1.5527e-02,\n",
       "                      -1.7126e-02, -1.5385e-02, -2.9271e-02, -1.9906e-02,  2.2261e-02,\n",
       "                       1.0310e-02, -1.4637e-02, -9.6434e-03,  1.2649e-02, -1.7200e-02,\n",
       "                       3.1112e-02, -6.6424e-03, -1.9605e-02, -2.1260e-03,  2.6921e-03,\n",
       "                      -4.1744e-03,  2.0540e-02, -2.7367e-02,  3.6314e-02,  1.0333e-02,\n",
       "                       8.9858e-04,  3.4318e-02, -2.5229e-02,  5.2111e-03, -3.5452e-03,\n",
       "                      -1.8607e-02, -8.3889e-04,  2.7060e-02, -1.4765e-03,  1.2425e-02,\n",
       "                      -3.2226e-02,  2.4261e-03,  6.3903e-02,  1.5437e-02,  4.7212e-02,\n",
       "                      -9.6980e-03,  1.5992e-02, -3.8823e-02,  1.2240e-02,  1.1996e-03,\n",
       "                      -1.8950e-02, -3.3637e-02,  3.2323e-02, -1.6761e-02,  3.8203e-03,\n",
       "                       1.4325e-02, -1.2668e-02, -2.9631e-02, -3.1804e-04, -5.5205e-03,\n",
       "                      -2.0691e-02, -1.4320e-02, -8.7662e-02, -7.7582e-03, -4.8041e-02,\n",
       "                       1.9216e-03, -7.1324e-04,  2.5906e-02, -7.6160e-02,  3.3911e-02,\n",
       "                      -2.7574e-02, -2.3239e-02,  7.9121e-03,  8.7516e-04, -8.0075e-02,\n",
       "                       2.6600e-02, -6.5851e-03,  3.6337e-02, -1.6171e-02, -4.5592e-03,\n",
       "                       8.3587e-02, -1.9892e-02])),\n",
       "             ('layer4.1.bn1.running_var',\n",
       "              tensor([0.0032, 0.0028, 0.0008, 0.0038, 0.0052, 0.0037, 0.0041, 0.0022, 0.0049,\n",
       "                      0.0021, 0.0033, 0.0015, 0.0033, 0.0044, 0.0063, 0.0033, 0.0028, 0.0018,\n",
       "                      0.0020, 0.0031, 0.0030, 0.0120, 0.0030, 0.0028, 0.0011, 0.0039, 0.0053,\n",
       "                      0.0033, 0.0024, 0.0036, 0.0101, 0.0019, 0.0046, 0.0039, 0.0028, 0.0042,\n",
       "                      0.0021, 0.0064, 0.0055, 0.0045, 0.0038, 0.0026, 0.0017, 0.0030, 0.0039,\n",
       "                      0.0024, 0.0031, 0.0070, 0.0035, 0.0080, 0.0067, 0.0026, 0.0072, 0.0017,\n",
       "                      0.0050, 0.0022, 0.0023, 0.0045, 0.0012, 0.0073, 0.0067, 0.0066, 0.0060,\n",
       "                      0.0075, 0.0009, 0.0061, 0.0068, 0.0021, 0.0032, 0.0030, 0.0051, 0.0010,\n",
       "                      0.0009, 0.0040, 0.0041, 0.0032, 0.0020, 0.0076, 0.0036, 0.0033, 0.0032,\n",
       "                      0.0043, 0.0010, 0.0036, 0.0005, 0.0036, 0.0029, 0.0029, 0.0010, 0.0035,\n",
       "                      0.0025, 0.0042, 0.0027, 0.0034, 0.0028, 0.0014, 0.0047, 0.0020, 0.0017,\n",
       "                      0.0057, 0.0037, 0.0060, 0.0065, 0.0053, 0.0060, 0.0033, 0.0070, 0.0029,\n",
       "                      0.0027, 0.0068, 0.0017, 0.0038, 0.0035, 0.0030, 0.0028, 0.0042, 0.0072,\n",
       "                      0.0041, 0.0055, 0.0043, 0.0015, 0.0021, 0.0022, 0.0022, 0.0048, 0.0043,\n",
       "                      0.0036, 0.0022, 0.0083, 0.0062, 0.0040, 0.0045, 0.0049, 0.0009, 0.0074,\n",
       "                      0.0037, 0.0037, 0.0051, 0.0040, 0.0025, 0.0033, 0.0034, 0.0037, 0.0087,\n",
       "                      0.0037, 0.0016, 0.0016, 0.0025, 0.0033, 0.0130, 0.0012, 0.0047, 0.0017,\n",
       "                      0.0019, 0.0022, 0.0084, 0.0045, 0.0060, 0.0033, 0.0090, 0.0043, 0.0035,\n",
       "                      0.0023, 0.0027, 0.0025, 0.0036, 0.0049, 0.0014, 0.0014, 0.0103, 0.0068,\n",
       "                      0.0025, 0.0058, 0.0026, 0.0070, 0.0018, 0.0069, 0.0029, 0.0033, 0.0043,\n",
       "                      0.0043, 0.0105, 0.0089, 0.0045, 0.0049, 0.0037, 0.0013, 0.0032, 0.0023,\n",
       "                      0.0047, 0.0164, 0.0016, 0.0060, 0.0034, 0.0017, 0.0030, 0.0055, 0.0048,\n",
       "                      0.0028, 0.0030, 0.0036, 0.0026, 0.0044, 0.0077, 0.0039, 0.0038, 0.0033,\n",
       "                      0.0020, 0.0011, 0.0033, 0.0037, 0.0019, 0.0054, 0.0016, 0.0043, 0.0027,\n",
       "                      0.0035, 0.0043, 0.0029, 0.0076, 0.0028, 0.0031, 0.0014, 0.0043, 0.0047,\n",
       "                      0.0019, 0.0011, 0.0048, 0.0021, 0.0055, 0.0031, 0.0018, 0.0028, 0.0016,\n",
       "                      0.0024, 0.0017, 0.0015, 0.0025, 0.0037, 0.0039, 0.0019, 0.0063, 0.0044,\n",
       "                      0.0025, 0.0044, 0.0023, 0.0015, 0.0039, 0.0018, 0.0011, 0.0047, 0.0030,\n",
       "                      0.0102, 0.0060, 0.0039, 0.0022, 0.0016, 0.0090, 0.0025, 0.0026, 0.0019,\n",
       "                      0.0026, 0.0051, 0.0027, 0.0048, 0.0028, 0.0037, 0.0022, 0.0099, 0.0119,\n",
       "                      0.0017, 0.0011, 0.0018, 0.0016, 0.0030, 0.0064, 0.0022, 0.0052, 0.0036,\n",
       "                      0.0013, 0.0014, 0.0041, 0.0020, 0.0022, 0.0080, 0.0072, 0.0019, 0.0065,\n",
       "                      0.0021, 0.0019, 0.0016, 0.0060, 0.0022, 0.0066, 0.0041, 0.0024, 0.0027,\n",
       "                      0.0033, 0.0029, 0.0048, 0.0068, 0.0017, 0.0033, 0.0042, 0.0033, 0.0041,\n",
       "                      0.0040, 0.0037, 0.0047, 0.0039, 0.0016, 0.0045, 0.0037, 0.0032, 0.0045,\n",
       "                      0.0014, 0.0019, 0.0023, 0.0017, 0.0051, 0.0059, 0.0050, 0.0042, 0.0056,\n",
       "                      0.0029, 0.0060, 0.0029, 0.0084, 0.0069, 0.0018, 0.0015, 0.0062, 0.0043,\n",
       "                      0.0018, 0.0032, 0.0057, 0.0070, 0.0053, 0.0040, 0.0027, 0.0041, 0.0047,\n",
       "                      0.0037, 0.0069, 0.0125, 0.0054, 0.0011, 0.0040, 0.0044, 0.0031, 0.0109,\n",
       "                      0.0042, 0.0046, 0.0025, 0.0049, 0.0026, 0.0067, 0.0040, 0.0050, 0.0032,\n",
       "                      0.0027, 0.0042, 0.0027, 0.0014, 0.0039, 0.0044, 0.0025, 0.0039, 0.0028,\n",
       "                      0.0008, 0.0021, 0.0028, 0.0034, 0.0015, 0.0064, 0.0067, 0.0039, 0.0104,\n",
       "                      0.0048, 0.0020, 0.0031, 0.0077, 0.0128, 0.0037, 0.0017, 0.0026, 0.0045,\n",
       "                      0.0058, 0.0053, 0.0051, 0.0023, 0.0016, 0.0013, 0.0041, 0.0059, 0.0029,\n",
       "                      0.0082, 0.0027, 0.0057, 0.0042, 0.0141, 0.0045, 0.0040, 0.0016, 0.0016,\n",
       "                      0.0062, 0.0048, 0.0033, 0.0039, 0.0046, 0.0034, 0.0024, 0.0035, 0.0011,\n",
       "                      0.0044, 0.0021, 0.0034, 0.0048, 0.0054, 0.0050, 0.0019, 0.0082, 0.0065,\n",
       "                      0.0012, 0.0030, 0.0026, 0.0040, 0.0018, 0.0039, 0.0063, 0.0045, 0.0047,\n",
       "                      0.0042, 0.0034, 0.0019, 0.0020, 0.0072, 0.0021, 0.0049, 0.0015, 0.0027,\n",
       "                      0.0018, 0.0033, 0.0048, 0.0023, 0.0053, 0.0031, 0.0017, 0.0016, 0.0034,\n",
       "                      0.0026, 0.0049, 0.0012, 0.0043, 0.0082, 0.0029, 0.0080, 0.0023, 0.0043,\n",
       "                      0.0020, 0.0031, 0.0037, 0.0055, 0.0047, 0.0038, 0.0053, 0.0029, 0.0047,\n",
       "                      0.0023, 0.0051, 0.0037, 0.0071, 0.0065, 0.0019, 0.0036, 0.0059, 0.0017,\n",
       "                      0.0040, 0.0018, 0.0022, 0.0033, 0.0028, 0.0044, 0.0021, 0.0073, 0.0035,\n",
       "                      0.0027, 0.0026, 0.0050, 0.0022, 0.0036, 0.0082, 0.0088, 0.0029, 0.0037,\n",
       "                      0.0045, 0.0046, 0.0068, 0.0054, 0.0021, 0.0020, 0.0065, 0.0038, 0.0016,\n",
       "                      0.0095, 0.0024, 0.0026, 0.0029, 0.0034, 0.0049, 0.0053, 0.0032])),\n",
       "             ('layer4.1.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.1.conv2.weight',\n",
       "              tensor([[[[-4.6287e-03, -1.1646e-03,  6.3681e-03],\n",
       "                        [-3.0360e-03,  2.2784e-03,  1.0058e-02],\n",
       "                        [-3.3755e-03, -3.0238e-03,  3.7849e-03]],\n",
       "              \n",
       "                       [[-1.3691e-03, -3.6343e-03, -2.4476e-03],\n",
       "                        [-3.3215e-04, -2.2913e-03,  1.0625e-03],\n",
       "                        [-1.8098e-03, -1.1541e-04,  3.0200e-03]],\n",
       "              \n",
       "                       [[ 6.1702e-04, -2.0160e-03, -4.1590e-03],\n",
       "                        [-5.2792e-04, -2.9746e-03, -2.6726e-03],\n",
       "                        [-2.3637e-04, -2.0364e-03,  9.1265e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4347e-03,  2.9408e-03,  8.4704e-03],\n",
       "                        [-1.8666e-03,  4.5232e-03,  1.0319e-02],\n",
       "                        [-2.7543e-03, -6.8752e-03, -4.9216e-03]],\n",
       "              \n",
       "                       [[ 6.7513e-05,  3.9087e-03,  5.8982e-03],\n",
       "                        [ 1.0155e-04, -1.3281e-03, -2.6771e-03],\n",
       "                        [-1.9025e-03, -6.8422e-03, -8.4379e-03]],\n",
       "              \n",
       "                       [[-5.0880e-03, -4.6759e-03,  2.6448e-04],\n",
       "                        [-3.3221e-03,  2.1289e-03,  8.1573e-03],\n",
       "                        [-4.3151e-03,  1.5069e-03,  4.0103e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.4365e-04, -2.5098e-03, -3.4853e-03],\n",
       "                        [ 5.6912e-04, -2.9438e-03, -3.2854e-03],\n",
       "                        [ 2.1829e-03,  6.7764e-04, -1.3354e-04]],\n",
       "              \n",
       "                       [[-8.5447e-04, -1.5019e-04, -2.8239e-04],\n",
       "                        [-8.1443e-04, -1.0974e-03, -1.7909e-03],\n",
       "                        [ 8.3376e-04,  3.8749e-04, -5.2516e-04]],\n",
       "              \n",
       "                       [[-4.6832e-04,  7.8755e-04,  4.7947e-04],\n",
       "                        [-9.9279e-05, -1.3264e-03, -1.7775e-03],\n",
       "                        [-4.0085e-04, -4.5545e-04, -8.8623e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.0299e-04, -5.8420e-04, -2.1012e-03],\n",
       "                        [ 7.5395e-04,  6.3571e-04, -1.0660e-03],\n",
       "                        [ 1.6831e-03,  8.8451e-04, -6.2684e-04]],\n",
       "              \n",
       "                       [[-1.4764e-03,  2.7887e-04,  7.9955e-04],\n",
       "                        [ 1.0570e-03, -2.2041e-03, -1.7958e-03],\n",
       "                        [ 1.4617e-03, -8.4882e-04, -2.6654e-03]],\n",
       "              \n",
       "                       [[ 5.1591e-04, -2.9551e-04, -3.3931e-03],\n",
       "                        [-1.9745e-03,  9.6028e-04,  5.7033e-04],\n",
       "                        [-2.0204e-04,  1.6872e-03,  3.1004e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.5918e-03,  3.2591e-03, -2.4975e-03],\n",
       "                        [ 6.0749e-04,  2.8346e-03,  1.4624e-04],\n",
       "                        [-1.8623e-03, -2.4352e-03,  4.1408e-03]],\n",
       "              \n",
       "                       [[ 2.0728e-03,  1.1364e-03, -9.7192e-04],\n",
       "                        [ 7.0132e-03,  4.1793e-03, -1.8345e-03],\n",
       "                        [-2.7105e-05, -2.0617e-03, -5.2694e-04]],\n",
       "              \n",
       "                       [[-1.1374e-03,  8.2002e-05,  6.5206e-04],\n",
       "                        [ 4.3368e-03,  3.5555e-03, -2.2358e-03],\n",
       "                        [ 5.6721e-03,  9.6026e-04, -2.4075e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1026e-03,  3.0564e-03,  2.1563e-03],\n",
       "                        [-5.0227e-03, -3.9834e-04, -9.8787e-04],\n",
       "                        [ 3.0309e-03, -7.1522e-03, -3.3925e-03]],\n",
       "              \n",
       "                       [[-4.4552e-03, -4.8555e-03, -2.7906e-03],\n",
       "                        [-4.5878e-04, -6.2527e-03, -2.9411e-03],\n",
       "                        [ 3.1426e-03, -5.0577e-03, -2.8527e-03]],\n",
       "              \n",
       "                       [[-3.9793e-04,  6.7468e-04,  1.1720e-03],\n",
       "                        [-4.4336e-03, -1.2843e-03,  5.7002e-03],\n",
       "                        [-2.0339e-03, -2.7802e-03,  1.7258e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.2643e-03, -3.9615e-03, -3.5315e-03],\n",
       "                        [-1.0606e-03, -1.5749e-03, -1.0283e-03],\n",
       "                        [-3.0451e-03, -3.5643e-03, -2.8704e-03]],\n",
       "              \n",
       "                       [[-2.1794e-03, -1.3777e-03, -1.1545e-03],\n",
       "                        [ 2.1234e-03,  4.7877e-04,  6.0727e-04],\n",
       "                        [ 2.1520e-04,  3.9294e-04, -7.2284e-05]],\n",
       "              \n",
       "                       [[-5.7370e-05, -2.3493e-04,  4.9491e-04],\n",
       "                        [ 1.0008e-03,  7.0131e-04,  1.1050e-03],\n",
       "                        [ 6.9987e-04,  3.6234e-04,  3.7562e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4965e-04,  1.8956e-03,  1.7750e-03],\n",
       "                        [ 2.4874e-03,  1.1519e-03,  2.9021e-03],\n",
       "                        [ 2.7055e-04,  6.7996e-04, -1.0599e-04]],\n",
       "              \n",
       "                       [[ 7.5170e-04, -4.0150e-04, -1.2665e-03],\n",
       "                        [-5.3367e-04, -1.7539e-03, -2.1380e-03],\n",
       "                        [-9.1943e-04, -4.1267e-03, -1.7676e-03]],\n",
       "              \n",
       "                       [[-3.0522e-03, -1.2343e-03, -1.4807e-03],\n",
       "                        [-7.8787e-04,  1.7096e-04, -1.3860e-03],\n",
       "                        [-1.1207e-03, -8.1950e-04, -8.7514e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4129e-03,  1.9711e-04,  6.8155e-04],\n",
       "                        [-5.9018e-04,  9.3266e-04,  2.0058e-04],\n",
       "                        [-1.7119e-03,  1.8607e-04, -2.2528e-04]],\n",
       "              \n",
       "                       [[-4.6609e-04, -8.1630e-04, -1.2291e-03],\n",
       "                        [-5.3728e-06,  1.9052e-03,  1.8882e-04],\n",
       "                        [-1.0840e-03,  2.9996e-04, -5.8696e-05]],\n",
       "              \n",
       "                       [[-9.2920e-04, -1.5757e-04,  9.8127e-06],\n",
       "                        [ 9.3316e-05,  1.5822e-03,  1.5953e-03],\n",
       "                        [ 2.7426e-04, -6.1750e-04, -3.4449e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.7886e-03, -9.9532e-05, -1.2485e-03],\n",
       "                        [ 2.3475e-04, -5.9125e-04, -3.8957e-04],\n",
       "                        [-3.9265e-04, -4.7596e-04, -2.2082e-03]],\n",
       "              \n",
       "                       [[-1.3091e-04, -3.1673e-04, -1.0019e-03],\n",
       "                        [ 1.9766e-03, -3.5000e-04, -1.4507e-03],\n",
       "                        [ 1.3399e-03, -1.1404e-03, -2.3009e-04]],\n",
       "              \n",
       "                       [[-1.5444e-03,  4.5124e-04,  1.3027e-04],\n",
       "                        [ 3.9776e-04,  1.0749e-04,  4.9718e-04],\n",
       "                        [ 1.4823e-03,  1.1833e-03,  1.1960e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7882e-03, -5.0524e-03, -6.2007e-03],\n",
       "                        [ 1.2844e-03,  2.7129e-03, -3.7862e-03],\n",
       "                        [-2.0252e-03, -3.9066e-04, -2.1308e-03]],\n",
       "              \n",
       "                       [[-9.9871e-04, -6.6815e-03, -5.0522e-03],\n",
       "                        [ 1.7755e-04,  2.3383e-03, -1.8538e-03],\n",
       "                        [-2.8936e-03, -1.0992e-03, -7.4128e-04]],\n",
       "              \n",
       "                       [[-5.9476e-04, -1.0354e-03,  2.9052e-04],\n",
       "                        [-9.8888e-04, -1.0302e-03, -1.6240e-03],\n",
       "                        [ 4.5565e-04,  2.4826e-03,  1.4670e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1677e-03, -4.6877e-03, -2.4069e-03],\n",
       "                        [ 5.3654e-03,  1.9564e-03, -1.9621e-03],\n",
       "                        [ 1.4721e-04,  1.4732e-03, -9.9975e-04]],\n",
       "              \n",
       "                       [[ 3.9721e-03, -1.6199e-04,  4.2636e-03],\n",
       "                        [-4.3708e-04, -1.2559e-03, -3.6340e-03],\n",
       "                        [ 2.6651e-04,  1.6424e-03, -2.0678e-03]],\n",
       "              \n",
       "                       [[-2.3945e-03, -2.1272e-03,  8.1972e-05],\n",
       "                        [ 4.1969e-03,  2.4209e-03,  2.3249e-03],\n",
       "                        [-2.2274e-03, -1.6694e-03, -1.8147e-03]]]])),\n",
       "             ('layer4.1.bn2.weight',\n",
       "              tensor([9.1890e-02, 4.0724e-02, 9.8030e-02, 5.7004e-02, 4.5751e-02, 6.3737e-02,\n",
       "                      4.3441e-02, 6.9824e-02, 7.2255e-02, 7.4691e-02, 4.4526e-02, 5.5171e-02,\n",
       "                      4.7728e-02, 4.3762e-02, 6.2829e-02, 3.8378e-02, 7.6347e-02, 6.6292e-02,\n",
       "                      7.9455e-02, 7.4408e-02, 2.5627e-02, 3.3764e-02, 5.5235e-02, 5.9466e-02,\n",
       "                      8.8790e-02, 5.0086e-02, 3.3962e-02, 5.3190e-02, 3.0633e-02, 4.2418e-02,\n",
       "                      6.9154e-02, 5.2274e-02, 4.7143e-02, 3.2395e-02, 3.4360e-02, 4.0115e-02,\n",
       "                      4.6932e-02, 6.0173e-02, 8.2686e-02, 6.7718e-02, 9.8425e-02, 6.8711e-02,\n",
       "                      4.5154e-02, 3.0086e-02, 6.3246e-02, 1.1512e-01, 8.3220e-02, 6.7690e-02,\n",
       "                      4.8713e-02, 7.9558e-02, 2.9023e-02, 6.0343e-02, 5.1149e-02, 1.8439e-02,\n",
       "                      4.8387e-02, 4.9864e-03, 3.9228e-02, 4.5716e-02, 9.2835e-02, 6.5069e-02,\n",
       "                      5.3758e-02, 6.0469e-02, 5.3983e-02, 3.4928e-02, 4.9612e-02, 2.2531e-02,\n",
       "                      7.2286e-02, 6.9494e-02, 9.1837e-02, 5.4185e-02, 3.8888e-02, 5.7314e-02,\n",
       "                      8.6975e-02, 4.4141e-02, 5.1561e-02, 8.2744e-02, 5.6976e-02, 5.8794e-02,\n",
       "                      6.9432e-02, 7.1572e-02, 5.8507e-02, 6.1123e-02, 8.0843e-02, 8.6382e-02,\n",
       "                      6.8431e-02, 6.5940e-02, 7.6999e-02, 6.1836e-02, 4.4080e-02, 5.8192e-02,\n",
       "                      7.0544e-02, 5.1689e-02, 8.4058e-02, 8.6636e-02, 4.2608e-02, 4.2202e-02,\n",
       "                      4.0404e-02, 5.4009e-02, 7.5993e-02, 8.8886e-02, 8.7695e-02, 5.6473e-02,\n",
       "                      5.4262e-02, 4.6993e-02, 3.8067e-02, 4.9585e-02, 8.4779e-02, 6.5906e-02,\n",
       "                      3.7304e-02, 7.0136e-02, 4.8478e-02, 7.9305e-02, 7.7195e-02, 3.1831e-02,\n",
       "                      7.2008e-02, 8.5870e-02, 4.7482e-02, 4.4437e-02, 7.9055e-02, 5.1233e-02,\n",
       "                      3.7855e-02, 6.5440e-02, 5.0526e-02, 6.8881e-02, 6.7173e-02, 7.9990e-02,\n",
       "                      3.4096e-02, 6.5984e-02, 6.1563e-02, 7.1365e-02, 6.9751e-02, 5.0815e-02,\n",
       "                      8.6329e-02, 4.7188e-02, 5.2787e-02, 4.7511e-02, 5.9047e-02, 9.3423e-02,\n",
       "                      3.5432e-02, 4.6143e-02, 5.8056e-02, 6.3399e-02, 4.1661e-02, 8.4858e-02,\n",
       "                      6.8961e-02, 5.2097e-02, 4.8685e-02, 4.5686e-02, 2.4167e-02, 8.3543e-02,\n",
       "                      2.4081e-02, 4.3343e-02, 5.7384e-02, 6.1795e-02, 5.2350e-02, 6.3122e-02,\n",
       "                      8.7308e-02, 7.6711e-02, 4.7499e-02, 4.9503e-02, 4.8263e-02, 6.0351e-02,\n",
       "                      6.8552e-02, 4.0178e-02, 2.6006e-02, 6.3328e-02, 5.1826e-02, 4.3858e-02,\n",
       "                      6.8108e-02, 5.1737e-02, 7.8598e-02, 5.4070e-02, 5.4737e-02, 7.5444e-02,\n",
       "                      8.6710e-02, 5.6688e-02, 6.7603e-02, 7.3149e-03, 4.4468e-02, 7.1879e-02,\n",
       "                      5.9163e-02, 5.9518e-02, 5.9261e-02, 3.5545e-02, 3.0941e-02, 6.4153e-02,\n",
       "                      5.1925e-02, 7.1381e-02, 4.8192e-02, 4.1035e-02, 5.6515e-02, 9.3179e-02,\n",
       "                      2.8014e-02, 4.3796e-02, 4.0459e-02, 5.8992e-02, 5.2954e-02, 7.5254e-02,\n",
       "                      6.7231e-02, 4.4842e-02, 3.4249e-02, 3.3347e-02, 2.6629e-02, 6.5201e-02,\n",
       "                      4.3419e-02, 7.3497e-02, 4.5329e-02, 5.2288e-02, 8.1537e-02, 7.5008e-02,\n",
       "                      3.5043e-02, 1.0162e-01, 6.3940e-02, 3.5890e-02, 5.1801e-02, 3.3472e-02,\n",
       "                      2.8113e-02, 6.1143e-02, 3.9583e-02, 5.2054e-02, 2.8602e-02, 5.5199e-02,\n",
       "                      4.5096e-02, 8.7631e-02, 8.3957e-02, 5.4709e-02, 5.2786e-02, 2.4982e-02,\n",
       "                      3.2039e-02, 5.6277e-02, 2.8740e-02, 8.3640e-02, 8.2701e-02, 7.3532e-02,\n",
       "                      2.5512e-02, 3.7372e-02, 7.0683e-02, 1.6207e-02, 4.7799e-02, 9.2784e-02,\n",
       "                      5.1543e-02, 4.9371e-02, 4.1470e-02, 3.3944e-02, 6.7537e-02, 5.6307e-02,\n",
       "                      4.3320e-02, 8.0551e-02, 6.1133e-02, 4.0999e-02, 5.4469e-02, 7.0255e-02,\n",
       "                      5.8492e-02, 4.9411e-02, 4.5282e-02, 4.4237e-02, 6.3149e-02, 1.1575e-01,\n",
       "                      3.3342e-02, 9.9134e-02, 4.2573e-02, 5.0744e-02, 5.5534e-02, 5.3948e-02,\n",
       "                      9.2906e-02, 6.6679e-02, 3.1794e-02, 3.5965e-02, 6.4662e-02, 7.1235e-02,\n",
       "                      4.5249e-02, 5.5221e-02, 6.0658e-02, 6.7278e-02, 8.2147e-02, 4.0358e-02,\n",
       "                      6.3064e-02, 3.2558e-02, 4.3339e-02, 3.3119e-02, 4.9086e-02, 4.3065e-02,\n",
       "                      6.3934e-02, 1.0059e-01, 6.7472e-02, 6.1009e-02, 6.4078e-02, 3.7110e-02,\n",
       "                      4.3168e-02, 1.0167e-01, 6.3670e-02, 3.1402e-02, 7.4866e-02, 7.2931e-02,\n",
       "                      4.1416e-02, 1.0025e-01, 7.6614e-02, 4.8781e-02, 6.6065e-02, 4.1473e-02,\n",
       "                      5.8959e-02, 6.1177e-02, 4.5937e-02, 5.2421e-02, 5.1543e-02, 9.9669e-02,\n",
       "                      4.0182e-02, 2.8508e-02, 8.5968e-02, 5.2976e-02, 4.1071e-02, 5.3787e-02,\n",
       "                      5.9631e-02, 5.7982e-02, 4.8923e-02, 6.1053e-02, 6.6367e-02, 4.4967e-02,\n",
       "                      4.8775e-02, 3.1025e-02, 5.0750e-02, 6.7151e-02, 5.9105e-02, 4.8038e-02,\n",
       "                      9.2627e-02, 4.8880e-02, 5.1602e-02, 4.9461e-02, 5.9924e-02, 6.1285e-02,\n",
       "                      5.6855e-02, 4.9179e-02, 4.9871e-02, 1.0581e-01, 8.3461e-02, 3.9963e-02,\n",
       "                      5.0636e-02, 6.6095e-02, 8.8016e-02, 5.4149e-02, 8.6778e-02, 6.1797e-02,\n",
       "                      6.3337e-03, 7.7894e-02, 2.8944e-02, 8.3155e-02, 4.0686e-02, 4.3993e-02,\n",
       "                      4.4630e-02, 6.4095e-02, 1.0471e-01, 7.0223e-02, 6.6248e-02, 6.5929e-02,\n",
       "                      4.9658e-02, 7.1304e-02, 6.1421e-02, 5.8341e-02, 6.1344e-02, 8.3034e-02,\n",
       "                      5.8055e-02, 9.2871e-02, 2.6381e-02, 6.1564e-02, 4.7291e-02, 4.6733e-02,\n",
       "                      3.9372e-02, 7.7955e-02, 7.7711e-02, 3.5993e-02, 4.0550e-02, 9.0985e-02,\n",
       "                      2.6646e-02, 9.5170e-02, 3.9801e-02, 3.7391e-02, 2.7380e-02, 5.1222e-02,\n",
       "                      3.9449e-02, 9.3434e-02, 3.3953e-02, 5.2955e-02, 5.4987e-02, 5.6545e-02,\n",
       "                      6.1998e-02, 7.7330e-02, 5.3960e-02, 6.8646e-02, 6.6836e-02, 5.5550e-02,\n",
       "                      4.5574e-02, 4.9913e-02, 7.8975e-02, 8.6737e-02, 5.1998e-02, 5.5034e-02,\n",
       "                      6.0330e-02, 4.2588e-02, 4.3627e-02, 7.9998e-02, 5.4616e-02, 6.1861e-02,\n",
       "                      5.4101e-02, 9.8419e-02, 8.6328e-02, 7.2824e-02, 6.1551e-02, 7.1128e-02,\n",
       "                      8.2187e-02, 1.1546e-01, 7.0881e-02, 7.1152e-02, 4.1059e-02, 6.0766e-02,\n",
       "                      6.7281e-02, 8.2721e-05, 3.6568e-02, 5.4748e-02, 4.6585e-02, 2.9078e-02,\n",
       "                      5.9324e-02, 1.0763e-01, 5.2832e-02, 5.4695e-02, 4.5277e-02, 5.6296e-02,\n",
       "                      6.1018e-02, 5.6816e-02, 7.8470e-02, 5.6225e-02, 6.3579e-02, 6.7589e-02,\n",
       "                      1.0265e-01, 6.7406e-02, 4.5366e-02, 5.8982e-02, 2.6495e-02, 2.5424e-02,\n",
       "                      7.2844e-02, 6.0765e-02, 6.7487e-02, 3.7177e-02, 5.6678e-02, 1.9265e-02,\n",
       "                      2.7802e-02, 6.0970e-02, 3.7872e-02, 6.3214e-02, 7.3888e-02, 5.0564e-02,\n",
       "                      5.3005e-02, 6.7619e-02, 3.7164e-02, 5.3026e-02, 4.8250e-02, 5.1163e-02,\n",
       "                      5.3315e-02, 5.7210e-02, 3.6739e-02, 6.6667e-02, 6.8152e-02, 6.8005e-02,\n",
       "                      5.9533e-02, 7.7337e-02, 8.5223e-02, 3.6951e-02, 3.7049e-02, 3.2536e-02,\n",
       "                      8.6614e-02, 6.6547e-02, 5.6144e-02, 5.9861e-02, 8.2001e-02, 7.7669e-02,\n",
       "                      4.3908e-02, 3.7853e-02, 9.3960e-02, 8.6172e-02, 7.3266e-02, 7.5030e-02,\n",
       "                      8.1203e-02, 4.3803e-02, 5.1615e-02, 6.5488e-02, 6.4112e-02, 4.8140e-02,\n",
       "                      4.8243e-02, 6.7165e-02, 5.4967e-02, 4.4193e-02, 4.9620e-02, 5.2180e-02,\n",
       "                      4.9791e-04, 7.7967e-02, 6.3318e-02, 4.9788e-02, 4.5014e-02, 3.7701e-02,\n",
       "                      4.5259e-02, 1.1091e-01, 7.4268e-02, 9.2356e-02, 7.9140e-02, 1.5607e-02,\n",
       "                      7.5170e-02, 1.5669e-02, 6.8748e-02, 2.4504e-02, 4.7470e-02, 4.9567e-02,\n",
       "                      3.6036e-02, 9.8840e-02])),\n",
       "             ('layer4.1.bn2.bias',\n",
       "              tensor([-7.7520e-02, -3.0505e-02, -5.0654e-02, -2.0071e-02, -2.3846e-02,\n",
       "                      -3.4868e-03, -2.6331e-02, -1.9622e-02, -3.6350e-02, -2.1474e-02,\n",
       "                      -4.0665e-03, -3.8514e-02,  8.9238e-04, -3.7321e-02, -3.2703e-02,\n",
       "                      -8.0241e-03, -3.4775e-02, -6.2958e-02, -1.0269e-01, -4.9330e-02,\n",
       "                      -6.8857e-03, -1.0973e-02, -6.5346e-02, -3.6604e-02, -6.6693e-02,\n",
       "                      -2.2788e-02, -1.0624e-02, -1.2167e-02, -2.0856e-02, -2.7770e-02,\n",
       "                      -2.2655e-02, -1.9982e-02, -2.7818e-02, -6.0563e-03, -1.4585e-02,\n",
       "                      -3.2854e-02, -2.4840e-02, -1.6280e-02, -2.8494e-02, -5.3188e-02,\n",
       "                      -5.7629e-02, -4.3471e-02, -1.5085e-02, -3.4440e-03, -2.0778e-02,\n",
       "                      -2.8074e-02, -2.3903e-02, -3.5225e-02, -3.5529e-02, -4.5793e-02,\n",
       "                      -1.9752e-02, -3.9404e-02, -1.1494e-02, -4.0282e-03, -1.5226e-02,\n",
       "                      -2.3839e-02, -4.3903e-03, -4.1813e-03, -4.4790e-02, -6.6929e-02,\n",
       "                      -3.3162e-02, -5.0018e-02, -5.8801e-02, -6.2061e-03, -3.9382e-02,\n",
       "                      -1.7389e-02, -7.5547e-02, -1.5675e-02, -3.7073e-02, -3.5216e-02,\n",
       "                      -1.2613e-02, -3.5613e-02, -5.5260e-02, -2.0565e-02, -1.9395e-02,\n",
       "                      -5.5669e-02, -4.8824e-03, -1.3234e-02, -3.2293e-02, -2.6403e-03,\n",
       "                      -4.2686e-02, -1.9039e-02, -1.6374e-02, -3.3450e-02, -6.9899e-02,\n",
       "                      -2.7596e-02, -7.6228e-02, -3.8132e-02, -1.1355e-02, -1.2943e-02,\n",
       "                      -1.3725e-02, -3.0731e-02, -4.0859e-02, -3.2651e-02, -2.4574e-02,\n",
       "                      -7.3354e-03, -1.9871e-02, -3.8623e-02, -6.4224e-02, -6.8111e-02,\n",
       "                      -1.4212e-02, -3.8072e-02, -1.1562e-02, -2.1587e-02, -1.9323e-02,\n",
       "                      -1.0600e-02, -5.5850e-02, -3.8942e-02,  1.2154e-02, -6.2604e-03,\n",
       "                      -4.5130e-02, -1.0598e-02, -9.5491e-02,  1.7747e-03, -5.8521e-02,\n",
       "                      -2.2512e-02, -3.5912e-02, -3.2553e-02, -6.6209e-02, -3.7123e-04,\n",
       "                      -2.0743e-02, -4.4168e-02, -9.6856e-04, -3.9849e-03, -2.8988e-02,\n",
       "                      -2.5980e-02, -2.2510e-02, -5.2319e-02, -2.7330e-02, -2.5399e-02,\n",
       "                      -4.3213e-02, -1.4270e-02, -3.7914e-02, -2.9439e-02, -3.4542e-02,\n",
       "                      -4.2981e-02, -4.5836e-02, -8.3124e-02, -1.9182e-02, -9.5469e-03,\n",
       "                      -6.3335e-03, -6.9582e-03, -2.9043e-02, -4.6498e-02, -4.6392e-02,\n",
       "                      -1.2716e-02, -1.1762e-02, -4.9797e-02, -2.4883e-04, -5.6313e-02,\n",
       "                       7.4551e-03, -4.9591e-02, -2.1936e-02, -5.0925e-02, -1.9828e-03,\n",
       "                      -2.6256e-02, -7.9820e-02, -4.8527e-02, -3.8235e-02, -6.3494e-02,\n",
       "                      -2.7786e-02, -2.1991e-02, -4.7295e-02,  8.3385e-03, -1.0289e-02,\n",
       "                      -2.5482e-02,  1.0810e-04, -3.4478e-02, -6.2478e-02, -1.2604e-02,\n",
       "                      -4.7122e-02, -1.0017e-02, -1.7342e-02, -2.3353e-02, -7.0170e-02,\n",
       "                       1.8554e-03, -8.5860e-02, -2.8146e-02, -5.8386e-02,  2.0655e-02,\n",
       "                      -2.9664e-02, -4.6471e-02, -2.9056e-02, -1.0942e-02,  9.3351e-03,\n",
       "                      -2.0981e-02, -2.8753e-02, -4.7284e-02, -2.9167e-02, -1.6264e-02,\n",
       "                      -3.4177e-02, -5.1928e-02, -2.1990e-02, -2.2643e-02, -3.3579e-02,\n",
       "                      -1.8896e-02, -2.9652e-02, -4.3382e-02, -3.4214e-02, -4.8255e-02,\n",
       "                      -1.6514e-02, -3.7584e-02, -3.4792e-02, -3.9707e-02, -1.5802e-02,\n",
       "                      -4.0121e-02, -1.5529e-02, -3.8744e-02, -1.8423e-02, -3.4289e-02,\n",
       "                      -2.0123e-02, -8.1082e-02, -5.0436e-02, -1.9350e-02, -1.0747e-02,\n",
       "                      -1.4292e-02, -7.5534e-03, -2.3220e-02, -3.3455e-02, -3.2144e-02,\n",
       "                      -1.4356e-02, -6.0931e-02, -5.0073e-02, -7.9551e-02, -5.1271e-02,\n",
       "                      -3.9590e-02, -3.6577e-02, -7.6771e-03, -7.9725e-03, -3.5366e-02,\n",
       "                      -3.4836e-02, -3.3893e-02, -2.1241e-02, -3.6511e-02, -1.3322e-02,\n",
       "                      -9.1299e-03, -3.8076e-02, -7.2277e-03, -3.4275e-02, -8.3553e-02,\n",
       "                      -5.4941e-02, -2.2450e-02, -3.7059e-02, -2.2010e-02,  1.8733e-03,\n",
       "                      -1.1982e-02, -3.3383e-02, -1.0917e-02, -2.2730e-02, -1.7280e-02,\n",
       "                      -1.6145e-02, -3.9279e-02, -2.4316e-02,  6.7996e-03, -3.3855e-02,\n",
       "                      -1.8837e-02, -4.2668e-02, -9.7744e-02, -2.8068e-02, -8.9989e-02,\n",
       "                      -2.9667e-02, -2.3218e-02, -3.0957e-02, -1.7491e-03, -6.7483e-02,\n",
       "                      -2.5427e-02, -5.8244e-03, -6.0099e-03, -6.8098e-02, -2.7572e-02,\n",
       "                      -1.0071e-02, -4.9573e-02, -3.3517e-02, -7.9155e-02, -6.4661e-02,\n",
       "                       1.1584e-02, -2.0900e-02,  2.0345e-03, -4.7310e-02, -2.0516e-02,\n",
       "                      -1.5352e-02, -2.2351e-02, -4.4769e-02, -6.8760e-02, -1.8040e-02,\n",
       "                      -3.8203e-02, -1.4021e-02, -1.0101e-02, -1.3511e-02, -7.6929e-02,\n",
       "                      -1.0488e-02, -4.0883e-03, -3.3166e-02, -3.8946e-02, -1.8179e-02,\n",
       "                      -8.8169e-02, -5.9414e-02, -1.6949e-02, -4.1379e-02, -1.9248e-02,\n",
       "                      -2.5340e-02, -3.8808e-02, -4.4328e-03, -2.9950e-02, -2.1640e-02,\n",
       "                      -7.0171e-02, -1.1694e-02, -6.1381e-03, -4.2539e-02, -2.2795e-02,\n",
       "                      -3.1018e-02, -1.1741e-02, -7.7130e-02, -1.9079e-02,  6.0155e-03,\n",
       "                      -2.7955e-02, -4.1601e-02, -8.9918e-03, -2.0170e-02, -6.1301e-03,\n",
       "                      -2.0567e-02, -3.7803e-02, -6.0415e-02, -4.8754e-02, -7.5024e-02,\n",
       "                      -3.4755e-02, -7.7023e-03, -3.3773e-02, -4.6400e-02, -4.1484e-02,\n",
       "                      -2.9127e-02,  9.1979e-04, -2.6165e-02, -9.3018e-02, -1.6080e-02,\n",
       "                      -3.8069e-02, -2.7617e-02, -2.1409e-02, -4.3007e-02, -3.0173e-02,\n",
       "                      -4.8819e-02,  8.3682e-03, -3.5136e-02, -2.9804e-02, -1.0086e-02,\n",
       "                      -5.6586e-02, -2.7907e-02, -3.3679e-02, -3.0874e-02, -4.9677e-02,\n",
       "                      -8.2013e-02, -2.0975e-02, -6.0762e-02, -2.7533e-02, -2.6680e-02,\n",
       "                      -6.1960e-02, -4.3811e-02, -2.4003e-02, -6.0571e-02, -1.1042e-02,\n",
       "                      -2.3162e-02, -7.9021e-02, -1.9690e-02, -1.7362e-02, -3.1333e-02,\n",
       "                      -1.3634e-02, -2.1887e-02, -5.3954e-02, -1.8653e-02, -2.0089e-02,\n",
       "                      -1.0341e-02, -1.4103e-01, -1.2317e-02, -6.6283e-02, -2.6034e-02,\n",
       "                      -4.6770e-02, -5.2263e-03, -3.2449e-02, -8.7962e-03, -8.8006e-02,\n",
       "                      -1.3050e-02, -3.7772e-02, -9.3982e-03, -1.7492e-02, -4.8613e-02,\n",
       "                      -7.1095e-02, -2.2043e-02, -1.2027e-02, -4.4487e-02, -1.5987e-02,\n",
       "                      -2.2928e-02, -1.4315e-02, -1.8679e-02, -7.4520e-02, -2.7610e-02,\n",
       "                      -5.7355e-03, -3.7906e-02, -3.3448e-02, -1.5882e-02, -5.2034e-02,\n",
       "                      -1.4406e-02, -3.9871e-03, -2.5864e-02, -5.4101e-02,  2.9223e-03,\n",
       "                      -4.1632e-02, -2.2803e-02, -7.2302e-02, -6.9300e-02, -6.9748e-02,\n",
       "                      -7.4749e-04, -2.9094e-02, -1.8452e-02, -3.0151e-02, -8.6073e-02,\n",
       "                      -6.6484e-04, -1.4368e-02, -3.1858e-02, -1.0642e-02, -3.6692e-03,\n",
       "                      -3.4392e-02, -3.4675e-02, -2.7275e-02, -4.0727e-02, -2.7170e-02,\n",
       "                      -4.0621e-02, -3.9270e-02, -3.3163e-02, -4.8513e-02, -8.7899e-02,\n",
       "                      -4.0169e-02, -2.6556e-02, -7.0007e-02,  7.5023e-03, -3.2803e-02,\n",
       "                      -2.7135e-02, -3.1220e-02, -7.4457e-03, -4.7437e-02, -3.9941e-02,\n",
       "                      -5.4310e-02, -2.3113e-02, -2.4612e-02, -1.8563e-02, -1.3807e-02,\n",
       "                      -2.7217e-02, -1.0281e-02, -3.5877e-02, -6.3472e-02, -2.3524e-02,\n",
       "                      -2.9464e-02, -8.6992e-03, -1.1457e-02, -3.0821e-02, -2.4311e-03,\n",
       "                       1.4494e-03, -1.6732e-02, -2.1383e-02, -1.5390e-02, -1.7984e-02,\n",
       "                      -3.7607e-02, -8.2214e-02, -2.0551e-02, -7.5309e-02, -6.5609e-02,\n",
       "                      -1.0946e-02, -2.3167e-02, -2.0305e-02, -3.0902e-02, -5.1501e-02,\n",
       "                      -2.6340e-02, -4.1423e-02, -1.1075e-01, -7.3577e-02, -3.7795e-03,\n",
       "                      -2.2849e-02, -5.5062e-02, -2.0184e-02, -7.1580e-02, -6.4996e-02,\n",
       "                      -6.1280e-02, -2.2249e-02, -2.4792e-02, -5.7834e-02, -3.9652e-02,\n",
       "                      -3.8030e-02, -1.3275e-02,  1.3868e-02, -5.8018e-02, -1.2447e-02,\n",
       "                      -1.2140e-02, -2.6426e-02, -2.3849e-03, -2.5702e-02, -2.9423e-03,\n",
       "                      -3.9677e-02, -3.3246e-02, -3.0522e-02, -1.9462e-02, -8.7328e-02,\n",
       "                      -5.8557e-02, -4.7041e-02, -5.3778e-02, -1.0606e-02, -2.6230e-02,\n",
       "                      -6.0373e-03, -3.1837e-02, -1.4010e-02, -2.2194e-02, -1.8673e-02,\n",
       "                      -7.8303e-03, -7.8792e-02])),\n",
       "             ('layer4.1.bn2.running_mean',\n",
       "              tensor([-1.1209e-02, -7.9187e-03, -1.5124e-02, -2.3043e-02,  2.4556e-03,\n",
       "                      -1.7377e-02, -8.5974e-03, -1.9598e-02, -1.9078e-02, -1.5814e-02,\n",
       "                      -1.7332e-02,  1.8661e-02, -1.4654e-02, -1.2766e-03, -1.3765e-02,\n",
       "                      -2.1228e-03, -1.4973e-02,  4.3793e-03, -3.9306e-03, -1.5299e-02,\n",
       "                      -1.2626e-02, -6.5765e-03, -6.3713e-03, -7.1117e-03, -2.3449e-02,\n",
       "                      -2.1094e-02, -4.5133e-03, -5.9748e-03, -5.7799e-03, -8.7557e-03,\n",
       "                       2.1490e-03, -6.5340e-03, -9.1777e-03, -9.6953e-03, -1.0677e-02,\n",
       "                      -1.0508e-02, -3.8499e-03, -1.4245e-02, -1.6570e-02, -8.5636e-03,\n",
       "                      -2.1915e-02, -6.9758e-04, -1.2253e-03,  1.7072e-03, -1.1806e-02,\n",
       "                      -2.3002e-02, -1.3411e-02, -1.9789e-02,  3.0782e-03, -2.2095e-02,\n",
       "                       2.5008e-05, -1.0353e-02, -2.2010e-02, -5.8342e-03, -1.1184e-02,\n",
       "                      -2.3091e-03, -7.6867e-03, -2.4894e-03, -1.6760e-02,  1.6513e-03,\n",
       "                      -1.1979e-02,  9.0173e-03, -1.3852e-02, -7.1950e-03,  1.4744e-03,\n",
       "                       2.7925e-03, -8.6845e-03, -1.7831e-02, -9.4674e-03, -6.7732e-03,\n",
       "                      -5.4085e-03, -2.2877e-02, -8.1081e-03,  5.1825e-03, -3.6002e-03,\n",
       "                      -8.8133e-03, -1.5634e-02, -1.8795e-02, -1.5404e-02, -2.3970e-02,\n",
       "                      -1.4866e-02, -1.4996e-02, -2.0334e-02, -1.2521e-02, -5.4897e-03,\n",
       "                      -4.8576e-03, -5.1796e-02, -8.3078e-03, -1.8239e-02, -1.1225e-02,\n",
       "                      -1.3001e-02, -6.4548e-03,  1.7739e-02, -2.5801e-02,  3.4775e-03,\n",
       "                      -4.5244e-03, -1.0938e-02, -4.4982e-03,  1.2182e-02, -1.9916e-02,\n",
       "                      -2.8771e-02,  3.5119e-03,  1.2092e-03, -1.4755e-02, -1.1005e-02,\n",
       "                       2.4273e-03, -1.8252e-02, -2.4601e-02, -2.5093e-02, -2.7342e-02,\n",
       "                       2.0275e-03, -2.0534e-02, -6.2083e-03, -1.8073e-02, -2.1929e-03,\n",
       "                      -8.8862e-03, -9.3073e-04, -4.0099e-03, -9.6154e-03, -4.2922e-03,\n",
       "                      -2.1797e-03, -7.6785e-03, -2.0420e-02, -2.8233e-02,  5.9794e-03,\n",
       "                      -1.9181e-02, -1.5946e-03,  2.2212e-03, -1.0950e-02, -3.9615e-03,\n",
       "                      -1.0812e-02, -1.1546e-02, -2.9653e-02, -9.6804e-03, -1.0537e-02,\n",
       "                      -6.0451e-03,  2.1736e-02, -5.8268e-03,  1.9765e-03, -1.3065e-02,\n",
       "                      -2.4655e-02, -2.4630e-02,  4.6527e-03, -2.3322e-02, -2.5706e-02,\n",
       "                      -1.1460e-02, -1.0737e-02, -5.3187e-03, -1.1527e-04, -3.2203e-03,\n",
       "                      -1.7612e-02,  1.7183e-03, -2.0049e-03, -1.6005e-02, -2.5894e-02,\n",
       "                      -1.5414e-03, -9.7037e-03, -1.9779e-02, -4.6409e-03, -3.5175e-02,\n",
       "                      -5.3878e-03, -2.1048e-02, -3.7384e-03, -2.4657e-02, -1.4465e-03,\n",
       "                      -5.8189e-03, -1.0855e-02,  6.4221e-04, -9.3412e-03, -1.6837e-02,\n",
       "                      -2.2178e-02, -2.0030e-02, -3.2146e-03, -1.7160e-02, -2.4926e-02,\n",
       "                      -1.1749e-02, -3.7975e-03, -8.4600e-03,  4.9831e-03, -5.7949e-02,\n",
       "                      -2.0080e-03, -1.4983e-02, -1.4100e-02, -9.7186e-03, -2.0318e-02,\n",
       "                       3.8045e-03, -7.9253e-03, -2.6981e-02, -8.6292e-03,  8.5643e-03,\n",
       "                      -4.2649e-03,  4.5589e-02,  8.0745e-04, -5.9762e-04,  3.2801e-03,\n",
       "                      -1.1381e-02,  4.0441e-03, -1.3125e-02,  1.0800e-03, -1.9325e-02,\n",
       "                      -7.7930e-03, -2.5327e-04, -7.5188e-03, -4.4878e-03, -4.9984e-03,\n",
       "                      -1.1825e-02, -6.8828e-03, -2.1069e-02, -1.4672e-02,  1.4665e-02,\n",
       "                      -2.8629e-03, -1.5314e-02, -1.3749e-02,  1.2170e-03, -5.8297e-03,\n",
       "                      -3.4138e-03, -6.4465e-03, -1.3444e-02, -5.0539e-03, -5.7716e-03,\n",
       "                      -6.8035e-03, -1.0249e-02, -5.9778e-03, -7.8604e-03, -4.8759e-03,\n",
       "                      -1.3713e-02, -1.5323e-02, -4.0199e-03, -1.5945e-03, -3.1090e-03,\n",
       "                      -3.0065e-04, -1.3899e-02, -1.4319e-02, -1.0831e-02,  2.6636e-03,\n",
       "                      -9.6419e-03, -7.4735e-03, -2.4929e-03, -8.2044e-03, -1.4044e-02,\n",
       "                      -5.0786e-03, -1.0144e-02,  8.1022e-03, -1.6450e-02, -2.3304e-02,\n",
       "                      -6.1012e-03,  6.2104e-03, -1.6469e-02, -7.9732e-03, -1.2447e-02,\n",
       "                      -2.7509e-02, -8.6304e-03, -1.5701e-02, -1.8768e-02, -1.7876e-03,\n",
       "                       1.2581e-04, -4.6577e-03, -2.9257e-02,  4.9516e-03, -1.1400e-02,\n",
       "                       3.2882e-03, -7.6483e-03, -7.1778e-03, -1.0034e-02,  2.0340e-02,\n",
       "                      -2.2651e-02, -5.5743e-03, -2.0022e-02,  3.0631e-03, -6.0719e-03,\n",
       "                      -7.1789e-03, -6.1532e-04, -1.3568e-02,  6.2725e-03,  4.8640e-03,\n",
       "                      -2.6360e-02, -1.3562e-02, -9.0461e-03, -9.6115e-03,  7.6434e-04,\n",
       "                      -7.8214e-03,  1.1470e-02, -3.9951e-03, -2.9847e-02, -7.5635e-03,\n",
       "                       7.7089e-03, -1.2781e-02, -6.1602e-03, -5.6699e-03,  5.5851e-02,\n",
       "                      -1.2619e-02, -8.2305e-03, -5.1414e-03, -1.8152e-02, -1.1668e-02,\n",
       "                       5.5908e-03,  6.2745e-03, -2.7295e-03, -7.0958e-03,  1.0298e-02,\n",
       "                      -1.0980e-02, -1.2256e-02, -1.8391e-03, -2.2739e-02, -5.2487e-03,\n",
       "                      -1.3350e-02, -9.9333e-03, -8.9735e-03, -2.7285e-02, -1.7723e-03,\n",
       "                       3.3882e-03, -3.6717e-03,  1.3732e-04, -8.5045e-03, -2.2618e-02,\n",
       "                      -9.8050e-03, -4.9385e-03,  9.5853e-03, -5.9166e-03, -3.0709e-03,\n",
       "                       4.4218e-03, -2.2871e-02, -5.1856e-03, -3.1218e-03, -1.7055e-02,\n",
       "                      -1.2643e-02, -1.1024e-02, -2.1865e-03, -1.3470e-02,  6.8106e-03,\n",
       "                      -3.3200e-03, -1.2531e-02, -1.4320e-03, -4.1944e-03, -2.1204e-02,\n",
       "                      -4.0423e-03, -3.3552e-03,  5.5047e-03, -6.3952e-03, -3.7963e-03,\n",
       "                      -1.0718e-02, -2.5994e-02, -1.4993e-03, -2.3009e-02, -1.6966e-03,\n",
       "                       5.7343e-05, -3.0516e-03,  1.1763e-03,  7.3988e-04, -1.4864e-02,\n",
       "                      -2.7740e-02, -3.4599e-02, -1.7356e-02, -1.9432e-02,  1.1230e-03,\n",
       "                       1.5239e-03, -6.2971e-03, -7.4612e-03, -1.9010e-02, -2.5049e-02,\n",
       "                       1.3845e-03,  4.7226e-04,  5.6909e-03, -1.2545e-02, -1.0237e-02,\n",
       "                      -2.6780e-03, -1.0627e-02,  1.0447e-02, -3.8640e-03, -4.1511e-03,\n",
       "                      -4.1326e-03, -2.5316e-04,  2.0847e-04, -1.9656e-02, -6.7790e-03,\n",
       "                      -9.8982e-03, -3.2192e-03, -4.7758e-03, -2.6299e-03, -3.4529e-03,\n",
       "                       7.9762e-04,  7.2235e-03, -1.5982e-02, -1.6374e-02, -1.3707e-02,\n",
       "                      -2.1775e-02, -7.5062e-03,  3.1239e-03, -2.3706e-02, -2.2302e-02,\n",
       "                       5.7783e-03, -1.6284e-02, -1.4688e-02,  1.6473e-02, -8.1117e-03,\n",
       "                      -1.5005e-02,  5.7922e-03, -2.7478e-03, -6.5039e-03, -7.5022e-03,\n",
       "                      -1.6027e-02, -1.7957e-02, -3.8093e-04,  3.2825e-03, -1.7012e-02,\n",
       "                       4.9360e-03, -1.4017e-02, -2.9409e-02, -4.9986e-02, -2.2345e-02,\n",
       "                      -2.1168e-02, -1.8076e-02, -1.0641e-02, -5.5723e-03,  1.3927e-02,\n",
       "                       1.2202e-05, -1.3671e-03, -9.2450e-03, -1.7312e-02, -5.1168e-03,\n",
       "                      -1.8517e-02,  1.1226e-02, -6.4296e-03, -4.1749e-03,  3.3937e-03,\n",
       "                      -2.7605e-03,  4.5525e-03,  2.4328e-04, -1.2850e-02,  1.6210e-02,\n",
       "                      -5.1816e-03,  2.3081e-03, -2.0367e-02, -2.3743e-02,  5.4793e-03,\n",
       "                      -5.8889e-03,  3.0398e-03,  2.8418e-03, -1.9871e-02,  4.0289e-05,\n",
       "                      -1.2691e-02, -3.8810e-04, -1.1261e-03, -3.3291e-03,  5.3450e-04,\n",
       "                      -2.7983e-03, -7.5524e-03, -5.5107e-03, -8.8745e-03, -1.2728e-02,\n",
       "                      -1.3408e-02, -2.3636e-02, -8.6978e-03,  2.3750e-02, -1.6531e-02,\n",
       "                      -2.7704e-02, -1.1383e-02, -6.2210e-03, -6.5224e-03, -1.7178e-02,\n",
       "                      -8.9187e-03, -1.0763e-02, -1.5293e-03, -2.3221e-03, -3.5190e-03,\n",
       "                      -8.4504e-03, -7.5200e-03,  9.6494e-04, -3.0296e-02,  9.0732e-03,\n",
       "                      -8.7107e-03, -5.8825e-03,  4.6336e-03, -4.0619e-02, -2.1174e-02,\n",
       "                      -7.7888e-03, -1.9460e-02, -2.1110e-02, -2.2477e-03,  4.6348e-04,\n",
       "                      -2.7301e-02, -9.9246e-03, -2.0416e-03,  3.2867e-03, -1.6464e-04,\n",
       "                      -8.6688e-03, -1.2938e-03, -4.0736e-02, -4.3519e-03,  2.7174e-03,\n",
       "                      -1.3491e-02, -5.8528e-03, -4.0739e-04, -2.6197e-02, -1.3817e-02,\n",
       "                      -8.3422e-03,  2.0216e-03,  3.8257e-03, -9.0758e-03, -2.5891e-02,\n",
       "                      -1.2930e-03, -1.3565e-02, -1.1239e-02, -5.4877e-03,  4.7678e-03,\n",
       "                      -3.1554e-03, -1.6331e-02, -3.7344e-03, -7.7759e-03, -2.0410e-02,\n",
       "                      -2.1183e-03, -8.4008e-03])),\n",
       "             ('layer4.1.bn2.running_var',\n",
       "              tensor([9.7897e-04, 2.0076e-04, 7.2571e-04, 3.9302e-04, 1.7432e-04, 3.7908e-04,\n",
       "                      2.3169e-04, 5.1063e-04, 3.3616e-04, 5.6561e-04, 3.1114e-04, 2.9258e-04,\n",
       "                      2.4536e-04, 2.4747e-04, 3.8368e-04, 1.2821e-04, 3.2715e-04, 3.3907e-04,\n",
       "                      6.6279e-04, 9.2478e-04, 1.0390e-04, 8.4709e-05, 2.7602e-04, 2.5444e-04,\n",
       "                      5.1030e-04, 3.1228e-04, 9.0635e-05, 2.3270e-04, 1.3735e-04, 1.7097e-04,\n",
       "                      5.5458e-04, 2.6822e-04, 2.8901e-04, 1.2620e-04, 2.4731e-04, 1.9236e-04,\n",
       "                      1.5807e-04, 3.3776e-04, 7.5508e-04, 3.6527e-04, 7.4963e-04, 3.4577e-04,\n",
       "                      2.1605e-04, 7.1340e-05, 4.7137e-04, 1.3282e-03, 4.9460e-04, 5.6243e-04,\n",
       "                      1.4023e-04, 5.4348e-04, 5.0959e-05, 3.1496e-04, 4.2070e-04, 7.4499e-05,\n",
       "                      1.9696e-04, 3.5183e-05, 1.4044e-04, 2.2922e-04, 7.2416e-04, 3.0638e-04,\n",
       "                      3.5282e-04, 6.0953e-04, 3.2998e-04, 1.3339e-04, 1.9861e-04, 8.4379e-05,\n",
       "                      3.7262e-04, 6.3184e-04, 6.1984e-04, 2.1878e-04, 2.3772e-04, 2.4176e-04,\n",
       "                      7.7155e-04, 1.5588e-04, 3.3586e-04, 4.5952e-04, 3.1001e-04, 2.6363e-04,\n",
       "                      2.7581e-04, 4.5343e-04, 4.7516e-04, 3.5923e-04, 8.1612e-04, 6.1454e-04,\n",
       "                      2.7674e-04, 3.3438e-04, 8.7230e-04, 2.6796e-04, 2.1411e-04, 3.1752e-04,\n",
       "                      4.3017e-04, 3.3759e-04, 9.1954e-04, 6.8252e-04, 1.9912e-04, 1.9610e-04,\n",
       "                      1.4513e-04, 1.8975e-04, 6.5717e-04, 5.0534e-04, 7.6265e-04, 2.6945e-04,\n",
       "                      1.7113e-04, 2.1636e-04, 1.6144e-04, 2.6743e-04, 8.6061e-04, 4.0240e-04,\n",
       "                      2.4478e-04, 5.4534e-04, 3.2736e-04, 4.8575e-04, 4.6479e-04, 1.6374e-04,\n",
       "                      3.4956e-04, 6.5956e-04, 3.1834e-04, 1.4678e-04, 5.5835e-04, 1.9149e-04,\n",
       "                      1.2593e-04, 3.9045e-04, 3.3990e-04, 3.5863e-04, 5.9793e-04, 4.7570e-04,\n",
       "                      1.3387e-04, 6.7516e-04, 3.6395e-04, 5.3805e-04, 4.1549e-04, 2.2366e-04,\n",
       "                      6.6546e-04, 1.6005e-04, 4.2802e-04, 2.4204e-04, 4.2507e-04, 6.5765e-04,\n",
       "                      1.0728e-04, 2.0055e-04, 3.8267e-04, 5.4030e-04, 1.0375e-04, 5.7166e-04,\n",
       "                      3.8344e-04, 2.3055e-04, 2.8884e-04, 2.6502e-04, 8.1554e-05, 6.2550e-04,\n",
       "                      1.3167e-04, 1.8708e-04, 2.3295e-04, 3.9867e-04, 3.5832e-04, 3.0003e-04,\n",
       "                      5.6309e-04, 4.6040e-04, 1.7294e-04, 4.6638e-04, 1.1791e-04, 3.1140e-04,\n",
       "                      4.9652e-04, 2.5465e-04, 4.0006e-05, 3.9972e-04, 3.3578e-04, 1.9471e-04,\n",
       "                      4.4711e-04, 3.1323e-04, 4.7513e-04, 2.1772e-04, 4.8162e-04, 4.7752e-04,\n",
       "                      7.1527e-04, 3.4025e-04, 2.7703e-04, 7.4387e-05, 1.6714e-04, 1.4863e-03,\n",
       "                      3.4636e-04, 3.0081e-04, 2.1687e-04, 1.9009e-04, 1.9199e-04, 1.5661e-04,\n",
       "                      2.5776e-04, 6.4800e-04, 2.7910e-04, 1.8022e-04, 3.6629e-04, 1.1507e-03,\n",
       "                      6.4311e-05, 1.3317e-04, 1.2817e-04, 3.2896e-04, 1.5388e-04, 5.2779e-04,\n",
       "                      4.0582e-04, 3.1398e-04, 1.3624e-04, 1.6025e-04, 5.6058e-05, 5.4976e-04,\n",
       "                      2.2072e-04, 3.3696e-04, 3.0096e-04, 3.1244e-04, 5.1805e-04, 6.9848e-04,\n",
       "                      7.7892e-05, 8.2431e-04, 5.1685e-04, 1.1194e-04, 4.3907e-04, 1.3023e-04,\n",
       "                      1.1691e-04, 3.0327e-04, 1.4006e-04, 1.8865e-04, 8.7919e-05, 2.7038e-04,\n",
       "                      2.3852e-04, 7.3852e-04, 5.7347e-04, 2.6751e-04, 3.8692e-04, 7.4535e-05,\n",
       "                      7.1929e-05, 1.9304e-04, 1.7017e-04, 7.5344e-04, 8.4394e-04, 7.9156e-04,\n",
       "                      1.1241e-04, 1.5586e-04, 3.8067e-04, 1.6714e-05, 2.7649e-04, 7.6788e-04,\n",
       "                      2.3839e-04, 2.3329e-04, 1.5751e-04, 2.3184e-04, 5.0535e-04, 5.2282e-04,\n",
       "                      1.5539e-04, 4.3909e-04, 3.6007e-04, 2.6258e-04, 3.4071e-04, 4.8340e-04,\n",
       "                      6.1293e-04, 2.1206e-04, 1.9446e-04, 1.5797e-04, 3.6971e-04, 1.0551e-03,\n",
       "                      8.9809e-05, 1.3163e-03, 3.4906e-04, 3.4829e-04, 2.5945e-04, 3.5615e-04,\n",
       "                      1.0341e-03, 5.1133e-04, 8.7754e-05, 2.2151e-04, 3.8360e-04, 5.1442e-04,\n",
       "                      1.2746e-04, 3.7225e-04, 2.8004e-04, 3.6671e-04, 9.5926e-04, 2.8194e-04,\n",
       "                      2.8494e-04, 6.1785e-05, 1.7009e-04, 8.1552e-05, 2.1024e-04, 2.3017e-04,\n",
       "                      2.8226e-04, 1.6249e-03, 3.9413e-04, 2.9674e-04, 3.1982e-04, 1.0060e-04,\n",
       "                      1.6954e-04, 1.8016e-03, 2.6462e-04, 1.0056e-04, 4.0742e-04, 4.0020e-04,\n",
       "                      1.7790e-04, 8.6632e-04, 3.2588e-04, 2.7411e-04, 5.0545e-04, 1.0275e-04,\n",
       "                      3.5170e-04, 4.7297e-04, 1.6724e-04, 2.4491e-04, 3.7181e-04, 8.3774e-04,\n",
       "                      1.9473e-04, 4.6996e-05, 6.0026e-04, 1.9071e-04, 9.5481e-05, 1.9002e-04,\n",
       "                      2.9084e-04, 3.4759e-04, 2.0035e-04, 2.9242e-04, 4.1194e-04, 2.1967e-04,\n",
       "                      1.3910e-04, 1.0291e-04, 3.4781e-04, 6.6564e-04, 1.9039e-04, 2.6829e-04,\n",
       "                      7.3697e-04, 1.6115e-04, 3.0349e-04, 2.2770e-04, 4.1109e-04, 2.7156e-04,\n",
       "                      3.2469e-04, 2.5681e-04, 2.7210e-04, 8.5067e-04, 5.5913e-04, 1.0726e-04,\n",
       "                      2.5912e-04, 1.9416e-04, 7.7289e-04, 1.9537e-04, 4.8900e-04, 4.8518e-04,\n",
       "                      8.7587e-05, 5.5777e-04, 7.9395e-05, 8.9832e-04, 1.6555e-04, 2.9756e-04,\n",
       "                      1.3447e-04, 3.7482e-04, 9.5427e-04, 8.0126e-04, 4.7360e-04, 4.2442e-04,\n",
       "                      2.3972e-04, 2.5486e-04, 2.5614e-04, 2.2746e-04, 3.4904e-04, 8.7838e-04,\n",
       "                      1.5436e-04, 1.0596e-03, 6.6288e-05, 3.6938e-04, 2.1641e-04, 1.9374e-04,\n",
       "                      2.0344e-04, 5.4046e-04, 5.2240e-04, 1.2320e-04, 9.8754e-05, 6.8407e-04,\n",
       "                      7.5018e-05, 1.2889e-03, 1.4372e-04, 1.6301e-04, 6.4042e-05, 1.8076e-04,\n",
       "                      1.7180e-04, 5.5845e-04, 5.8752e-05, 2.8862e-04, 2.9399e-04, 5.0095e-04,\n",
       "                      2.7018e-04, 4.6985e-04, 2.7942e-04, 4.9072e-04, 5.8425e-04, 3.1724e-04,\n",
       "                      4.0587e-04, 3.4434e-04, 3.4923e-04, 8.5873e-04, 3.0956e-04, 3.6538e-04,\n",
       "                      2.9262e-04, 2.0589e-04, 4.5798e-04, 7.6884e-04, 3.2962e-04, 3.0287e-04,\n",
       "                      2.8532e-04, 8.0259e-04, 7.3905e-04, 4.3718e-04, 4.0718e-04, 6.3238e-04,\n",
       "                      5.2162e-04, 1.6081e-03, 5.1507e-04, 5.9805e-04, 1.3246e-04, 3.1871e-04,\n",
       "                      4.5408e-04, 3.8321e-08, 1.0462e-04, 2.2057e-04, 2.1065e-04, 1.1044e-04,\n",
       "                      2.5711e-04, 9.1917e-04, 2.1379e-04, 2.5015e-04, 2.0532e-04, 3.6969e-04,\n",
       "                      3.9213e-04, 3.3243e-04, 5.2610e-04, 4.6121e-04, 3.4212e-04, 3.3479e-04,\n",
       "                      1.0878e-03, 5.5025e-04, 2.3845e-04, 2.4115e-04, 1.0538e-04, 4.4596e-05,\n",
       "                      4.8364e-04, 3.2316e-04, 3.0681e-04, 9.3435e-05, 2.2634e-04, 3.9322e-05,\n",
       "                      9.0272e-05, 4.6938e-04, 1.4619e-04, 2.3448e-04, 5.9816e-04, 3.1881e-04,\n",
       "                      1.9233e-04, 5.6118e-04, 7.7045e-05, 3.3660e-04, 1.9494e-04, 4.3442e-04,\n",
       "                      2.0081e-04, 2.9984e-04, 1.9666e-04, 6.5996e-04, 5.7969e-04, 3.5969e-04,\n",
       "                      2.4702e-04, 9.5771e-04, 4.5521e-04, 1.2298e-04, 1.5919e-04, 1.2258e-04,\n",
       "                      7.9226e-04, 6.6219e-04, 3.5782e-04, 3.9603e-04, 5.7470e-04, 3.8745e-04,\n",
       "                      2.4318e-04, 8.0589e-05, 9.7113e-04, 6.2276e-04, 7.6264e-04, 6.1656e-04,\n",
       "                      7.2193e-04, 1.8976e-04, 1.9897e-04, 3.0434e-04, 3.3520e-04, 1.9598e-04,\n",
       "                      2.1408e-04, 8.2606e-04, 2.7507e-04, 1.4262e-04, 4.1578e-04, 1.9097e-04,\n",
       "                      3.8721e-07, 3.8353e-04, 3.3921e-04, 1.7553e-04, 1.3936e-04, 9.1911e-05,\n",
       "                      1.1259e-04, 1.4285e-03, 5.2009e-04, 4.5054e-04, 7.8418e-04, 3.3425e-05,\n",
       "                      5.8491e-04, 2.6020e-05, 2.8267e-04, 5.9283e-05, 3.6527e-04, 2.9945e-04,\n",
       "                      2.0588e-04, 8.1942e-04])),\n",
       "             ('layer4.1.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.1.conv3.weight',\n",
       "              tensor([[[[ 1.8377e-04]],\n",
       "              \n",
       "                       [[ 8.1858e-05]],\n",
       "              \n",
       "                       [[-1.5549e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5275e-04]],\n",
       "              \n",
       "                       [[-6.8696e-06]],\n",
       "              \n",
       "                       [[ 1.6123e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.0089e-05]],\n",
       "              \n",
       "                       [[-1.0736e-04]],\n",
       "              \n",
       "                       [[ 5.2041e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.2744e-04]],\n",
       "              \n",
       "                       [[-1.9275e-04]],\n",
       "              \n",
       "                       [[ 5.3621e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4892e-04]],\n",
       "              \n",
       "                       [[-9.4766e-04]],\n",
       "              \n",
       "                       [[-1.0586e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1674e-03]],\n",
       "              \n",
       "                       [[-1.3783e-03]],\n",
       "              \n",
       "                       [[-8.7817e-04]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 8.9796e-06]],\n",
       "              \n",
       "                       [[-2.4902e-06]],\n",
       "              \n",
       "                       [[-4.2533e-06]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.3218e-06]],\n",
       "              \n",
       "                       [[ 1.4066e-06]],\n",
       "              \n",
       "                       [[ 2.9782e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2210e-04]],\n",
       "              \n",
       "                       [[ 8.5893e-05]],\n",
       "              \n",
       "                       [[-3.9435e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9701e-03]],\n",
       "              \n",
       "                       [[-1.4221e-03]],\n",
       "              \n",
       "                       [[ 3.1709e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.6643e-03]],\n",
       "              \n",
       "                       [[ 6.9140e-03]],\n",
       "              \n",
       "                       [[-6.6537e-06]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9750e-03]],\n",
       "              \n",
       "                       [[-9.0453e-04]],\n",
       "              \n",
       "                       [[-1.5297e-03]]]])),\n",
       "             ('layer4.1.bn3.weight',\n",
       "              tensor([-1.6053e-03, -6.1724e-03,  2.2745e-02,  ..., -5.4454e-05,\n",
       "                      -1.8830e-02,  8.5764e-02])),\n",
       "             ('layer4.1.bn3.bias',\n",
       "              tensor([0.0021, 0.0238, 0.0175,  ..., 0.0103, 0.0161, 0.0142])),\n",
       "             ('layer4.1.bn3.running_mean',\n",
       "              tensor([-7.5228e-05, -8.5142e-05,  4.9925e-04,  ...,  1.6785e-06,\n",
       "                      -5.6625e-04,  1.0450e-04])),\n",
       "             ('layer4.1.bn3.running_var',\n",
       "              tensor([2.4534e-08, 4.2695e-07, 4.4642e-06,  ..., 3.2111e-11, 3.1288e-06,\n",
       "                      2.9299e-05])),\n",
       "             ('layer4.1.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.2.conv1.weight',\n",
       "              tensor([[[[-1.1728e-03]],\n",
       "              \n",
       "                       [[ 2.1578e-04]],\n",
       "              \n",
       "                       [[ 3.2495e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.1288e-03]],\n",
       "              \n",
       "                       [[-1.7975e-03]],\n",
       "              \n",
       "                       [[-2.7425e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.6376e-04]],\n",
       "              \n",
       "                       [[ 1.0069e-03]],\n",
       "              \n",
       "                       [[-1.7558e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2216e-03]],\n",
       "              \n",
       "                       [[ 1.2825e-06]],\n",
       "              \n",
       "                       [[-2.3929e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9175e-04]],\n",
       "              \n",
       "                       [[-1.5465e-03]],\n",
       "              \n",
       "                       [[-6.2303e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.0290e-05]],\n",
       "              \n",
       "                       [[ 2.3215e-04]],\n",
       "              \n",
       "                       [[-1.0225e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.9797e-05]],\n",
       "              \n",
       "                       [[ 7.3785e-03]],\n",
       "              \n",
       "                       [[ 1.0836e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1221e-03]],\n",
       "              \n",
       "                       [[-3.8854e-04]],\n",
       "              \n",
       "                       [[-2.0828e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8710e-04]],\n",
       "              \n",
       "                       [[-7.8036e-04]],\n",
       "              \n",
       "                       [[-5.6865e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.5787e-05]],\n",
       "              \n",
       "                       [[ 2.0528e-03]],\n",
       "              \n",
       "                       [[ 2.5393e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8799e-04]],\n",
       "              \n",
       "                       [[ 4.5040e-04]],\n",
       "              \n",
       "                       [[-1.7041e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2713e-03]],\n",
       "              \n",
       "                       [[-3.2594e-03]],\n",
       "              \n",
       "                       [[ 3.9919e-03]]]])),\n",
       "             ('layer4.2.bn1.weight',\n",
       "              tensor([0.0260, 0.0354, 0.0398, 0.0401, 0.0386, 0.0290, 0.0213, 0.0260, 0.0297,\n",
       "                      0.0428, 0.0533, 0.0251, 0.0662, 0.0258, 0.0418, 0.0439, 0.0409, 0.0291,\n",
       "                      0.0220, 0.0530, 0.0316, 0.0230, 0.0282, 0.0863, 0.0504, 0.0447, 0.0328,\n",
       "                      0.0662, 0.0320, 0.0577, 0.0204, 0.0117, 0.0532, 0.0472, 0.0437, 0.0377,\n",
       "                      0.0399, 0.0579, 0.0730, 0.0422, 0.0366, 0.0386, 0.0399, 0.0457, 0.0324,\n",
       "                      0.0418, 0.0430, 0.0497, 0.0212, 0.0412, 0.0254, 0.0192, 0.0481, 0.0388,\n",
       "                      0.0391, 0.0291, 0.0398, 0.0459, 0.0548, 0.0303, 0.0148, 0.0425, 0.0369,\n",
       "                      0.0464, 0.0766, 0.0393, 0.0516, 0.0894, 0.0323, 0.0306, 0.0209, 0.0336,\n",
       "                      0.0444, 0.0487, 0.0332, 0.0323, 0.0622, 0.0545, 0.0486, 0.0280, 0.0326,\n",
       "                      0.0290, 0.0419, 0.0289, 0.0536, 0.0573, 0.0324, 0.0471, 0.0392, 0.0370,\n",
       "                      0.0510, 0.0483, 0.0599, 0.0293, 0.0292, 0.0405, 0.0514, 0.0445, 0.0601,\n",
       "                      0.0354, 0.0483, 0.0494, 0.0368, 0.0509, 0.0430, 0.0356, 0.0272, 0.0603,\n",
       "                      0.0591, 0.0340, 0.0460, 0.0151, 0.0481, 0.0530, 0.0447, 0.0269, 0.0435,\n",
       "                      0.0657, 0.0230, 0.0369, 0.0569, 0.0339, 0.0592, 0.0415, 0.0518, 0.0401,\n",
       "                      0.0273, 0.0343, 0.0411, 0.0648, 0.0502, 0.0425, 0.0219, 0.0179, 0.0447,\n",
       "                      0.0300, 0.0233, 0.0258, 0.0165, 0.0366, 0.0319, 0.0276, 0.0386, 0.0345,\n",
       "                      0.0231, 0.0457, 0.0194, 0.0546, 0.0295, 0.0221, 0.0227, 0.0549, 0.0209,\n",
       "                      0.0259, 0.0318, 0.0307, 0.0444, 0.0280, 0.0411, 0.0494, 0.0636, 0.0463,\n",
       "                      0.0322, 0.0330, 0.0300, 0.0409, 0.0416, 0.0416, 0.0723, 0.0228, 0.0371,\n",
       "                      0.0512, 0.0377, 0.0371, 0.0568, 0.0255, 0.0312, 0.0213, 0.0201, 0.0296,\n",
       "                      0.0532, 0.0316, 0.0315, 0.0247, 0.0219, 0.0535, 0.0365, 0.0128, 0.0278,\n",
       "                      0.0254, 0.0401, 0.0449, 0.0275, 0.0356, 0.0281, 0.0331, 0.0379, 0.0375,\n",
       "                      0.0477, 0.0254, 0.0295, 0.0313, 0.0287, 0.0252, 0.0257, 0.0343, 0.0384,\n",
       "                      0.0167, 0.0849, 0.0267, 0.0404, 0.0289, 0.0267, 0.0353, 0.0543, 0.0721,\n",
       "                      0.0376, 0.0294, 0.0444, 0.0546, 0.0295, 0.0237, 0.0380, 0.0456, 0.0409,\n",
       "                      0.0782, 0.0511, 0.0485, 0.0162, 0.0365, 0.0206, 0.0369, 0.0606, 0.0526,\n",
       "                      0.0237, 0.0504, 0.0223, 0.0374, 0.0374, 0.0292, 0.0496, 0.0326, 0.0467,\n",
       "                      0.0489, 0.0306, 0.0310, 0.0334, 0.0257, 0.0540, 0.0713, 0.0372, 0.0627,\n",
       "                      0.0257, 0.0246, 0.0479, 0.0314, 0.0452, 0.0742, 0.0558, 0.0559, 0.0347,\n",
       "                      0.0446, 0.0611, 0.0675, 0.0252, 0.0743, 0.0553, 0.0501, 0.0417, 0.0561,\n",
       "                      0.0408, 0.0267, 0.0313, 0.0397, 0.0519, 0.0603, 0.0300, 0.0354, 0.0413,\n",
       "                      0.0455, 0.0504, 0.0675, 0.0530, 0.0550, 0.0503, 0.0739, 0.0930, 0.0241,\n",
       "                      0.0284, 0.0324, 0.0558, 0.0494, 0.0388, 0.0444, 0.0369, 0.0348, 0.0292,\n",
       "                      0.0339, 0.0440, 0.0324, 0.0589, 0.0545, 0.0364, 0.0516, 0.0352, 0.0361,\n",
       "                      0.0431, 0.0514, 0.0265, 0.0458, 0.0407, 0.0457, 0.0382, 0.0564, 0.0196,\n",
       "                      0.0426, 0.0627, 0.0244, 0.0245, 0.0466, 0.0247, 0.0451, 0.0231, 0.0397,\n",
       "                      0.0412, 0.0306, 0.0398, 0.0451, 0.0284, 0.0234, 0.0289, 0.0337, 0.0310,\n",
       "                      0.0424, 0.0463, 0.0501, 0.0346, 0.0287, 0.0569, 0.0376, 0.0316, 0.0529,\n",
       "                      0.0275, 0.0459, 0.0603, 0.0275, 0.0371, 0.0231, 0.0309, 0.0497, 0.0635,\n",
       "                      0.0291, 0.0635, 0.0304, 0.0215, 0.0184, 0.0460, 0.0399, 0.0541, 0.0391,\n",
       "                      0.0397, 0.0359, 0.0326, 0.0865, 0.0287, 0.0373, 0.0538, 0.0295, 0.0313,\n",
       "                      0.0446, 0.0479, 0.0257, 0.0917, 0.0761, 0.0308, 0.0603, 0.0372, 0.0207,\n",
       "                      0.0198, 0.0581, 0.0607, 0.0328, 0.0250, 0.0223, 0.0398, 0.0535, 0.0268,\n",
       "                      0.0467, 0.0459, 0.0283, 0.0213, 0.0234, 0.0495, 0.0437, 0.0197, 0.0611,\n",
       "                      0.0303, 0.0297, 0.0145, 0.0790, 0.0435, 0.0325, 0.0393, 0.0421, 0.0500,\n",
       "                      0.0569, 0.0757, 0.0295, 0.0691, 0.0446, 0.0630, 0.0286, 0.0327, 0.0200,\n",
       "                      0.0260, 0.0419, 0.0303, 0.0267, 0.0761, 0.0546, 0.0308, 0.0217, 0.0681,\n",
       "                      0.0850, 0.0388, 0.0281, 0.0364, 0.0243, 0.0374, 0.0151, 0.0402, 0.0644,\n",
       "                      0.0641, 0.0495, 0.0800, 0.0327, 0.0267, 0.0206, 0.0327, 0.0564, 0.0585,\n",
       "                      0.0304, 0.0261, 0.0346, 0.0446, 0.0703, 0.0365, 0.0529, 0.0225, 0.0395,\n",
       "                      0.0323, 0.0469, 0.0430, 0.0530, 0.0548, 0.0257, 0.0203, 0.0353, 0.0314,\n",
       "                      0.0505, 0.0620, 0.0366, 0.0396, 0.0628, 0.0544, 0.0611, 0.0416, 0.0496,\n",
       "                      0.0462, 0.0253, 0.0402, 0.0225, 0.0492, 0.0548, 0.0264, 0.0359, 0.0656,\n",
       "                      0.0292, 0.0158, 0.0473, 0.0136, 0.0414, 0.0577, 0.0699, 0.0712, 0.0461,\n",
       "                      0.0591, 0.0377, 0.0297, 0.0433, 0.0665, 0.0348, 0.0321, 0.0417, 0.0632,\n",
       "                      0.0958, 0.0218, 0.0535, 0.0628, 0.0304, 0.0637, 0.0279, 0.0408, 0.0513,\n",
       "                      0.0548, 0.0336, 0.0523, 0.0206, 0.0504, 0.0217, 0.0159, 0.0373])),\n",
       "             ('layer4.2.bn1.bias',\n",
       "              tensor([-0.0028, -0.0070, -0.0173, -0.0129, -0.0029,  0.0045,  0.0041, -0.0070,\n",
       "                      -0.0010,  0.0007,  0.0134, -0.0131,  0.0311,  0.0100, -0.0100,  0.0080,\n",
       "                      -0.0148, -0.0145, -0.0026, -0.0066, -0.0094,  0.0109, -0.0173, -0.0458,\n",
       "                      -0.0037, -0.0156,  0.0113, -0.0284,  0.0112, -0.0294, -0.0057,  0.0019,\n",
       "                      -0.0204, -0.0109, -0.0014, -0.0207, -0.0133, -0.0132, -0.0095, -0.0144,\n",
       "                      -0.0221, -0.0185,  0.0034, -0.0012, -0.0193,  0.0065, -0.0051, -0.0330,\n",
       "                       0.0123, -0.0209, -0.0044,  0.0065, -0.0105,  0.0140, -0.0069,  0.0048,\n",
       "                      -0.0158, -0.0295, -0.0328,  0.0019,  0.0085, -0.0121,  0.0015, -0.0008,\n",
       "                       0.0170, -0.0266, -0.0166, -0.0390,  0.0120, -0.0192,  0.0046, -0.0124,\n",
       "                      -0.0149, -0.0174, -0.0047, -0.0030, -0.0002, -0.0218, -0.0012, -0.0167,\n",
       "                      -0.0073, -0.0075,  0.0034, -0.0016, -0.0242, -0.0191, -0.0012, -0.0229,\n",
       "                      -0.0268, -0.0161,  0.0038, -0.0162, -0.0086, -0.0062,  0.0092, -0.0017,\n",
       "                      -0.0088, -0.0281, -0.0283, -0.0006, -0.0032, -0.0182,  0.0106, -0.0194,\n",
       "                      -0.0225,  0.0019, -0.0039, -0.0207,  0.0007, -0.0060, -0.0158, -0.0018,\n",
       "                      -0.0007, -0.0292, -0.0177,  0.0073,  0.0102, -0.0015,  0.0068,  0.0039,\n",
       "                      -0.0264, -0.0053, -0.0240,  0.0007, -0.0062, -0.0226, -0.0074, -0.0144,\n",
       "                      -0.0005, -0.0165, -0.0080,  0.0073, -0.0029,  0.0067,  0.0214, -0.0150,\n",
       "                      -0.0002,  0.0138,  0.0039, -0.0140, -0.0024,  0.0043, -0.0116, -0.0017,\n",
       "                      -0.0011, -0.0041,  0.0056, -0.0259,  0.0068, -0.0078, -0.0041, -0.0255,\n",
       "                      -0.0046,  0.0029,  0.0042,  0.0120,  0.0028,  0.0174, -0.0153, -0.0216,\n",
       "                      -0.0427, -0.0209, -0.0137, -0.0180, -0.0055,  0.0140,  0.0115, -0.0227,\n",
       "                      -0.0257,  0.0103, -0.0123, -0.0083, -0.0020,  0.0118,  0.0044, -0.0194,\n",
       "                      -0.0077,  0.0090,  0.0063,  0.0018, -0.0314,  0.0074,  0.0024, -0.0172,\n",
       "                      -0.0046,  0.0111, -0.0108,  0.0165,  0.0078, -0.0142, -0.0109, -0.0218,\n",
       "                       0.0018, -0.0007, -0.0062, -0.0219,  0.0030, -0.0148, -0.0110, -0.0057,\n",
       "                      -0.0014,  0.0059, -0.0089,  0.0062,  0.0036, -0.0005, -0.0044,  0.0067,\n",
       "                      -0.0323,  0.0005, -0.0022, -0.0109,  0.0059,  0.0049, -0.0223, -0.0368,\n",
       "                      -0.0192,  0.0215, -0.0173, -0.0216,  0.0068,  0.0055, -0.0179, -0.0010,\n",
       "                      -0.0285, -0.0184, -0.0231, -0.0043,  0.0007, -0.0111,  0.0002, -0.0098,\n",
       "                      -0.0174, -0.0104, -0.0027, -0.0332,  0.0056, -0.0078, -0.0205, -0.0163,\n",
       "                      -0.0146, -0.0097, -0.0144,  0.0095, -0.0145, -0.0112, -0.0124, -0.0006,\n",
       "                      -0.0155, -0.0145, -0.0021, -0.0314,  0.0061,  0.0065,  0.0151, -0.0032,\n",
       "                      -0.0308,  0.0003, -0.0255, -0.0140, -0.0011, -0.0091, -0.0134, -0.0260,\n",
       "                      -0.0079, -0.0365, -0.0227, -0.0302, -0.0152, -0.0303, -0.0235, -0.0116,\n",
       "                      -0.0092, -0.0100, -0.0175, -0.0378,  0.0028, -0.0072, -0.0057, -0.0002,\n",
       "                      -0.0163, -0.0198, -0.0192, -0.0054, -0.0112, -0.0324, -0.0333, -0.0077,\n",
       "                      -0.0009, -0.0034,  0.0046, -0.0266, -0.0080, -0.0044,  0.0045, -0.0025,\n",
       "                      -0.0036, -0.0128, -0.0241,  0.0061, -0.0202, -0.0131, -0.0153, -0.0148,\n",
       "                       0.0030,  0.0013, -0.0069, -0.0225, -0.0023, -0.0098, -0.0218, -0.0189,\n",
       "                      -0.0030, -0.0214, -0.0009, -0.0223, -0.0333,  0.0036, -0.0069, -0.0160,\n",
       "                       0.0025, -0.0115, -0.0090,  0.0149, -0.0138,  0.0015, -0.0189, -0.0244,\n",
       "                       0.0037, -0.0073, -0.0033, -0.0136, -0.0082, -0.0139, -0.0047, -0.0127,\n",
       "                      -0.0055, -0.0049,  0.0084, -0.0081, -0.0061,  0.0172,  0.0020, -0.0196,\n",
       "                      -0.0374, -0.0056, -0.0005,  0.0032,  0.0018, -0.0251, -0.0212, -0.0108,\n",
       "                      -0.0176,  0.0055,  0.0059,  0.0016, -0.0249, -0.0129, -0.0280, -0.0074,\n",
       "                      -0.0159, -0.0089,  0.0029, -0.0052, -0.0134, -0.0180, -0.0134, -0.0123,\n",
       "                      -0.0122, -0.0237, -0.0035, -0.0008,  0.0090, -0.0374, -0.0023, -0.0228,\n",
       "                       0.0026,  0.0080,  0.0064, -0.0116, -0.0345, -0.0079, -0.0083,  0.0011,\n",
       "                      -0.0078, -0.0120, -0.0081, -0.0218, -0.0156, -0.0067,  0.0066,  0.0043,\n",
       "                      -0.0162, -0.0011, -0.0052, -0.0179, -0.0117,  0.0079,  0.0161, -0.0371,\n",
       "                       0.0130, -0.0164, -0.0139, -0.0034, -0.0028, -0.0041, -0.0094, -0.0088,\n",
       "                      -0.0041, -0.0086, -0.0104,  0.0102, -0.0004,  0.0026, -0.0051, -0.0185,\n",
       "                       0.0109, -0.0107, -0.0311, -0.0054,  0.0061,  0.0039, -0.0458, -0.0266,\n",
       "                      -0.0216,  0.0075,  0.0118, -0.0100, -0.0092,  0.0165, -0.0094, -0.0353,\n",
       "                      -0.0336, -0.0169, -0.0337, -0.0106,  0.0093,  0.0024,  0.0054, -0.0209,\n",
       "                      -0.0135, -0.0141, -0.0039,  0.0079, -0.0218, -0.0336,  0.0130, -0.0129,\n",
       "                      -0.0015, -0.0018, -0.0090, -0.0133, -0.0183, -0.0074, -0.0197,  0.0128,\n",
       "                      -0.0061, -0.0182, -0.0193, -0.0232, -0.0217, -0.0049, -0.0068, -0.0197,\n",
       "                       0.0206, -0.0351, -0.0214, -0.0289,  0.0146, -0.0043, -0.0046,  0.0034,\n",
       "                      -0.0186,  0.0181,  0.0067,  0.0060, -0.0121,  0.0007,  0.0067, -0.0051,\n",
       "                       0.0038, -0.0127, -0.0438, -0.0146, -0.0294, -0.0201, -0.0328,  0.0024,\n",
       "                      -0.0141, -0.0151, -0.0231,  0.0010, -0.0071, -0.0159, -0.0242,  0.0173,\n",
       "                       0.0012, -0.0304, -0.0055, -0.0112, -0.0281, -0.0012, -0.0175, -0.0029,\n",
       "                      -0.0186, -0.0043, -0.0290, -0.0008, -0.0159,  0.0032,  0.0138, -0.0060])),\n",
       "             ('layer4.2.bn1.running_mean',\n",
       "              tensor([-1.5053e-02,  3.7359e-02,  1.2477e-02,  4.3125e-02,  2.4221e-02,\n",
       "                       7.6232e-03, -1.1723e-02, -6.4941e-02, -2.9640e-02, -3.0663e-02,\n",
       "                      -9.0899e-02, -1.2331e-02, -9.5708e-03, -7.1057e-03,  2.0222e-02,\n",
       "                      -1.9170e-02, -7.5314e-03,  3.3304e-03, -7.6725e-03, -2.1322e-02,\n",
       "                      -2.5554e-02, -4.9643e-03,  2.3523e-02, -3.8145e-02,  5.3216e-03,\n",
       "                       2.6157e-03,  1.5325e-02,  3.3616e-02, -3.1829e-02,  4.7718e-02,\n",
       "                       1.7965e-02,  7.6967e-03,  7.7373e-02,  3.4714e-02, -4.6473e-02,\n",
       "                      -3.3319e-02, -1.0364e-02,  1.7635e-02, -6.6470e-02, -4.3070e-02,\n",
       "                       2.2557e-02,  9.7470e-03, -3.2217e-02, -6.0573e-02,  3.7403e-02,\n",
       "                       1.2214e-02, -1.3622e-02,  2.6979e-02, -1.5132e-03,  1.4947e-02,\n",
       "                      -8.0327e-03, -3.4106e-02, -5.3147e-04,  2.2288e-03,  4.9232e-02,\n",
       "                      -3.3374e-02, -1.3594e-02,  6.1313e-02, -1.6255e-02, -2.0511e-02,\n",
       "                      -1.2183e-02,  2.3418e-03, -1.5036e-02,  2.8494e-02, -9.8070e-03,\n",
       "                       6.5413e-02,  3.4147e-02,  5.3088e-02, -7.0200e-02, -1.9264e-03,\n",
       "                       9.1800e-03,  4.3811e-02, -1.2595e-02,  3.0132e-02,  3.2875e-02,\n",
       "                       3.2061e-02, -1.5280e-02,  2.7803e-02,  3.2212e-02,  2.3669e-02,\n",
       "                      -6.2643e-02,  2.1471e-02, -2.9156e-02,  6.8601e-03,  3.9283e-02,\n",
       "                       4.3573e-02, -1.9015e-02,  3.3552e-03,  2.3362e-02,  7.2187e-02,\n",
       "                       4.7868e-02,  3.6367e-02,  2.2067e-02,  3.5963e-02, -4.5456e-02,\n",
       "                      -1.5309e-02, -3.6340e-02,  1.0088e-01,  1.0024e-01,  6.1102e-02,\n",
       "                       9.4113e-02,  9.3758e-03,  1.5947e-02,  4.1684e-02,  1.8921e-02,\n",
       "                       1.0169e-02,  9.6162e-03,  2.5692e-02,  4.0903e-02,  4.3591e-02,\n",
       "                       1.7765e-02,  1.3610e-02,  9.1759e-02,  2.3819e-02,  2.1541e-02,\n",
       "                      -1.0835e-02,  4.2409e-02, -3.1824e-02, -2.3488e-02, -3.1253e-02,\n",
       "                       2.1429e-02,  1.3455e-02,  1.7027e-02, -5.0155e-02, -4.6167e-02,\n",
       "                       6.3491e-02,  2.3210e-02,  6.2939e-02, -6.4921e-03, -2.5155e-02,\n",
       "                      -5.2149e-02, -5.6896e-02, -4.9187e-02, -1.8246e-02, -2.0842e-03,\n",
       "                      -3.1140e-04, -3.0750e-02, -3.5156e-02, -2.3523e-02,  2.7784e-02,\n",
       "                      -1.5424e-02, -5.8278e-02,  2.9994e-02, -6.6188e-05, -9.9119e-03,\n",
       "                       1.6346e-02,  1.0605e-03,  2.7409e-02,  4.2342e-02, -4.8570e-03,\n",
       "                      -3.1068e-02,  2.1993e-02, -1.8462e-02, -1.0194e-04, -2.2085e-02,\n",
       "                      -2.8331e-02, -5.8262e-02, -3.9330e-02,  2.2863e-02, -1.2567e-02,\n",
       "                       1.7911e-02,  3.0248e-02,  2.1707e-02,  1.5669e-02,  4.7785e-02,\n",
       "                      -2.8308e-02,  7.7178e-03,  4.0756e-02,  6.2624e-02, -4.5657e-02,\n",
       "                       3.8729e-02, -2.0064e-03,  4.8496e-03, -2.0503e-02,  6.8461e-03,\n",
       "                       2.4936e-02,  4.3267e-02, -6.6826e-04, -1.5621e-02, -3.6383e-02,\n",
       "                       2.6749e-02, -2.7399e-02, -1.0782e-02,  5.5765e-03,  2.6711e-02,\n",
       "                      -1.4311e-01,  3.9639e-02, -2.0190e-02, -5.6706e-02,  3.0604e-02,\n",
       "                       3.0025e-02,  4.4875e-02, -1.1255e-02, -3.1507e-02, -1.5255e-02,\n",
       "                       1.3137e-02, -3.0238e-02, -2.3296e-03,  8.2469e-02,  2.4158e-02,\n",
       "                      -1.2097e-02, -3.2559e-02,  3.3859e-02, -6.6365e-03, -3.0862e-02,\n",
       "                      -3.5181e-02, -2.8710e-02, -3.0693e-02,  9.8044e-03,  2.2059e-02,\n",
       "                      -5.0953e-02,  2.7107e-02, -4.1267e-02, -5.6108e-02,  1.6364e-02,\n",
       "                       4.3803e-03,  6.1935e-02, -1.7999e-02, -1.8922e-02,  1.1487e-02,\n",
       "                      -6.8652e-02, -4.6505e-03,  9.1082e-03, -2.3426e-02,  3.9431e-02,\n",
       "                      -3.5407e-02,  1.4881e-02, -1.0063e-01, -1.7293e-02,  4.3104e-04,\n",
       "                      -2.9578e-02,  1.8123e-02, -5.6380e-02,  4.0200e-02, -2.0740e-03,\n",
       "                      -6.6148e-03, -2.4864e-02,  3.8950e-02,  2.9910e-02,  2.4000e-02,\n",
       "                       1.2078e-02,  2.8590e-02,  3.1164e-02, -4.1248e-02,  2.7554e-02,\n",
       "                       2.3824e-02,  7.3495e-02, -2.1138e-02, -1.9233e-02, -5.6633e-02,\n",
       "                       2.6106e-03,  1.1262e-01, -1.7993e-02, -2.0235e-02, -1.3660e-02,\n",
       "                      -2.1749e-02, -6.0907e-04, -2.2732e-02, -4.7526e-03,  3.7011e-02,\n",
       "                       1.4310e-02,  4.0463e-02,  3.3732e-02,  7.3572e-02,  2.7553e-02,\n",
       "                       4.5588e-02,  6.4417e-02,  7.0834e-02,  5.6859e-02, -6.1968e-03,\n",
       "                      -3.4619e-02,  4.0159e-02,  2.2571e-02,  2.4710e-02,  7.6283e-02,\n",
       "                      -8.6077e-03, -2.6226e-02, -1.2417e-02,  5.0031e-02, -7.7254e-02,\n",
       "                      -1.0691e-02,  2.7085e-02,  3.4655e-02,  2.6953e-03,  1.9204e-02,\n",
       "                       2.9383e-02,  5.2701e-02, -2.6043e-04,  2.4846e-02,  1.1979e-03,\n",
       "                      -6.7291e-03,  3.7284e-03,  6.4180e-02,  5.5705e-02, -3.9296e-03,\n",
       "                       2.1608e-02,  9.5331e-03,  1.1278e-02,  2.6939e-02, -5.2688e-02,\n",
       "                       2.3543e-02, -4.9758e-02,  4.0833e-02,  1.0098e-02, -3.2073e-02,\n",
       "                      -6.1586e-02, -6.8077e-02, -6.1152e-03,  1.1294e-03,  1.2618e-02,\n",
       "                       1.2636e-02, -1.8516e-02,  1.8926e-02,  4.7129e-02,  9.5585e-03,\n",
       "                      -2.7720e-02,  4.5125e-02,  7.7724e-03,  5.0723e-03,  1.5156e-02,\n",
       "                      -2.2770e-02,  6.2440e-02,  2.8510e-02,  2.7985e-02,  8.0124e-02,\n",
       "                      -5.0621e-02,  7.1790e-02,  2.7889e-02, -2.7512e-03,  2.6110e-02,\n",
       "                       5.8685e-02,  9.0038e-03,  2.9113e-02, -2.7470e-02,  8.2215e-02,\n",
       "                       2.8567e-02,  4.5676e-02, -3.3551e-02, -4.3021e-02, -1.0491e-03,\n",
       "                      -6.6759e-02, -5.8542e-03, -2.5607e-02,  1.1192e-02,  4.6736e-02,\n",
       "                       2.7087e-02,  2.7658e-02,  3.7705e-03, -1.1502e-03, -9.5030e-03,\n",
       "                       3.8152e-02,  2.4978e-05, -6.7235e-02, -4.9299e-02, -1.3755e-03,\n",
       "                       3.3263e-03, -2.2573e-02, -1.0187e-02, -5.3950e-03, -1.3706e-03,\n",
       "                       7.9770e-02, -5.4878e-02,  4.6905e-04, -3.6196e-02,  4.7852e-02,\n",
       "                      -1.4072e-02, -5.0850e-02,  2.2840e-02,  5.5285e-02, -2.8405e-03,\n",
       "                       2.6213e-02, -1.7155e-02,  5.7158e-02,  3.0396e-02,  7.4877e-03,\n",
       "                       4.5308e-03,  2.6699e-02, -1.3359e-02, -1.2090e-02,  5.2701e-02,\n",
       "                       9.9558e-03,  7.0994e-02,  1.7080e-02, -1.8748e-02,  1.9074e-02,\n",
       "                       2.0125e-02,  1.5686e-02,  5.0905e-03, -6.3515e-02,  3.1447e-02,\n",
       "                       6.0318e-03, -3.0374e-03,  8.0220e-02, -2.2853e-02, -5.9415e-03,\n",
       "                       3.7697e-03, -2.3746e-03,  4.1960e-02, -4.6379e-02,  4.2799e-02,\n",
       "                      -1.0012e-01,  4.1855e-03,  3.3749e-02,  1.9929e-02, -2.7084e-02,\n",
       "                      -2.5865e-02,  2.3890e-02,  2.8691e-02,  7.7249e-02, -4.9875e-02,\n",
       "                       6.6470e-02, -2.6545e-02,  2.0909e-02, -1.6931e-02, -9.9413e-03,\n",
       "                       2.2809e-02, -6.9110e-02,  4.8306e-02,  3.5910e-02, -7.0596e-03,\n",
       "                      -6.5198e-04, -1.9811e-02, -6.4948e-02,  3.3184e-02,  2.4440e-02,\n",
       "                      -3.3025e-02,  6.9175e-03,  2.3322e-02,  5.4926e-02, -4.6764e-02,\n",
       "                       1.1024e-02, -1.4472e-02,  1.5040e-02,  7.6807e-02, -2.4464e-02,\n",
       "                      -3.0371e-02,  3.3000e-03, -4.8181e-03,  6.0805e-02, -3.8068e-02,\n",
       "                      -6.6294e-04,  2.9181e-03, -2.7741e-02, -5.0978e-02,  3.6702e-02,\n",
       "                       1.0312e-01,  1.8054e-02,  2.2572e-02,  4.0055e-02,  1.4241e-02,\n",
       "                       3.3432e-02,  2.3391e-04,  3.4936e-02,  2.1405e-02,  6.1009e-02,\n",
       "                       2.3550e-03, -1.4606e-02,  2.7429e-02,  2.0983e-02,  6.1697e-03,\n",
       "                      -1.1898e-02,  2.8361e-02,  1.6777e-02,  7.3051e-02, -2.9748e-02,\n",
       "                       2.9609e-02,  4.5744e-02, -1.0490e-02, -1.9427e-02, -2.0421e-03,\n",
       "                      -5.5257e-04,  5.5799e-04,  1.0461e-01,  1.0138e-02,  8.2573e-03,\n",
       "                      -8.3315e-03,  1.2706e-01, -2.7301e-02, -9.0688e-03, -4.3692e-02,\n",
       "                      -2.6189e-02, -2.6459e-03, -3.3391e-03,  4.5563e-02,  3.9515e-02,\n",
       "                       3.4645e-02,  4.7756e-02, -3.3380e-02,  1.8752e-03,  1.3625e-02,\n",
       "                      -4.8018e-02, -3.0323e-02, -9.7415e-04,  2.6463e-02,  2.6692e-02,\n",
       "                      -1.8513e-02,  1.3110e-02,  2.6613e-02,  5.4680e-02, -5.3949e-02,\n",
       "                       1.6244e-02,  2.8291e-02, -2.7183e-02, -1.0070e-02,  5.6225e-02,\n",
       "                      -5.8161e-02, -1.4320e-02, -3.4991e-02,  6.5547e-03, -1.3677e-02,\n",
       "                      -3.3565e-02,  9.7237e-03])),\n",
       "             ('layer4.2.bn1.running_var',\n",
       "              tensor([0.0046, 0.0076, 0.0085, 0.0125, 0.0049, 0.0024, 0.0020, 0.0022, 0.0041,\n",
       "                      0.0075, 0.0074, 0.0024, 0.0085, 0.0020, 0.0090, 0.0163, 0.0065, 0.0042,\n",
       "                      0.0053, 0.0046, 0.0034, 0.0088, 0.0046, 0.0093, 0.0080, 0.0125, 0.0033,\n",
       "                      0.0158, 0.0050, 0.0204, 0.0026, 0.0028, 0.0181, 0.0127, 0.0039, 0.0099,\n",
       "                      0.0065, 0.0103, 0.0192, 0.0056, 0.0073, 0.0082, 0.0046, 0.0083, 0.0070,\n",
       "                      0.0022, 0.0061, 0.0159, 0.0016, 0.0100, 0.0058, 0.0016, 0.0138, 0.0036,\n",
       "                      0.0159, 0.0083, 0.0057, 0.0164, 0.0123, 0.0075, 0.0016, 0.0081, 0.0035,\n",
       "                      0.0081, 0.0144, 0.0126, 0.0118, 0.0412, 0.0068, 0.0053, 0.0025, 0.0080,\n",
       "                      0.0082, 0.0105, 0.0133, 0.0065, 0.0052, 0.0128, 0.0204, 0.0053, 0.0020,\n",
       "                      0.0062, 0.0045, 0.0062, 0.0188, 0.0204, 0.0061, 0.0143, 0.0092, 0.0109,\n",
       "                      0.0054, 0.0175, 0.0165, 0.0070, 0.0078, 0.0074, 0.0148, 0.0092, 0.0220,\n",
       "                      0.0072, 0.0204, 0.0223, 0.0071, 0.0133, 0.0077, 0.0015, 0.0064, 0.0172,\n",
       "                      0.0140, 0.0084, 0.0130, 0.0030, 0.0251, 0.0159, 0.0068, 0.0075, 0.0094,\n",
       "                      0.0058, 0.0052, 0.0078, 0.0203, 0.0064, 0.0135, 0.0076, 0.0093, 0.0113,\n",
       "                      0.0066, 0.0117, 0.0053, 0.0145, 0.0100, 0.0119, 0.0021, 0.0018, 0.0044,\n",
       "                      0.0058, 0.0028, 0.0062, 0.0020, 0.0104, 0.0088, 0.0045, 0.0082, 0.0124,\n",
       "                      0.0032, 0.0082, 0.0036, 0.0166, 0.0098, 0.0013, 0.0022, 0.0122, 0.0014,\n",
       "                      0.0033, 0.0024, 0.0083, 0.0084, 0.0042, 0.0091, 0.0149, 0.0158, 0.0145,\n",
       "                      0.0066, 0.0047, 0.0052, 0.0071, 0.0014, 0.0114, 0.0202, 0.0057, 0.0071,\n",
       "                      0.0144, 0.0078, 0.0032, 0.0274, 0.0053, 0.0073, 0.0084, 0.0051, 0.0039,\n",
       "                      0.0175, 0.0076, 0.0052, 0.0036, 0.0057, 0.0153, 0.0101, 0.0039, 0.0030,\n",
       "                      0.0062, 0.0105, 0.0107, 0.0036, 0.0026, 0.0082, 0.0060, 0.0043, 0.0052,\n",
       "                      0.0164, 0.0050, 0.0037, 0.0038, 0.0048, 0.0025, 0.0059, 0.0027, 0.0069,\n",
       "                      0.0012, 0.0231, 0.0073, 0.0077, 0.0054, 0.0041, 0.0051, 0.0191, 0.0150,\n",
       "                      0.0099, 0.0049, 0.0077, 0.0125, 0.0053, 0.0014, 0.0086, 0.0107, 0.0087,\n",
       "                      0.0153, 0.0148, 0.0126, 0.0016, 0.0051, 0.0011, 0.0078, 0.0111, 0.0109,\n",
       "                      0.0024, 0.0131, 0.0037, 0.0116, 0.0095, 0.0043, 0.0144, 0.0077, 0.0124,\n",
       "                      0.0091, 0.0047, 0.0035, 0.0132, 0.0021, 0.0055, 0.0142, 0.0061, 0.0276,\n",
       "                      0.0021, 0.0070, 0.0117, 0.0051, 0.0096, 0.0072, 0.0113, 0.0193, 0.0023,\n",
       "                      0.0164, 0.0063, 0.0188, 0.0057, 0.0373, 0.0125, 0.0207, 0.0128, 0.0219,\n",
       "                      0.0049, 0.0057, 0.0071, 0.0144, 0.0186, 0.0153, 0.0040, 0.0025, 0.0120,\n",
       "                      0.0081, 0.0068, 0.0202, 0.0176, 0.0168, 0.0114, 0.0215, 0.0357, 0.0032,\n",
       "                      0.0044, 0.0026, 0.0045, 0.0095, 0.0089, 0.0053, 0.0071, 0.0111, 0.0045,\n",
       "                      0.0107, 0.0158, 0.0054, 0.0165, 0.0108, 0.0099, 0.0124, 0.0082, 0.0070,\n",
       "                      0.0058, 0.0057, 0.0038, 0.0098, 0.0112, 0.0077, 0.0056, 0.0158, 0.0019,\n",
       "                      0.0073, 0.0216, 0.0051, 0.0040, 0.0130, 0.0017, 0.0125, 0.0036, 0.0166,\n",
       "                      0.0150, 0.0045, 0.0146, 0.0137, 0.0073, 0.0021, 0.0080, 0.0061, 0.0068,\n",
       "                      0.0068, 0.0145, 0.0130, 0.0082, 0.0044, 0.0229, 0.0088, 0.0033, 0.0052,\n",
       "                      0.0041, 0.0088, 0.0247, 0.0048, 0.0023, 0.0036, 0.0030, 0.0153, 0.0141,\n",
       "                      0.0034, 0.0081, 0.0029, 0.0055, 0.0026, 0.0107, 0.0052, 0.0104, 0.0062,\n",
       "                      0.0119, 0.0032, 0.0098, 0.0074, 0.0055, 0.0080, 0.0063, 0.0050, 0.0074,\n",
       "                      0.0072, 0.0172, 0.0052, 0.0106, 0.0218, 0.0033, 0.0126, 0.0031, 0.0014,\n",
       "                      0.0025, 0.0182, 0.0201, 0.0094, 0.0023, 0.0017, 0.0069, 0.0138, 0.0061,\n",
       "                      0.0073, 0.0083, 0.0048, 0.0054, 0.0019, 0.0165, 0.0044, 0.0011, 0.0147,\n",
       "                      0.0053, 0.0130, 0.0052, 0.0335, 0.0095, 0.0062, 0.0118, 0.0126, 0.0039,\n",
       "                      0.0150, 0.0072, 0.0057, 0.0261, 0.0079, 0.0131, 0.0051, 0.0026, 0.0017,\n",
       "                      0.0044, 0.0100, 0.0074, 0.0055, 0.0132, 0.0230, 0.0015, 0.0026, 0.0178,\n",
       "                      0.0341, 0.0082, 0.0023, 0.0118, 0.0047, 0.0100, 0.0052, 0.0092, 0.0209,\n",
       "                      0.0179, 0.0221, 0.0189, 0.0065, 0.0034, 0.0019, 0.0031, 0.0123, 0.0132,\n",
       "                      0.0050, 0.0025, 0.0048, 0.0103, 0.0384, 0.0061, 0.0121, 0.0078, 0.0066,\n",
       "                      0.0085, 0.0111, 0.0095, 0.0099, 0.0142, 0.0016, 0.0020, 0.0051, 0.0055,\n",
       "                      0.0125, 0.0152, 0.0170, 0.0119, 0.0217, 0.0040, 0.0234, 0.0099, 0.0153,\n",
       "                      0.0137, 0.0037, 0.0091, 0.0024, 0.0222, 0.0153, 0.0036, 0.0064, 0.0194,\n",
       "                      0.0040, 0.0019, 0.0074, 0.0006, 0.0079, 0.0162, 0.0085, 0.0211, 0.0089,\n",
       "                      0.0177, 0.0121, 0.0040, 0.0104, 0.0131, 0.0093, 0.0055, 0.0051, 0.0170,\n",
       "                      0.0128, 0.0050, 0.0159, 0.0242, 0.0038, 0.0105, 0.0057, 0.0067, 0.0162,\n",
       "                      0.0073, 0.0034, 0.0128, 0.0020, 0.0023, 0.0038, 0.0016, 0.0121])),\n",
       "             ('layer4.2.bn1.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.2.conv2.weight',\n",
       "              tensor([[[[-1.3890e-03, -1.5837e-03, -9.5336e-04],\n",
       "                        [ 3.3813e-04,  6.3187e-04,  1.7981e-03],\n",
       "                        [ 1.9094e-04,  7.4583e-04,  6.9409e-04]],\n",
       "              \n",
       "                       [[-1.3714e-03, -2.0862e-03, -1.2018e-03],\n",
       "                        [-1.9994e-03, -2.4780e-03, -1.4494e-03],\n",
       "                        [ 2.0819e-03,  3.1436e-03,  3.6156e-03]],\n",
       "              \n",
       "                       [[ 2.5026e-04, -1.4488e-04, -1.6897e-04],\n",
       "                        [ 7.1025e-04,  9.3837e-04,  1.9402e-03],\n",
       "                        [-3.5557e-04, -5.4761e-04, -4.1049e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.0499e-05,  5.5963e-04,  2.7558e-04],\n",
       "                        [-3.8836e-04,  2.5133e-05, -1.8645e-04],\n",
       "                        [-1.1068e-03,  3.7870e-05, -3.2872e-04]],\n",
       "              \n",
       "                       [[ 2.7499e-03,  3.1805e-03,  2.0837e-03],\n",
       "                        [-2.5301e-04, -6.3363e-04, -1.3210e-03],\n",
       "                        [ 3.0588e-04, -5.0153e-04, -4.8089e-04]],\n",
       "              \n",
       "                       [[-1.5188e-03, -1.8069e-03, -2.0016e-03],\n",
       "                        [ 1.5435e-03,  1.1107e-03,  3.0673e-03],\n",
       "                        [ 1.1904e-03,  1.9764e-03,  1.8516e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1265e-03,  9.6110e-04, -2.8920e-03],\n",
       "                        [ 1.6838e-03,  1.3857e-03, -5.1504e-03],\n",
       "                        [-1.8258e-03, -4.2608e-04, -1.8865e-03]],\n",
       "              \n",
       "                       [[-1.3836e-03,  1.3052e-04, -3.0696e-05],\n",
       "                        [-2.3188e-03, -1.3758e-04, -7.1174e-04],\n",
       "                        [-1.7810e-03, -1.8526e-03, -1.1128e-03]],\n",
       "              \n",
       "                       [[-1.3965e-03, -1.9975e-03, -6.5056e-04],\n",
       "                        [-9.0422e-04, -3.1404e-03, -2.6655e-03],\n",
       "                        [ 1.4491e-03, -1.7803e-03, -1.4324e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.7109e-03,  1.2277e-03, -1.3108e-03],\n",
       "                        [ 1.3467e-03, -1.0446e-03, -3.2048e-03],\n",
       "                        [ 1.8457e-05, -2.6633e-04, -1.3294e-03]],\n",
       "              \n",
       "                       [[ 2.9927e-03,  8.8888e-04,  3.1544e-04],\n",
       "                        [-1.0315e-04, -8.3633e-04, -7.4458e-04],\n",
       "                        [ 5.1523e-04,  5.7903e-04,  1.0556e-03]],\n",
       "              \n",
       "                       [[-2.9107e-03, -1.7119e-03, -4.7776e-03],\n",
       "                        [-1.3850e-03, -3.0230e-05, -2.9126e-03],\n",
       "                        [-3.1422e-03,  3.6404e-05, -4.2420e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9838e-04, -3.0516e-04,  5.8535e-04],\n",
       "                        [-1.9704e-04, -3.3015e-04,  1.2736e-03],\n",
       "                        [ 3.6097e-04, -1.2550e-04, -4.8368e-04]],\n",
       "              \n",
       "                       [[-1.0922e-03, -1.4114e-03, -2.0520e-03],\n",
       "                        [-1.4282e-03, -1.5641e-03, -2.1156e-03],\n",
       "                        [-1.1242e-03, -1.4387e-03, -1.1333e-03]],\n",
       "              \n",
       "                       [[-6.5738e-04, -4.4068e-04, -8.0443e-04],\n",
       "                        [-5.7577e-04,  2.2173e-04, -5.8191e-04],\n",
       "                        [ 5.7984e-04,  1.1268e-03,  1.8505e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.9472e-03,  2.0952e-03,  1.7048e-03],\n",
       "                        [ 3.1283e-04, -7.0174e-04, -3.8313e-04],\n",
       "                        [-1.0393e-03, -8.3079e-04, -1.0051e-03]],\n",
       "              \n",
       "                       [[ 3.5999e-04, -1.0639e-04, -6.6944e-04],\n",
       "                        [ 1.2412e-03, -5.9314e-04, -6.5141e-04],\n",
       "                        [ 1.6476e-03,  4.8695e-04,  2.1408e-04]],\n",
       "              \n",
       "                       [[-1.8815e-04, -1.1943e-03, -1.9360e-03],\n",
       "                        [ 5.3908e-04, -1.2591e-03, -2.2469e-03],\n",
       "                        [ 1.4278e-03,  1.5329e-03,  1.7585e-04]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-7.4261e-04,  4.1310e-04, -2.3926e-04],\n",
       "                        [-6.9139e-04,  1.9030e-03,  2.9185e-04],\n",
       "                        [-9.5966e-04,  8.9420e-05, -9.6257e-04]],\n",
       "              \n",
       "                       [[-9.0205e-04, -7.1425e-04, -1.0793e-03],\n",
       "                        [ 3.0036e-04, -8.5447e-04, -2.2960e-03],\n",
       "                        [-1.3990e-03, -4.1382e-04, -1.6994e-03]],\n",
       "              \n",
       "                       [[-8.7813e-04, -9.2243e-04, -2.2312e-03],\n",
       "                        [-5.1021e-04, -1.8190e-05, -1.5954e-03],\n",
       "                        [-1.3508e-03, -4.5768e-04, -9.3940e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.5239e-04,  3.0065e-03,  1.1727e-03],\n",
       "                        [-2.5480e-03, -2.5455e-03, -3.0349e-03],\n",
       "                        [-1.3576e-03, -3.5331e-04, -1.3431e-03]],\n",
       "              \n",
       "                       [[ 1.3874e-03, -6.6021e-04, -6.9522e-04],\n",
       "                        [ 6.2564e-05, -3.6158e-03, -1.8552e-03],\n",
       "                        [ 3.8191e-04, -5.9062e-04,  1.2914e-04]],\n",
       "              \n",
       "                       [[ 2.5602e-04,  1.0879e-04,  7.0444e-05],\n",
       "                        [ 1.3930e-03,  1.6597e-03,  4.1224e-04],\n",
       "                        [-7.6733e-04, -8.5795e-04, -5.6839e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.8315e-04, -4.0244e-04, -1.8352e-03],\n",
       "                        [-1.9782e-03, -1.2094e-04, -1.7268e-03],\n",
       "                        [-2.3875e-03, -1.1160e-03, -2.1774e-03]],\n",
       "              \n",
       "                       [[ 9.7544e-04,  1.4761e-03,  4.9757e-04],\n",
       "                        [ 2.4050e-03,  1.4888e-03,  1.5416e-03],\n",
       "                        [ 2.4786e-03,  1.0830e-04, -1.3645e-03]],\n",
       "              \n",
       "                       [[-1.6631e-03, -1.0047e-03, -1.4382e-03],\n",
       "                        [-1.6796e-03, -3.3364e-04, -1.4890e-03],\n",
       "                        [-1.4842e-03, -4.1203e-04, -2.8314e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.9029e-04,  9.6892e-04,  9.5854e-04],\n",
       "                        [ 8.6512e-04,  6.3228e-04,  5.0980e-04],\n",
       "                        [ 1.1907e-03,  1.8499e-03,  2.2778e-03]],\n",
       "              \n",
       "                       [[ 2.0290e-03,  2.2574e-03,  2.6907e-03],\n",
       "                        [-1.5967e-03, -2.0388e-03, -1.0978e-03],\n",
       "                        [ 1.1749e-05, -3.8637e-05,  2.9154e-04]],\n",
       "              \n",
       "                       [[ 4.6461e-04,  6.4050e-04,  4.3345e-04],\n",
       "                        [ 3.1575e-04,  8.0433e-04,  4.7684e-04],\n",
       "                        [-1.9627e-03, -9.0483e-04, -1.9149e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0994e-03,  2.2554e-03,  1.3896e-03],\n",
       "                        [ 5.4735e-04,  1.1628e-03,  1.5382e-03],\n",
       "                        [-6.4395e-04, -7.2331e-04, -2.5023e-04]],\n",
       "              \n",
       "                       [[ 5.3099e-04,  2.5463e-04, -5.0939e-04],\n",
       "                        [-5.6889e-04, -8.9297e-04, -7.6089e-04],\n",
       "                        [-1.6682e-03, -1.3427e-03, -1.4688e-03]],\n",
       "              \n",
       "                       [[-1.6771e-03, -1.5383e-03, -1.6591e-03],\n",
       "                        [-2.6781e-04, -6.0272e-05, -1.3445e-03],\n",
       "                        [ 3.4518e-04,  5.5539e-04, -3.6338e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1984e-04, -1.1708e-05, -3.2874e-04],\n",
       "                        [-6.5369e-05, -6.8724e-04,  1.7433e-05],\n",
       "                        [ 2.0905e-04, -3.5475e-04, -2.2941e-04]],\n",
       "              \n",
       "                       [[ 1.4435e-04, -5.9344e-04, -1.3249e-04],\n",
       "                        [ 3.8191e-04, -3.0650e-04, -4.2539e-04],\n",
       "                        [ 7.5313e-04,  4.6529e-04,  2.9186e-04]],\n",
       "              \n",
       "                       [[ 2.1277e-03,  3.4987e-03,  2.6812e-03],\n",
       "                        [ 1.8301e-04,  2.1063e-04,  1.1642e-03],\n",
       "                        [ 6.4717e-04,  4.1318e-04,  7.6539e-04]]]])),\n",
       "             ('layer4.2.bn2.weight',\n",
       "              tensor([0.0460, 0.0784, 0.0518, 0.0612, 0.0493, 0.0717, 0.0322, 0.0909, 0.0410,\n",
       "                      0.0521, 0.0572, 0.0354, 0.0947, 0.0577, 0.0592, 0.0475, 0.0350, 0.0496,\n",
       "                      0.0297, 0.0361, 0.0344, 0.0767, 0.0739, 0.0393, 0.0528, 0.0650, 0.0695,\n",
       "                      0.0720, 0.0339, 0.0588, 0.1000, 0.0567, 0.0368, 0.0703, 0.0726, 0.0648,\n",
       "                      0.0403, 0.0763, 0.0512, 0.0595, 0.0606, 0.0497, 0.0535, 0.0542, 0.0391,\n",
       "                      0.0410, 0.0725, 0.0795, 0.1123, 0.0336, 0.0513, 0.0466, 0.0297, 0.0908,\n",
       "                      0.0719, 0.0535, 0.0434, 0.0863, 0.0688, 0.0403, 0.0524, 0.0689, 0.0960,\n",
       "                      0.0740, 0.0627, 0.0674, 0.0501, 0.0463, 0.0487, 0.0405, 0.0207, 0.0799,\n",
       "                      0.0462, 0.0318, 0.0530, 0.0332, 0.0543, 0.0380, 0.0380, 0.0831, 0.0651,\n",
       "                      0.0650, 0.0674, 0.0660, 0.0792, 0.0917, 0.0453, 0.0511, 0.0705, 0.0710,\n",
       "                      0.0737, 0.0606, 0.0774, 0.0454, 0.0651, 0.0411, 0.0757, 0.0646, 0.0624,\n",
       "                      0.0479, 0.0597, 0.0606, 0.0826, 0.0674, 0.0488, 0.0713, 0.0276, 0.0635,\n",
       "                      0.0889, 0.0860, 0.0282, 0.0612, 0.1210, 0.0242, 0.0670, 0.0483, 0.0661,\n",
       "                      0.0597, 0.0369, 0.0573, 0.0648, 0.0444, 0.0606, 0.0468, 0.0639, 0.0566,\n",
       "                      0.0519, 0.0784, 0.0514, 0.0605, 0.0643, 0.0329, 0.0916, 0.0500, 0.0589,\n",
       "                      0.0556, 0.0701, 0.0419, 0.0620, 0.0530, 0.0702, 0.0829, 0.0570, 0.0938,\n",
       "                      0.0390, 0.0554, 0.0560, 0.0641, 0.0649, 0.0550, 0.0740, 0.0542, 0.0680,\n",
       "                      0.0800, 0.0290, 0.0634, 0.0390, 0.0336, 0.0465, 0.0425, 0.0724, 0.0386,\n",
       "                      0.0411, 0.0501, 0.0228, 0.0392, 0.0391, 0.0745, 0.0553, 0.0641, 0.0734,\n",
       "                      0.0923, 0.0458, 0.0383, 0.0762, 0.0561, 0.0678, 0.0735, 0.0540, 0.0405,\n",
       "                      0.1035, 0.0575, 0.0777, 0.0661, 0.0477, 0.0599, 0.0331, 0.0777, 0.1195,\n",
       "                      0.0420, 0.0504, 0.0624, 0.0534, 0.1071, 0.0686, 0.0548, 0.0334, 0.0454,\n",
       "                      0.0869, 0.0663, 0.0374, 0.0859, 0.0820, 0.0567, 0.0425, 0.0687, 0.0718,\n",
       "                      0.0920, 0.0387, 0.0885, 0.0390, 0.0779, 0.0637, 0.0501, 0.1121, 0.0311,\n",
       "                      0.0773, 0.0296, 0.1059, 0.0369, 0.0668, 0.0768, 0.0349, 0.0651, 0.0751,\n",
       "                      0.0499, 0.0344, 0.0946, 0.0262, 0.0491, 0.0497, 0.0812, 0.0994, 0.0470,\n",
       "                      0.0770, 0.0752, 0.0595, 0.0438, 0.0866, 0.1120, 0.0563, 0.0474, 0.0368,\n",
       "                      0.0471, 0.1069, 0.0991, 0.0504, 0.0603, 0.0526, 0.0633, 0.0408, 0.0757,\n",
       "                      0.0528, 0.0690, 0.0380, 0.0374, 0.0583, 0.0448, 0.0562, 0.0540, 0.1053,\n",
       "                      0.1052, 0.0656, 0.0779, 0.0338, 0.0436, 0.0515, 0.0355, 0.0615, 0.0795,\n",
       "                      0.0323, 0.0588, 0.0462, 0.0272, 0.0705, 0.1122, 0.0591, 0.0589, 0.0411,\n",
       "                      0.0549, 0.0348, 0.0584, 0.0583, 0.0764, 0.0422, 0.0685, 0.0555, 0.0607,\n",
       "                      0.0566, 0.0396, 0.0595, 0.0622, 0.1056, 0.0884, 0.0530, 0.0400, 0.0297,\n",
       "                      0.0438, 0.0497, 0.0372, 0.0422, 0.0541, 0.0426, 0.0438, 0.0665, 0.0386,\n",
       "                      0.0437, 0.0423, 0.0707, 0.0416, 0.0260, 0.0666, 0.0494, 0.0742, 0.0780,\n",
       "                      0.0666, 0.0590, 0.0387, 0.0588, 0.0506, 0.0371, 0.0859, 0.0773, 0.0281,\n",
       "                      0.0578, 0.0370, 0.0422, 0.0630, 0.0386, 0.0546, 0.0839, 0.0435, 0.0670,\n",
       "                      0.0795, 0.0656, 0.0866, 0.0655, 0.0700, 0.0545, 0.0457, 0.0559, 0.0790,\n",
       "                      0.0632, 0.0723, 0.0701, 0.0658, 0.0584, 0.0536, 0.0492, 0.0757, 0.0620,\n",
       "                      0.0419, 0.0521, 0.0305, 0.0592, 0.1012, 0.0655, 0.0687, 0.0399, 0.0380,\n",
       "                      0.0574, 0.0698, 0.0792, 0.0480, 0.0506, 0.0381, 0.0550, 0.0561, 0.0575,\n",
       "                      0.0737, 0.0588, 0.0548, 0.0631, 0.0385, 0.0333, 0.0490, 0.0421, 0.0689,\n",
       "                      0.0830, 0.0673, 0.0339, 0.0888, 0.0478, 0.0538, 0.0567, 0.0201, 0.0560,\n",
       "                      0.0497, 0.0328, 0.0837, 0.0659, 0.0437, 0.0435, 0.0578, 0.0781, 0.0557,\n",
       "                      0.0665, 0.0808, 0.0586, 0.0561, 0.0979, 0.0304, 0.0657, 0.0694, 0.0731,\n",
       "                      0.0817, 0.0513, 0.0236, 0.0422, 0.0840, 0.0383, 0.0821, 0.0622, 0.0847,\n",
       "                      0.0384, 0.0384, 0.0495, 0.0536, 0.0672, 0.0615, 0.0717, 0.0479, 0.0436,\n",
       "                      0.0419, 0.0597, 0.0449, 0.0711, 0.0659, 0.0794, 0.0413, 0.0589, 0.0871,\n",
       "                      0.0640, 0.0901, 0.0694, 0.0436, 0.0553, 0.0874, 0.0529, 0.0522, 0.0816,\n",
       "                      0.0612, 0.0514, 0.0464, 0.0563, 0.0390, 0.0742, 0.0748, 0.0551, 0.0700,\n",
       "                      0.0566, 0.0634, 0.0867, 0.0602, 0.0417, 0.0426, 0.0378, 0.0655, 0.0600,\n",
       "                      0.0364, 0.0315, 0.0504, 0.0408, 0.0568, 0.0452, 0.0612, 0.0357, 0.0428,\n",
       "                      0.0687, 0.0515, 0.0479, 0.0534, 0.0975, 0.1035, 0.0466, 0.0554, 0.0652,\n",
       "                      0.0266, 0.0916, 0.0338, 0.0853, 0.0505, 0.0514, 0.0537, 0.0511, 0.0502,\n",
       "                      0.0328, 0.0503, 0.0580, 0.0635, 0.0736, 0.0388, 0.0621, 0.0583, 0.0486,\n",
       "                      0.0549, 0.0468, 0.0673, 0.0645, 0.0355, 0.0573, 0.0499, 0.0460, 0.0361,\n",
       "                      0.0628, 0.0765, 0.0454, 0.0609, 0.0511, 0.0809, 0.0748, 0.0392])),\n",
       "             ('layer4.2.bn2.bias',\n",
       "              tensor([-1.2608e-02, -6.3704e-03, -2.3148e-02, -8.0858e-03, -6.9222e-03,\n",
       "                      -2.9397e-02, -8.9301e-03, -6.7537e-03, -1.2584e-02, -2.1710e-02,\n",
       "                      -5.1810e-03, -1.7074e-02, -1.4683e-02, -1.1197e-02, -1.3319e-02,\n",
       "                      -5.0618e-03, -1.2355e-02, -1.6000e-02, -5.0075e-03, -7.8922e-03,\n",
       "                      -9.1944e-03,  1.0413e-02, -1.2297e-02, -7.7555e-03, -1.0714e-02,\n",
       "                      -2.4721e-02, -3.0652e-02, -1.6430e-02,  1.0616e-03,  4.0650e-03,\n",
       "                      -1.2428e-02, -1.8734e-02, -8.0254e-03, -2.0283e-02, -1.6769e-02,\n",
       "                      -1.3886e-02, -4.9399e-03, -6.9027e-03, -3.0148e-02,  1.8814e-02,\n",
       "                      -1.8429e-02, -1.8492e-02, -1.5750e-02, -1.9379e-02, -9.9972e-03,\n",
       "                       6.7020e-03, -1.6365e-02,  3.8547e-03, -4.1781e-02, -1.1959e-02,\n",
       "                      -9.6245e-03, -1.7921e-03, -6.4632e-03, -3.8152e-02, -2.4790e-03,\n",
       "                      -1.6995e-02, -1.1581e-03, -1.5737e-02, -1.0275e-02, -2.4309e-02,\n",
       "                      -1.6788e-02, -4.0157e-02, -2.1822e-02, -1.3956e-02, -1.6128e-02,\n",
       "                      -2.8513e-03, -7.1259e-03, -7.1374e-03, -1.9624e-02, -9.7581e-03,\n",
       "                       4.6525e-03, -2.1424e-02, -8.8640e-03, -9.8286e-03,  2.4103e-02,\n",
       "                      -1.2721e-02, -2.5361e-02, -1.3309e-02,  1.4893e-02, -2.7004e-02,\n",
       "                      -6.4034e-03, -4.6187e-03, -9.8341e-03, -2.1103e-02, -1.6962e-02,\n",
       "                      -2.3577e-02, -4.4181e-03, -1.4949e-02, -2.7561e-02, -3.5020e-02,\n",
       "                      -2.5221e-02, -1.7867e-02, -1.2975e-02,  1.1860e-02, -1.7401e-02,\n",
       "                      -1.0805e-02, -2.4210e-02, -1.3529e-02, -3.3928e-03,  1.4312e-03,\n",
       "                      -6.4193e-03, -1.4651e-02, -1.2235e-02, -2.0325e-02, -6.8355e-03,\n",
       "                      -1.4810e-02, -4.0446e-03, -1.6715e-02, -3.7969e-02, -3.0680e-02,\n",
       "                       3.7981e-04, -1.1866e-02, -2.5477e-02, -3.0393e-03, -2.7330e-02,\n",
       "                      -1.0857e-02, -1.5218e-02, -1.8309e-02,  6.7468e-04, -1.4427e-02,\n",
       "                      -8.6783e-03, -9.1052e-03, -2.1705e-02, -1.3769e-02, -2.7027e-02,\n",
       "                      -1.2974e-02, -5.3556e-03,  5.2465e-04, -1.2237e-02, -2.2187e-02,\n",
       "                      -1.0172e-02,  2.7041e-03, -2.4124e-02, -9.1081e-03, -1.2500e-02,\n",
       "                      -2.3711e-02, -1.0255e-02, -2.3083e-02, -1.4084e-02, -1.3495e-02,\n",
       "                      -1.4954e-02, -1.3087e-02, -2.2262e-02, -2.7322e-02, -5.3448e-03,\n",
       "                      -1.1489e-02, -7.7170e-03, -9.7964e-03, -1.3686e-02, -1.2851e-02,\n",
       "                      -2.3647e-02, -2.4585e-02,  1.4855e-02, -1.4924e-03,  3.3082e-03,\n",
       "                      -2.7298e-02, -1.0919e-02, -9.2913e-03, -7.7629e-03, -6.0810e-03,\n",
       "                      -2.4063e-02, -7.4280e-03, -1.6593e-02, -1.0992e-02, -1.7927e-03,\n",
       "                      -1.1821e-02, -8.9098e-04, -9.5556e-03, -2.2692e-02, -7.2074e-03,\n",
       "                      -1.3197e-02, -2.6814e-02, -5.4422e-04, -1.5622e-02, -2.0456e-02,\n",
       "                      -1.4903e-02,  1.1842e-03, -4.6565e-03, -2.3616e-02, -1.0304e-02,\n",
       "                      -5.5025e-02, -1.5858e-02, -2.0288e-02, -2.2779e-03, -1.7663e-02,\n",
       "                      -2.5072e-02, -1.4820e-02, -1.5698e-02, -1.8891e-02,  5.5435e-03,\n",
       "                      -1.7549e-02, -4.9528e-03, -4.7808e-03, -1.2279e-02, -1.7255e-02,\n",
       "                      -2.1791e-02,  1.7016e-03,  1.7344e-03, -2.8424e-02, -3.2784e-03,\n",
       "                      -1.6174e-02, -3.6429e-02, -2.0553e-02, -1.0559e-02, -1.6158e-02,\n",
       "                      -2.4799e-02, -1.6083e-02, -2.6342e-02, -5.1184e-03, -3.1609e-02,\n",
       "                      -7.3120e-03, -1.6986e-02, -1.4944e-02, -3.9599e-03, -1.0624e-02,\n",
       "                      -1.3328e-02, -1.2848e-02, -8.4717e-03, -5.8776e-03, -1.4122e-02,\n",
       "                      -1.9723e-02, -2.2435e-02, -1.1628e-02, -1.2410e-03, -1.6371e-02,\n",
       "                      -1.1975e-02, -1.4137e-02, -1.6092e-02, -9.6819e-03, -1.0363e-02,\n",
       "                      -8.6601e-03, -1.9197e-02, -5.2011e-03,  6.2940e-03, -1.4339e-02,\n",
       "                      -3.7975e-02, -2.1799e-02, -7.8930e-03, -1.7502e-02, -3.0317e-02,\n",
       "                      -5.9715e-03, -1.1482e-02, -7.3317e-03, -1.8691e-02, -3.6743e-02,\n",
       "                      -2.6397e-02, -5.9389e-03,  2.7124e-03,  3.3816e-03, -1.4622e-02,\n",
       "                       2.0160e-03, -2.3992e-02, -2.9820e-02, -1.8469e-02, -1.2976e-02,\n",
       "                      -5.6935e-03, -1.2671e-02, -1.2130e-02,  1.5878e-02, -1.3201e-02,\n",
       "                      -1.4263e-02, -2.5579e-03, -6.2245e-03, -2.2357e-02, -1.2191e-02,\n",
       "                      -4.3544e-03, -2.2828e-02, -1.8657e-02, -2.8238e-03, -7.3594e-04,\n",
       "                      -2.0102e-03, -1.1944e-02, -2.0555e-03, -7.1072e-03, -3.7240e-02,\n",
       "                      -2.9427e-02, -1.2287e-02, -1.0358e-02, -1.4434e-02, -2.5123e-02,\n",
       "                      -1.3410e-02,  7.8770e-03, -2.8166e-02, -1.8829e-02, -1.1424e-02,\n",
       "                      -2.0756e-02, -1.5197e-02, -9.9946e-04, -2.5264e-02,  8.2391e-03,\n",
       "                      -1.2730e-02, -2.7470e-02,  1.2478e-02, -1.1852e-02, -2.3679e-02,\n",
       "                      -7.0854e-03,  5.7059e-03, -5.5539e-03, -1.0838e-02, -1.5576e-02,\n",
       "                      -6.2427e-03, -1.7588e-02, -1.6054e-02, -6.7202e-03, -1.1745e-02,\n",
       "                      -9.7758e-03, -1.1976e-02, -8.2963e-03, -2.9136e-02, -7.5870e-03,\n",
       "                      -2.0129e-03, -3.6969e-02, -8.8262e-03, -4.7077e-02, -2.2990e-02,\n",
       "                      -2.7359e-03, -9.6021e-03, -8.5940e-03, -1.3649e-02, -7.0010e-03,\n",
       "                       4.1590e-03, -1.0240e-03, -3.1378e-02, -1.0693e-02, -1.9012e-02,\n",
       "                      -1.5028e-02, -9.4056e-03, -1.4217e-02, -9.9831e-03,  1.3996e-02,\n",
       "                      -2.6534e-02, -1.6245e-03, -1.6422e-02, -2.7332e-02, -2.4931e-02,\n",
       "                      -2.5810e-02, -2.5310e-02, -2.2396e-02, -7.3399e-03, -1.3616e-02,\n",
       "                      -4.5158e-04, -1.3339e-02, -1.2805e-02, -9.2360e-03, -3.4570e-02,\n",
       "                       1.4329e-02, -1.1555e-02, -1.5545e-02, -1.7536e-02, -5.7507e-03,\n",
       "                      -1.7043e-03, -2.3227e-02, -1.5323e-02, -7.5518e-03, -4.3014e-03,\n",
       "                      -8.8172e-03,  1.9455e-03,  2.1913e-02,  7.0315e-04, -1.4172e-02,\n",
       "                      -7.1042e-04, -2.6820e-02, -2.6097e-02, -1.8797e-02, -8.5895e-03,\n",
       "                      -9.7800e-03, -1.0716e-02, -1.7633e-02, -1.6799e-02, -1.0406e-02,\n",
       "                      -3.4157e-03, -1.8535e-02, -5.8793e-03, -1.6954e-02,  8.8709e-03,\n",
       "                      -1.4208e-02,  1.4122e-03,  1.1873e-04, -2.9847e-03, -2.5441e-02,\n",
       "                      -1.0317e-02, -5.1622e-02, -8.2190e-03, -1.5179e-02, -1.8339e-02,\n",
       "                      -5.4314e-03, -5.1953e-03, -8.9587e-03, -9.2915e-03, -3.6640e-02,\n",
       "                      -1.2935e-02, -8.1747e-03, -2.0010e-02, -1.6658e-02, -3.6289e-02,\n",
       "                      -1.4909e-02, -1.1730e-02, -1.5966e-02, -1.0583e-02,  3.2519e-03,\n",
       "                      -2.0843e-02, -4.9565e-03, -1.6698e-02, -1.1792e-02, -1.6370e-02,\n",
       "                      -1.6211e-02, -1.9501e-02, -7.1952e-03, -1.6367e-02, -1.4173e-02,\n",
       "                      -5.4318e-03, -2.5409e-02, -2.7271e-02, -1.0477e-02, -1.1705e-02,\n",
       "                      -1.5257e-02, -1.1922e-02, -2.8758e-02, -1.4505e-02, -1.1147e-02,\n",
       "                      -2.3524e-02, -2.1819e-02,  2.0277e-04, -1.3603e-02, -3.0887e-02,\n",
       "                      -4.1420e-03, -2.9996e-03, -2.0857e-02,  6.6921e-04, -9.3728e-04,\n",
       "                      -2.9717e-02, -1.8052e-03, -1.3535e-02, -2.1467e-02, -1.5685e-02,\n",
       "                      -3.9475e-03, -3.0550e-02, -1.0324e-02, -2.2364e-02, -2.5437e-04,\n",
       "                      -7.0437e-06, -2.2850e-02, -1.5901e-02, -5.8796e-03, -2.0717e-02,\n",
       "                      -1.4587e-02, -1.0287e-02, -3.9318e-02, -1.8206e-02,  1.3888e-03,\n",
       "                      -1.8809e-02, -1.8908e-02, -2.6446e-03,  6.9638e-03, -6.9620e-03,\n",
       "                      -1.7628e-02, -1.0695e-02, -2.1035e-02, -8.5722e-03, -9.2599e-03,\n",
       "                      -2.1193e-02, -1.2219e-02, -1.2717e-02, -2.0301e-02, -9.7733e-03,\n",
       "                      -2.7548e-02, -7.6925e-03, -9.7708e-04, -1.2797e-02, -2.5483e-02,\n",
       "                      -1.5024e-02, -1.2686e-02, -2.0440e-02, -3.8356e-02, -1.5593e-02,\n",
       "                      -2.1748e-02, -2.6263e-02, -1.0142e-02, -1.1301e-02, -9.6911e-03,\n",
       "                      -3.5636e-02, -1.2047e-02, -1.8399e-02, -3.2414e-02, -7.9825e-03,\n",
       "                      -1.9450e-02, -4.3625e-03, -1.0285e-02, -1.3093e-02, -2.8653e-03,\n",
       "                      -2.4570e-02, -9.7922e-03, -2.8818e-02, -4.8570e-03, -6.1328e-03,\n",
       "                      -1.3909e-02, -1.7957e-03, -1.1415e-03,  3.5312e-03, -6.2283e-03,\n",
       "                      -2.3029e-02,  4.7413e-03, -1.6899e-02, -3.5690e-03, -1.6485e-02,\n",
       "                      -2.3528e-02,  7.3084e-03, -1.3668e-02, -1.3853e-02, -1.0915e-02,\n",
       "                       9.9998e-04, -8.9662e-03])),\n",
       "             ('layer4.2.bn2.running_mean',\n",
       "              tensor([-3.4572e-03, -1.0428e-02,  6.4380e-04, -1.2515e-02, -4.7866e-03,\n",
       "                      -8.3016e-03, -4.1171e-03, -4.0938e-03, -1.0066e-02, -8.5634e-03,\n",
       "                      -8.5757e-03,  3.2552e-03, -1.7937e-02, -8.1390e-03, -7.1479e-03,\n",
       "                      -5.6415e-03, -5.2966e-04, -8.1527e-03, -8.0641e-04, -3.0924e-03,\n",
       "                      -5.1953e-03, -1.5316e-02, -1.8156e-02, -1.8521e-02, -6.5637e-03,\n",
       "                      -1.6000e-02, -7.2976e-03, -7.8474e-03, -2.4388e-03, -8.9786e-03,\n",
       "                      -1.4720e-02, -4.5130e-03, -3.3889e-03, -1.6575e-02, -6.3216e-03,\n",
       "                      -1.4317e-02, -1.0559e-02, -1.7048e-02, -6.2130e-03, -2.3871e-02,\n",
       "                      -6.9999e-03, -9.3590e-03, -1.6951e-02, -7.0063e-03, -3.7878e-03,\n",
       "                      -1.5411e-02, -1.1192e-02, -1.8893e-02, -5.2812e-03, -4.6471e-03,\n",
       "                      -5.6488e-03, -8.6295e-03, -3.3734e-03, -2.0408e-02, -1.1583e-02,\n",
       "                      -6.4676e-03, -6.2544e-03, -1.1970e-02, -1.2236e-02, -7.4457e-03,\n",
       "                      -1.5244e-02, -9.7976e-03, -5.8513e-03, -1.2840e-02, -8.0670e-03,\n",
       "                      -9.8389e-03, -8.6508e-03, -3.8547e-03, -5.1012e-03, -4.9422e-03,\n",
       "                      -3.4261e-03, -1.5689e-02, -5.4819e-03, -5.9980e-03, -1.6683e-02,\n",
       "                      -8.9216e-03, -1.1306e-02, -6.9461e-03, -1.3725e-02, -1.4279e-02,\n",
       "                      -9.7460e-03, -1.0365e-02, -1.6789e-02, -1.0715e-02, -9.4317e-03,\n",
       "                      -2.2039e-02, -1.3299e-02, -5.9225e-03, -1.6929e-02, -4.1684e-03,\n",
       "                      -1.5508e-02, -8.4833e-03, -4.1135e-03, -1.3937e-02, -1.4383e-02,\n",
       "                      -1.4409e-03, -1.8722e-02, -5.1871e-03, -1.5747e-02, -8.9975e-03,\n",
       "                      -4.0279e-03, -5.4072e-03, -1.4837e-02, -1.2279e-02, -7.2314e-03,\n",
       "                      -1.1602e-02, -8.0540e-03, -9.1462e-03, -7.9981e-03, -3.0069e-03,\n",
       "                      -6.7959e-03, -1.8517e-02, -9.4738e-03, -6.3078e-03, -1.7626e-03,\n",
       "                      -9.0570e-03, -1.0240e-02, -1.8697e-02, -8.8353e-03, -4.6120e-03,\n",
       "                      -1.1289e-02, -9.8936e-03, -1.4347e-02, -4.5372e-03, -1.2631e-02,\n",
       "                      -3.8482e-03, -7.8341e-03, -1.1145e-02, -1.2144e-02, -9.5436e-03,\n",
       "                      -1.6441e-02, -1.4704e-02, -2.1739e-02, -6.6733e-03, -6.6284e-03,\n",
       "                      -2.6418e-03, -1.6212e-02, -9.3977e-03, -1.0339e-02,  1.3006e-03,\n",
       "                      -1.5295e-02, -1.3190e-02, -1.0073e-02, -2.2431e-02, -1.0606e-02,\n",
       "                      -1.8089e-03, -1.3001e-02, -9.9148e-03, -2.7368e-03, -9.5507e-03,\n",
       "                      -4.1277e-03, -8.4728e-03, -2.0490e-02, -1.6851e-02, -4.9078e-03,\n",
       "                      -4.3276e-03, -8.3242e-03, -3.7632e-03, -1.2009e-02, -8.6692e-03,\n",
       "                      -1.0617e-02, -6.2166e-03, -8.6631e-03, -5.6538e-03, -6.1859e-03,\n",
       "                      -5.9684e-03, -1.2643e-02, -1.4080e-02, -6.3676e-03, -9.2430e-03,\n",
       "                      -1.7305e-02, -1.3547e-02, -7.7354e-03, -1.0315e-02, -3.6614e-03,\n",
       "                      -1.0304e-02, -2.3685e-02, -1.3275e-02, -1.3330e-02, -8.5724e-03,\n",
       "                      -9.0141e-03, -8.5241e-03, -1.3451e-02, -6.4735e-03, -3.9499e-03,\n",
       "                      -1.4355e-02, -4.2431e-03, -1.3454e-02, -1.9110e-02, -9.4525e-03,\n",
       "                      -9.0908e-03, -9.1430e-03, -8.2652e-03, -2.5667e-02, -9.8136e-03,\n",
       "                      -1.5924e-02, -1.1793e-02, -1.2992e-02, -1.6257e-02, -4.8980e-03,\n",
       "                      -3.7087e-03, -1.0248e-02, -8.1810e-03, -1.2587e-02, -1.7379e-04,\n",
       "                      -6.6777e-03, -1.8089e-02, -7.8423e-03, -8.4005e-03, -9.3369e-03,\n",
       "                      -1.9322e-03, -1.4460e-02, -7.9088e-03, -1.2684e-02, -1.9867e-02,\n",
       "                      -7.3219e-03, -1.2480e-02, -6.4440e-03, -2.6204e-02, -3.8789e-03,\n",
       "                      -2.3441e-03, -1.3263e-02, -3.7767e-03, -6.5653e-03, -9.2343e-03,\n",
       "                      -1.1398e-02, -4.3913e-03, -1.7511e-02, -2.5464e-03, -1.1706e-02,\n",
       "                      -7.8672e-03, -1.8542e-02, -1.9777e-02, -6.0711e-03, -2.0891e-02,\n",
       "                      -8.2529e-03, -5.7381e-03, -9.2952e-03, -9.3761e-03, -9.7374e-03,\n",
       "                      -1.1267e-02, -7.1342e-03, -6.2213e-03, -7.5836e-03, -2.0143e-02,\n",
       "                      -1.9351e-02, -1.2678e-02, -6.9291e-03, -1.0285e-02, -1.1874e-02,\n",
       "                      -1.2719e-02, -7.1560e-03, -6.3697e-03, -1.1449e-02, -4.9568e-03,\n",
       "                      -1.0142e-02, -4.6564e-03, -9.4279e-03, -1.5453e-02, -8.0398e-03,\n",
       "                      -1.7051e-02, -2.6009e-02, -1.3025e-02, -2.0263e-02, -5.6759e-03,\n",
       "                      -8.5637e-03, -9.2050e-03, -9.7345e-05, -1.0713e-02, -1.7446e-02,\n",
       "                      -1.2652e-02, -1.0902e-02, -1.2741e-02, -5.6048e-03, -2.2261e-03,\n",
       "                      -1.1682e-02, -1.5648e-02, -1.2939e-02, -6.1041e-03, -9.4978e-03,\n",
       "                      -7.9407e-03, -5.6493e-03, -1.3442e-02, -6.8258e-03, -7.3319e-03,\n",
       "                      -9.6519e-03, -1.3396e-02, -1.8193e-02, -9.5814e-03, -1.3458e-02,\n",
       "                      -9.6642e-03, -1.6202e-02, -2.2642e-02, -1.4588e-02, -9.3343e-03,\n",
       "                      -6.8435e-03, -6.5744e-03, -7.3907e-03, -1.9125e-03, -4.3080e-03,\n",
       "                      -9.3429e-03, -2.1437e-02, -1.9323e-03, -8.5285e-03, -8.3010e-03,\n",
       "                      -1.1974e-02, -3.0900e-03, -1.0707e-02, -8.0199e-03, -1.7807e-02,\n",
       "                      -3.5676e-03, -3.5318e-03, -7.1382e-03, -5.6521e-03, -5.3265e-03,\n",
       "                      -1.0476e-02, -9.2789e-03, -9.9320e-03, -5.4250e-03, -8.7352e-03,\n",
       "                      -9.4470e-03, -1.7654e-02, -7.9543e-03, -7.0836e-03, -1.2372e-02,\n",
       "                      -4.3932e-03, -3.7438e-03, -1.9862e-03, -1.0683e-02, -1.4494e-02,\n",
       "                      -1.1515e-02, -8.8026e-03, -5.7990e-03, -1.9001e-02, -1.7795e-02,\n",
       "                      -1.5485e-02, -1.0539e-02, -1.1510e-02, -8.2186e-03, -1.0204e-02,\n",
       "                      -7.1391e-03, -1.6948e-02, -1.7415e-02, -1.4681e-02, -7.8083e-03,\n",
       "                      -2.0165e-02, -5.9059e-03, -4.4194e-03, -4.2884e-03, -1.1293e-02,\n",
       "                      -9.9230e-03, -3.8364e-03, -1.4219e-02, -4.0983e-03, -1.3690e-02,\n",
       "                      -2.2364e-02, -1.4641e-02, -1.2634e-02, -8.8486e-03, -3.3211e-03,\n",
       "                      -1.7619e-02, -1.2570e-03, -1.2854e-02, -8.8673e-03, -1.4967e-02,\n",
       "                      -7.4872e-03, -1.1552e-02, -7.8649e-03, -1.1328e-02, -2.3680e-03,\n",
       "                      -8.2416e-03, -7.7249e-03, -1.0887e-02, -3.0395e-03, -1.2483e-02,\n",
       "                      -8.0772e-03, -1.3910e-02, -2.0045e-02, -5.4072e-03, -1.0630e-02,\n",
       "                      -4.4207e-03, -1.9931e-02, -3.6460e-03, -6.2202e-03, -1.1956e-02,\n",
       "                      -5.7775e-04, -1.0218e-02, -9.5141e-03, -1.5447e-03,  2.7224e-03,\n",
       "                      -1.0474e-02, -7.0671e-03, -5.9462e-03, -7.6006e-03,  1.0227e-02,\n",
       "                      -9.9762e-03, -1.3922e-02, -1.6409e-02, -6.1319e-03, -1.1923e-02,\n",
       "                      -5.7035e-03, -8.7475e-03, -1.9728e-02, -1.7755e-02, -8.9354e-03,\n",
       "                      -1.9805e-02, -4.8679e-04, -4.4169e-03, -3.5560e-03, -9.4348e-03,\n",
       "                      -6.0739e-03, -2.5693e-02, -1.3154e-02, -2.0119e-02, -2.1778e-03,\n",
       "                      -2.4913e-03, -9.0802e-03, -5.5100e-03, -2.0668e-02, -6.7271e-03,\n",
       "                      -1.3592e-02, -1.3004e-02, -1.1879e-02, -6.0891e-03,  6.8899e-04,\n",
       "                      -1.2084e-02, -1.5520e-02, -6.0007e-03, -1.2843e-02, -7.2221e-03,\n",
       "                      -6.5592e-03, -1.9093e-02, -6.3959e-03, -6.6797e-03, -4.3381e-03,\n",
       "                      -1.1750e-02, -8.9538e-03, -2.2438e-02, -6.7635e-03, -1.0875e-02,\n",
       "                      -2.0524e-02, -2.8356e-03, -1.0085e-02, -1.1332e-02, -4.2801e-03,\n",
       "                      -3.6301e-03, -1.0220e-02, -1.4697e-02, -9.5091e-03, -1.5503e-02,\n",
       "                      -8.2597e-03, -1.2382e-02, -1.3733e-02, -1.3367e-02, -6.8108e-03,\n",
       "                      -1.5390e-03, -8.6496e-03, -1.0419e-02, -9.6319e-03, -5.7552e-03,\n",
       "                      -7.9548e-03, -4.3532e-03, -5.6168e-03, -1.2569e-02,  7.2388e-03,\n",
       "                      -7.5151e-03, -2.3454e-03, -1.0863e-02, -9.7658e-03, -9.5343e-03,\n",
       "                      -8.3529e-03, -1.2375e-02, -2.4484e-02, -2.2436e-02, -8.6349e-03,\n",
       "                      -3.2432e-03, -1.3814e-02, -2.7346e-03, -2.2656e-02, -7.4456e-03,\n",
       "                      -1.0810e-02, -7.1810e-03, -9.2986e-03, -7.2229e-03, -5.4100e-03,\n",
       "                      -6.5747e-03, -7.7010e-03, -1.1318e-02, -1.4738e-02, -8.9907e-03,\n",
       "                      -1.2581e-02, -9.7116e-03, -5.6638e-03, -1.3897e-02, -9.3102e-03,\n",
       "                      -7.4402e-03, -8.1638e-03, -1.7062e-02, -1.1101e-02, -6.6907e-03,\n",
       "                      -6.6875e-03, -1.8680e-02, -3.8940e-03, -1.5044e-02, -1.2732e-02,\n",
       "                      -3.2063e-03, -1.2611e-02, -1.2018e-02, -5.0075e-03, -2.6514e-02,\n",
       "                      -1.7711e-02, -8.4219e-03])),\n",
       "             ('layer4.2.bn2.running_var',\n",
       "              tensor([1.4938e-04, 3.7376e-04, 1.7154e-04, 2.6253e-04, 1.6631e-04, 2.8161e-04,\n",
       "                      8.4947e-05, 5.5696e-04, 2.0532e-04, 2.8593e-04, 2.6349e-04, 9.7785e-05,\n",
       "                      5.5512e-04, 2.4352e-04, 2.5318e-04, 1.5389e-04, 9.9659e-05, 1.5997e-04,\n",
       "                      7.6541e-05, 8.1852e-05, 9.6773e-05, 5.1976e-04, 4.4172e-04, 1.4517e-04,\n",
       "                      2.7343e-04, 3.2114e-04, 3.0572e-04, 2.4166e-04, 1.8218e-04, 1.7642e-04,\n",
       "                      3.8677e-04, 2.0194e-04, 5.3321e-05, 2.7798e-04, 3.1851e-04, 3.6590e-04,\n",
       "                      1.1782e-04, 5.0131e-04, 1.3763e-04, 5.3056e-04, 1.9045e-04, 1.7828e-04,\n",
       "                      2.3839e-04, 2.2173e-04, 1.2409e-04, 1.4249e-04, 2.1828e-04, 3.6016e-04,\n",
       "                      1.1396e-03, 9.2518e-05, 2.0790e-04, 1.3470e-04, 5.6120e-05, 8.5923e-04,\n",
       "                      2.8872e-04, 3.0446e-04, 1.2946e-04, 8.3534e-04, 2.9058e-04, 2.7161e-04,\n",
       "                      2.1065e-04, 3.1041e-04, 5.7487e-04, 4.0966e-04, 4.5125e-04, 4.2022e-04,\n",
       "                      1.7118e-04, 1.4318e-04, 3.8330e-04, 1.7554e-04, 3.6960e-05, 9.2755e-04,\n",
       "                      3.3615e-04, 1.0477e-04, 3.4859e-04, 1.0628e-04, 2.8943e-04, 8.8456e-05,\n",
       "                      1.3895e-04, 5.6326e-04, 2.9615e-04, 2.6086e-04, 5.1567e-04, 5.2602e-04,\n",
       "                      3.6552e-04, 5.8745e-04, 1.9262e-04, 9.6793e-05, 3.5357e-04, 5.3219e-04,\n",
       "                      3.9269e-04, 2.7316e-04, 7.0520e-04, 1.8122e-04, 3.3959e-04, 1.2514e-04,\n",
       "                      5.3379e-04, 2.3867e-04, 2.0563e-04, 1.6661e-04, 1.1119e-04, 1.7308e-04,\n",
       "                      6.5992e-04, 3.3133e-04, 1.7830e-04, 2.0995e-04, 9.0925e-05, 4.0726e-04,\n",
       "                      4.3537e-04, 7.0740e-04, 8.2290e-05, 3.3040e-04, 1.4042e-03, 4.5422e-05,\n",
       "                      2.3571e-04, 2.5715e-04, 3.3787e-04, 4.2280e-04, 9.1625e-05, 1.5532e-04,\n",
       "                      3.1103e-04, 2.2063e-04, 3.3331e-04, 1.3336e-04, 3.8418e-04, 2.5811e-04,\n",
       "                      3.9033e-04, 4.7310e-04, 2.7438e-04, 1.9880e-04, 3.1714e-04, 8.3830e-05,\n",
       "                      8.0048e-04, 1.2727e-04, 4.4086e-04, 2.1773e-04, 3.1847e-04, 1.4272e-04,\n",
       "                      3.1752e-04, 3.0608e-04, 5.2927e-04, 4.5649e-04, 2.5251e-04, 6.7063e-04,\n",
       "                      1.3592e-04, 2.5451e-04, 1.5062e-04, 2.3624e-04, 2.2000e-04, 2.2547e-04,\n",
       "                      3.1322e-04, 3.3586e-04, 5.1014e-04, 6.6257e-04, 5.2701e-05, 3.2992e-04,\n",
       "                      1.8116e-04, 1.0541e-04, 8.7361e-05, 9.1504e-05, 5.7402e-04, 1.3778e-04,\n",
       "                      2.0071e-04, 1.8712e-04, 6.7026e-05, 1.0654e-04, 1.4123e-04, 5.1853e-04,\n",
       "                      3.5308e-04, 2.1554e-04, 5.0294e-04, 3.3536e-04, 9.7980e-05, 1.4693e-04,\n",
       "                      3.8477e-04, 2.3370e-04, 2.8012e-04, 4.7535e-04, 2.2897e-04, 1.2073e-04,\n",
       "                      4.9532e-04, 2.6804e-04, 4.1955e-04, 2.5751e-04, 1.7305e-04, 2.4638e-04,\n",
       "                      8.5845e-05, 4.8462e-04, 1.1898e-03, 1.4858e-04, 1.8714e-04, 3.4054e-04,\n",
       "                      2.4140e-04, 9.0254e-04, 3.6196e-04, 1.7647e-04, 1.3698e-04, 2.2825e-04,\n",
       "                      2.7013e-04, 4.0778e-04, 9.1758e-05, 5.1392e-04, 5.7230e-04, 2.2395e-04,\n",
       "                      1.2932e-04, 2.8701e-04, 4.1380e-04, 3.6911e-04, 1.8621e-04, 4.1447e-04,\n",
       "                      1.4604e-04, 3.7980e-04, 2.5300e-04, 2.0412e-04, 8.7686e-04, 1.1144e-04,\n",
       "                      3.9251e-04, 8.7624e-05, 6.4889e-04, 2.1255e-04, 4.6400e-04, 4.4206e-04,\n",
       "                      1.4308e-04, 3.0601e-04, 3.6750e-04, 1.9420e-04, 1.1470e-04, 6.5784e-04,\n",
       "                      9.2074e-05, 1.4524e-04, 1.8306e-04, 3.9711e-04, 7.3701e-04, 2.0130e-04,\n",
       "                      2.7447e-04, 4.8686e-04, 2.3604e-04, 1.4143e-04, 5.1042e-04, 4.8527e-04,\n",
       "                      1.8416e-04, 1.6287e-04, 1.2991e-04, 1.6011e-04, 7.2263e-04, 9.6587e-04,\n",
       "                      2.7373e-04, 6.1086e-04, 9.5963e-05, 2.5233e-04, 1.2719e-04, 3.4212e-04,\n",
       "                      2.8807e-04, 2.5487e-04, 1.3167e-04, 1.2640e-04, 1.6409e-04, 1.5710e-04,\n",
       "                      2.6877e-04, 1.6712e-04, 7.0785e-04, 1.1009e-03, 3.3881e-04, 3.6254e-04,\n",
       "                      7.2981e-05, 1.1602e-04, 1.7680e-04, 7.2434e-05, 2.3030e-04, 5.3756e-04,\n",
       "                      8.5124e-05, 2.3727e-04, 3.1844e-04, 2.9058e-05, 3.5523e-04, 5.0026e-04,\n",
       "                      2.9738e-04, 2.3667e-04, 9.0322e-05, 1.3998e-04, 9.7723e-05, 2.2614e-04,\n",
       "                      3.1747e-04, 5.6709e-04, 2.2513e-04, 2.3827e-04, 1.9627e-04, 4.2781e-04,\n",
       "                      1.8651e-04, 1.6027e-04, 2.8687e-04, 3.3879e-04, 6.3743e-04, 6.1500e-04,\n",
       "                      1.4567e-04, 9.5536e-05, 4.7390e-05, 2.1137e-04, 1.9104e-04, 1.0233e-04,\n",
       "                      1.4667e-04, 2.8762e-04, 1.2536e-04, 2.3023e-04, 3.2692e-04, 1.2821e-04,\n",
       "                      1.5351e-04, 1.4207e-04, 3.2678e-04, 1.3759e-04, 1.0194e-04, 2.6120e-04,\n",
       "                      2.1060e-04, 2.9608e-04, 5.1166e-04, 4.2901e-04, 2.6054e-04, 1.8598e-04,\n",
       "                      3.5848e-04, 2.0803e-04, 8.3312e-05, 3.2061e-04, 2.8628e-04, 7.8978e-05,\n",
       "                      2.3462e-04, 1.1055e-04, 1.2629e-04, 1.5723e-04, 2.2127e-04, 3.8256e-04,\n",
       "                      4.8237e-04, 1.5061e-04, 2.3788e-04, 4.9429e-04, 2.9167e-04, 6.8821e-04,\n",
       "                      2.2353e-04, 5.8056e-04, 1.3561e-04, 1.4717e-04, 4.2133e-04, 5.3514e-04,\n",
       "                      2.9574e-04, 3.7472e-04, 2.7502e-04, 4.7939e-04, 3.2539e-04, 3.1197e-04,\n",
       "                      3.0792e-04, 4.9816e-04, 2.4192e-04, 1.2037e-04, 1.7068e-04, 8.5800e-05,\n",
       "                      3.2431e-04, 1.1297e-03, 4.3263e-04, 2.4347e-04, 1.2509e-04, 1.6027e-04,\n",
       "                      2.3596e-04, 3.8733e-04, 3.9728e-04, 2.3012e-04, 2.2343e-04, 2.2279e-04,\n",
       "                      3.4081e-04, 1.9329e-04, 2.3975e-04, 6.4421e-04, 3.6732e-04, 2.0679e-04,\n",
       "                      3.4231e-04, 1.0640e-04, 1.2309e-04, 2.1471e-04, 1.2050e-04, 4.3878e-04,\n",
       "                      6.4868e-04, 3.4446e-04, 6.0710e-05, 4.9282e-04, 1.6592e-04, 2.0680e-04,\n",
       "                      2.2184e-04, 5.5443e-05, 1.7376e-04, 2.6588e-04, 5.2967e-05, 6.6661e-04,\n",
       "                      1.4916e-04, 1.6088e-04, 8.9062e-05, 3.4445e-04, 4.6577e-04, 3.4056e-04,\n",
       "                      3.1910e-04, 3.5274e-04, 3.0702e-04, 1.8513e-04, 7.5114e-04, 6.9881e-05,\n",
       "                      4.2161e-04, 5.7971e-04, 2.7132e-04, 4.9185e-04, 1.9613e-04, 4.9405e-05,\n",
       "                      7.1043e-05, 6.1746e-04, 8.4835e-05, 5.6426e-04, 3.8754e-04, 7.4561e-04,\n",
       "                      6.3405e-05, 1.8931e-04, 2.3093e-04, 1.7452e-04, 2.8393e-04, 2.1311e-04,\n",
       "                      3.0291e-04, 2.1856e-04, 1.9364e-04, 1.0023e-04, 2.9551e-04, 1.8188e-04,\n",
       "                      2.2980e-04, 3.6789e-04, 6.9107e-04, 1.7543e-04, 1.6203e-04, 5.4551e-04,\n",
       "                      5.6492e-04, 5.8468e-04, 2.4741e-04, 1.3966e-04, 2.0359e-04, 5.9210e-04,\n",
       "                      4.3775e-04, 3.0128e-04, 3.6986e-04, 3.0637e-04, 1.7839e-04, 1.8203e-04,\n",
       "                      2.7404e-04, 1.7092e-04, 4.2417e-04, 4.1498e-04, 2.6961e-04, 2.6795e-04,\n",
       "                      1.3808e-04, 4.6662e-04, 7.0573e-04, 3.6256e-04, 1.6239e-04, 1.2920e-04,\n",
       "                      9.3979e-05, 3.1870e-04, 2.6703e-04, 8.7521e-05, 9.4822e-05, 1.6218e-04,\n",
       "                      1.7012e-04, 2.5183e-04, 1.7061e-04, 2.2407e-04, 8.4984e-05, 1.8750e-04,\n",
       "                      2.6772e-04, 1.8604e-04, 1.8931e-04, 2.9969e-04, 5.0111e-04, 9.7082e-04,\n",
       "                      1.6172e-04, 2.0833e-04, 3.2244e-04, 7.7057e-05, 6.7375e-04, 1.7840e-04,\n",
       "                      4.3846e-04, 2.8392e-04, 2.4869e-04, 1.6863e-04, 1.3663e-04, 1.8769e-04,\n",
       "                      5.8295e-05, 2.7881e-04, 2.6424e-04, 3.5545e-04, 4.9624e-04, 1.0936e-04,\n",
       "                      2.9948e-04, 2.8375e-04, 2.8384e-04, 2.5825e-04, 1.9669e-04, 2.5034e-04,\n",
       "                      4.5820e-04, 6.3990e-05, 3.6805e-04, 2.3502e-04, 1.3855e-04, 1.3342e-04,\n",
       "                      2.8636e-04, 4.5576e-04, 1.7397e-04, 2.4153e-04, 2.3707e-04, 5.7785e-04,\n",
       "                      5.3736e-04, 1.6554e-04])),\n",
       "             ('layer4.2.bn2.num_batches_tracked', tensor(111435)),\n",
       "             ('layer4.2.conv3.weight',\n",
       "              tensor([[[[-1.4709e-04]],\n",
       "              \n",
       "                       [[ 6.9087e-04]],\n",
       "              \n",
       "                       [[ 3.3444e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.9731e-04]],\n",
       "              \n",
       "                       [[ 2.2693e-05]],\n",
       "              \n",
       "                       [[-8.7219e-05]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7319e-03]],\n",
       "              \n",
       "                       [[-6.2291e-03]],\n",
       "              \n",
       "                       [[-1.3705e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.1992e-04]],\n",
       "              \n",
       "                       [[-2.1596e-03]],\n",
       "              \n",
       "                       [[ 2.3507e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1931e-03]],\n",
       "              \n",
       "                       [[ 1.0195e-03]],\n",
       "              \n",
       "                       [[ 4.7436e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.2202e-03]],\n",
       "              \n",
       "                       [[-2.2994e-03]],\n",
       "              \n",
       "                       [[ 2.6009e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5737e-04]],\n",
       "              \n",
       "                       [[-2.1117e-04]],\n",
       "              \n",
       "                       [[-4.2671e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.6600e-03]],\n",
       "              \n",
       "                       [[-3.8119e-04]],\n",
       "              \n",
       "                       [[-4.9615e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0642e-03]],\n",
       "              \n",
       "                       [[-6.5770e-04]],\n",
       "              \n",
       "                       [[ 3.9933e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.1652e-03]],\n",
       "              \n",
       "                       [[-1.2131e-03]],\n",
       "              \n",
       "                       [[-3.2760e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.8184e-04]],\n",
       "              \n",
       "                       [[ 7.7705e-04]],\n",
       "              \n",
       "                       [[-9.9235e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.8837e-03]],\n",
       "              \n",
       "                       [[-2.1044e-03]],\n",
       "              \n",
       "                       [[ 8.8692e-04]]]])),\n",
       "             ('layer4.2.bn3.weight',\n",
       "              tensor([ 0.0057,  0.1005,  0.0743,  ...,  0.0350, -0.0886,  0.1194])),\n",
       "             ('layer4.2.bn3.bias',\n",
       "              tensor([0.0011, 0.0152, 0.0129,  ..., 0.0061, 0.0108, 0.0161])),\n",
       "             ('layer4.2.bn3.running_mean',\n",
       "              tensor([ 0.0001, -0.0038,  0.0001,  ..., -0.0006, -0.0003,  0.0010])),\n",
       "             ('layer4.2.bn3.running_var',\n",
       "              tensor([5.8874e-07, 5.3573e-05, 2.7357e-05,  ..., 9.7393e-06, 3.8379e-05,\n",
       "                      5.8101e-05])),\n",
       "             ('layer4.2.bn3.num_batches_tracked', tensor(111435)),\n",
       "             ('fc.weight',\n",
       "              tensor([[-0.0184,  0.0816, -0.0163,  ..., -0.0342,  0.0448, -0.0544],\n",
       "                      [ 0.0003,  0.0403, -0.0185,  ...,  0.0205, -0.0322, -0.0295],\n",
       "                      [ 0.0091, -0.0840,  0.0311,  ...,  0.0045,  0.0649, -0.0696],\n",
       "                      ...,\n",
       "                      [ 0.0094,  0.0746,  0.0804,  ...,  0.0256, -0.0475, -0.0189],\n",
       "                      [ 0.0029, -0.0452, -0.0397,  ..., -0.0032, -0.0340,  0.1742],\n",
       "                      [ 0.0032,  0.0379, -0.0168,  ...,  0.0461, -0.0391, -0.0157]])),\n",
       "             ('fc.bias',\n",
       "              tensor([ 0.0395, -0.0778,  0.0263,  0.0457,  0.0559,  0.0091,  0.0036, -0.0234,\n",
       "                      -0.0177, -0.0613]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
=======
   "execution_count": 48,
>>>>>>> QAT_experiments
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Processing pruning factor: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QuantizedResNet18:\n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 2048]) from checkpoint, the shape in current model is torch.Size([10, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m student_path \u001b[38;5;241m=\u001b[39m student_path_template\u001b[38;5;241m.\u001b[39mformat(scaling_factor\u001b[38;5;241m=\u001b[39mpruning_factor)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the student network\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m student_model \u001b[38;5;241m=\u001b[39m \u001b[43mnetworks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStudentNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#student_model = student_model.to(fast_device)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load the checkpoint\u001b[39;00m\n\u001b[0;32m     14\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(student_path)\n",
      "File \u001b[1;32mc:\\Users\\daniel\\dsc180b\\DSC180B_Q2_Project\\networks.py:132\u001b[0m, in \u001b[0;36mStudentNetwork.__init__\u001b[1;34m(self, prune_amount, teacher_net, q, fuse, qat, dif_arch)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmaxpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantizedResNet18:\n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\". \n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 2048]) from checkpoint, the shape in current model is torch.Size([10, 512])."
     ]
    }
   ],
   "source": [
    "pruning_factor= 0\n",
    "print(f\"Processing pruning factor: {pruning_factor}\")\n",
    "\n",
    "student_path_template = \"checkpoints_student_QAT/checkpoint_epoch_150.tar\"\n",
    "\n",
    "# Generate the student path based on the pruning factor\n",
    "student_path = student_path_template.format(scaling_factor=pruning_factor)\n",
    "\n",
    "# Initialize the student network\n",
    "student_model = networks.StudentNetwork(pruning_factor, teacher_net, q=True)\n",
    "#student_model = student_model.to(fast_device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(student_path)\n",
    "student_model.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "student_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\3884408939.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\3884408939.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8577\n",
      "test accuracy:  0.8928\n",
      "test accuracy:  0.9065\n",
      "test accuracy:  0.9135\n",
      "test accuracy:  0.9159\n",
      "test accuracy:  0.9217\n",
      "test accuracy:  0.926\n",
      "test accuracy:  0.9257\n",
      "test accuracy:  0.9282\n",
      "test accuracy:  0.9313\n",
      "test accuracy:  0.935\n",
      "test accuracy:  0.9325\n",
      "test accuracy:  0.9334\n",
      "test accuracy:  0.933\n",
      "test accuracy:  0.9379\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved model\n",
    "\n",
    "pruning_factor = 0\n",
    "teacher_path = \"checkpoints_teacher/dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_final.tar\"\n",
    "\n",
    "# Initialize the network\n",
    "teacher_net = networks.TeacherNetwork50()\n",
    "teacher_net = teacher_net.to(fast_device)\n",
    "\n",
    "checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
    "\n",
    "\n",
    "teacher_net.model.load_state_dict(checkpoint)\n",
    "teacher_net.to(fast_device)\n",
    "\n",
    "checkpoints = [x for x in range(25, 376, 25)]\n",
    "test_accs = []\n",
    "\n",
    "for cp in checkpoints:\n",
    "    student_net = networks.StudentNetwork(teacher_net, q=True, fuse=True, qat=True, dif_arch=True)\n",
    "    checkpoint = torch.load(f'student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n",
    "\n",
    "    student_net.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
    "    prepared_student = torch.quantization.prepare_qat(student_net)\n",
    "\n",
    "    prepared_student.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    quantized_model = torch.quantization.convert(prepared_student)\n",
    "    reproducibilitySeed()\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, 'cpu')\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    test_accs.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\3250247352.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'student_BiT_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAHHCAYAAABwaWYjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcG0lEQVR4nO3dd1QU198G8GcpuyBVkKqIBbGixobYW8RG7LELBksS0NjLm8SuWKLRGEtMoqjBmNhiYkdBjRFLUCyo2LEBIhaKSL3vHzk7P5c+uDR9Pufs0Z25e/c7s8PwMHNnViGEECAiIiKiAtMp6QKIiIiIyhoGKCIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJgYoIiIiIpkYoIiIiIhkYoAiIiIikokBioiIiEgmBiiZqlSpAi8vr5Iu4523dOlSVKtWDbq6umjYsKHW+r137x4UCgX8/f0L/dpvvvlGa/W8i44dOwaFQoFjx45J07y8vFClSpUSqymrnGrUhvd9/6BQKODr66u1/vz9/aFQKPDvv//m27Zdu3Zo166d9Dynn/XZs2dDoVDIeu979+7JrJreF+91gMrvh7Ndu3aoV6/eW7/P/v37MXv27Lfu531x+PBhTJ06FS1btsTGjRuxcOHCXNt6eXnB2Ng41/na3qGXBLnbT7t27aBQKKSHhYUFmjZtig0bNiAzM7PoCi0CCxcuxB9//FHSZeD27dsYM2YMqlWrBgMDA5iamqJly5ZYuXIlkpOTS7q8PKn3c+qHgYEBnJ2d4evri5iYmJIur8SVlm2sJBw7dgx9+vSBra0tlEolrK2t4eHhgV27dkltcvrDUf0HSE6PgQMHarzH/v37oVAoYG9vn+v+p0qVKhp9GBkZoVmzZti8eXOBl+W3337D0KFDUaNGDSgUCo0wnVVKSgqmTZsGe3t7GBoawtXVFYGBgQV+LzU92a94z0VEREBHR17u3L9/P1avXs0QVUBBQUHQ0dHBzz//DKVSqdW+HR0dkZycDH19fa32W5QKs/1UqlQJfn5+AIDY2Fhs3rwZ3t7euHHjBhYtWlRElebuxx9/LFR4W7hwIfr164devXppv6gC2rdvH/r37w+VSoXhw4ejXr16SE1NxcmTJzFlyhSEh4dj/fr1JVZfQc2dOxdVq1bF69evcfLkSaxduxb79+/HlStXUK5cuZIu760dPnw43zZfffUVpk+frjEtt21s2LBhGDhwIFQqlTbLLDVmzZqFuXPnokaNGhgzZgwcHR0RFxeH/fv3o2/fvggICMDgwYPz7GPcuHFo2rSpxrSsR5oDAgJQpUoV3Lt3D0FBQejUqVOOfTVs2BCTJk0CAERFReGnn36Cp6cnUlJSMGrUqHyXZ+3atQgNDUXTpk0RFxeXZ1svLy/s2LED48ePR40aNeDv749u3bohODgYrVq1yve91BigZCqLP0xJSUkwMjIq6TIK7MmTJzA0NNR6eAIg/QX+rjMzM8PQoUOl52PGjEHNmjXx/fffY968eTkGyMzMTKSmphbJ+ilLgfVNd+/excCBA+Ho6IigoCDY2dlJ83x8fHDr1i3s27evBCssuK5du6JJkyYAgJEjR8LS0hLLly/Hnj17MGjQoBxfU5b2HQXZX+jp6UFPr2C/9nR1daGrq/u2ZZVKO3bswNy5c9GvXz9s3bpV4+dzypQpOHToENLS0vLtp3Xr1ujXr1+u85OSkrBnzx74+flh48aNCAgIyDVAVaxYUWOf5eXlhWrVquHbb78tUIDasmULKlasCB0dnTzPHJ09exbbtm3D0qVLMXnyZACQ/jCaOnUqTp06le97qb3Xp/AKI+sYh7S0NMyZMwc1atSAgYEBLC0t0apVK+lwoJeXF1avXg0AGoco1ZKSkjBp0iQ4ODhApVKhZs2a+OabbyCE0Hjf5ORkjBs3DhUqVICJiQk++ugjPHr0CAqFQuPIhPoc/9WrVzF48GCUL19eStSXLl2SNkoDAwPY2trik08+yZbW1X3cuHEDQ4cOhZmZGaysrPD1119DCIEHDx6gZ8+eMDU1ha2tLZYtW1agdZeeno558+ahevXqUKlUqFKlCv7v//4PKSkpUhuFQoGNGzciKSlJWleFGa+Um9zGQG3fvh116tSBgYEB6tWrh927d+c5bmf9+vXScjRt2hTnzp3L1ub69evo168fLCwsYGBggCZNmuDPP//UaPO2209BlStXDs2bN0dSUhJiY2Ol/nx9fREQEIC6detCpVLh4MGDAIBHjx7hk08+gY2NDVQqFerWrYsNGzZk6/fhw4fo1asXjIyMYG1tjQkTJmh8nmo5rcvMzEysXLkSLi4uMDAwgJWVFbp06SKdUlcoFEhKSsKmTZuk5X7zZ0/bNeZkyZIlSExMxM8//6wRntScnJzwxRdf5Pr6Z8+eYfLkyXBxcYGxsTFMTU3RtWtXXLx4MVvbVatWoW7duihXrhzKly+PJk2aYOvWrdL8hIQEjB8/HlWqVIFKpYK1tTU+/PBDnD9/vkDLklWHDh0A/BcSgf+dDr99+za6desGExMTDBkyBEDB91NqAQEBqFmzJgwMDNC4cWOcOHFCY35kZCQ+//xz1KxZE4aGhrC0tET//v1zHW/06tUrjBkzBpaWljA1NcXw4cPx/PlzjTZZx0DlJOsYqLy2sdzGQB04cACtW7eGkZERTExM0L17d4SHh2u0iY6OxogRI1CpUiWoVCrY2dmhZ8+eBRpPFRQUJPVvbm6Onj174tq1azkux61bt+Dl5QVzc3OYmZlhxIgRePXqVb7v8fXXX8PCwgIbNmzI8Y8bd3d39OjRI99+8rN7924kJyejf//+GDhwIHbt2oXXr18X6LVWVlaoVasWbt++XaD2Dg4OBTo7tGPHDujq6mL06NHSNAMDA3h7eyMkJAQPHjwo0PsBPAIFAHj58iWePn2abXpBEvjs2bPh5+eHkSNHolmzZoiPj8e///6L8+fP48MPP8SYMWPw+PFjBAYGYsuWLRqvFULgo48+QnBwMLy9vdGwYUMcOnQIU6ZMwaNHj/Dtt99Kbb28vPD7779j2LBhaN68OY4fP47u3bvnWlf//v1Ro0YNLFy4UNrJBQYG4s6dOxgxYgRsbW2lUw/h4eE4ffp0tl/MAwYMQO3atbFo0SLs27cP8+fPh4WFBX744Qd06NABixcvRkBAACZPnoymTZuiTZs2ea6rkSNHYtOmTejXrx8mTZqEM2fOwM/PD9euXcPu3bsB/PdXxPr163H27Fn89NNPAIAWLVrk+znk9PkV1L59+zBgwAC4uLjAz88Pz58/h7e3NypWrJhj+61btyIhIQFjxoyBQqHAkiVL0KdPH9y5c0faGYWHh6Nly5aoWLEipk+fDiMjI/z+++/o1asXdu7cid69ewN4u+1Hrjt37kBXVxfm5ubStKCgIPz+++/w9fVFhQoVUKVKFcTExKB58+ZSwLKyssKBAwfg7e2N+Ph4jB8/HsB/ob5jx464f/8+xo0bB3t7e2zZsgVBQUEFqsfb2xv+/v7o2rUrRo4cifT0dPz99984ffo0mjRpgi1btkjrRb2zq169OgAUW41//fUXqlWrVqBtMCd37tzBH3/8gf79+6Nq1aqIiYnBDz/8gLZt2+Lq1auwt7cH8N8pznHjxqFfv3744osv8Pr1a1y6dAlnzpyRTqN8+umn2LFjB3x9fVGnTh3ExcXh5MmTuHbtGho1aiS7NvUvJktLS2laeno63N3d0apVK3zzzTcoV66crP0UABw/fhy//fYbxo0bB5VKhTVr1qBLly44e/asdGTg3LlzOHXqFAYOHIhKlSrh3r17WLt2Ldq1a4erV69mO6Xo6+sLc3NzzJ49GxEREVi7di0iIyOlsTiFldc2llt7T09PuLu7Y/HixXj16hXWrl2LVq1a4cKFC9IfCX379kV4eDjGjh2LKlWq4MmTJwgMDMT9+/fzvJjiyJEj6Nq1K6pVq4bZs2cjOTkZq1atQsuWLXH+/Plsr/34449RtWpV+Pn54fz58/jpp59gbW2NxYsX5/oeN2/exPXr1/HJJ5/AxMSkwOsqJwkJCdn2vRYWFlKQCQgIQPv27WFra4uBAwdi+vTp+Ouvv9C/f/98+05PT8fDhw9Rvnz5t6oxqwsXLsDZ2RmmpqYa05s1awYACAsLg4ODQ8E6E++xjRs3CgB5PurWravxGkdHR+Hp6Sk9b9CggejevXue7+Pj4yNyWtV//PGHACDmz5+vMb1fv35CoVCIW7duCSGECA0NFQDE+PHjNdp5eXkJAGLWrFnStFmzZgkAYtCgQdne79WrV9mm/frrrwKAOHHiRLY+Ro8eLU1LT08XlSpVEgqFQixatEia/vz5c2FoaKixTnISFhYmAIiRI0dqTJ88ebIAIIKCgqRpnp6ewsjIKM/+3myb32fo4+Mjtb97964AIDZu3ChNc3FxEZUqVRIJCQnStGPHjgkAwtHRMdtrLS0txbNnz6Tpe/bsEQDEX3/9JU3r2LGjcHFxEa9fv5amZWZmihYtWogaNWpI095m+8lN27ZtRa1atURsbKyIjY0V165dE+PGjRMAhIeHh9QOgNDR0RHh4eEar/f29hZ2dnbi6dOnGtMHDhwozMzMpO1oxYoVAoD4/fffpTZJSUnCyclJABDBwcHSdE9PT411GRQUJACIcePGZas/MzNT+r+RkVGO21ZR1JjVy5cvBQDRs2fPXNtklXX/8Pr1a5GRkaHR5u7du0KlUom5c+dK03r27JltX5OVmZmZxrZcUOr93JEjR0RsbKx48OCB2LZtm7C0tBSGhobi4cOHQoj//SxNnz5d4/UF3U8JIaSfuX///VeaFhkZKQwMDETv3r2laTnti0JCQgQAsXnz5my1N27cWKSmpkrTlyxZIgCIPXv2SNPatm0r2rZtKz3P6WddvW97U27bmPq97969K4QQIiEhQZibm4tRo0ZptIuOjhZmZmbS9OfPnwsAYunSpdn6zE/Dhg2FtbW1iIuLk6ZdvHhR6OjoiOHDh2dbjk8++UTj9b179xaWlpZ5vod6f/Xtt98WqCb1enxzeYKDg3Pd36rXV0xMjNDT0xM//vij9LoWLVrk+PPk6OgoOnfuLO2zLl++LIYNG5Zt/11QdevW1dgWss7r0KFDtunh4eECgFi3bl2B34en8ACsXr0agYGB2R7169fP97Xm5uYIDw/HzZs3Zb/v/v37oauri3HjxmlMnzRpEoQQOHDgAABIp1U+//xzjXZjx47Nte9PP/002zRDQ0Pp/69fv8bTp0/RvHlzAMjxNMDIkSOl/+vq6qJJkyYQQsDb21uabm5ujpo1a+LOnTu51gL8t6wAMHHiRI3p6kGDbzOOxMDAIMfPryBXVTx+/BiXL1/G8OHDNa7ma9u2LVxcXHJ8zYABAzT+KmrdujUASOvg2bNnCAoKwscffyz9hfb06VPExcXB3d0dN2/exKNHjwC83faTl+vXr8PKygpWVlaoXbs2Vq1ahe7du2c7xdW2bVvUqVNHei6EwM6dO+Hh4QEhhFT706dP4e7ujpcvX0rbyv79+2FnZ6cxBqJcuXIah8Zzs3PnTigUCsyaNSvbvPyOKBRXjfHx8QDwVn+lq1Qq6a/xjIwMxMXFwdjYGDVr1tT4mTM3N8fDhw9zPBX8ZpszZ87g8ePHhaqlU6dOsLKygoODAwYOHAhjY2Ps3r0725HWzz77TON5QfdTam5ubmjcuLH0vHLlyujZsycOHTqEjIwMAJr7orS0NMTFxcHJyQnm5uY57otGjx6tcarps88+g56enrRfKQ6BgYF48eIFBg0apLHN6erqwtXVFcHBwQAgjd88duxYttOMeYmKikJYWBi8vLxgYWEhTa9fvz4+/PDDHJc1636+devWiIuLk7bdnGhju1abOXNmtn2ura0tAGDbtm3Q0dFB3759pfaDBg3CgQMHclwvhw8flvZZLi4u2LJlC0aMGIGlS5e+dZ1vSk5OznEss3rsp5yrankKD/8dulMPrnxT+fLl8z01NHfuXPTs2RPOzs6oV68eunTpgmHDhhUofEVGRsLe3j7bhly7dm1pvvpfHR0dVK1aVaOdk5NTrn1nbQv894t9zpw52LZtG548eaIx7+XLl9naV65cWeO5mZkZDAwMUKFChWzT87vqQb0MWWu2tbWFubm5tKyFoaurm+vAxPyo3zendenk5JTjzjzrelGHKfVO4datWxBC4Ouvv8bXX3+d4/s+efIEFStWfKvtJy9VqlTBjz/+KA2ar1GjBqytrbO1y7qdxMbG4sWLF1i/fn2uV5apt53IyEg4OTllCzw1a9bMt77bt2/D3t5e4xdFQRVXjepD/AkJCbJrVFOP81qzZg3u3r0rBQhA89TZtGnTcOTIETRr1gxOTk7o3LkzBg8ejJYtW0ptlixZAk9PTzg4OKBx48bo1q0bhg8fjmrVqhWoltWrV8PZ2Rl6enqwsbFBzZo1s40Z0dPTQ6VKlTSmFXQ/pVajRo1s7+3s7IxXr14hNjYWtra2SE5OlgYWP3r0SGMsVU77oqx9Ghsbw87Orljv0aT+I0c9diwr9faiUqmwePFiTJo0CTY2NmjevDl69OiB4cOHS+EiJ+r1mNO2Wbt2bRw6dCjboP689kVZT1FlrfNttms1FxeXXPe9v/zyC5o1a4a4uDjp98MHH3yA1NRUbN++PdsfMa6urpg/fz4yMjJw5coVzJ8/H8+fP9e4OODZs2dITU2VnhsaGsLMzExWzYaGhjmOgVSPzXoz3OeHAeottWnTBrdv38aePXtw+PBh/PTTT/j222+xbt06jSM4xS2njeDjjz/GqVOnMGXKFDRs2BDGxsbIzMxEly5dcrzEPKcrUHK7KkXkMpg0q7cZr1Ba5LcO1Oty8uTJcHd3z7GtOrAV1fZjZGRUoFCZdTtR1z506FB4enrm+Jq3DXdvq7hqNDU1hb29Pa5cuVLoPhYuXIivv/4an3zyCebNmyeNDxk/frzGz1zt2rURERGBvXv34uDBg9i5cyfWrFmDmTNnYs6cOQD++/lt3bo1du/ejcOHD2Pp0qVYvHgxdu3aha5du+ZbS25/KL7pzSNmRWns2LHYuHEjxo8fDzc3N5iZmUn3ECqt9ypT17Vly5Ycg9CbV/eNHz8eHh4e+OOPP3Do0CF8/fXX8PPzQ1BQED744AOt1VSY/XGtWrUAAJcvX9ZaHVndvHlTOpqaU6AOCAjIFqAqVKgg7bPc3d1Rq1Yt9OjRAytXrpTOXPTp0wfHjx+XXuPp6Sn7IiM7OzvpDMCboqKiAEAal1gQDFBaYGFhgREjRmDEiBFITExEmzZtMHv2bOkXYG6hwdHREUeOHEFCQoLGX3fXr1+X5qv/zczMxN27dzU2xlu3bhW4xufPn+Po0aOYM2cOZs6cKU3X9qmj3KiX4ebNm9JfrsB/g4FfvHghLWtxU79vTutSzvp9k/qIgL6+foFCTGG3n6JgZWUFExMTZGRk5Fu7o6Mjrly5AiGERo0RERH5vk/16tVx6NAhPHv2LM+jUDkte3HVCAA9evTA+vXrERISAjc3twK95k07duxA+/bt8fPPP2tMf/HiRbYjuUZGRhgwYAAGDBiA1NRU9OnTBwsWLMCMGTOk0wt2dnb4/PPP8fnnn+PJkydo1KgRFixYUKAAVVgF3U+p5bRPuXHjBsqVKwcrKysA/60XT09PjSt4X79+jRcvXuRYw82bN9G+fXvpeWJiIqKiotCtW7dCL5daQX++1IPLra2tC/RzXb16dUyaNAmTJk3CzZs30bBhQyxbtgy//PJLju3V6zGnbfP69euoUKGCVm4p4ezsjJo1a2LPnj1YuXJlnjciLqyAgADo6+tjy5Yt2ULeyZMn8d133+H+/fvZjqC9qXv37mjbti0WLlyIMWPGwMjICMuWLdM4/Scn7Kg1bNgQwcHBiI+P1zhKd+bMGWl+QXEM1FvKeurK2NgYTk5OGocI1Rt91p1Dt27dkJGRge+//15j+rfffguFQiHtFNVHMdasWaPRbtWqVQWuU70RZ/3LZMWKFQXu422od3RZ32/58uUAkOcVhUXJ3t4e9erVw+bNm5GYmChNP378eKH/QrO2tka7du3www8/SH/VvEl9GwHg7bafoqCrq4u+ffti586dOR55ebP2bt264fHjx9ixY4c07dWrVwW6qWTfvn0hhJCOrrzpzW3UyMgo23IXV40AMHXqVBgZGWHkyJE53rX79u3bWLlyZa6v19XVzfYzt3379mx/AWfdDpRKJerUqQMhBNLS0pCRkZHt1Ja1tTXs7e0LfEuGwirofkotJCRE49T3gwcPsGfPHnTu3FnaD+W0XlatWqVxivNN69ev17gqeu3atUhPT9dKcMxpG8uJu7s7TE1NsXDhwhyv0FZvd69evcp2qX716tVhYmKS52dlZ2eHhg0bYtOmTRr1XLlyBYcPH9ZKWFSbM2cO4uLipKtfszp8+DD27t1b6P4DAgLQunVrDBgwAP369dN4TJkyBQDw66+/5tvPtGnTEBcXhx9//BEA0LhxY3Tq1El6vDl+s6D69euHjIwMjX1ASkoKNm7cCFdX14JfgQcegXprderUQbt27dC4cWNYWFjg33//lS41VlMPqBw3bhzc3d2hq6uLgQMHwsPDA+3bt8eXX36Je/fuoUGDBjh8+DD27NmD8ePHS3/xNG7cGH379sWKFSsQFxcn3cbgxo0bAAr2F5SpqSnatGmDJUuWIC0tDRUrVsThw4ele8AUtQYNGsDT0xPr16/Hixcv0LZtW5w9exabNm1Cr169NP66LG4LFy5Ez5490bJlS4wYMQLPnz/H999/j3r16mmEKjlWr16NVq1awcXFBaNGjUK1atUQExODkJAQPHz4ULoP0NtsP0Vl0aJFCA4OhqurK0aNGoU6derg2bNnOH/+PI4cOYJnz54BAEaNGoXvv/8ew4cPR2hoKOzs7LBly5YC3dW6ffv2GDZsGL777jvcvHlTOo38999/o3379tLyN27cGEeOHMHy5cthb2+PqlWrwtXVtVhqBP77xbd161bplh5v3on81KlT2L59e57ffdejRw/MnTsXI0aMQIsWLXD58mUEBARkG7fUuXNn2NraomXLlrCxscG1a9fw/fffo3v37jAxMcGLFy9QqVIl9OvXDw0aNICxsTGOHDmCc+fOFfg+bIVV0P2UWr169eDu7q5xGwMAGmG5R48e2LJlC8zMzFCnTh2EhITgyJEjGuPC3pSamoqOHTvi448/RkREBNasWYNWrVrho48+euvly20by8rU1BRr167FsGHD0KhRIwwcOBBWVla4f/8+9u3bh5YtW+L777/HjRs3pFrr1KkDPT097N69GzExMfn+3C5duhRdu3aFm5sbvL29pdsYmJmZafWbLAYMGIDLly9jwYIFuHDhAgYNGiTdifzgwYM4evSoxj3I5Dhz5gxu3bqV61doVaxYEY0aNUJAQACmTZuWZ19du3ZFvXr1sHz5cvj4+OR5Q94TJ05I9xuLjY1FUlIS5s+fD+C/oRLq2+y4urqif//+mDFjBp48eQInJyds2rQJ9+7dy3akOF8Fvl7vHaS+TPXcuXM5zm/btm2+tzGYP3++aNasmTA3NxeGhoaiVq1aYsGCBRqX3Kanp4uxY8cKKysroVAoNC6jTUhIEBMmTBD29vZCX19f1KhRQyxdulTjUm4h/rv02sfHR1hYWAhjY2PRq1cvERERIQBo3FZAfXlrbGxstuV5+PCh6N27tzA3NxdmZmaif//+4vHjx7neCiFrH7ndXiCn9ZSTtLQ0MWfOHFG1alWhr68vHBwcxIwZMzQu9c/rfXKSX1sU4DYGQgixbds2UatWLaFSqUS9evXEn3/+Kfr27Stq1aqV7bU5XZ6cdR0KIcTt27fF8OHDha2trdDX1xcVK1YUPXr0EDt27JDavO32k5OCfh5Z182bYmJihI+Pj3BwcBD6+vrC1tZWdOzYUaxfv16jXWRkpPjoo49EuXLlRIUKFcQXX3whDh48mO9tDNTLtXTpUlGrVi2hVCqFlZWV6Nq1qwgNDZXaXL9+XbRp00YYGhoKABo/e9quMS83btwQo0aNElWqVBFKpVKYmJiIli1bilWrVmlsvzndxmDSpEnCzs5OGBoaipYtW4qQkJBsl9z/8MMPok2bNsLS0lKoVCpRvXp1MWXKFPHy5UshhBApKSliypQpokGDBsLExEQYGRmJBg0aiDVr1uRbe377ObW8fpYKup9Sb1O//PKLqFGjhlCpVOKDDz7Itp6fP38uRowYISpUqCCMjY2Fu7u7uH79erb1p679+PHjYvTo0aJ8+fLC2NhYDBkyRONSfyEKfxuD3LaxrLcxUAsODhbu7u7CzMxMGBgYiOrVqwsvLy/p1g1Pnz4VPj4+olatWsLIyEiYmZkJV1dXjVtp5OXIkSOiZcuWwtDQUJiamgoPDw9x9epVjTa57aNzqzk3R48eFT179hTW1tZCT09PWFlZCQ8PD43bQ+R1G4Pt27dn63Ps2LECgLh9+3au7zt79mwBQFy8eFEI8d/PTW63c/H3989xn52Vep3k9Mi6b05OThaTJ08Wtra2QqVSiaZNm4qDBw/m2X9OFEIUcPQvlTphYWH44IMP8Msvv0h3DCbtadiwIaysrAr1JZNERPRu4xioMiKne1OsWLECOjo6+d4BnPKWlpaWbRzAsWPHcPHixXy/GoKIiN5PHANVRixZsgShoaFo37499PT0cODAARw4cACjR4+WNeiNsnv06BE6deqEoUOHwt7eHtevX8e6detga2ub4w1JiYiIeAqvjAgMDMScOXNw9epVJCYmonLlyhg2bBi+/PLLAn+7OOXs5cuXGD16NP755x/ExsbCyMgIHTt2xKJFi/L8XiwiInp/MUARERERycQxUEREREQyMUARERERycTBMwWQmZmJx48fw8TE5J34LjciIqL3gRACCQkJsLe31/r3PDJAFcDjx495pRsREVEZ9eDBA1SqVEmrfTJAFYD6CzQfPHig8eWDREREVHrFx8fDwcFB44uwtYUBqgDUp+1MTU0ZoIiIiMqYohh+U6KDyP38/NC0aVOYmJjA2toavXr1QkREhEabdu3aQaFQaDyy3tzw/v376N69O8qVKwdra2tMmTIlxztLN2rUCCqVCk5OTvD39y/qxSMiIqJ3VIkGqOPHj8PHxwenT59GYGAg0tLS0LlzZyQlJWm0GzVqFKKioqTHkiVLpHkZGRno3r279O3omzZtgr+/P2bOnCm1uXv3Lrp374727dsjLCwM48ePx8iRI3Ho0KFiW1YiIiJ6d5SqG2nGxsbC2toax48fl77frV27dmjYsCFWrFiR42sOHDiAHj164PHjx7CxsQEArFu3DtOmTUNsbCyUSiWmTZuGffv24cqVK9LrBg4ciBcvXuDgwYP51hUfHw8zMzO8fPmSp/CIiIjKiKL8/V2q7gP18uVLAICFhYXG9ICAAFSoUAH16tXDjBkz8OrVK2leSEgIXFxcpPAEAO7u7oiPj0d4eLjUplOnThp9uru7IyQkJMc6UlJSEB8fr/EgIiIiUis1g8gzMzMxfvx4tGzZEvXq1ZOmDx48GI6OjrC3t8elS5cwbdo0REREYNeuXQCA6OhojfAEQHoeHR2dZ5v4+HgkJyfD0NBQY56fnx/mzJmj9WUkIiKid0OpCVA+Pj64cuUKTp48qTF99OjR0v9dXFxgZ2eHjh074vbt20X2Ra8zZszAxIkTpefqyyCJiIiIgFJyCs/X1xd79+5FcHBwvje6cnV1BQDcunULAGBra4uYmBiNNurntra2ebYxNTXNdvQJAFQqlXTLAt66gIiIiLIq0QAlhICvry92796NoKAgVK1aNd/XhIWFAQDs7OwAAG5ubrh8+TKePHkitQkMDISpqSnq1KkjtTl69KhGP4GBgXBzc9PSkhAREdH7pEQDlI+PD3755Rds3boVJiYmiI6ORnR0NJKTkwEAt2/fxrx58xAaGop79+7hzz//xPDhw9GmTRvUr18fANC5c2fUqVMHw4YNw8WLF3Ho0CF89dVX8PHxgUqlAgB8+umnuHPnDqZOnYrr169jzZo1+P333zFhwoQSW3YiIiIqu0r0Nga53Rl048aN8PLywoMHDzB06FBcuXIFSUlJcHBwQO/evfHVV19pnFaLjIzEZ599hmPHjsHIyAienp5YtGgR9PT+N8Tr2LFjmDBhAq5evYpKlSrh66+/hpeXV4Hq5G0MiIiIyp6i/P1dqu4DVVoxQBEREZU97819oIiIiIjKAgYoIiIiIpkYoIiIiIhkKjU30iQiIiLtiI2NLZKvITM1NYWVlZXW+y2LGKCIiIjeIbGxsRg8+DPExaVovW9LSxW2bl3LEAUGKCIiohJRVEeJIiMjEROTBCOjaTA01N7XkCUnP0Bc3DLEx8czQIEBioiIqNgV5VGilJQkPHgQgwYNrGFkpN3vjE3RfrllFgMUERFRMYuPj0dcXApUqklaPUoEAM+fn0Z6+gKkp2dotV/SxABFRERUQgwNHbR+lCg5OVKr/VHOeBsDIiIiIpkYoIiIiIhkYoAiIiIikokBioiIiEgmBigiIiIimRigiIiIiGRigCIiIiKSiQGKiIiISCYGKCIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJgYoIiIiIpkYoIiIiIhkYoAiIiIikokBioiIiEgmBigiIiIimRigiIiIiGRigCIiIiKSiQGKiIiISCYGKCIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJgYoIiIiIpkYoIiIiIhkYoAiIiIikokBioiIiEgmBigiIiIimRigiIiIiGRigCIiIiKSiQGKiIiISCYGKCIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJr2SLoCIiIjKhrS0FERGRhZJ36amprCysiqSvosCAxQRERHlKzU1DpGRdzB27CKoVCqt929pqcLWrWvLTIhigCIiIqJ8ZWQkIj1dCaVyAszNnbXad3LyA8TFLUN8fDwDFBEREb17DAwqwcioutb7TUnRepdFioPIiYiIiGRigCIiIiKSiQGKiIiISCYGKCIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJgYoIiIiIpkYoIiIiIhkYoAiIiIikokBioiIiEgmBigiIiIimUo0QPn5+aFp06YwMTGBtbU1evXqhYiICI02r1+/ho+PDywtLWFsbIy+ffsiJiZGo839+/fRvXt3lCtXDtbW1pgyZQrS09M12hw7dgyNGjWCSqWCk5MT/P39i3rxiIiI6B1VogHq+PHj8PHxwenTpxEYGIi0tDR07twZSUlJUpsJEybgr7/+wvbt23H8+HE8fvwYffr0keZnZGSge/fuSE1NxalTp7Bp0yb4+/tj5syZUpu7d++ie/fuaN++PcLCwjB+/HiMHDkShw4dKtblJSIioneDXkm++cGDBzWe+/v7w9raGqGhoWjTpg1evnyJn3/+GVu3bkWHDh0AABs3bkTt2rVx+vRpNG/eHIcPH8bVq1dx5MgR2NjYoGHDhpg3bx6mTZuG2bNnQ6lUYt26dahatSqWLVsGAKhduzZOnjyJb7/9Fu7u7sW+3ERERFS2laoxUC9fvgQAWFhYAABCQ0ORlpaGTp06SW1q1aqFypUrIyQkBAAQEhICFxcX2NjYSG3c3d0RHx+P8PBwqc2bfajbqPsgIiIikqNEj0C9KTMzE+PHj0fLli1Rr149AEB0dDSUSiXMzc012trY2CA6Olpq82Z4Us9Xz8urTXx8PJKTk2FoaKgxLyUlBSkpKdLz+Pj4t19AIiIiemeUmiNQPj4+uHLlCrZt21bSpcDPzw9mZmbSw8HBoaRLIiIiolKkVAQoX19f7N27F8HBwahUqZI03dbWFqmpqXjx4oVG+5iYGNja2kptsl6Vp36eXxtTU9NsR58AYMaMGXj58qX0ePDgwVsvIxEREb07SjRACSHg6+uL3bt3IygoCFWrVtWY37hxY+jr6+Po0aPStIiICNy/fx9ubm4AADc3N1y+fBlPnjyR2gQGBsLU1BR16tSR2rzZh7qNuo+sVCoVTE1NNR5EREREaiU6BsrHxwdbt27Fnj17YGJiIo1ZMjMzg6GhIczMzODt7Y2JEyfCwsICpqamGDt2LNzc3NC8eXMAQOfOnVGnTh0MGzYMS5YsQXR0NL766iv4+PhApVIBAD799FN8//33mDp1Kj755BMEBQXh999/x759+0ps2YmIiKjsKtEjUGvXrsXLly/Rrl072NnZSY/ffvtNavPtt9+iR48e6Nu3L9q0aQNbW1vs2rVLmq+rq4u9e/dCV1cXbm5uGDp0KIYPH465c+dKbapWrYp9+/YhMDAQDRo0wLJly/DTTz/xFgZERERUKCV6BEoIkW8bAwMDrF69GqtXr861jaOjI/bv359nP+3atcOFCxdk10hERESUVakYRE5ERERUljBAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREclUogHqxIkT8PDwgL29PRQKBf744w+N+V5eXlAoFBqPLl26aLR59uwZhgwZAlNTU5ibm8Pb2xuJiYkabS5duoTWrVvDwMAADg4OWLJkSVEvGhEREb3DSjRAJSUloUGDBli9enWubbp06YKoqCjp8euvv2rMHzJkCMLDwxEYGIi9e/fixIkTGD16tDQ/Pj4enTt3hqOjI0JDQ7F06VLMnj0b69evL7LlIiIionebXkm+edeuXdG1a9c826hUKtja2uY479q1azh48CDOnTuHJk2aAABWrVqFbt264ZtvvoG9vT0CAgKQmpqKDRs2QKlUom7duggLC8Py5cs1ghYRERFRQZX6MVDHjh2DtbU1atasic8++wxxcXHSvJCQEJibm0vhCQA6deoEHR0dnDlzRmrTpk0bKJVKqY27uzsiIiLw/PnzHN8zJSUF8fHxGg8iIiIitUIFqDt37mi7jhx16dIFmzdvxtGjR7F48WIcP34cXbt2RUZGBgAgOjoa1tbWGq/R09ODhYUFoqOjpTY2NjYabdTP1W2y8vPzg5mZmfRwcHDQ9qIRERFRGVaoAOXk5IT27dvjl19+wevXr7Vdk2TgwIH46KOP4OLigl69emHv3r04d+4cjh07VmTvCQAzZszAy5cvpceDBw+K9P2IiIiobClUgDp//jzq16+PiRMnwtbWFmPGjMHZs2e1XVs21apVQ4UKFXDr1i0AgK2tLZ48eaLRJj09Hc+ePZPGTdna2iImJkajjfp5bmOrVCoVTE1NNR5EREREaoUKUA0bNsTKlSvx+PFjbNiwAVFRUWjVqhXq1auH5cuXIzY2Vtt1AgAePnyIuLg42NnZAQDc3Nzw4sULhIaGSm2CgoKQmZkJV1dXqc2JEyeQlpYmtQkMDETNmjVRvnz5IqmTiIiI3m1vNYhcT08Pffr0wfbt27F48WLcunULkydPhoODA4YPH46oqKg8X5+YmIiwsDCEhYUBAO7evYuwsDDcv38fiYmJmDJlCk6fPo179+7h6NGj6NmzJ5ycnODu7g4AqF27Nrp06YJRo0bh7Nmz+Oeff+Dr64uBAwfC3t4eADB48GAolUp4e3sjPDwcv/32G1auXImJEye+zaITERHRe+ytAtS///6Lzz//HHZ2dli+fDkmT56M27dvIzAwEI8fP0bPnj3zff0HH3yADz74AAAwceJEfPDBB5g5cyZ0dXVx6dIlfPTRR3B2doa3tzcaN26Mv//+GyqVSuojICAAtWrVQseOHdGtWze0atVK4x5PZmZmOHz4MO7evYvGjRtj0qRJmDlzJm9hQERERIVWqPtALV++HBs3bkRERAS6deuGzZs3o1u3btDR+S+PVa1aFf7+/qhSpUqe/bRr1w5CiFznHzp0KN9aLCwssHXr1jzb1K9fH3///Xe+fREREREVRKEC1Nq1a/HJJ5/Ay8tLGo+UlbW1NX7++ee3Ko6IiIioNCpUgLp582a+bZRKJTw9PQvTPREREVGpVqgxUBs3bsT27duzTd++fTs2bdr01kURERERlWaFClB+fn6oUKFCtunW1tZYuHDhWxdFREREVJoVKkDdv38fVatWzTbd0dER9+/ff+uiiIiIiEqzQgUoa2trXLp0Kdv0ixcvwtLS8q2LIiIiIirNChWgBg0ahHHjxiE4OBgZGRnIyMhAUFAQvvjiCwwcOFDbNRIRERGVKoW6Cm/evHm4d+8eOnbsCD29/7rIzMzE8OHDOQaKiIiI3nmFClBKpRK//fYb5s2bh4sXL8LQ0BAuLi5wdHTUdn1EREREpU6hApSas7MznJ2dtVULERERUZlQqACVkZEBf39/HD16FE+ePEFmZqbG/KCgIK0UR0RERFQaFSpAffHFF/D390f37t1Rr149KBQKbddFREREVGoVKkBt27YNv//+O7p166bteoiIiIhKvULdxkCpVMLJyUnbtRARERGVCYU6AjVp0iSsXLkS33//PU/fERHROy02Nhbx8fFa7TMyMhLp6ela7ZOKV6EC1MmTJxEcHIwDBw6gbt260NfX15i/a9curRRHRERUkmJjYzF48GeIi0vRar8pKUl48CAGZmba7ZeKT6EClLm5OXr37q3tWoiIiEqV+Ph4xMWlQKWaBENDB631+/z5aaSnL0B6eobW+qTiVagAtXHjRm3XQUREVGoZGjrAyKi61vpLTo7UWl9UMgo1iBwA0tPTceTIEfzwww9ISEgAADx+/BiJiYlaK46IiIioNCrUEajIyEh06dIF9+/fR0pKCj788EOYmJhg8eLFSElJwbp167RdJxEREVGpUagjUF988QWaNGmC58+fw9DQUJreu3dvHD16VGvFEREREZVGhToC9ffff+PUqVNQKpUa06tUqYJHjx5ppTAiIiKi0qpQR6AyMzORkZH9yoGHDx/CxMTkrYsiIiIiKs0KFaA6d+6MFStWSM8VCgUSExMxa9Ysfr0LERERvfMKdQpv2bJlcHd3R506dfD69WsMHjwYN2/eRIUKFfDrr79qu0YiIiKiUqVQAapSpUq4ePEitm3bhkuXLiExMRHe3t4YMmSIxqByIiIiondRoQIUAOjp6WHo0KHarIWIiIioTChUgNq8eXOe84cPH16oYoiIiIjKgkIFqC+++ELjeVpaGl69egWlUoly5coxQBEREdE7rVBX4T1//lzjkZiYiIiICLRq1YqDyImIiOidV+jvwsuqRo0aWLRoUbajU0RERETvGq0FKOC/geWPHz/WZpdEREREpU6hxkD9+eefGs+FEIiKisL333+Pli1baqUwIiIiotKqUAGqV69eGs8VCgWsrKzQoUMHLFu2TBt1EREREZVahQpQmZmZ2q6DiIiIqMzQ6hgoIiIiovdBoY5ATZw4scBtly9fXpi3ICIiIiq1ChWgLly4gAsXLiAtLQ01a9YEANy4cQO6urpo1KiR1E6hUGinSiIiIqJSpFABysPDAyYmJti0aRPKly8P4L+ba44YMQKtW7fGpEmTtFokERERUWlSqDFQy5Ytg5+fnxSeAKB8+fKYP38+r8IjIiKid16hAlR8fDxiY2OzTY+NjUVCQsJbF0VERERUmhUqQPXu3RsjRozArl278PDhQzx8+BA7d+6Et7c3+vTpo+0aiYiIiEqVQo2BWrduHSZPnozBgwcjLS3tv4709ODt7Y2lS5dqtUAiIiKi0qZQAapcuXJYs2YNli5ditu3bwMAqlevDiMjI60WR0RERFQavdWNNKOiohAVFYUaNWrAyMgIQght1UVERERUahUqQMXFxaFjx45wdnZGt27dEBUVBQDw9vbmLQyIiIjonVeoADVhwgTo6+vj/v37KFeunDR9wIABOHjwoNaKIyIiIiqNCjUG6vDhwzh06BAqVaqkMb1GjRqIjIzUSmFEREREpVWhjkAlJSVpHHlSe/bsGVQq1VsXRURERFSaFSpAtW7dGps3b5aeKxQKZGZmYsmSJWjfvr3WiiMiIiIqjQp1Cm/JkiXo2LEj/v33X6SmpmLq1KkIDw/Hs2fP8M8//2i7RiIiIqJSpVBHoOrVq4cbN26gVatW6NmzJ5KSktCnTx9cuHAB1atX13aNRERERKWK7CNQaWlp6NKlC9atW4cvv/yyKGoiIiIiKtVkH4HS19fHpUuXiqIWIiIiojKhUKfwhg4dip9//lnbtRARERGVCYUaRJ6eno4NGzbgyJEjaNy4cbbvwFu+fLlWiiMiIiIqjWQFqDt37qBKlSq4cuUKGjVqBAC4ceOGRhuFQqG96oiIiIhKIVkBqkaNGoiKikJwcDCA/7665bvvvoONjU2RFEdERERUGskaAyWE0Hh+4MABJCUlabUgIiIiotKuUIPI1bIGKiIiIqL3gawApVAoso1x4pgnIiIiet/IPoXn5eWFPn36oE+fPnj9+jU+/fRT6bn6UVAnTpyAh4cH7O3toVAo8Mcff2R7v5kzZ8LOzg6Ghobo1KkTbt68qdHm2bNnGDJkCExNTWFubg5vb28kJiZqtLl06RJat24NAwMDODg4YMmSJXIWm4iIiEiDrADl6ekJa2trmJmZwczMDEOHDoW9vb30XP0oqKSkJDRo0ACrV6/Ocf6SJUvw3XffYd26dThz5gyMjIzg7u6O169fS22GDBmC8PBwBAYGYu/evThx4gRGjx4tzY+Pj0fnzp3h6OiI0NBQLF26FLNnz8b69evlLDoRERGRRNZVeBs3btTqm3ft2hVdu3bNcZ4QAitWrMBXX32Fnj17AgA2b94MGxsb/PHHHxg4cCCuXbuGgwcP4ty5c2jSpAkAYNWqVejWrRu++eYb2NvbIyAgAKmpqdiwYQOUSiXq1q2LsLAwLF++XCNoERERERXUWw0iL0p3795FdHQ0OnXqJE0zMzODq6srQkJCAAAhISEwNzeXwhMAdOrUCTo6Ojhz5ozUpk2bNlAqlVIbd3d3RERE4Pnz58W0NERERPQuKdSdyItDdHQ0AGS7x5SNjY00Lzo6GtbW1hrz9fT0YGFhodGmatWq2fpQzytfvny2905JSUFKSor0PD4+/i2XhoiIiN4lpfYIVEny8/PTGNPl4OBQ0iURERFRKVJqA5StrS0AICYmRmN6TEyMNM/W1hZPnjzRmJ+eno5nz55ptMmpjzffI6sZM2bg5cuX0uPBgwdvv0BERET0zii1Aapq1aqwtbXF0aNHpWnx8fE4c+YM3NzcAABubm548eIFQkNDpTZBQUHIzMyEq6ur1ObEiRNIS0uT2gQGBqJmzZo5nr4DAJVKBVNTU40HERERkVqJBqjExESEhYUhLCwMwH8Dx8PCwnD//n0oFAqMHz8e8+fPx59//onLly9j+PDhsLe3R69evQAAtWvXRpcuXTBq1CicPXsW//zzD3x9fTFw4EDY29sDAAYPHgylUglvb2+Eh4fjt99+w8qVKzFx4sQSWmoiIiIq60p0EPm///6L9u3bS8/VocbT0xP+/v6YOnUqkpKSMHr0aLx48QKtWrXCwYMHYWBgIL0mICAAvr6+6NixI3R0dNC3b19899130nwzMzMcPnwYPj4+aNy4MSpUqICZM2fyFgZERERUaCUaoNq1a5fn9+kpFArMnTsXc+fOzbWNhYUFtm7dmuf71K9fH3///Xeh6yQiIiJ6U6kdA0VERERUWjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQy6ZV0AURERG8rNjYW8fHxWu83MjIS6enpWu+Xyj4GKCIiKtNiY2MxePBniItL0XrfKSlJePAgBmZm2u+byjYGKCIiKtPi4+MRF5cClWoSDA0dtNr38+enkZ6+AOnpGVrtl8o+BigiInonGBo6wMioulb7TE6O1Gp/9O7gIHIiIiIimRigiIiIiGRigCIiIiKSiQGKiIiISCYGKCIiIiKZSnWAmj17NhQKhcajVq1a0vzXr1/Dx8cHlpaWMDY2Rt++fRETE6PRx/3799G9e3eUK1cO1tbWmDJlCm+KRkRERG+l1N/GoG7dujhy5Ij0XE/vfyVPmDAB+/btw/bt22FmZgZfX1/06dMH//zzDwAgIyMD3bt3h62tLU6dOoWoqCgMHz4c+vr6WLhwYbEvCxEREb0bSn2A0tPTg62tbbbpL1++xM8//4ytW7eiQ4cOAICNGzeidu3aOH36NJo3b47Dhw/j6tWrOHLkCGxsbNCwYUPMmzcP06ZNw+zZs6FUKot7cYiIiOgdUKpP4QHAzZs3YW9vj2rVqmHIkCG4f/8+ACA0NBRpaWno1KmT1LZWrVqoXLkyQkJCAAAhISFwcXGBjY2N1Mbd3R3x8fEIDw/P9T1TUlIQHx+v8SAiIiJSK9UBytXVFf7+/jh48CDWrl2Lu3fvonXr1khISEB0dDSUSiXMzc01XmNjY4Po6GgAQHR0tEZ4Us9Xz8uNn58fzMzMpIeDg3a/GoCIiIjKtlJ9Cq9r167S/+vXrw9XV1c4Ojri999/h6GhYZG974wZMzBx4kTpeXx8PEMUERERSUr1EaiszM3N4ezsjFu3bsHW1hapqal48eKFRpuYmBhpzJStrW22q/LUz3MaV6WmUqlgamqq8SAiIiJSK1MBKjExEbdv34adnR0aN24MfX19HD16VJofERGB+/fvw83NDQDg5uaGy5cv48mTJ1KbwMBAmJqaok6dOsVePxEREb0bSvUpvMmTJ8PDwwOOjo54/PgxZs2aBV1dXQwaNAhmZmbw9vbGxIkTYWFhAVNTU4wdOxZubm5o3rw5AKBz586oU6cOhg0bhiVLliA6OhpfffUVfHx8oFKpSnjpiIiIqKwq1QHq4cOHGDRoEOLi4mBlZYVWrVrh9OnTsLKyAgB8++230NHRQd++fZGSkgJ3d3esWbNGer2uri727t2Lzz77DG5ubjAyMoKnpyfmzp1bUotERERE74BSHaC2bduW53wDAwOsXr0aq1evzrWNo6Mj9u/fr+3SiIiI6D1WpsZAEREREZUGDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQkEwMUERERkUwMUEREREQyMUARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcmkV9IFEBHR+yM2Nhbx8fFa7TMyMhLp6ela7ZMoPwxQRERULGJjYzF48GeIi0vRar8pKUl48CAGZmba7ZcoLwxQRERULOLj4xEXlwKVahIMDR201u/z56eRnr4A6ekZWuuTKD8MUEREVKwMDR1gZFRda/0lJ0dqrS+iguIgciIiIiKZGKCIiIiIZGKAIiIiIpKJAYqIiIhIJgYoIiIiIpkYoIiIiIhk4m0MiIhIQ1HcLRzgHcPp3cIARUREkqK6WzjAO4bTu4UBioiIJEV1t3CAdwyndwsDFBERZaPtu4UDvGM4vVs4iJyIiIhIJgYoIiIiIpkYoIiIiIhkYoAiIiIikomDyImIilBR3VPJ1NQUVlZWWu+XiAqGAYqIqIgU5T2VLC1V2Lp1LUMUUQlhgCIiKiJFdU+l5OQHiI5eiMuXL8PR0VFr/QK8WzhRQTFAEREVMW3fUyk1NQ6RkXcwduwiqFQqrfUL8G7hRAXFAEVEVMZkZCQiPV0JpXICzM2dtdo37xZOVDAMUERUZnBAtiYDg0q8WzhRCWGAIqIygQOyiag0YYAiIq0qqqNEkZGRiIlJgpHRNA7IJqISxwBFRFpTlEeJ1IObGzSw5oBsIipxDFBEpDVFddk+UHSDmzkgm4gKgwGKiLRO25ftA0U/uJkDsolIDgYoovdUUYxV4pgfInpfMEARvYeKaqwSx/wQ0fuCAYroPVRUY5U45oeI3hcMUETvMW2PVeKYHyJ6X+iUdAFEREREZc17dQRq9erVWLp0KaKjo9GgQQOsWrUKzZo1K+myiHJVlDel5GBvIqLCe28C1G+//YaJEydi3bp1cHV1xYoVK+Du7o6IiAhYW1uXdHlUxhVF0ImLi8OUKfORkCC02i/Awd5ERG/rvQlQy5cvx6hRozBixAgAwLp167Bv3z5s2LAB06dPL+HqqCwr6ivaatb8FiYm2r0/EQd7ExG9nfciQKWmpiI0NBQzZsyQpuno6KBTp04ICQkpwcqoOJW172hThxw9PTve4JGIqJR5LwLU06dPkZGRARsbG43pNjY2uH79erb2KSkpSEn539GEly9fAkCR/PIFgGfPnuHFixdF0jf959mzZ5g58xskJGRqve+UlFd49OgJatR4Dn19c631m5HxCkJkICnpBvT1tXukKCnpdpH0XVT9FmXfrLns982ai6fvoqw5OfkRMjLSkJCQoNXfteq+hND+UAiI98CjR48EAHHq1CmN6VOmTBHNmjXL1n7WrFkCAB988MEHH3zw8Q48Hjx4oPVs8V4cgapQoQJ0dXURExOjMT0mJga2trbZ2s+YMQMTJ06UnmdmZuLZs2ewtLSEQqHQam3x8fFwcHDAgwcPYGpqqtW+6X+4nosH13Px4HouPlzXxaOo1rMQAgkJCbC3t9dan2rvRYBSKpVo3Lgxjh49il69egH4LxQdPXoUvr6+2dqrVCqoVCqNaebm5kVao6mpKX84iwHXc/Hgei4eXM/Fh+u6eBTFejYzM9Nqf2rvRYACgIkTJ8LT0xNNmjRBs2bNsGLFCiQlJUlX5REREREV1HsToAYMGIDY2FjMnDkT0dHRaNiwIQ4ePJhtYDkRERFRft6bAAUAvr6+OZ6yK0kqlQqzZs3KdsqQtIvruXhwPRcPrufiw3VdPMrielYIURTX9hERERG9u/hlwkREREQyMUARERERycQARURERCQTAxQRERGRTAxQxWD16tWoUqUKDAwM4OrqirNnz+bZfvv27ahVqxYMDAzg4uKC/fv3F1OlZZuc9fzjjz+idevWKF++PMqXL49OnTrl+7nQf+Ruz2rbtm2DQqGQbmZLeZO7nl+8eAEfHx/Y2dlBpVLB2dmZ+44CkruuV6xYgZo1a8LQ0BAODg6YMGECXr9+XUzVlj0nTpyAh4cH7O3toVAo8Mcff+T7mmPHjqFRo0ZQqVRwcnKCv79/kdcpm9a/HIY0bNu2TSiVSrFhwwYRHh4uRo0aJczNzUVMTEyO7f/55x+hq6srlixZIq5evSq++uoroa+vLy5fvlzMlZctctfz4MGDxerVq8WFCxfEtWvXhJeXlzAzMxMPHz4s5srLFrnrWe3u3buiYsWKonXr1qJnz57FU2wZJnc9p6SkiCZNmohu3bqJkydPirt374pjx46JsLCwYq687JG7rgMCAoRKpRIBAQHi7t274tChQ8LOzk5MmDChmCsvO/bv3y++/PJLsWvXLgFA7N69O8/2d+7cEeXKlRMTJ04UV69eFatWrRK6urri4MGDxVNwATFAFbFmzZoJHx8f6XlGRoawt7cXfn5+Obb/+OOPRffu3TWmubq6ijFjxhRpnWWd3PWcVXp6ujAxMRGbNm0qqhLfCYVZz+np6aJFixbip59+Ep6engxQBSB3Pa9du1ZUq1ZNpKamFleJ7wy569rHx0d06NBBY9rEiRNFy5Yti7TOd0VBAtTUqVNF3bp1NaYNGDBAuLu7F2Fl8vEUXhFKTU1FaGgoOnXqJE3T0dFBp06dEBISkuNrQkJCNNoDgLu7e67tqXDrOatXr14hLS0NFhYWRVVmmVfY9Tx37lxYW1vD29u7OMos8wqznv/880+4ubnBx8cHNjY2qFevHhYuXIiMjIziKrtMKsy6btGiBUJDQ6XTfHfu3MH+/fvRrVu3Yqn5fVBWfg++V3ciL25Pnz5FRkZGtq+LsbGxwfXr13N8TXR0dI7to6Oji6zOsq4w6zmradOmwd7ePtsPLf1PYdbzyZMn8fPPPyMsLKwYKnw3FGY937lzB0FBQRgyZAj279+PW7du4fPPP0daWhpmzZpVHGWXSYVZ14MHD8bTp0/RqlUrCCGQnp6OTz/9FP/3f/9XHCW/F3L7PRgfH4/k5GQYGhqWUGWaeASK3nuLFi3Ctm3bsHv3bhgYGJR0Oe+MhIQEDBs2DD/++CMqVKhQ0uW80zIzM2FtbY3169ejcePGGDBgAL788kusW7eupEt75xw7dgwLFy7EmjVrcP78eezatQv79u3DvHnzSro0KmY8AlWEKlSoAF1dXcTExGhMj4mJga2tbY6vsbW1ldWeCree1b755hssWrQIR44cQf369YuyzDJP7nq+ffs27t27Bw8PD2laZmYmAEBPTw8RERGoXr160RZdBhVme7azs4O+vj50dXWlabVr10Z0dDRSU1OhVCqLtOayqjDr+uuvv8awYcMwcuRIAICLiwuSkpIwevRofPnll9DR4XGJt5Xb70FTU9NSc/QJ4BGoIqVUKtG4cWMcPXpUmpaZmYmjR4/Czc0tx9e4ublptAeAwMDAXNtT4dYzACxZsgTz5s3DwYMH0aRJk+IotUyTu55r1aqFy5cvIywsTHp89NFHaN++PcLCwuDg4FCc5ZcZhdmeW7ZsiVu3bkkBFQBu3LgBOzs7hqc8FGZdv3r1KltIUgdXwa+W1Yoy83uwpEexv+u2bdsmVCqV8Pf3F1evXhWjR48W5ubmIjo6WgghxLBhw8T06dOl9v/884/Q09MT33zzjbh27ZqYNWsWb2NQAHLX86JFi4RSqRQ7duwQUVFR0iMhIaGkFqFMkLues+JVeAUjdz3fv39fmJiYCF9fXxERESH27t0rrK2txfz580tqEcoMuet61qxZwsTERPz666/izp074vDhw6J69eri448/LqlFKPUSEhLEhQsXxIULFwQAsXz5cnHhwgURGRkphBBi+vTpYtiwYVJ79W0MpkyZIq5duyZWr17N2xi8r1atWiUqV64slEqlaNasmTh9+rQ0r23btsLT01Oj/e+//y6cnZ2FUqkUdevWFfv27SvmissmOevZ0dFRAMj2mDVrVvEXXsbI3Z7fxABVcHLX86lTp4Srq6tQqVSiWrVqYsGCBSI9Pb2Yqy6b5KzrtLQ0MXv2bFG9enVhYGAgHBwcxOeffy6eP39e/IWXEcHBwTnub9Xr1dPTU7Rt2zbbaxo2bCiUSqWoVq2a2LhxY7HXnR+FEDzmSERERCQHx0ARERERycQARURERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEWuTv7w9zc3NZr/Hy8kKvXr2KpJ7S6tixY1AoFHjx4gWAwq03balSpQpWrFhRavopTbSxbd67dw8KhQJhYWG5tslve5g9ezYaNmz4VnUQaRsDFFEB5PaLJOuOf8CAAbhx40bxFpePrDXm1079sLGxQd++fXHnzp0ir1HueivusBIfH48vv/wStWrVgoGBAWxtbdGpUyfs2rWrVHz/mZeXl/S5KZVKODk5Ye7cuUhPTy/p0gqkRYsWiIqKgpmZWY7zJ0+erPHdaO/jHx1U+uiVdAFE7xJDQ8NS9W3hhREREQETExPcvHkTo0ePhoeHBy5duiR9YaqaEAIZGRnQ03v73UhpXm8vXrxAq1at8PLlS8yfPx9NmzaFnp4ejh8/jqlTp6JDhw4ldvTsTV26dMHGjRuRkpKC/fv3w8fHB/r6+pgxY0a2tqmpqaXqS4aVSiVsbW1znW9sbAxjY+NirIgofzwCRaRFOZ2Kmj9/PqytrWFiYoKRI0di+vTpOZ6O+Oabb2BnZwdLS0v4+PggLS1NmpeSkoLJkyejYsWKMDIygqurK44dOybNj4yMhIeHB8qXLw8jIyPUrVsX+/fvx71799C+fXsAQPny5aFQKODl5ZXnMlhbW8POzg5t2rTBzJkzcfXqVdy6dUs6QnXgwAE0btwYKpUKJ0+eRGZmJvz8/FC1alUYGhqiQYMG2LFjh0af+/fvh7OzMwwNDdG+fXvcu3cv3/X2119/oWnTpjAwMECFChXQu3dvAEC7du0QGRmJCRMmSEdd1E6ePInWrVvD0NAQDg4OGDduHJKSkqT5T548gYeHBwwNDVG1alUEBATkuS4A4P/+7/9w7949nDlzBp6enqhTpw6cnZ0xatQohIWF5fqLffny5XBxcYGRkREcHBzw+eefIzExUZqf22cGAM+fP8eQIUNgZWUFQ0ND1KhRAxs3bsyzTpVKBVtbWzg6OuKzzz5Dp06d8OeffwL43xGbBQsWwN7eHjVr1gQAXL58GR06dIChoSEsLS0xevRojRrV5syZAysrK5iamuLTTz9FamqqNO/gwYNo1aoVzM3NYWlpiR49euD27dvZ+rh+/TpatGgBAwMD1KtXD8ePH5fm5XeU9M1TeLNnz8amTZuwZ88e6fM/duwYOnToAF9fX43XxcbGQqlUahy9ItIWBiiiIhQQEIAFCxZg8eLFCA0NReXKlbF27dps7YKDg3H79m0EBwdj06ZN8Pf3h7+/vzTf19cXISEh2LZtGy5duoT+/fujS5cuuHnzJgDAx8cHKSkpOHHiBC5fvozFixfD2NgYDg4O2LlzJ4D/jixFRUVh5cqVBa5ffVTozV+Y06dPx6JFi3Dt2jXUr18ffn5+2Lx5M9atW4fw8HBMmDABQ4cOlX5BPnjwAH369IGHhwfCwsKkEJmXffv2oXfv3ujWrRsuXLiAo0ePolmzZgCAXbt2oVKlSpg7dy6ioqIQFRUFALh9+za6dOmCvn374tKlS/jtt99w8uRJjV+qXl5eePDgAYKDg7Fjxw6sWbMGT548ybWOzMxMbNu2DUOGDIG9vX22+cbGxrkegdPR0cF3332H8PBwbNq0CUFBQZg6dao0P7fPDAC+/vprXL16FQcOHMC1a9ewdu1aVKhQIc91lpWhoaHG53b06FFEREQgMDAQe/fuRVJSEtzd3VG+fHmcO3cO27dvx5EjR7KFkKNHj+LatWs4duwYfv31V+zatQtz5syR5iclJWHixIn4999/cfToUejo6KB3797IzMzU6GfKlCmYNGkSLly4ADc3N3h4eCAuLk7WMgH/nc77+OOP0aVLF+nzb9GiBUaOHImtW7ciJSVFavvLL7+gYsWK6NChg+z3IcpXyX6XMVHZ4OnpKXR1dYWRkZHGw8DAQACQvol948aNwszMTHqdq6ur8PHx0eirZcuWokGDBhp9Ozo6ivT0dGla//79xYABA4QQQkRGRgpdXV3x6NEjjX46duwoZsyYIYQQwsXFRcyePTvH2tXfhJ7ft8Vnbff48WPRokULUbFiRZGSkiLN/+OPP6TXvH79WpQrV06cOnVKoy9vb28xaNAgIYQQM2bMEHXq1NGYP23atDzXm5ubmxgyZEiutTo6Oopvv/0223uOHj1aY9rff/8tdHR0RHJysoiIiBAAxNmzZ6X5165dEwCy9aUWExMjAIjly5fnWkteNb1p+/btwtLSUnqe12fm4eEhRowYke97qnl6eoqePXsKIYTIzMwUgYGBQqVSicmTJ0vzbWxsREpKivSa9evXi/Lly4vExERp2r59+4SOjo6Ijo6WXmdhYSGSkpKkNmvXrhXGxsYiIyMjx1piY2MFAHH58mUhhBB3794VAMSiRYukNmlpaaJSpUpi8eLFQojs217W7WHWrFnZfmbUy6uWnJwsypcvL3777TdpWv369XNdx0Rvi0egiAqoffv2CAsL03j89NNPeb4mIiJCOnKilvU5ANStW1djjJGdnZ10ZOTy5cvIyMiAs7OzNBbE2NgYx48fl06VjBs3DvPnz0fLli0xa9YsXLp0qdDLWalSJRgZGcHe3h5JSUnYuXOnxniZJk2aSP+/desWXr16hQ8//FCjts2bN0u1Xbt2Da6urhrv4ebmlmcNYWFh6Nixo6y6L168CH9/f4063N3dkZmZibt37+LatWvQ09ND48aNpdfUqlUrz/FL4i0GiB85cgQdO3ZExYoVYWJigmHDhiEuLg6vXr0CkPdn9tlnn2Hbtm1o2LAhpk6dilOnTuX7fnv37oWxsTEMDAzQtWtXDBgwALNnz5bmu7i4aHyO165dQ4MGDWBkZCRNa9myJTIzMxERESFNa9CgAcqVKyc9d3NzQ2JiIh48eAAAuHnzJgYNGoRq1arB1NQUVapUAQDcv39fo743P3M9PT00adIE165dy3e5CsrAwADDhg3Dhg0bAADnz5/HlStX8j1lTVRYHEROVEBGRkZwcnLSmPbw4UOt9K2vr6/xXKFQSKdAEhMToauri9DQ0GwDudWnfEaOHAl3d3fs27cPhw8fhp+fH5YtW4axY8fKruXvv/+GqampNG4rqzd/4arHy+zbtw8VK1bUaKdSqWS/t1phBpQnJiZizJgxGDduXLZ5lStXLtTVkVZWVjA3N8f169dlve7evXvo0aMHPvvsMyxYsAAWFhY4efIkvL29kZqainLlyuX5mXXt2hWRkZHYv38/AgMD0bFjR/j4+OCbb77J9T3bt2+PtWvXQqlUwt7ePtupxTc/N23y8PCAo6MjfvzxR9jb2yMzMxP16tXTOH1YXEaOHImGDRvi4cOH2LhxIzp06ABHR8dir4PeDzwCRVSEatasiXPnzmlMy/o8Px988AEyMjLw5MkTODk5aTzevHLJwcEBn376KXbt2oVJkybhxx9/BADpqENGRkaB3q9q1aqoXr16juEpqzp16kClUuH+/fvZanNwcAAA1K5dG2fPntV43enTp/Pst379+nkO/FUqldmWp1GjRrh69Wq2OpycnKBUKlGrVi2kp6cjNDRUek1ERESet3fQ0dHBwIEDERAQgMePH2ebn5iYmOOtAkJDQ5GZmYlly5ahefPmcHZ2zvH1uX1mwH/hzdPTE7/88gtWrFiB9evX51on8L+AX7ly5QJdGVm7dm1cvHhRY5D9P//8Ax0dHWmQOfDfkb3k5GTp+enTp6XxdXFxcYiIiMBXX32Fjh07onbt2nj+/HmO7/fmZ67+HGrXrp1vnTnJ6fMH/jvK1qRJE/z444/YunUrPvnkk0L1T1QQDFBERWjs2LH4+eefsWnTJty8eRPz58/HpUuXNK4cy4+zszOGDBmC4cOHY9euXbh79y7Onj0LPz8/7Nu3DwAwfvx4HDp0CHfv3sX58+cRHBws/XJydHSEQqHA3r17ERsbm+NVVoVlYmKCyZMnY8KECdi0aRNu376N8+fPY9WqVdi0aRMA4NNPP8XNmzcxZcoUREREYOvWrRoD5HMya9Ys/Prrr5g1axauXbsmDbJWq1KlCk6cOIFHjx7h6dOnAIBp06bh1KlT8PX1RVhYGG7evIk9e/ZIg6Jr1qyJLl26YMyYMThz5gxCQ0MxcuTIfI92LViwAA4ODnB1dcXmzZtx9epV3Lx5Exs2bMAHH3yQ4/p0cnJCWloaVq1ahTt37mDLli1Yt26dRpu8PrOZM2diz549uHXrFsLDw7F3795Ch43cDBkyBAYGBvD09MSVK1cQHByMsWPHYtiwYbCxsZHapaamwtvbG1evXsX+/fsxa9Ys+Pr6QkdHB+XLl4elpSXWr1+PW7duISgoCBMnTszx/VavXo3du3fj+vXr8PHxwfPnzwsdcKpUqYJLly4hIiICT58+1bhideTIkVi0aBGEENKVm0RFoqQHYRGVBTkNWhUi/8GvQggxd+5cUaFCBWFsbCw++eQTMW7cONG8efM8+/7iiy9E27Ztpeepqali5syZokqVKkJfX1/Y2dmJ3r17i0uXLgkhhPD19RXVq1cXKpVKWFlZiWHDhomnT59q1GBraysUCoXw9PTMcRnzG2ye2/zMzEyxYsUKUbNmTaGvry+srKyEu7u7OH78uNTmr7/+Ek5OTkKlUonWrVuLDRs25Lvedu7cKRo2bCiUSqWoUKGC6NOnjzQvJCRE1K9fX6hUKvHmbuzs2bPiww8/FMbGxsLIyEjUr19fLFiwQJofFRUlunfvLlQqlahcubLYvHlzvoO/hRDixYsXYvr06aJGjRpCqVQKGxsb0alTJ7F7926RmZkphMg+iHz58uXCzs5OGBoaCnd3d7F582aNZc7rM5s3b56oXbu2MDQ0FBYWFqJnz57izp07udaX2/aZ3/xLly6J9u3bCwMDA2FhYSFGjRolEhISsr1u5syZwtLSUhgbG4tRo0aJ169fS20CAwNF7dq1hUqlEvXr1xfHjh0TAMTu3buFEP8bRL5161bRrFkzoVQqRZ06dURQUJDUh9xB5E+ePJE+ZwAiODhYmpeQkCDKlSsnPv/881zXB5E2KIQoBbfRJXqPfPjhh7C1tcWWLVtKuhSid869e/dQvXp1nDt3Do0aNSrpcugdxkHkREXo1atXWLduHdzd3aGrq4tff/0VR44cQWBgYEmXRvROSUtLQ1xcHL766is0b96c4YmKHAMUURFSKBTYv38/FixYgNevX6NmzZrYuXMnOnXqVNKlEb1T/vnnH7Rv3x7Ozs7Z7oRPVBR4Co+IiIhIJl6FR0RERCQTAxQRERGRTAxQRERERDIxQBERERHJxABFREREJBMDFBEREZFMDFBEREREMjFAEREREcnEAEVEREQk0/8D2plH9yye16QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_net = networks.TeacherNetwork50()\n",
    "teacher_net = teacher_net.to(fast_device)\n",
    "\n",
    "#checkpoint = torch.load('best_resnet50_cifar10.pth')\n",
    "\n",
    "\n",
    "#teacher_net.model.load_state_dict(checkpoint)\n",
    "\n",
    "cp = 300\n",
    "\n",
    "student_net = networks.StudentNetwork(teacher_net, q=True, fuse=False, qat=False, dif_arch=True)\n",
    "checkpoint = torch.load(f'student_BiT_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n",
    "student_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "student_net.to(fast_device)\n",
    "\n",
    "\n",
    "student_net.eval()\n",
    "\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(fast_device)\n",
    "        outputs = student_net(images)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        max_probs, _ = torch.max(probabilities, dim=1)\n",
    "        probs.extend(max_probs.cpu().numpy())\n",
    "        \n",
    "# Plot histogram of highest predicted class probabilities\n",
    "plt.hist(probs, bins=np.linspace(0, 1, 21), alpha=0.7, color='b', edgecolor='black')\n",
    "plt.xlabel('Highest Predicted Class Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Highest Predicted Class Probabilities on CIFAR-10')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def zip_lists_to_json(list1, list2, folder_path, filename=\"test_scores.json\"):\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Convert zipped lists into a dictionary format\n",
    "    zipped_dict = {epoch: accuracy for epoch, accuracy in zip(list1, list2)}\n",
    "    \n",
    "    # Define JSON file path\n",
    "    json_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(zipped_dict, json_file, indent=4)\n",
    "    \n",
    "    print(f\"JSON saved at: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved at: logs\\t_BiT_QAT_quantized.json\n"
     ]
    }
   ],
   "source": [
    "zip_lists_to_json(checkpoints, test_accs, 'logs', 't_BiT_QAT_quantized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.convert(prepared_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, 'cpu')\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "def test_student_network(\n",
    "    start_checkpoint, \n",
    "    end_checkpoint, \n",
    "    file_names,\n",
    "    save_folder, \n",
    "    filename=\"test_scores.json\", \n",
    "    quantize=False, \n",
    "    pruning_factor=None, \n",
    "    teacher_net=None, \n",
    "    fast_device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a student model from checkpoints within a range, optionally quantizes it, evaluates test accuracy, and saves results to JSON.\n",
    "\n",
    "    Parameters:\n",
    "        start_checkpoint (int): The starting epoch.\n",
    "        end_checkpoint (int): The ending epoch.\n",
    "        folder_path (str): Path to the folder where checkpoint files are stored.\n",
    "        save_folder (str): Folder where JSON results will be saved.\n",
    "        filename (str): Name of the output JSON file.\n",
    "        quantize (bool): Whether to quantize the model before testing.\n",
    "        pruning_factor (any): Pruning factor for StudentNetwork (if applicable).\n",
    "        teacher_net (torch.nn.Module): Teacher network model (if required).\n",
    "        fast_device (str): Device to run the model on (default: auto-detect CUDA).\n",
    "\n",
    "    Saves:\n",
    "        JSON file with test accuracy results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure save folder exists\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Generate checkpoint range (increments of 25)\n",
    "    checkpoints = list(range(start_checkpoint, end_checkpoint + 1, 25))\n",
    "\n",
    "    # Initialize accuracy dictionary\n",
    "    test_accs = {}\n",
    "\n",
    "    for cp in checkpoints:\n",
    "        print(f\"Testing checkpoint: {cp}\")\n",
    "\n",
    "        # Load the student network\n",
    "        student_net = networks.StudentNetwork(teacher_net, q=quantize, fuse=False, qat=False, dif_arch=True)\n",
    "        checkpoint_path = f\"{file_names}{cp}.tar\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        student_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        student_net.to(fast_device)\n",
    "        student_net.eval()\n",
    "\n",
    "        # Apply quantization if enabled\n",
    "        if quantize:\n",
    "            student_net.qconfig = torch.ao.quantization.get_default_qconfig(\"x86\")\n",
    "            student_net_prepared = torch.ao.quantization.prepare(student_net)\n",
    "\n",
    "            # Run a forward pass to collect activation statistics\n",
    "            with torch.no_grad():\n",
    "                for inputs, _ in train_loader:\n",
    "                    inputs = inputs.to(fast_device)\n",
    "                    student_net_prepared(inputs)\n",
    "\n",
    "            student_net_prepared.to(\"cpu\")\n",
    "            student_net = torch.ao.quantization.convert(student_net_prepared)\n",
    "\n",
    "        # Ensure reproducibility before testing\n",
    "        reproducibilitySeed()\n",
    "\n",
    "        # Evaluate test accuracy\n",
    "        _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, \"cpu\" if quantize else fast_device)\n",
    "        print(f\"Test accuracy at epoch {cp}: {test_accuracy}\")\n",
    "\n",
    "        # Store results in dictionary\n",
    "        test_accs[cp] = test_accuracy\n",
    "\n",
    "    # Save results to JSON file\n",
    "    json_path = os.path.join(save_folder, filename)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(test_accs, json_file, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_70004\\1970378868.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('student_BiT_R101x1\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_50.tar')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy at epoch 50: 0.9427\n"
     ]
    }
   ],
   "source": [
    "import networks\n",
    "\n",
    "import torch\n",
    "teacher_net = networks.TeacherNetwork()\n",
    "student_net = networks.StudentNetwork(teacher_net, q=False, fuse=False, qat=False, dif_arch=True)\n",
    "checkpoint = torch.load('student_BiT_R101x1\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_50.tar')\n",
    "student_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "student_net.to(fast_device)\n",
    "student_net.eval()\n",
    "\n",
    "# Ensure reproducibility before testing\n",
    "reproducibilitySeed()\n",
    "\n",
    "# Evaluate test accuracy\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "print(f\"Test accuracy at epoch {50}: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing checkpoint: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\3966538732.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy at epoch 25: 0.9425\n",
      "Testing checkpoint: 50\n",
      "Test accuracy at epoch 50: 0.95\n",
      "Testing checkpoint: 75\n",
      "Test accuracy at epoch 75: 0.9548\n",
      "Testing checkpoint: 100\n",
      "Test accuracy at epoch 100: 0.9564\n",
      "Testing checkpoint: 125\n",
      "Test accuracy at epoch 125: 0.958\n",
      "Testing checkpoint: 150\n",
      "Test accuracy at epoch 150: 0.9575\n",
      "Testing checkpoint: 175\n",
      "Test accuracy at epoch 175: 0.9607\n",
      "Testing checkpoint: 200\n",
      "Test accuracy at epoch 200: 0.9578\n",
      "Testing checkpoint: 225\n",
      "Test accuracy at epoch 225: 0.9583\n",
      "Testing checkpoint: 250\n",
      "Test accuracy at epoch 250: 0.9606\n",
      "Testing checkpoint: 275\n",
      "Test accuracy at epoch 275: 0.9605\n",
      "Testing checkpoint: 300\n",
      "Test accuracy at epoch 300: 0.96\n",
      "Testing checkpoint: 325\n",
      "Test accuracy at epoch 325: 0.9585\n",
      "Testing checkpoint: 350\n",
      "Test accuracy at epoch 350: 0.9591\n",
      "Testing checkpoint: 375\n",
      "Test accuracy at epoch 375: 0.9586\n",
      "Testing checkpoint: 400\n",
      "Test accuracy at epoch 400: 0.9586\n",
      "Results saved to logs\\t_resnet50_no_QAT_quantized.json\n"
     ]
    }
   ],
   "source": [
    "test_student_network(25, 400, 'student_resnet50_no_QAT/T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_', 'logs', \"t_resnet50_no_QAT_quantized.json\", quantize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\52760316.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_resnet50_cifar10.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8996\n"
     ]
    }
   ],
   "source": [
    "teacher_net = networks.TeacherNetwork50()\n",
    "checkpoint = torch.load('best_resnet50_cifar10.pth')\n",
    "\n",
    "\n",
    "teacher_net.model.load_state_dict(checkpoint)\n",
    "\n",
    "teacher_net = teacher_net.to(fast_device)\n",
    "\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train Epoch [1/300]: Loss: 2.9030, Accuracy: 18.08%\n",
      "Train Epoch [2/300]: Loss: 1.8696, Accuracy: 28.16%\n",
      "Train Epoch [3/300]: Loss: 1.6474, Accuracy: 37.99%\n",
      "Train Epoch [4/300]: Loss: 1.5380, Accuracy: 42.82%\n",
      "Train Epoch [5/300]: Loss: 1.4056, Accuracy: 48.42%\n",
      "Validation Epoch [5]: Loss: 1.5715, Accuracy: 46.20%\n",
      "New best model saved with accuracy: 46.20%\n",
      "Train Epoch [6/300]: Loss: 1.2722, Accuracy: 53.69%\n",
      "Train Epoch [7/300]: Loss: 1.1439, Accuracy: 59.07%\n",
      "Train Epoch [8/300]: Loss: 1.0371, Accuracy: 63.08%\n",
      "Train Epoch [9/300]: Loss: 0.9477, Accuracy: 66.06%\n",
      "Train Epoch [10/300]: Loss: 0.8567, Accuracy: 69.59%\n",
      "Validation Epoch [10]: Loss: 1.1347, Accuracy: 61.52%\n",
      "New best model saved with accuracy: 61.52%\n",
      "Train Epoch [11/300]: Loss: 0.7962, Accuracy: 71.68%\n",
      "Train Epoch [12/300]: Loss: 0.7267, Accuracy: 74.43%\n",
      "Train Epoch [13/300]: Loss: 0.6819, Accuracy: 76.15%\n",
      "Train Epoch [14/300]: Loss: 0.6322, Accuracy: 78.07%\n",
      "Train Epoch [15/300]: Loss: 0.6092, Accuracy: 78.93%\n",
      "Validation Epoch [15]: Loss: 0.9909, Accuracy: 67.16%\n",
      "New best model saved with accuracy: 67.16%\n",
      "Train Epoch [16/300]: Loss: 0.5869, Accuracy: 79.77%\n",
      "Train Epoch [17/300]: Loss: 0.5681, Accuracy: 80.43%\n",
      "Train Epoch [18/300]: Loss: 0.5515, Accuracy: 80.87%\n",
      "Train Epoch [19/300]: Loss: 0.5537, Accuracy: 80.96%\n",
      "Train Epoch [20/300]: Loss: 0.5231, Accuracy: 82.09%\n",
      "Validation Epoch [20]: Loss: 2.5216, Accuracy: 51.84%\n",
      "Train Epoch [21/300]: Loss: 0.5208, Accuracy: 82.05%\n",
      "Train Epoch [22/300]: Loss: 0.5311, Accuracy: 81.74%\n",
      "Train Epoch [23/300]: Loss: 0.4859, Accuracy: 83.29%\n",
      "Train Epoch [24/300]: Loss: 0.4924, Accuracy: 83.01%\n",
      "Train Epoch [25/300]: Loss: 0.4771, Accuracy: 83.51%\n",
      "Validation Epoch [25]: Loss: 1.3841, Accuracy: 63.44%\n",
      "Train Epoch [26/300]: Loss: 0.4812, Accuracy: 83.34%\n",
      "Train Epoch [27/300]: Loss: 0.4653, Accuracy: 83.83%\n",
      "Train Epoch [28/300]: Loss: 0.4689, Accuracy: 83.75%\n",
      "Train Epoch [29/300]: Loss: 0.4510, Accuracy: 84.65%\n",
      "Train Epoch [30/300]: Loss: 0.4503, Accuracy: 84.60%\n",
      "Validation Epoch [30]: Loss: 0.7322, Accuracy: 75.96%\n",
      "New best model saved with accuracy: 75.96%\n",
      "Train Epoch [31/300]: Loss: 0.4429, Accuracy: 84.71%\n",
      "Train Epoch [32/300]: Loss: 0.4405, Accuracy: 84.83%\n",
      "Train Epoch [33/300]: Loss: 0.4357, Accuracy: 84.93%\n",
      "Train Epoch [34/300]: Loss: 0.4256, Accuracy: 85.27%\n",
      "Train Epoch [35/300]: Loss: 0.4334, Accuracy: 85.21%\n",
      "Validation Epoch [35]: Loss: 0.8677, Accuracy: 70.88%\n",
      "Train Epoch [36/300]: Loss: 0.4193, Accuracy: 85.58%\n",
      "Train Epoch [37/300]: Loss: 0.4146, Accuracy: 85.85%\n",
      "Train Epoch [38/300]: Loss: 0.4229, Accuracy: 85.54%\n",
      "Train Epoch [39/300]: Loss: 0.4193, Accuracy: 85.62%\n",
      "Train Epoch [40/300]: Loss: 0.4120, Accuracy: 85.72%\n",
      "Validation Epoch [40]: Loss: 0.9503, Accuracy: 73.08%\n",
      "Train Epoch [41/300]: Loss: 0.4188, Accuracy: 85.66%\n",
      "Train Epoch [42/300]: Loss: 0.4041, Accuracy: 85.96%\n",
      "Train Epoch [43/300]: Loss: 0.4067, Accuracy: 85.93%\n",
      "Train Epoch [44/300]: Loss: 0.4006, Accuracy: 86.20%\n",
      "Train Epoch [45/300]: Loss: 0.4090, Accuracy: 85.95%\n",
      "Validation Epoch [45]: Loss: 1.1914, Accuracy: 63.36%\n",
      "Train Epoch [46/300]: Loss: 0.4046, Accuracy: 86.23%\n",
      "Train Epoch [47/300]: Loss: 0.3888, Accuracy: 86.52%\n",
      "Train Epoch [48/300]: Loss: 0.4140, Accuracy: 85.67%\n",
      "Train Epoch [49/300]: Loss: 0.4050, Accuracy: 86.10%\n",
      "Train Epoch [50/300]: Loss: 0.3995, Accuracy: 86.29%\n",
      "Validation Epoch [50]: Loss: 2.8645, Accuracy: 54.80%\n",
      "Train Epoch [51/300]: Loss: 0.2428, Accuracy: 91.73%\n",
      "Train Epoch [52/300]: Loss: 0.1800, Accuracy: 93.87%\n",
      "Train Epoch [53/300]: Loss: 0.1608, Accuracy: 94.50%\n",
      "Train Epoch [54/300]: Loss: 0.1457, Accuracy: 95.03%\n",
      "Train Epoch [55/300]: Loss: 0.1376, Accuracy: 95.32%\n",
      "Validation Epoch [55]: Loss: 0.2653, Accuracy: 91.28%\n",
      "New best model saved with accuracy: 91.28%\n",
      "Train Epoch [56/300]: Loss: 0.1247, Accuracy: 95.73%\n",
      "Train Epoch [57/300]: Loss: 0.1148, Accuracy: 96.01%\n",
      "Train Epoch [58/300]: Loss: 0.1087, Accuracy: 96.33%\n",
      "Train Epoch [59/300]: Loss: 0.1020, Accuracy: 96.47%\n",
      "Train Epoch [60/300]: Loss: 0.0946, Accuracy: 96.77%\n",
      "Validation Epoch [60]: Loss: 0.2771, Accuracy: 91.44%\n",
      "New best model saved with accuracy: 91.44%\n",
      "Train Epoch [61/300]: Loss: 0.0899, Accuracy: 96.93%\n",
      "Train Epoch [62/300]: Loss: 0.0855, Accuracy: 97.07%\n",
      "Train Epoch [63/300]: Loss: 0.0874, Accuracy: 96.96%\n",
      "Train Epoch [64/300]: Loss: 0.0842, Accuracy: 97.15%\n",
      "Train Epoch [65/300]: Loss: 0.0740, Accuracy: 97.49%\n",
      "Validation Epoch [65]: Loss: 0.3101, Accuracy: 90.76%\n",
      "Train Epoch [66/300]: Loss: 0.0806, Accuracy: 97.19%\n",
      "Train Epoch [67/300]: Loss: 0.0765, Accuracy: 97.42%\n",
      "Train Epoch [68/300]: Loss: 0.0753, Accuracy: 97.45%\n",
      "Train Epoch [69/300]: Loss: 0.0714, Accuracy: 97.63%\n",
      "Train Epoch [70/300]: Loss: 0.0786, Accuracy: 97.29%\n",
      "Validation Epoch [70]: Loss: 0.2801, Accuracy: 91.56%\n",
      "New best model saved with accuracy: 91.56%\n",
      "Train Epoch [71/300]: Loss: 0.0709, Accuracy: 97.65%\n",
      "Train Epoch [72/300]: Loss: 0.0785, Accuracy: 97.29%\n",
      "Train Epoch [73/300]: Loss: 0.0682, Accuracy: 97.63%\n",
      "Train Epoch [74/300]: Loss: 0.0677, Accuracy: 97.66%\n",
      "Train Epoch [75/300]: Loss: 0.0685, Accuracy: 97.63%\n",
      "Validation Epoch [75]: Loss: 0.3442, Accuracy: 90.40%\n",
      "Train Epoch [76/300]: Loss: 0.0693, Accuracy: 97.59%\n",
      "Train Epoch [77/300]: Loss: 0.0786, Accuracy: 97.27%\n",
      "Train Epoch [78/300]: Loss: 0.0771, Accuracy: 97.31%\n",
      "Train Epoch [79/300]: Loss: 0.0662, Accuracy: 97.80%\n",
      "Train Epoch [80/300]: Loss: 0.0708, Accuracy: 97.58%\n",
      "Validation Epoch [80]: Loss: 0.4199, Accuracy: 89.32%\n",
      "Train Epoch [81/300]: Loss: 0.0745, Accuracy: 97.42%\n",
      "Train Epoch [82/300]: Loss: 0.0877, Accuracy: 97.01%\n",
      "Train Epoch [83/300]: Loss: 0.0764, Accuracy: 97.35%\n",
      "Train Epoch [84/300]: Loss: 0.0829, Accuracy: 97.11%\n",
      "Train Epoch [85/300]: Loss: 0.0651, Accuracy: 97.77%\n",
      "Validation Epoch [85]: Loss: 0.6013, Accuracy: 84.08%\n",
      "Train Epoch [86/300]: Loss: 0.1159, Accuracy: 96.07%\n",
      "Train Epoch [87/300]: Loss: 0.0860, Accuracy: 97.07%\n",
      "Train Epoch [88/300]: Loss: 0.0644, Accuracy: 97.76%\n",
      "Train Epoch [89/300]: Loss: 0.0849, Accuracy: 97.01%\n",
      "Train Epoch [90/300]: Loss: 0.0735, Accuracy: 97.45%\n",
      "Validation Epoch [90]: Loss: 0.3545, Accuracy: 90.00%\n",
      "Train Epoch [91/300]: Loss: 0.0356, Accuracy: 98.95%\n",
      "Train Epoch [92/300]: Loss: 0.0237, Accuracy: 99.38%\n",
      "Train Epoch [93/300]: Loss: 0.0193, Accuracy: 99.51%\n",
      "Train Epoch [94/300]: Loss: 0.0166, Accuracy: 99.60%\n",
      "Train Epoch [95/300]: Loss: 0.0153, Accuracy: 99.65%\n",
      "Validation Epoch [95]: Loss: 0.2403, Accuracy: 93.08%\n",
      "New best model saved with accuracy: 93.08%\n",
      "Train Epoch [96/300]: Loss: 0.0136, Accuracy: 99.69%\n",
      "Train Epoch [97/300]: Loss: 0.0137, Accuracy: 99.67%\n",
      "Train Epoch [98/300]: Loss: 0.0107, Accuracy: 99.80%\n",
      "Train Epoch [99/300]: Loss: 0.0111, Accuracy: 99.75%\n",
      "Train Epoch [100/300]: Loss: 0.0103, Accuracy: 99.78%\n",
      "Validation Epoch [100]: Loss: 0.2477, Accuracy: 93.20%\n",
      "New best model saved with accuracy: 93.20%\n",
      "Train Epoch [101/300]: Loss: 0.0098, Accuracy: 99.78%\n",
      "Train Epoch [102/300]: Loss: 0.0093, Accuracy: 99.81%\n",
      "Train Epoch [103/300]: Loss: 0.0092, Accuracy: 99.81%\n",
      "Train Epoch [104/300]: Loss: 0.0078, Accuracy: 99.86%\n",
      "Train Epoch [105/300]: Loss: 0.0089, Accuracy: 99.82%\n",
      "Validation Epoch [105]: Loss: 0.2358, Accuracy: 93.24%\n",
      "New best model saved with accuracy: 93.24%\n",
      "Train Epoch [106/300]: Loss: 0.0075, Accuracy: 99.88%\n",
      "Train Epoch [107/300]: Loss: 0.0072, Accuracy: 99.87%\n",
      "Train Epoch [108/300]: Loss: 0.0071, Accuracy: 99.86%\n",
      "Train Epoch [109/300]: Loss: 0.0074, Accuracy: 99.85%\n",
      "Train Epoch [110/300]: Loss: 0.0069, Accuracy: 99.87%\n",
      "Validation Epoch [110]: Loss: 0.2482, Accuracy: 93.28%\n",
      "New best model saved with accuracy: 93.28%\n",
      "Train Epoch [111/300]: Loss: 0.0061, Accuracy: 99.90%\n",
      "Train Epoch [112/300]: Loss: 0.0055, Accuracy: 99.93%\n",
      "Train Epoch [113/300]: Loss: 0.0059, Accuracy: 99.89%\n",
      "Train Epoch [114/300]: Loss: 0.0057, Accuracy: 99.91%\n",
      "Train Epoch [115/300]: Loss: 0.0057, Accuracy: 99.89%\n",
      "Validation Epoch [115]: Loss: 0.2572, Accuracy: 93.32%\n",
      "New best model saved with accuracy: 93.32%\n",
      "Train Epoch [116/300]: Loss: 0.0052, Accuracy: 99.91%\n",
      "Train Epoch [117/300]: Loss: 0.0052, Accuracy: 99.93%\n",
      "Train Epoch [118/300]: Loss: 0.0056, Accuracy: 99.88%\n",
      "Train Epoch [119/300]: Loss: 0.0051, Accuracy: 99.93%\n",
      "Train Epoch [120/300]: Loss: 0.0052, Accuracy: 99.92%\n",
      "Validation Epoch [120]: Loss: 0.2532, Accuracy: 93.76%\n",
      "New best model saved with accuracy: 93.76%\n",
      "Train Epoch [121/300]: Loss: 0.0049, Accuracy: 99.92%\n",
      "Train Epoch [122/300]: Loss: 0.0049, Accuracy: 99.93%\n",
      "Train Epoch [123/300]: Loss: 0.0045, Accuracy: 99.93%\n",
      "Train Epoch [124/300]: Loss: 0.0047, Accuracy: 99.91%\n",
      "Train Epoch [125/300]: Loss: 0.0050, Accuracy: 99.93%\n",
      "Validation Epoch [125]: Loss: 0.2573, Accuracy: 93.00%\n",
      "Train Epoch [126/300]: Loss: 0.0050, Accuracy: 99.92%\n",
      "Train Epoch [127/300]: Loss: 0.0043, Accuracy: 99.94%\n",
      "Train Epoch [128/300]: Loss: 0.0047, Accuracy: 99.91%\n",
      "Train Epoch [129/300]: Loss: 0.0040, Accuracy: 99.94%\n",
      "Train Epoch [130/300]: Loss: 0.0039, Accuracy: 99.96%\n",
      "Validation Epoch [130]: Loss: 0.2665, Accuracy: 93.56%\n",
      "Train Epoch [131/300]: Loss: 0.0041, Accuracy: 99.94%\n",
      "Train Epoch [132/300]: Loss: 0.0040, Accuracy: 99.95%\n",
      "Train Epoch [133/300]: Loss: 0.0039, Accuracy: 99.94%\n",
      "Train Epoch [134/300]: Loss: 0.0039, Accuracy: 99.94%\n",
      "Train Epoch [135/300]: Loss: 0.0045, Accuracy: 99.93%\n",
      "Validation Epoch [135]: Loss: 0.2512, Accuracy: 93.68%\n",
      "Train Epoch [136/300]: Loss: 0.0037, Accuracy: 99.96%\n",
      "Train Epoch [137/300]: Loss: 0.0038, Accuracy: 99.95%\n",
      "Train Epoch [138/300]: Loss: 0.0036, Accuracy: 99.94%\n",
      "Train Epoch [139/300]: Loss: 0.0039, Accuracy: 99.95%\n",
      "Train Epoch [140/300]: Loss: 0.0045, Accuracy: 99.94%\n",
      "Validation Epoch [140]: Loss: 0.2846, Accuracy: 93.04%\n",
      "Train Epoch [141/300]: Loss: 0.0036, Accuracy: 99.96%\n",
      "Train Epoch [142/300]: Loss: 0.0033, Accuracy: 99.97%\n",
      "Train Epoch [143/300]: Loss: 0.0035, Accuracy: 99.96%\n",
      "Train Epoch [144/300]: Loss: 0.0034, Accuracy: 99.96%\n",
      "Train Epoch [145/300]: Loss: 0.0034, Accuracy: 99.96%\n",
      "Validation Epoch [145]: Loss: 0.2645, Accuracy: 92.96%\n",
      "Train Epoch [146/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [147/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [148/300]: Loss: 0.0035, Accuracy: 99.95%\n",
      "Train Epoch [149/300]: Loss: 0.0037, Accuracy: 99.95%\n",
      "Train Epoch [150/300]: Loss: 0.0035, Accuracy: 99.95%\n",
      "Validation Epoch [150]: Loss: 0.2638, Accuracy: 93.72%\n",
      "Train Epoch [151/300]: Loss: 0.0035, Accuracy: 99.95%\n",
      "Train Epoch [152/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [153/300]: Loss: 0.0034, Accuracy: 99.96%\n",
      "Train Epoch [154/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [155/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Validation Epoch [155]: Loss: 0.2702, Accuracy: 93.04%\n",
      "Train Epoch [156/300]: Loss: 0.0034, Accuracy: 99.95%\n",
      "Train Epoch [157/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [158/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [159/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [160/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Validation Epoch [160]: Loss: 0.2602, Accuracy: 93.28%\n",
      "Train Epoch [161/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Train Epoch [162/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [163/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Train Epoch [164/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [165/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Validation Epoch [165]: Loss: 0.2788, Accuracy: 93.04%\n",
      "Train Epoch [166/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [167/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [168/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [169/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [170/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [170]: Loss: 0.2611, Accuracy: 93.40%\n",
      "Train Epoch [171/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [172/300]: Loss: 0.0030, Accuracy: 99.98%\n",
      "Train Epoch [173/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [174/300]: Loss: 0.0031, Accuracy: 99.98%\n",
      "Train Epoch [175/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Validation Epoch [175]: Loss: 0.2897, Accuracy: 93.04%\n",
      "Train Epoch [176/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [177/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [178/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [179/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [180/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [180]: Loss: 0.2701, Accuracy: 93.68%\n",
      "Train Epoch [181/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [182/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [183/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [184/300]: Loss: 0.0030, Accuracy: 99.98%\n",
      "Train Epoch [185/300]: Loss: 0.0029, Accuracy: 99.96%\n",
      "Validation Epoch [185]: Loss: 0.2750, Accuracy: 93.08%\n",
      "Train Epoch [186/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [187/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [188/300]: Loss: 0.0031, Accuracy: 99.95%\n",
      "Train Epoch [189/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Train Epoch [190/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Validation Epoch [190]: Loss: 0.2710, Accuracy: 93.32%\n",
      "Train Epoch [191/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [192/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [193/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [194/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [195/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Validation Epoch [195]: Loss: 0.2733, Accuracy: 93.12%\n",
      "Train Epoch [196/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [197/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [198/300]: Loss: 0.0035, Accuracy: 99.95%\n",
      "Train Epoch [199/300]: Loss: 0.0029, Accuracy: 99.96%\n",
      "Train Epoch [200/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Validation Epoch [200]: Loss: 0.2610, Accuracy: 93.40%\n",
      "Train Epoch [201/300]: Loss: 0.0033, Accuracy: 99.95%\n",
      "Train Epoch [202/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [203/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [204/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Train Epoch [205/300]: Loss: 0.0033, Accuracy: 99.95%\n",
      "Validation Epoch [205]: Loss: 0.2434, Accuracy: 93.88%\n",
      "New best model saved with accuracy: 93.88%\n",
      "Train Epoch [206/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [207/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [208/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [209/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [210/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Validation Epoch [210]: Loss: 0.2793, Accuracy: 93.32%\n",
      "Train Epoch [211/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [212/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Train Epoch [213/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [214/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [215/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Validation Epoch [215]: Loss: 0.2572, Accuracy: 93.24%\n",
      "Train Epoch [216/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [217/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [218/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [219/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [220/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Validation Epoch [220]: Loss: 0.2769, Accuracy: 93.04%\n",
      "Train Epoch [221/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [222/300]: Loss: 0.0030, Accuracy: 99.98%\n",
      "Train Epoch [223/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [224/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [225/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Validation Epoch [225]: Loss: 0.2657, Accuracy: 93.08%\n",
      "Train Epoch [226/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [227/300]: Loss: 0.0027, Accuracy: 99.98%\n",
      "Train Epoch [228/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [229/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [230/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [230]: Loss: 0.2541, Accuracy: 93.88%\n",
      "Train Epoch [231/300]: Loss: 0.0036, Accuracy: 99.95%\n",
      "Train Epoch [232/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [233/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Train Epoch [234/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [235/300]: Loss: 0.0031, Accuracy: 99.95%\n",
      "Validation Epoch [235]: Loss: 0.2645, Accuracy: 93.56%\n",
      "Train Epoch [236/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [237/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [238/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [239/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [240/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Validation Epoch [240]: Loss: 0.2545, Accuracy: 93.60%\n",
      "Train Epoch [241/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [242/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [243/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [244/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [245/300]: Loss: 0.0034, Accuracy: 99.94%\n",
      "Validation Epoch [245]: Loss: 0.2497, Accuracy: 93.72%\n",
      "Train Epoch [246/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [247/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [248/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [249/300]: Loss: 0.0033, Accuracy: 99.95%\n",
      "Train Epoch [250/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Validation Epoch [250]: Loss: 0.2646, Accuracy: 93.16%\n",
      "Train Epoch [251/300]: Loss: 0.0030, Accuracy: 99.98%\n",
      "Train Epoch [252/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [253/300]: Loss: 0.0028, Accuracy: 99.98%\n",
      "Train Epoch [254/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [255/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Validation Epoch [255]: Loss: 0.2516, Accuracy: 93.68%\n",
      "Train Epoch [256/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [257/300]: Loss: 0.0026, Accuracy: 99.98%\n",
      "Train Epoch [258/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Train Epoch [259/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Train Epoch [260/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [260]: Loss: 0.2714, Accuracy: 93.68%\n",
      "Train Epoch [261/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [262/300]: Loss: 0.0028, Accuracy: 99.97%\n",
      "Train Epoch [263/300]: Loss: 0.0033, Accuracy: 99.96%\n",
      "Train Epoch [264/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [265/300]: Loss: 0.0027, Accuracy: 99.98%\n",
      "Validation Epoch [265]: Loss: 0.2537, Accuracy: 94.12%\n",
      "New best model saved with accuracy: 94.12%\n",
      "Train Epoch [266/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [267/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [268/300]: Loss: 0.0030, Accuracy: 99.98%\n",
      "Train Epoch [269/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [270/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Validation Epoch [270]: Loss: 0.2793, Accuracy: 93.16%\n",
      "Train Epoch [271/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [272/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [273/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [274/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [275/300]: Loss: 0.0028, Accuracy: 99.97%\n",
      "Validation Epoch [275]: Loss: 0.2693, Accuracy: 92.80%\n",
      "Train Epoch [276/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [277/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [278/300]: Loss: 0.0035, Accuracy: 99.95%\n",
      "Train Epoch [279/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [280/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [280]: Loss: 0.2719, Accuracy: 93.80%\n",
      "Train Epoch [281/300]: Loss: 0.0028, Accuracy: 99.97%\n",
      "Train Epoch [282/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [283/300]: Loss: 0.0031, Accuracy: 99.97%\n",
      "Train Epoch [284/300]: Loss: 0.0029, Accuracy: 99.96%\n",
      "Train Epoch [285/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Validation Epoch [285]: Loss: 0.2851, Accuracy: 93.40%\n",
      "Train Epoch [286/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Train Epoch [287/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [288/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Train Epoch [289/300]: Loss: 0.0030, Accuracy: 99.97%\n",
      "Train Epoch [290/300]: Loss: 0.0031, Accuracy: 99.96%\n",
      "Validation Epoch [290]: Loss: 0.2701, Accuracy: 93.20%\n",
      "Train Epoch [291/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [292/300]: Loss: 0.0032, Accuracy: 99.95%\n",
      "Train Epoch [293/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [294/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [295/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Validation Epoch [295]: Loss: 0.2575, Accuracy: 93.52%\n",
      "Train Epoch [296/300]: Loss: 0.0032, Accuracy: 99.96%\n",
      "Train Epoch [297/300]: Loss: 0.0032, Accuracy: 99.97%\n",
      "Train Epoch [298/300]: Loss: 0.0029, Accuracy: 99.98%\n",
      "Train Epoch [299/300]: Loss: 0.0030, Accuracy: 99.96%\n",
      "Train Epoch [300/300]: Loss: 0.0029, Accuracy: 99.97%\n",
      "Validation Epoch [300]: Loss: 0.2710, Accuracy: 93.12%\n",
      "Training complete. Best validation accuracy: 94.12%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters and settings\n",
    "config = {\n",
    "    \"model\": \"resnet50_cifar10\",\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 300,\n",
    "    \"validation_frequency\": 5,\n",
    "    \"seed\": 1,\n",
    "    \"criterion\": nn.CrossEntropyLoss,\n",
    "    \"optimizer\": optim.SGD,\n",
    "    \"lr\": 0.1,\n",
    "    \"optimizer_kwargs\": {\"momentum\": 0.9, \"weight_decay\": 0.0005, \"nesterov\": True},\n",
    "    \"scheduler\": ReduceLROnPlateau,\n",
    "    \"scheduler_kwargs\": {\"factor\": 0.1, \"patience\": 3, \"threshold\": 0.001, \"mode\": \"max\"},\n",
    "}\n",
    "\n",
    "# Set the seed\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Model: ResNet-50\n",
    "model = models.resnet50(pretrained=False, num_classes=10)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "# Criterion\n",
    "criterion = config[\"criterion\"]()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = config[\"optimizer\"](model.parameters(), lr=config[\"lr\"], **config[\"optimizer_kwargs\"])\n",
    "\n",
    "# Scheduler\n",
    "scheduler = config[\"scheduler\"](optimizer, **config[\"scheduler_kwargs\"])\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate stats\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100.0 * correct / total\n",
    "    print(f\"Train Epoch [{epoch}/{config['epochs']}]: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Calculate stats\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = 100.0 * correct / total\n",
    "    print(f\"Validation Epoch [{epoch}]: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Training loop\n",
    "best_accuracy = 0.0\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    train_loss, train_accuracy = train_one_epoch(epoch)\n",
    "    \n",
    "    if epoch % config[\"validation_frequency\"] == 0:\n",
    "        val_loss, val_accuracy = validate(epoch)\n",
    "\n",
    "        # Adjust learning rate based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        # Save best model\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"best_resnet50_cifar10.pth\")\n",
    "            print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Training complete. Best validation accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\333305979.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'student_resnet50_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9423\n",
      "test accuracy:  0.9482\n",
      "test accuracy:  0.9545\n",
      "test accuracy:  0.956\n",
      "test accuracy:  0.958\n",
      "test accuracy:  0.9574\n",
      "test accuracy:  0.9603\n",
      "test accuracy:  0.9576\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from torch.quantization import fuse_modules\n",
    "\n",
    "checkpoints = [x for x in range(25, 201, 25)]\n",
    "test_accs = []\n",
    "\n",
    "for cp in checkpoints:\n",
    "    student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=True, fuse=False, qat=False, dif_arch=True)\n",
    "    checkpoint = torch.load(f'student_resnet50_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n",
    "    student_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    student_net.to(fast_device)\n",
    "    student_net.eval()\n",
    "\n",
    "    student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "    student_net_prepared = torch.ao.quantization.prepare(student_net)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in train_loader:\n",
    "            inputs = inputs.to(fast_device)\n",
    "            student_net_prepared(inputs)  # Run a forward pass to collect activation statistics\n",
    "    student_net_prepared.to('cpu')\n",
    "\n",
    "    student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
    "    reproducibilitySeed()\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net_int8, test_loader, 'cpu')\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    test_accs.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_68156\\1959200769.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'student_teacher_learn_BiT\\T=7, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9429\n",
      "test accuracy:  0.9459\n",
      "test accuracy:  0.9468\n",
      "test accuracy:  0.9487\n",
      "test accuracy:  0.9502\n",
      "test accuracy:  0.9498\n",
      "test accuracy:  0.9519\n",
      "test accuracy:  0.9518\n",
      "test accuracy:  0.9525\n",
      "test accuracy:  0.9513\n",
      "test accuracy:  0.953\n",
      "test accuracy:  0.9549\n",
      "test accuracy:  0.9517\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [x for x in range(100, 401, 25)]\n",
    "test_accs = []\n",
    "\n",
    "for cp in checkpoints:\n",
    "    student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=False, fuse=False, qat=False, dif_arch=True)\n",
    "    checkpoint = torch.load(f'student_teacher_learn_BiT\\T=7, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_{cp}.tar')\n",
    "    student_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    student_net.to(fast_device)\n",
    "    student_net.eval()\n",
    "\n",
    "    reproducibilitySeed()\n",
    "    _, test_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "    print('test accuracy: ', test_accuracy)\n",
    "    test_accs.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved at: student_teacher_learn_BiT\\unquantized_scores.json\n"
     ]
    }
   ],
   "source": [
    "zip_lists_to_json(checkpoints, test_accs, 'student_teacher_learn_BiT', filename='unquantized_scores.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_net.eval()\n",
    "\n",
    "student_net.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "student_net_prepared = torch.ao.quantization.prepare(student_net)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.to(fast_device)\n",
    "        student_net_prepared(inputs)  # Run a forward pass to collect activation statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): Conv2d(\n",
       "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (activation_post_process): HistogramObserver(min_val=-4.539089679718018, max_val=4.928720951080322)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(\n",
       "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "        (activation_post_process): HistogramObserver(min_val=-4.455965995788574, max_val=6.05128288269043)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.419429302215576, max_val=9.073027610778809)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.119362831115723, max_val=8.924439430236816)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-9.640437126159668, max_val=4.69738245010376)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.7801079750061035, max_val=6.7098822593688965)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.095983982086182, max_val=7.026950359344482)\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-11.257030487060547, max_val=9.500932693481445)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.715474605560303, max_val=4.953790664672852)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.470907211303711, max_val=3.534721851348877)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-12.049206733703613, max_val=5.953272342681885)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-12.049206733703613, max_val=10.328696250915527)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-10.030402183532715, max_val=8.610897064208984)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.001376628875732, max_val=7.3359375)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.158465385437012, max_val=2.337920904159546)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.541430950164795, max_val=3.9949324131011963)\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(\n",
       "              64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=-4.314838409423828, max_val=4.607681751251221)\n",
       "            )\n",
       "            (1): BatchNorm2d(\n",
       "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (activation_post_process): HistogramObserver(min_val=-5.464797019958496, max_val=5.353374481201172)\n",
       "            )\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-6.597156524658203, max_val=6.537437438964844)\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.435968399047852, max_val=3.804121732711792)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.245702266693115, max_val=3.5925838947296143)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-2.4180848598480225, max_val=2.3326497077941895)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.617290496826172, max_val=4.642918109893799)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.617290496826172, max_val=8.645076751708984)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-6.769078254699707, max_val=5.233593463897705)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-3.2542190551757812, max_val=5.039158821105957)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-3.96917724609375, max_val=3.6997570991516113)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-2.938992500305176, max_val=5.10924768447876)\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(\n",
       "              128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=-1.3613437414169312, max_val=1.091618299484253)\n",
       "            )\n",
       "            (1): BatchNorm2d(\n",
       "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (activation_post_process): HistogramObserver(min_val=-2.3111073970794678, max_val=1.1978989839553833)\n",
       "            )\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-3.4471869468688965, max_val=5.693908214569092)\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-3.1337218284606934, max_val=3.4005911350250244)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-3.448746919631958, max_val=4.947140693664551)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-1.9595959186553955, max_val=3.2705020904541016)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-6.024975299835205, max_val=8.156042098999023)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.941572189331055, max_val=9.472945213317871)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-4.532700538635254, max_val=7.804793834686279)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.255317211151123, max_val=5.475815296173096)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-5.097604751586914, max_val=2.327594518661499)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-10.071745872497559, max_val=8.0810546875)\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=-2.8778750896453857, max_val=4.012026786804199)\n",
       "            )\n",
       "            (1): BatchNorm2d(\n",
       "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (activation_post_process): HistogramObserver(min_val=-4.684586048126221, max_val=4.628907680511475)\n",
       "            )\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-9.287775993347168, max_val=9.57108211517334)\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.4746479988098145, max_val=6.154197692871094)\n",
       "          )\n",
       "          (bn1): BatchNorm2d(\n",
       "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-7.083560943603516, max_val=5.636285305023193)\n",
       "          )\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=-1.3981186151504517, max_val=1.7423951625823975)\n",
       "          )\n",
       "          (bn2): BatchNorm2d(\n",
       "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=-22.24444007873535, max_val=29.99346160888672)\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): HistogramObserver(min_val=-20.929405212402344, max_val=29.99346160888672)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(\n",
       "        in_features=512, out_features=10, bias=True\n",
       "        (activation_post_process): HistogramObserver(min_val=-3.2373692989349365, max_val=8.57269287109375)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=-2.429065704345703, max_val=2.7537312507629395)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_net_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.047721993178129196, zero_point=63, padding=(1, 1), bias=False)\n",
       "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06531138718128204, zero_point=73, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.047628093510866165, zero_point=63, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.05907723680138588, zero_point=58\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0780530646443367, zero_point=80, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04019875079393387, zero_point=60, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.07192700356245041, zero_point=60\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.06973572820425034, zero_point=67, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.028673456981778145, zero_point=76, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.037289224565029144, zero_point=70, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.0399952232837677, zero_point=68\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.03351845219731331, zero_point=69, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.020256230607628822, zero_point=64, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.04395361617207527, zero_point=56\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.04347055405378342, zero_point=69, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.030015746131539345, zero_point=68, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.011798163875937462, zero_point=71, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.04755129665136337, zero_point=47\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03416686877608299, zero_point=58, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.029519038274884224, zero_point=51, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.08069150149822235, zero_point=42\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.07138875871896744, zero_point=51, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0423080138862133, zero_point=87, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.038012925535440445, zero_point=55, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.09121417999267578, zero_point=72\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07288740575313568, zero_point=72, padding=(1, 1), bias=False)\n",
       "          (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.015672625973820686, zero_point=49, padding=(1, 1), bias=False)\n",
       "          (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.2823218107223511, zero_point=44\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.08209504932165146, zero_point=32, qscheme=torch.per_channel_affine)\n",
       "    )\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_net_prepared.to('cpu')\n",
    "\n",
    "student_net_int8 = torch.ao.quantization.convert(student_net_prepared)\n",
    "\n",
    "student_net_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9518\n"
     ]
    }
   ],
   "source": [
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(student_net_int8, test_loader, 'cpu')\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\276973708.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9616\n"
     ]
    }
   ],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get logits from both models\n",
    "        logits1 = self.model1(x)\n",
    "        logits2 = self.model2(x)\n",
    "        \n",
    "        # Average the logits\n",
    "        averaged_logits = (logits1 + logits2) / 2\n",
    "        return averaged_logits\n",
    "\n",
    "# Instantiate the teacher networks\n",
    "teacher_net_1 = networks.TeacherNetwork50()\n",
    "checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
    "\n",
    "\n",
    "teacher_net_1.model.load_state_dict(checkpoint)\n",
    "teacher_net_2 = networks.TeacherNetworkBiT()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = EnsembleModel(teacher_net_1, teacher_net_2)\n",
    "\n",
    "# Move the ensemble model to the appropriate device (e.g., GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(ensemble_model, test_loader, device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\438022654.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_1 = torch.load('student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\438022654.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_2 = torch.load('student_teacher_learn_BiT\\T=7, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n"
     ]
    }
   ],
   "source": [
    "checkpoint_1 = torch.load('student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n",
    "checkpoint_2 = torch.load('student_teacher_learn_BiT\\T=7, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def moving_average(data, window_size=20):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Apply moving average smoothing\n",
    "smoothed_checkpoint = moving_average(checkpoint_1['val_acc'][75:], window_size=15)\n",
    "smoothed_checkpoint_2 = moving_average(checkpoint_2['val_acc'], window_size=15)\n",
    "\n",
    "# Plot original data\n",
    "plt.plot(checkpoint_1['val_acc'][75:], alpha=0.3, label='Raw Checkpoint 1', color='blue')\n",
    "plt.plot(checkpoint_2['val_acc'], alpha=0.3, label='Raw Checkpoint 2', color='orange')\n",
    "\n",
    "# Plot smoothed trend lines\n",
    "plt.plot(smoothed_checkpoint, label='Trend Checkpoint 1', color='blue')\n",
    "plt.plot(smoothed_checkpoint_2, label='Trend Checkpoint 2', color='orange')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['val_acc'][199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\2141263293.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('resnet18_pretrained.bin')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9324\n"
     ]
    }
   ],
   "source": [
    "teacher_net = networks.TeacherNetwork()\n",
    "\n",
    "checkpoint = torch.load('resnet18_pretrained.bin')\n",
    "\n",
    "teacher_net.model.load_state_dict(checkpoint)\n",
    "teacher_net.to(fast_device)\n",
    "\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\3685477170.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_with_CE = torch.load('student_BiT_adam\\T=4, alpha=0.9, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_50.tar')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "checkpoint_with_CE = torch.load('student_BiT_adam\\T=4, alpha=0.9, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_50.tar')\n",
    "# Example data: Replace this with your actual data\n",
    "val_acc_CE = checkpoint_with_CE['val_acc']  # Assuming this is your validation accuracy data\n",
    "val_acc_no_CE = checkpoint['val_acc']\n",
    "\n",
    "# Define a function for moving average\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Apply smoothing\n",
    "window_size = 10  # Adjust this to control the degree of smoothing\n",
    "\n",
    "# Plot original and smoothed curves\n",
    "plt.plot(val_acc_CE, label=\"CE Accuracy\")\n",
    "plt.plot(val_acc_no_CE, label='No CE Validaiton')\n",
    "plt.axhline(y=.9752, color='green', linestyle='--', label='BiT-M-R50x1 - .9752')\n",
    "plt.legend()\n",
    "plt.title(\"S-Resnet18, T-BiT-M-R50x1 Alpha .9 vs 1.0\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "#plt.savefig(\"T-BiT-M-R50x1_alpha_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_student.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): ConvBnReLU2d(\n",
       "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([2.5067e-04, 2.8638e-04, 1.1144e-10, 1.5957e-03, 5.1110e-05, 5.3746e-04,\n",
       "                  1.1691e-03, 1.0979e-06, 3.4271e-04, 2.8620e-09, 1.3318e-03, 1.3878e-03,\n",
       "                  1.4170e-03, 7.4514e-09, 3.9458e-04, 3.1306e-04, 3.7666e-04, 1.2327e-03,\n",
       "                  1.0044e-03, 9.7093e-04, 5.0394e-04, 7.7511e-04, 1.9837e-03, 3.9344e-04,\n",
       "                  1.4284e-03, 5.3648e-04, 3.8778e-04, 9.5485e-04, 6.9514e-04, 3.4946e-04,\n",
       "                  1.0932e-03, 6.8335e-04, 1.4066e-03, 6.1146e-04, 1.0076e-03, 4.8166e-04,\n",
       "                  5.1811e-06, 7.7614e-04, 2.6815e-05, 4.1370e-04, 5.8854e-04, 7.8094e-04,\n",
       "                  1.5081e-03, 2.1072e-04, 8.6365e-04, 1.8344e-03, 5.8992e-04, 6.7660e-04,\n",
       "                  6.3378e-09, 7.4354e-04, 1.6671e-03, 2.0686e-03, 6.6972e-04, 2.5065e-04,\n",
       "                  8.2445e-04, 9.3383e-04, 3.1615e-04, 1.0150e-03, 1.2514e-03, 8.5081e-04,\n",
       "                  1.9907e-03, 8.8760e-04, 1.4076e-03, 4.3241e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-3.2085e-02, -3.0577e-02, -1.3777e-08, -2.0425e-01, -6.5421e-03,\n",
       "                    -6.6461e-02, -1.4965e-01, -1.2973e-04, -4.3581e-02, -3.4019e-07,\n",
       "                    -1.5350e-01, -1.7763e-01, -1.8137e-01, -9.5378e-07, -4.8736e-02,\n",
       "                    -4.0071e-02, -4.5950e-02, -1.5779e-01, -1.2856e-01, -1.2422e-01,\n",
       "                    -6.4504e-02, -9.9214e-02, -2.5391e-01, -4.7962e-02, -1.6914e-01,\n",
       "                    -6.8670e-02, -4.9636e-02, -1.2222e-01, -7.8741e-02, -3.4048e-02,\n",
       "                    -1.3993e-01, -7.7081e-02, -1.8005e-01, -7.8267e-02, -1.2628e-01,\n",
       "                    -6.1653e-02, -6.6318e-04, -9.4442e-02, -2.9827e-03, -5.2308e-02,\n",
       "                    -7.5334e-02, -9.9961e-02, -1.9304e-01, -2.6973e-02, -1.1055e-01,\n",
       "                    -2.3481e-01, -7.5510e-02, -8.6605e-02, -7.3713e-07, -8.3349e-02,\n",
       "                    -2.1338e-01, -2.6478e-01, -8.5724e-02, -3.2084e-02, -1.0553e-01,\n",
       "                    -1.1929e-01, -3.5924e-02, -1.0720e-01, -1.6017e-01, -1.0890e-01,\n",
       "                    -2.5481e-01, -9.3020e-02, -1.4632e-01, -5.5349e-02]), max_val=tensor([3.0255e-02, 3.6371e-02, 1.4153e-08, 1.9516e-01, 6.0909e-03, 6.8257e-02,\n",
       "                    1.4789e-01, 1.3943e-04, 4.3524e-02, 3.6348e-07, 1.6914e-01, 1.5168e-01,\n",
       "                    1.7542e-01, 9.3387e-07, 5.0112e-02, 3.9450e-02, 4.7836e-02, 1.4938e-01,\n",
       "                    1.2598e-01, 1.2331e-01, 5.2769e-02, 9.2190e-02, 1.7581e-01, 4.9967e-02,\n",
       "                    1.8141e-01, 6.5110e-02, 4.3844e-02, 1.2032e-01, 8.8283e-02, 4.4382e-02,\n",
       "                    1.3400e-01, 8.6786e-02, 1.4247e-01, 5.9403e-02, 1.2797e-01, 4.5843e-02,\n",
       "                    6.0045e-04, 9.8570e-02, 3.4055e-03, 5.2540e-02, 7.4736e-02, 9.3982e-02,\n",
       "                    1.9003e-01, 2.6309e-02, 1.0895e-01, 2.1467e-01, 7.1051e-02, 7.8497e-02,\n",
       "                    8.0490e-07, 9.4430e-02, 1.8851e-01, 2.4787e-01, 8.1930e-02, 3.1632e-02,\n",
       "                    1.0460e-01, 1.1860e-01, 4.0151e-02, 1.2891e-01, 1.4256e-01, 1.0269e-01,\n",
       "                    2.4427e-01, 1.1273e-01, 1.7876e-01, 4.8613e-02])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0277]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.5221753120422363)\n",
       "        )\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0056, 0.0019, 0.0034, 0.0027, 0.0031, 0.0022, 0.0021, 0.0010, 0.0030,\n",
       "                      0.0017, 0.0007, 0.0022, 0.0026, 0.0008, 0.0018, 0.0019, 0.0032, 0.0024,\n",
       "                      0.0024, 0.0023, 0.0053, 0.0024, 0.0045, 0.0013, 0.0025, 0.0027, 0.0007,\n",
       "                      0.0016, 0.0021, 0.0037, 0.0022, 0.0040, 0.0015, 0.0031, 0.0044, 0.0028,\n",
       "                      0.0033, 0.0021, 0.0026, 0.0051, 0.0017, 0.0030, 0.0010, 0.0011, 0.0012,\n",
       "                      0.0032, 0.0036, 0.0024, 0.0036, 0.0015, 0.0015, 0.0022, 0.0017, 0.0058,\n",
       "                      0.0026, 0.0032, 0.0026, 0.0025, 0.0032, 0.0027, 0.0019, 0.0035, 0.0007,\n",
       "                      0.0028]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.7228, -0.2467, -0.4098, -0.3447, -0.3975, -0.2840, -0.1453, -0.1209,\n",
       "                        -0.3868, -0.2174, -0.0795, -0.2822, -0.3304, -0.0980, -0.2300, -0.2399,\n",
       "                        -0.4062, -0.3047, -0.3037, -0.2953, -0.6790, -0.3045, -0.5746, -0.1676,\n",
       "                        -0.3157, -0.3481, -0.0852, -0.2069, -0.2207, -0.1557, -0.1181, -0.5077,\n",
       "                        -0.1890, -0.3975, -0.5582, -0.3635, -0.4171, -0.2690, -0.3278, -0.6575,\n",
       "                        -0.2214, -0.3849, -0.1253, -0.0455, -0.1479, -0.4069, -0.4610, -0.3117,\n",
       "                        -0.4620, -0.1881, -0.1972, -0.2776, -0.2117, -0.7427, -0.3346, -0.1008,\n",
       "                        -0.3283, -0.3199, -0.4050, -0.2872, -0.2036, -0.4493, -0.0432, -0.3609]), max_val=tensor([0.1208, 0.1648, 0.4340, 0.1457, 0.1298, 0.1412, 0.2690, 0.1238, 0.1003,\n",
       "                        0.1135, 0.0862, 0.0973, 0.1392, 0.0877, 0.1437, 0.1790, 0.1613, 0.1617,\n",
       "                        0.1491, 0.1449, 0.2295, 0.0782, 0.1732, 0.0947, 0.2822, 0.0839, 0.0828,\n",
       "                        0.0695, 0.2633, 0.4694, 0.2836, 0.1648, 0.1068, 0.1037, 0.1086, 0.1250,\n",
       "                        0.1757, 0.2594, 0.0743, 0.1836, 0.2015, 0.1313, 0.1236, 0.1376, 0.0592,\n",
       "                        0.2164, 0.1437, 0.1040, 0.1473, 0.0963, 0.1127, 0.1102, 0.1777, 0.2147,\n",
       "                        0.0680, 0.4053, 0.1660, 0.1744, 0.1589, 0.3450, 0.2411, 0.1670, 0.0950,\n",
       "                        0.1415])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0192]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.4346745014190674)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0013, 0.0011, 0.0011, 0.0030, 0.0006, 0.0018, 0.0011, 0.0017, 0.0020,\n",
       "                      0.0026, 0.0004, 0.0033, 0.0029, 0.0029, 0.0031, 0.0014, 0.0015, 0.0029,\n",
       "                      0.0022, 0.0014, 0.0037, 0.0013, 0.0026, 0.0019, 0.0016, 0.0009, 0.0017,\n",
       "                      0.0014, 0.0025, 0.0018, 0.0018, 0.0025, 0.0010, 0.0030, 0.0011, 0.0009,\n",
       "                      0.0018, 0.0037, 0.0015, 0.0016, 0.0019, 0.0007, 0.0020, 0.0018, 0.0015,\n",
       "                      0.0011, 0.0029, 0.0014, 0.0029, 0.0034, 0.0026, 0.0014, 0.0013, 0.0010,\n",
       "                      0.0031, 0.0010, 0.0021, 0.0022, 0.0026, 0.0022, 0.0011, 0.0008, 0.0038,\n",
       "                      0.0054]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1622, -0.1231, -0.1158, -0.3870, -0.0487, -0.2167, -0.1387, -0.1680,\n",
       "                        -0.2604, -0.2193, -0.0564, -0.4226, -0.3714, -0.1420, -0.3495, -0.1809,\n",
       "                        -0.1952, -0.2321, -0.0979, -0.1796, -0.4692, -0.1013, -0.3362, -0.2419,\n",
       "                        -0.2047, -0.1118, -0.1936, -0.1750, -0.3248, -0.2329, -0.1910, -0.3155,\n",
       "                        -0.1127, -0.3878, -0.1098, -0.1150, -0.2110, -0.4753, -0.1826, -0.2110,\n",
       "                        -0.2375, -0.0920, -0.2513, -0.2279, -0.1857, -0.1353, -0.3765, -0.1758,\n",
       "                        -0.3115, -0.4353, -0.3389, -0.1759, -0.1174, -0.1318, -0.2856, -0.1250,\n",
       "                        -0.2739, -0.2561, -0.1845, -0.2832, -0.1438, -0.1039, -0.4836, -0.6922]), max_val=tensor([0.0596, 0.1431, 0.1417, 0.3713, 0.0768, 0.2228, 0.1143, 0.2155, 0.2235,\n",
       "                        0.3363, 0.0417, 0.3565, 0.3017, 0.3639, 0.3887, 0.0707, 0.1415, 0.3693,\n",
       "                        0.2822, 0.1204, 0.3762, 0.1633, 0.3269, 0.1014, 0.0971, 0.1069, 0.2117,\n",
       "                        0.1536, 0.2593, 0.1696, 0.2252, 0.2173, 0.1262, 0.1171, 0.1411, 0.0947,\n",
       "                        0.2344, 0.0942, 0.1968, 0.1102, 0.1722, 0.0667, 0.0739, 0.2250, 0.1467,\n",
       "                        0.1028, 0.3562, 0.0817, 0.3721, 0.4237, 0.2457, 0.1228, 0.1618, 0.0627,\n",
       "                        0.3999, 0.0725, 0.2556, 0.2795, 0.3262, 0.1563, 0.0873, 0.1051, 0.4355,\n",
       "                        0.4973])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0643]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.829538583755493, max_val=4.331381320953369)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0648]), zero_point=tensor([56], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.662999391555786, max_val=4.572308540344238)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0007, 0.0015, 0.0007, 0.0007, 0.0014, 0.0010, 0.0007, 0.0014, 0.0012,\n",
       "                      0.0014, 0.0006, 0.0006, 0.0011, 0.0014, 0.0008, 0.0009, 0.0013, 0.0009,\n",
       "                      0.0004, 0.0014, 0.0007, 0.0006, 0.0007, 0.0006, 0.0017, 0.0011, 0.0005,\n",
       "                      0.0006, 0.0008, 0.0010, 0.0005, 0.0011, 0.0012, 0.0013, 0.0008, 0.0008,\n",
       "                      0.0006, 0.0007, 0.0008, 0.0006, 0.0008, 0.0007, 0.0005, 0.0007, 0.0007,\n",
       "                      0.0008, 0.0008, 0.0010, 0.0011, 0.0009, 0.0017, 0.0009, 0.0008, 0.0009,\n",
       "                      0.0011, 0.0015, 0.0010, 0.0011, 0.0005, 0.0008, 0.0007, 0.0005, 0.0010,\n",
       "                      0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0953, -0.1442, -0.0845, -0.0850, -0.1811, -0.1250, -0.0864, -0.1577,\n",
       "                        -0.1577, -0.1841, -0.0825, -0.0726, -0.1125, -0.1831, -0.0698, -0.0591,\n",
       "                        -0.1618, -0.1203, -0.0504, -0.1841, -0.0584, -0.0709, -0.0944, -0.0811,\n",
       "                        -0.2042, -0.1357, -0.0630, -0.0827, -0.0950, -0.0738, -0.0545, -0.1368,\n",
       "                        -0.1588, -0.1278, -0.1023, -0.0963, -0.0812, -0.0795, -0.1086, -0.0807,\n",
       "                        -0.0659, -0.0928, -0.0359, -0.0887, -0.0867, -0.1001, -0.0733, -0.1298,\n",
       "                        -0.1375, -0.1175, -0.1426, -0.1160, -0.0975, -0.1098, -0.1336, -0.1715,\n",
       "                        -0.1259, -0.1363, -0.0465, -0.1002, -0.0941, -0.0651, -0.0909, -0.0993]), max_val=tensor([0.0747, 0.1964, 0.0575, 0.0587, 0.1738, 0.0573, 0.0572, 0.1784, 0.1486,\n",
       "                        0.1336, 0.0798, 0.0672, 0.1388, 0.1147, 0.1014, 0.1128, 0.1313, 0.0817,\n",
       "                        0.0307, 0.1011, 0.0951, 0.0636, 0.0720, 0.0767, 0.2097, 0.0744, 0.0687,\n",
       "                        0.0798, 0.1006, 0.1214, 0.0614, 0.0687, 0.0919, 0.1659, 0.0847, 0.0914,\n",
       "                        0.0810, 0.0933, 0.0996, 0.0517, 0.0962, 0.0884, 0.0579, 0.0579, 0.0629,\n",
       "                        0.0760, 0.1053, 0.1235, 0.1454, 0.0983, 0.2113, 0.0508, 0.0647, 0.0690,\n",
       "                        0.1359, 0.1846, 0.1078, 0.0987, 0.0592, 0.0718, 0.0597, 0.0464, 0.1234,\n",
       "                        0.0620])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0221]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.806917667388916)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0015, 0.0038, 0.0025, 0.0025, 0.0008, 0.0037, 0.0028, 0.0042, 0.0038,\n",
       "                      0.0036, 0.0010, 0.0028, 0.0042, 0.0019, 0.0018, 0.0029, 0.0024, 0.0034,\n",
       "                      0.0048, 0.0010, 0.0026, 0.0014, 0.0024, 0.0026, 0.0011, 0.0005, 0.0012,\n",
       "                      0.0010, 0.0040, 0.0016, 0.0021, 0.0018, 0.0034, 0.0023, 0.0013, 0.0011,\n",
       "                      0.0036, 0.0030, 0.0029, 0.0022, 0.0051, 0.0008, 0.0019, 0.0022, 0.0017,\n",
       "                      0.0016, 0.0022, 0.0013, 0.0031, 0.0046, 0.0025, 0.0014, 0.0060, 0.0008,\n",
       "                      0.0025, 0.0008, 0.0013, 0.0008, 0.0027, 0.0017, 0.0009, 0.0003, 0.0025,\n",
       "                      0.0028]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1931, -0.4818, -0.1952, -0.3210, -0.1052, -0.4735, -0.3629, -0.5320,\n",
       "                        -0.4895, -0.4570, -0.1301, -0.3614, -0.5370, -0.2413, -0.2175, -0.2377,\n",
       "                        -0.3018, -0.3510, -0.3491, -0.0750, -0.2083, -0.0992, -0.2621, -0.2262,\n",
       "                        -0.1393, -0.0490, -0.1547, -0.1296, -0.5141, -0.2056, -0.2627, -0.1696,\n",
       "                        -0.4334, -0.2960, -0.1253, -0.0918, -0.4643, -0.3873, -0.3705, -0.2230,\n",
       "                        -0.6564, -0.0964, -0.2372, -0.2782, -0.2207, -0.2069, -0.2859, -0.1448,\n",
       "                        -0.3941, -0.5944, -0.3182, -0.1745, -0.7736, -0.0976, -0.3256, -0.0951,\n",
       "                        -0.1679, -0.1026, -0.3429, -0.1931, -0.1115, -0.0344, -0.3224, -0.3639]), max_val=tensor([0.1053, 0.2859, 0.3120, 0.3195, 0.1026, 0.4296, 0.3487, 0.3404, 0.3576,\n",
       "                        0.3575, 0.1237, 0.3497, 0.4363, 0.1542, 0.2245, 0.3622, 0.1599, 0.4356,\n",
       "                        0.6116, 0.1261, 0.3325, 0.1728, 0.3090, 0.3259, 0.1026, 0.0627, 0.1205,\n",
       "                        0.1319, 0.5102, 0.1700, 0.1909, 0.2229, 0.4097, 0.1931, 0.1713, 0.1386,\n",
       "                        0.4359, 0.2523, 0.2794, 0.2820, 0.6274, 0.1019, 0.1763, 0.2717, 0.1485,\n",
       "                        0.1239, 0.2288, 0.1673, 0.3241, 0.3206, 0.2507, 0.1747, 0.6309, 0.0883,\n",
       "                        0.2676, 0.1011, 0.1664, 0.1031, 0.3317, 0.2219, 0.0899, 0.0435, 0.2570,\n",
       "                        0.2827])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0919]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.813136577606201, max_val=4.862631797790527)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.1201]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.7559614181518555, max_val=7.496256351470947)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0004, 0.0002, 0.0003, 0.0002, 0.0004, 0.0004, 0.0002, 0.0003, 0.0003,\n",
       "                      0.0004, 0.0007, 0.0003, 0.0005, 0.0002, 0.0002, 0.0003, 0.0007, 0.0005,\n",
       "                      0.0009, 0.0007, 0.0005, 0.0004, 0.0007, 0.0005, 0.0003, 0.0005, 0.0006,\n",
       "                      0.0004, 0.0007, 0.0004, 0.0003, 0.0005, 0.0003, 0.0004, 0.0004, 0.0006,\n",
       "                      0.0004, 0.0004, 0.0004, 0.0005, 0.0006, 0.0005, 0.0010, 0.0004, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0007, 0.0004, 0.0008, 0.0004, 0.0003, 0.0005, 0.0003,\n",
       "                      0.0003, 0.0005, 0.0003, 0.0002, 0.0003, 0.0011, 0.0003, 0.0004, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0005, 0.0003, 0.0005, 0.0006, 0.0006, 0.0006, 0.0004,\n",
       "                      0.0005, 0.0005, 0.0007, 0.0005, 0.0006, 0.0003, 0.0003, 0.0005, 0.0003,\n",
       "                      0.0004, 0.0004, 0.0005, 0.0005, 0.0003, 0.0005, 0.0003, 0.0004, 0.0004,\n",
       "                      0.0006, 0.0002, 0.0005, 0.0008, 0.0002, 0.0004, 0.0005, 0.0003, 0.0003,\n",
       "                      0.0005, 0.0005, 0.0004, 0.0007, 0.0003, 0.0002, 0.0003, 0.0006, 0.0002,\n",
       "                      0.0005, 0.0004, 0.0009, 0.0005, 0.0007, 0.0004, 0.0002, 0.0005, 0.0005,\n",
       "                      0.0003, 0.0003, 0.0005, 0.0003, 0.0005, 0.0003, 0.0005, 0.0007, 0.0004,\n",
       "                      0.0003, 0.0005]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0357, -0.0280, -0.0314, -0.0220, -0.0502, -0.0383, -0.0257, -0.0289,\n",
       "                        -0.0404, -0.0499, -0.0513, -0.0336, -0.0563, -0.0284, -0.0215, -0.0302,\n",
       "                        -0.0749, -0.0621, -0.1119, -0.0948, -0.0633, -0.0392, -0.0600, -0.0620,\n",
       "                        -0.0373, -0.0658, -0.0397, -0.0483, -0.0663, -0.0450, -0.0369, -0.0438,\n",
       "                        -0.0402, -0.0542, -0.0478, -0.0586, -0.0494, -0.0537, -0.0450, -0.0668,\n",
       "                        -0.0717, -0.0700, -0.0474, -0.0575, -0.0251, -0.0295, -0.0290, -0.0539,\n",
       "                        -0.0475, -0.0601, -0.0543, -0.0407, -0.0559, -0.0436, -0.0329, -0.0653,\n",
       "                        -0.0341, -0.0188, -0.0338, -0.0957, -0.0308, -0.0340, -0.0303, -0.0209,\n",
       "                        -0.0230, -0.0625, -0.0429, -0.0656, -0.0676, -0.0802, -0.0434, -0.0466,\n",
       "                        -0.0615, -0.0548, -0.0521, -0.0465, -0.0521, -0.0425, -0.0373, -0.0603,\n",
       "                        -0.0429, -0.0522, -0.0315, -0.0577, -0.0328, -0.0292, -0.0587, -0.0382,\n",
       "                        -0.0457, -0.0442, -0.0706, -0.0289, -0.0625, -0.0586, -0.0250, -0.0426,\n",
       "                        -0.0555, -0.0347, -0.0398, -0.0499, -0.0422, -0.0571, -0.0588, -0.0243,\n",
       "                        -0.0303, -0.0351, -0.0591, -0.0253, -0.0481, -0.0491, -0.1137, -0.0653,\n",
       "                        -0.0858, -0.0465, -0.0266, -0.0685, -0.0408, -0.0293, -0.0345, -0.0582,\n",
       "                        -0.0325, -0.0683, -0.0442, -0.0529, -0.0539, -0.0383, -0.0315, -0.0532]), max_val=tensor([0.0461, 0.0305, 0.0419, 0.0256, 0.0328, 0.0494, 0.0239, 0.0424, 0.0384,\n",
       "                        0.0521, 0.0867, 0.0195, 0.0589, 0.0263, 0.0163, 0.0350, 0.0909, 0.0499,\n",
       "                        0.1000, 0.0321, 0.0590, 0.0497, 0.0872, 0.0663, 0.0357, 0.0508, 0.0773,\n",
       "                        0.0410, 0.0833, 0.0485, 0.0389, 0.0603, 0.0416, 0.0416, 0.0477, 0.0763,\n",
       "                        0.0462, 0.0396, 0.0398, 0.0628, 0.0605, 0.0461, 0.1298, 0.0473, 0.0222,\n",
       "                        0.0267, 0.0368, 0.0849, 0.0444, 0.1015, 0.0388, 0.0358, 0.0670, 0.0377,\n",
       "                        0.0308, 0.0491, 0.0276, 0.0206, 0.0331, 0.1371, 0.0332, 0.0507, 0.0291,\n",
       "                        0.0334, 0.0179, 0.0336, 0.0281, 0.0557, 0.0740, 0.0520, 0.0705, 0.0528,\n",
       "                        0.0687, 0.0598, 0.0896, 0.0575, 0.0790, 0.0292, 0.0257, 0.0659, 0.0243,\n",
       "                        0.0430, 0.0451, 0.0552, 0.0575, 0.0377, 0.0407, 0.0406, 0.0264, 0.0475,\n",
       "                        0.0719, 0.0255, 0.0569, 0.1064, 0.0255, 0.0475, 0.0604, 0.0384, 0.0275,\n",
       "                        0.0680, 0.0585, 0.0430, 0.0874, 0.0318, 0.0286, 0.0273, 0.0769, 0.0280,\n",
       "                        0.0622, 0.0379, 0.1107, 0.0560, 0.0887, 0.0287, 0.0295, 0.0673, 0.0618,\n",
       "                        0.0343, 0.0395, 0.0542, 0.0318, 0.0639, 0.0374, 0.0652, 0.0865, 0.0532,\n",
       "                        0.0340, 0.0662])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0274]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.478572130203247)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0013, 0.0015, 0.0028, 0.0012, 0.0021, 0.0027, 0.0024, 0.0011, 0.0023,\n",
       "                      0.0008, 0.0011, 0.0022, 0.0016, 0.0019, 0.0007, 0.0010, 0.0017, 0.0020,\n",
       "                      0.0010, 0.0016, 0.0057, 0.0027, 0.0007, 0.0049, 0.0007, 0.0015, 0.0019,\n",
       "                      0.0011, 0.0010, 0.0012, 0.0020, 0.0025, 0.0014, 0.0007, 0.0018, 0.0020,\n",
       "                      0.0014, 0.0016, 0.0021, 0.0025, 0.0016, 0.0009, 0.0016, 0.0010, 0.0009,\n",
       "                      0.0015, 0.0018, 0.0022, 0.0030, 0.0034, 0.0012, 0.0034, 0.0015, 0.0010,\n",
       "                      0.0018, 0.0009, 0.0013, 0.0012, 0.0023, 0.0016, 0.0010, 0.0017, 0.0043,\n",
       "                      0.0019, 0.0019, 0.0032, 0.0020, 0.0020, 0.0014, 0.0028, 0.0014, 0.0053,\n",
       "                      0.0051, 0.0018, 0.0014, 0.0015, 0.0018, 0.0014, 0.0013, 0.0031, 0.0013,\n",
       "                      0.0022, 0.0017, 0.0007, 0.0021, 0.0008, 0.0021, 0.0038, 0.0019, 0.0032,\n",
       "                      0.0010, 0.0050, 0.0021, 0.0027, 0.0022, 0.0025, 0.0012, 0.0008, 0.0031,\n",
       "                      0.0014, 0.0023, 0.0006, 0.0021, 0.0022, 0.0027, 0.0018, 0.0013, 0.0010,\n",
       "                      0.0031, 0.0013, 0.0023, 0.0019, 0.0031, 0.0054, 0.0009, 0.0020, 0.0022,\n",
       "                      0.0018, 0.0022, 0.0011, 0.0030, 0.0007, 0.0022, 0.0032, 0.0017, 0.0047,\n",
       "                      0.0009, 0.0030]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1632, -0.1946, -0.2494, -0.1383, -0.2695, -0.3109, -0.3094, -0.1264,\n",
       "                        -0.1937, -0.1027, -0.1418, -0.2794, -0.1219, -0.1087, -0.0607, -0.1304,\n",
       "                        -0.2117, -0.2580, -0.0842, -0.1501, -0.2417, -0.3394, -0.0859, -0.3865,\n",
       "                        -0.0898, -0.1955, -0.2058, -0.1194, -0.0772, -0.1543, -0.2538, -0.2596,\n",
       "                        -0.1831, -0.0914, -0.2364, -0.2402, -0.1068, -0.2037, -0.1514, -0.3247,\n",
       "                        -0.1990, -0.1127, -0.1962, -0.0721, -0.1111, -0.1887, -0.1011, -0.2853,\n",
       "                        -0.3902, -0.2845, -0.1059, -0.3188, -0.1913, -0.1272, -0.2249, -0.1215,\n",
       "                        -0.1316, -0.0953, -0.2475, -0.1501, -0.1300, -0.2116, -0.5510, -0.1697,\n",
       "                        -0.2174, -0.4128, -0.2507, -0.2540, -0.1779, -0.2596, -0.1256, -0.3221,\n",
       "                        -0.2382, -0.2350, -0.1776, -0.1605, -0.2219, -0.1773, -0.1453, -0.2591,\n",
       "                        -0.1634, -0.2860, -0.1888, -0.0922, -0.1663, -0.0994, -0.2673, -0.4920,\n",
       "                        -0.2451, -0.4094, -0.1048, -0.2676, -0.2711, -0.3520, -0.2109, -0.3180,\n",
       "                        -0.1172, -0.1047, -0.3983, -0.1852, -0.1774, -0.0816, -0.2740, -0.2752,\n",
       "                        -0.1785, -0.1997, -0.1185, -0.0952, -0.3927, -0.1680, -0.2296, -0.2407,\n",
       "                        -0.1637, -0.2956, -0.1064, -0.1657, -0.2765, -0.1872, -0.1818, -0.1307,\n",
       "                        -0.3828, -0.0632, -0.1845, -0.4037, -0.1988, -0.3175, -0.0836, -0.2676]), max_val=tensor([0.1303, 0.1871, 0.3539, 0.1513, 0.2347, 0.3466, 0.2817, 0.1446, 0.2862,\n",
       "                        0.0785, 0.1372, 0.2752, 0.2069, 0.2400, 0.0936, 0.1290, 0.1693, 0.2597,\n",
       "                        0.1235, 0.2062, 0.7210, 0.3114, 0.0776, 0.6256, 0.0649, 0.1942, 0.2436,\n",
       "                        0.1369, 0.1328, 0.1257, 0.2141, 0.3121, 0.1359, 0.0694, 0.1933, 0.2550,\n",
       "                        0.1720, 0.1582, 0.2729, 0.1646, 0.0946, 0.0588, 0.1990, 0.1214, 0.0908,\n",
       "                        0.1562, 0.2276, 0.2100, 0.2037, 0.4262, 0.1562, 0.4358, 0.1365, 0.0703,\n",
       "                        0.2096, 0.0590, 0.1678, 0.1510, 0.2983, 0.2083, 0.1209, 0.1800, 0.5343,\n",
       "                        0.2422, 0.2382, 0.3503, 0.1777, 0.1583, 0.1292, 0.3552, 0.1772, 0.6747,\n",
       "                        0.6493, 0.1647, 0.0925, 0.1927, 0.2239, 0.1508, 0.1624, 0.3970, 0.1266,\n",
       "                        0.1993, 0.2147, 0.0906, 0.2699, 0.0744, 0.1453, 0.3633, 0.1370, 0.2579,\n",
       "                        0.1250, 0.6343, 0.1697, 0.2758, 0.2838, 0.2869, 0.1505, 0.1019, 0.2600,\n",
       "                        0.1413, 0.2879, 0.0656, 0.2442, 0.2158, 0.3419, 0.2254, 0.1685, 0.1321,\n",
       "                        0.2979, 0.1222, 0.2934, 0.1019, 0.3942, 0.6812, 0.1110, 0.2591, 0.2782,\n",
       "                        0.2275, 0.2834, 0.1353, 0.1357, 0.0877, 0.2834, 0.2609, 0.2127, 0.5977,\n",
       "                        0.1133, 0.3870])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0732]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.104598522186279, max_val=4.190437316894531)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([2.8194e-03, 3.0093e-04, 3.0498e-04, 2.0398e-03, 5.0094e-04, 6.9321e-04,\n",
       "                        8.5593e-04, 1.4938e-03, 8.9711e-04, 1.3246e-03, 2.3350e-03, 8.8028e-04,\n",
       "                        2.2271e-03, 5.1927e-04, 1.4172e-03, 1.7638e-03, 6.9988e-04, 5.8124e-04,\n",
       "                        2.3113e-03, 3.0738e-04, 4.4487e-03, 2.5284e-04, 2.4104e-03, 3.5771e-04,\n",
       "                        1.5586e-03, 1.5043e-04, 6.2989e-04, 1.7200e-03, 1.4522e-03, 3.7584e-03,\n",
       "                        8.1231e-04, 1.7730e-04, 2.3431e-03, 1.1381e-03, 1.9753e-03, 2.8576e-03,\n",
       "                        4.5983e-04, 2.7899e-03, 8.8208e-04, 6.0584e-04, 6.9572e-04, 1.8611e-03,\n",
       "                        1.8028e-03, 1.4645e-03, 1.4267e-03, 1.6683e-03, 1.2141e-03, 5.8680e-04,\n",
       "                        1.3196e-04, 9.8630e-04, 1.9778e-03, 5.5256e-04, 3.0430e-04, 2.5654e-03,\n",
       "                        4.3721e-04, 1.3410e-03, 1.3143e-03, 2.7540e-03, 4.2768e-04, 7.9179e-04,\n",
       "                        6.3628e-04, 1.1708e-03, 6.5748e-04, 2.6666e-04, 1.8075e-04, 2.3816e-04,\n",
       "                        5.1579e-04, 8.4975e-04, 3.2893e-04, 2.4419e-04, 9.2224e-04, 5.3393e-04,\n",
       "                        5.6519e-04, 3.3961e-04, 3.9685e-04, 2.8994e-04, 1.9038e-04, 1.7079e-04,\n",
       "                        2.3171e-03, 6.3897e-04, 1.3340e-03, 6.5692e-04, 3.2452e-04, 9.8552e-04,\n",
       "                        1.2162e-03, 5.4893e-03, 6.4218e-04, 7.4313e-04, 4.7477e-04, 6.6423e-04,\n",
       "                        1.4311e-03, 1.3057e-03, 9.6484e-04, 5.2153e-04, 2.3570e-03, 5.9227e-04,\n",
       "                        3.0855e-04, 1.0820e-03, 8.2847e-04, 8.7792e-04, 7.5241e-04, 1.2713e-03,\n",
       "                        2.0334e-03, 9.4663e-04, 2.8343e-03, 1.0090e-03, 1.6248e-04, 2.1017e-04,\n",
       "                        1.9124e-04, 1.0538e-03, 5.0096e-04, 5.1218e-04, 1.4720e-03, 3.6099e-04,\n",
       "                        1.3237e-03, 6.1797e-04, 1.2696e-03, 5.3891e-04, 3.1675e-04, 1.2707e-03,\n",
       "                        1.0536e-04, 2.0330e-03, 8.1128e-04, 2.6689e-05, 1.0144e-03, 3.0367e-04,\n",
       "                        5.4701e-03, 5.1924e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.3609, -0.0385, -0.0390, -0.1109, -0.0641, -0.0792, -0.0785, -0.1912,\n",
       "                          -0.0908, -0.1695, -0.2989, -0.1127, -0.2851, -0.0665, -0.1382, -0.2258,\n",
       "                          -0.0896, -0.0556, -0.2104, -0.0374, -0.3694, -0.0314, -0.3085, -0.0458,\n",
       "                          -0.1539, -0.0143, -0.0806, -0.2202, -0.1723, -0.3504, -0.0696, -0.0218,\n",
       "                          -0.2999, -0.1457, -0.2528, -0.3658, -0.0589, -0.1214, -0.1129, -0.0592,\n",
       "                          -0.0891, -0.2382, -0.1327, -0.1875, -0.1826, -0.2135, -0.1554, -0.0581,\n",
       "                          -0.0169, -0.1262, -0.1071, -0.0707, -0.0390, -0.3284, -0.0490, -0.1717,\n",
       "                          -0.0842, -0.2573, -0.0512, -0.1013, -0.0814, -0.1499, -0.0842, -0.0341,\n",
       "                          -0.0231, -0.0305, -0.0660, -0.1088, -0.0358, -0.0238, -0.0960, -0.0545,\n",
       "                          -0.0643, -0.0414, -0.0459, -0.0310, -0.0244, -0.0190, -0.2966, -0.0818,\n",
       "                          -0.1264, -0.0841, -0.0415, -0.1261, -0.1557, -0.7026, -0.0822, -0.0790,\n",
       "                          -0.0500, -0.0779, -0.1832, -0.0669, -0.1235, -0.0668, -0.0981, -0.0596,\n",
       "                          -0.0395, -0.1385, -0.1017, -0.1124, -0.0963, -0.1627, -0.1264, -0.1212,\n",
       "                          -0.2256, -0.1292, -0.0208, -0.0269, -0.0245, -0.1349, -0.0641, -0.0656,\n",
       "                          -0.1884, -0.0462, -0.1694, -0.0410, -0.1046, -0.0690, -0.0277, -0.1626,\n",
       "                          -0.0135, -0.2602, -0.1038, -0.0030, -0.1298, -0.0389, -0.1020, -0.0665]), max_val=tensor([0.1877, 0.0191, 0.0161, 0.2591, 0.0628, 0.0880, 0.1087, 0.1103, 0.1139,\n",
       "                          0.1086, 0.1253, 0.0697, 0.2802, 0.0646, 0.1800, 0.1849, 0.0534, 0.0738,\n",
       "                          0.2935, 0.0390, 0.5650, 0.0321, 0.1972, 0.0446, 0.1979, 0.0191, 0.0788,\n",
       "                          0.1709, 0.1844, 0.4773, 0.1032, 0.0225, 0.1036, 0.1386, 0.1564, 0.1164,\n",
       "                          0.0373, 0.3543, 0.0498, 0.0769, 0.0753, 0.0843, 0.2290, 0.1070, 0.1216,\n",
       "                          0.2052, 0.1118, 0.0745, 0.0157, 0.1082, 0.2512, 0.0570, 0.0253, 0.1262,\n",
       "                          0.0555, 0.1004, 0.1669, 0.3498, 0.0543, 0.0925, 0.0745, 0.0969, 0.0773,\n",
       "                          0.0286, 0.0195, 0.0215, 0.0606, 0.0541, 0.0418, 0.0310, 0.1171, 0.0678,\n",
       "                          0.0718, 0.0431, 0.0504, 0.0368, 0.0225, 0.0217, 0.1291, 0.0708, 0.1694,\n",
       "                          0.0409, 0.0235, 0.0645, 0.1327, 0.3807, 0.0303, 0.0944, 0.0603, 0.0844,\n",
       "                          0.1045, 0.1658, 0.0680, 0.0517, 0.2993, 0.0752, 0.0284, 0.1108, 0.1052,\n",
       "                          0.0701, 0.0485, 0.1122, 0.2582, 0.0641, 0.3600, 0.0823, 0.0163, 0.0205,\n",
       "                          0.0173, 0.1129, 0.0512, 0.0337, 0.0927, 0.0432, 0.1225, 0.0785, 0.1612,\n",
       "                          0.0595, 0.0402, 0.0863, 0.0112, 0.1223, 0.0614, 0.0034, 0.0486, 0.0280,\n",
       "                          0.6947, 0.0472])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0508]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6241023540496826, max_val=2.8211660385131836)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0730]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.981338024139404, max_val=4.295375823974609)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0009, 0.0006, 0.0005, 0.0005, 0.0008, 0.0005, 0.0004, 0.0012, 0.0009,\n",
       "                      0.0008, 0.0006, 0.0006, 0.0006, 0.0010, 0.0004, 0.0017, 0.0008, 0.0007,\n",
       "                      0.0007, 0.0005, 0.0009, 0.0005, 0.0007, 0.0006, 0.0003, 0.0008, 0.0010,\n",
       "                      0.0012, 0.0014, 0.0008, 0.0004, 0.0010, 0.0001, 0.0009, 0.0005, 0.0007,\n",
       "                      0.0010, 0.0004, 0.0009, 0.0010, 0.0010, 0.0007, 0.0010, 0.0008, 0.0007,\n",
       "                      0.0007, 0.0011, 0.0010, 0.0006, 0.0010, 0.0019, 0.0010, 0.0006, 0.0009,\n",
       "                      0.0009, 0.0009, 0.0007, 0.0007, 0.0006, 0.0008, 0.0008, 0.0007, 0.0006,\n",
       "                      0.0005, 0.0005, 0.0008, 0.0006, 0.0006, 0.0007, 0.0004, 0.0006, 0.0013,\n",
       "                      0.0005, 0.0006, 0.0005, 0.0010, 0.0005, 0.0007, 0.0005, 0.0010, 0.0004,\n",
       "                      0.0008, 0.0008, 0.0005, 0.0005, 0.0006, 0.0005, 0.0007, 0.0019, 0.0006,\n",
       "                      0.0005, 0.0007, 0.0006, 0.0010, 0.0005, 0.0011, 0.0004, 0.0005, 0.0015,\n",
       "                      0.0005, 0.0008, 0.0008, 0.0008, 0.0016, 0.0008, 0.0005, 0.0006, 0.0005,\n",
       "                      0.0005, 0.0010, 0.0006, 0.0007, 0.0007, 0.0004, 0.0005, 0.0004, 0.0006,\n",
       "                      0.0006, 0.0009, 0.0006, 0.0007, 0.0008, 0.0005, 0.0006, 0.0004, 0.0011,\n",
       "                      0.0006, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0841, -0.0620, -0.0577, -0.0428, -0.0796, -0.0684, -0.0512, -0.0752,\n",
       "                        -0.0926, -0.0956, -0.0723, -0.0778, -0.0412, -0.1261, -0.0527, -0.1797,\n",
       "                        -0.1062, -0.0894, -0.0318, -0.0565, -0.0786, -0.0526, -0.0818, -0.0514,\n",
       "                        -0.0353, -0.1016, -0.0722, -0.0701, -0.1047, -0.1001, -0.0565, -0.1240,\n",
       "                        -0.0115, -0.0546, -0.0581, -0.0645, -0.1253, -0.0447, -0.0829, -0.0686,\n",
       "                        -0.1237, -0.0824, -0.1188, -0.1082, -0.0722, -0.0910, -0.1357, -0.1298,\n",
       "                        -0.0820, -0.1287, -0.1596, -0.1257, -0.0823, -0.0920, -0.1118, -0.0911,\n",
       "                        -0.0834, -0.0834, -0.0658, -0.0991, -0.0699, -0.0930, -0.0688, -0.0622,\n",
       "                        -0.0568, -0.0745, -0.0501, -0.0709, -0.0862, -0.0228, -0.0613, -0.1612,\n",
       "                        -0.0580, -0.0821, -0.0620, -0.1321, -0.0411, -0.0889, -0.0690, -0.1266,\n",
       "                        -0.0422, -0.0713, -0.0982, -0.0606, -0.0496, -0.0463, -0.0584, -0.0805,\n",
       "                        -0.1680, -0.0320, -0.0681, -0.0585, -0.0786, -0.0963, -0.0603, -0.1415,\n",
       "                        -0.0568, -0.0571, -0.1113, -0.0617, -0.1070, -0.1034, -0.0871, -0.1546,\n",
       "                        -0.0995, -0.0517, -0.0451, -0.0662, -0.0639, -0.1198, -0.0785, -0.0474,\n",
       "                        -0.0803, -0.0273, -0.0320, -0.0515, -0.0757, -0.0791, -0.0703, -0.0806,\n",
       "                        -0.0883, -0.0785, -0.0656, -0.0762, -0.0456, -0.1366, -0.0732, -0.1204]), max_val=tensor([0.1080, 0.0738, 0.0477, 0.0623, 0.1031, 0.0561, 0.0466, 0.1520, 0.1110,\n",
       "                        0.0961, 0.0802, 0.0695, 0.0702, 0.0699, 0.0546, 0.2132, 0.0838, 0.0869,\n",
       "                        0.0849, 0.0592, 0.1198, 0.0652, 0.0871, 0.0718, 0.0422, 0.1030, 0.1219,\n",
       "                        0.1565, 0.1832, 0.1000, 0.0467, 0.0900, 0.0177, 0.1115, 0.0523, 0.0835,\n",
       "                        0.0983, 0.0539, 0.1118, 0.1234, 0.1169, 0.0846, 0.1324, 0.0941, 0.0892,\n",
       "                        0.0948, 0.1213, 0.1257, 0.0751, 0.0659, 0.2368, 0.0806, 0.0695, 0.1154,\n",
       "                        0.0831, 0.1129, 0.0668, 0.0804, 0.0768, 0.0972, 0.0983, 0.0949, 0.0742,\n",
       "                        0.0662, 0.0575, 0.1046, 0.0706, 0.0662, 0.0814, 0.0503, 0.0732, 0.1227,\n",
       "                        0.0688, 0.0799, 0.0554, 0.1004, 0.0619, 0.0889, 0.0680, 0.1309, 0.0470,\n",
       "                        0.0957, 0.1072, 0.0431, 0.0584, 0.0758, 0.0622, 0.0920, 0.2381, 0.0733,\n",
       "                        0.0585, 0.0911, 0.0703, 0.1225, 0.0692, 0.0576, 0.0490, 0.0613, 0.1858,\n",
       "                        0.0660, 0.0797, 0.0900, 0.1047, 0.2006, 0.0957, 0.0584, 0.0768, 0.0675,\n",
       "                        0.0515, 0.1272, 0.0683, 0.0841, 0.0913, 0.0510, 0.0651, 0.0488, 0.0808,\n",
       "                        0.0722, 0.1134, 0.0593, 0.0913, 0.1018, 0.0673, 0.0820, 0.0566, 0.1216,\n",
       "                        0.0825, 0.1190])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0238]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.0218100547790527)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0010, 0.0009, 0.0023, 0.0026, 0.0017, 0.0022, 0.0020, 0.0013, 0.0016,\n",
       "                      0.0012, 0.0021, 0.0012, 0.0015, 0.0018, 0.0014, 0.0014, 0.0013, 0.0020,\n",
       "                      0.0007, 0.0013, 0.0018, 0.0027, 0.0010, 0.0034, 0.0026, 0.0023, 0.0015,\n",
       "                      0.0017, 0.0013, 0.0014, 0.0013, 0.0029, 0.0017, 0.0014, 0.0033, 0.0071,\n",
       "                      0.0017, 0.0014, 0.0011, 0.0011, 0.0008, 0.0011, 0.0026, 0.0008, 0.0014,\n",
       "                      0.0010, 0.0013, 0.0012, 0.0022, 0.0020, 0.0012, 0.0040, 0.0021, 0.0015,\n",
       "                      0.0011, 0.0011, 0.0011, 0.0011, 0.0021, 0.0014, 0.0016, 0.0017, 0.0030,\n",
       "                      0.0010, 0.0011, 0.0010, 0.0011, 0.0015, 0.0017, 0.0039, 0.0025, 0.0031,\n",
       "                      0.0026, 0.0032, 0.0009, 0.0022, 0.0042, 0.0010, 0.0013, 0.0033, 0.0012,\n",
       "                      0.0011, 0.0005, 0.0015, 0.0015, 0.0007, 0.0017, 0.0016, 0.0026, 0.0011,\n",
       "                      0.0021, 0.0032, 0.0043, 0.0009, 0.0019, 0.0018, 0.0013, 0.0010, 0.0022,\n",
       "                      0.0009, 0.0015, 0.0011, 0.0009, 0.0016, 0.0036, 0.0021, 0.0018, 0.0006,\n",
       "                      0.0020, 0.0025, 0.0015, 0.0013, 0.0019, 0.0048, 0.0015, 0.0035, 0.0022,\n",
       "                      0.0017, 0.0044, 0.0011, 0.0011, 0.0003, 0.0018, 0.0009, 0.0037, 0.0025,\n",
       "                      0.0012, 0.0027]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1056, -0.0974, -0.2055, -0.1718, -0.1547, -0.2764, -0.1983, -0.1041,\n",
       "                        -0.1894, -0.1547, -0.2241, -0.1339, -0.1326, -0.1319, -0.1394, -0.1771,\n",
       "                        -0.1109, -0.1135, -0.0811, -0.1635, -0.2258, -0.3163, -0.1110, -0.4204,\n",
       "                        -0.3353, -0.2591, -0.1022, -0.0816, -0.1497, -0.1159, -0.1630, -0.3745,\n",
       "                        -0.1569, -0.1493, -0.4272, -0.9126, -0.2235, -0.1624, -0.1471, -0.1106,\n",
       "                        -0.0854, -0.1429, -0.1880, -0.1006, -0.1728, -0.1318, -0.1091, -0.1591,\n",
       "                        -0.2842, -0.2510, -0.1551, -0.5110, -0.2650, -0.1811, -0.1096, -0.1423,\n",
       "                        -0.1383, -0.1076, -0.1946, -0.1741, -0.2059, -0.2212, -0.3793, -0.0802,\n",
       "                        -0.1350, -0.0900, -0.0981, -0.1981, -0.1712, -0.3096, -0.2069, -0.3973,\n",
       "                        -0.3342, -0.4141, -0.0674, -0.1524, -0.3698, -0.1080, -0.1666, -0.4244,\n",
       "                        -0.1444, -0.1357, -0.0627, -0.1933, -0.1070, -0.0918, -0.1907, -0.2017,\n",
       "                        -0.3295, -0.1308, -0.2620, -0.3423, -0.4733, -0.0941, -0.1544, -0.2157,\n",
       "                        -0.1094, -0.1298, -0.2820, -0.1200, -0.1936, -0.1015, -0.1104, -0.2058,\n",
       "                        -0.1967, -0.2696, -0.1699, -0.0607, -0.1806, -0.3156, -0.1894, -0.1382,\n",
       "                        -0.2487, -0.4066, -0.1895, -0.2780, -0.2639, -0.1628, -0.4289, -0.1372,\n",
       "                        -0.1356, -0.0359, -0.2333, -0.1004, -0.4792, -0.2455, -0.0709, -0.2820]), max_val=tensor([0.1287, 0.1132, 0.2859, 0.3272, 0.2191, 0.2848, 0.2490, 0.1659, 0.2034,\n",
       "                        0.1402, 0.2605, 0.1500, 0.1928, 0.2241, 0.1722, 0.1585, 0.1666, 0.2515,\n",
       "                        0.0881, 0.1513, 0.1922, 0.3380, 0.1314, 0.4375, 0.2499, 0.2939, 0.1963,\n",
       "                        0.2108, 0.1647, 0.1805, 0.1684, 0.3298, 0.2156, 0.1751, 0.1992, 0.4753,\n",
       "                        0.1623, 0.1774, 0.1386, 0.1393, 0.0965, 0.1378, 0.3271, 0.0753, 0.1782,\n",
       "                        0.1116, 0.1607, 0.1479, 0.1704, 0.2174, 0.1414, 0.4851, 0.2443, 0.1869,\n",
       "                        0.1414, 0.1209, 0.1406, 0.1369, 0.2727, 0.1292, 0.1482, 0.1913, 0.3273,\n",
       "                        0.1225, 0.1256, 0.1232, 0.1377, 0.1592, 0.2153, 0.4918, 0.3146, 0.3050,\n",
       "                        0.2902, 0.3388, 0.1165, 0.2772, 0.5353, 0.1303, 0.1301, 0.3325, 0.1482,\n",
       "                        0.1405, 0.0591, 0.1645, 0.1885, 0.0930, 0.2214, 0.1734, 0.1345, 0.1434,\n",
       "                        0.2674, 0.4013, 0.5501, 0.1104, 0.2463, 0.2303, 0.1636, 0.1188, 0.1668,\n",
       "                        0.0873, 0.1492, 0.1369, 0.0907, 0.2086, 0.4561, 0.1727, 0.2339, 0.0729,\n",
       "                        0.2549, 0.2641, 0.1852, 0.1652, 0.1969, 0.6110, 0.1563, 0.4506, 0.2752,\n",
       "                        0.2133, 0.5555, 0.1233, 0.0800, 0.0336, 0.2054, 0.1104, 0.3719, 0.3116,\n",
       "                        0.1516, 0.3418])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0761]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.177951335906982, max_val=4.488216876983643)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0945]), zero_point=tensor([72], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.7727580070495605, max_val=5.234442710876465)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0004, 0.0004, 0.0003, 0.0004, 0.0002, 0.0003, 0.0003, 0.0003, 0.0004,\n",
       "                      0.0003, 0.0005, 0.0008, 0.0002, 0.0003, 0.0003, 0.0004, 0.0004, 0.0003,\n",
       "                      0.0006, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0002, 0.0005, 0.0004,\n",
       "                      0.0003, 0.0008, 0.0002, 0.0003, 0.0004, 0.0003, 0.0003, 0.0005, 0.0003,\n",
       "                      0.0003, 0.0005, 0.0003, 0.0003, 0.0004, 0.0003, 0.0004, 0.0004, 0.0004,\n",
       "                      0.0003, 0.0007, 0.0004, 0.0003, 0.0003, 0.0004, 0.0005, 0.0005, 0.0004,\n",
       "                      0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0006, 0.0004, 0.0006,\n",
       "                      0.0005, 0.0004, 0.0002, 0.0004, 0.0009, 0.0004, 0.0006, 0.0003, 0.0008,\n",
       "                      0.0004, 0.0003, 0.0006, 0.0004, 0.0004, 0.0005, 0.0007, 0.0003, 0.0004,\n",
       "                      0.0005, 0.0007, 0.0005, 0.0003, 0.0005, 0.0004, 0.0003, 0.0003, 0.0003,\n",
       "                      0.0004, 0.0003, 0.0005, 0.0007, 0.0006, 0.0004, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0004, 0.0007, 0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0003, 0.0004,\n",
       "                      0.0003, 0.0005, 0.0004, 0.0006, 0.0005, 0.0004, 0.0003, 0.0003, 0.0011,\n",
       "                      0.0003, 0.0005, 0.0003, 0.0002, 0.0004, 0.0003, 0.0005, 0.0008, 0.0004,\n",
       "                      0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0005, 0.0007, 0.0002,\n",
       "                      0.0004, 0.0003, 0.0004, 0.0002, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004,\n",
       "                      0.0002, 0.0003, 0.0004, 0.0006, 0.0004, 0.0003, 0.0007, 0.0003, 0.0003,\n",
       "                      0.0003, 0.0006, 0.0007, 0.0004, 0.0006, 0.0004, 0.0003, 0.0003, 0.0004,\n",
       "                      0.0005, 0.0006, 0.0004, 0.0007, 0.0003, 0.0004, 0.0003, 0.0005, 0.0004,\n",
       "                      0.0004, 0.0004, 0.0003, 0.0006, 0.0003, 0.0003, 0.0006, 0.0006, 0.0002,\n",
       "                      0.0002, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004, 0.0003, 0.0005, 0.0003,\n",
       "                      0.0006, 0.0004, 0.0003, 0.0005, 0.0006, 0.0004, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0003, 0.0004, 0.0003, 0.0002, 0.0005, 0.0004, 0.0003, 0.0003, 0.0006,\n",
       "                      0.0004, 0.0006, 0.0007, 0.0003, 0.0007, 0.0004, 0.0005, 0.0003, 0.0004,\n",
       "                      0.0004, 0.0003, 0.0004, 0.0002, 0.0003, 0.0002, 0.0003, 0.0004, 0.0006,\n",
       "                      0.0005, 0.0003, 0.0005, 0.0004, 0.0003, 0.0005, 0.0003, 0.0003, 0.0004,\n",
       "                      0.0005, 0.0006, 0.0005, 0.0004, 0.0002, 0.0002, 0.0004, 0.0003, 0.0003,\n",
       "                      0.0008, 0.0003, 0.0005, 0.0003, 0.0008, 0.0007, 0.0007, 0.0002, 0.0003,\n",
       "                      0.0003, 0.0004, 0.0004, 0.0002]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0554, -0.0382, -0.0339, -0.0515, -0.0242, -0.0280, -0.0395, -0.0331,\n",
       "                        -0.0528, -0.0334, -0.0637, -0.0310, -0.0295, -0.0322, -0.0433, -0.0298,\n",
       "                        -0.0263, -0.0296, -0.0294, -0.0420, -0.0243, -0.0274, -0.0325, -0.0375,\n",
       "                        -0.0311, -0.0462, -0.0490, -0.0262, -0.0364, -0.0218, -0.0321, -0.0347,\n",
       "                        -0.0368, -0.0317, -0.0426, -0.0364, -0.0393, -0.0469, -0.0349, -0.0222,\n",
       "                        -0.0490, -0.0329, -0.0433, -0.0375, -0.0375, -0.0434, -0.0512, -0.0400,\n",
       "                        -0.0419, -0.0365, -0.0340, -0.0551, -0.0338, -0.0395, -0.0225, -0.0570,\n",
       "                        -0.0517, -0.0469, -0.0404, -0.0301, -0.0526, -0.0332, -0.0746, -0.0401,\n",
       "                        -0.0425, -0.0304, -0.0243, -0.0763, -0.0457, -0.0340, -0.0335, -0.0367,\n",
       "                        -0.0387, -0.0361, -0.0363, -0.0266, -0.0449, -0.0583, -0.0556, -0.0319,\n",
       "                        -0.0358, -0.0586, -0.0905, -0.0405, -0.0408, -0.0545, -0.0271, -0.0342,\n",
       "                        -0.0308, -0.0341, -0.0502, -0.0441, -0.0598, -0.0482, -0.0279, -0.0363,\n",
       "                        -0.0363, -0.0314, -0.0292, -0.0472, -0.0457, -0.0463, -0.0274, -0.0371,\n",
       "                        -0.0449, -0.0413, -0.0435, -0.0527, -0.0348, -0.0484, -0.0544, -0.0742,\n",
       "                        -0.0594, -0.0386, -0.0239, -0.0313, -0.0640, -0.0297, -0.0629, -0.0386,\n",
       "                        -0.0258, -0.0249, -0.0269, -0.0522, -0.1007, -0.0486, -0.0335, -0.0313,\n",
       "                        -0.0400, -0.0268, -0.0425, -0.0434, -0.0430, -0.0541, -0.0269, -0.0464,\n",
       "                        -0.0401, -0.0396, -0.0310, -0.0398, -0.0250, -0.0518, -0.0417, -0.0488,\n",
       "                        -0.0293, -0.0382, -0.0423, -0.0773, -0.0515, -0.0373, -0.0921, -0.0405,\n",
       "                        -0.0223, -0.0379, -0.0332, -0.0401, -0.0498, -0.0725, -0.0541, -0.0372,\n",
       "                        -0.0300, -0.0424, -0.0582, -0.0624, -0.0455, -0.0401, -0.0343, -0.0482,\n",
       "                        -0.0395, -0.0464, -0.0235, -0.0455, -0.0471, -0.0296, -0.0780, -0.0271,\n",
       "                        -0.0309, -0.0659, -0.0373, -0.0196, -0.0221, -0.0297, -0.0371, -0.0460,\n",
       "                        -0.0638, -0.0365, -0.0318, -0.0352, -0.0306, -0.0367, -0.0481, -0.0285,\n",
       "                        -0.0417, -0.0378, -0.0273, -0.0278, -0.0423, -0.0268, -0.0280, -0.0562,\n",
       "                        -0.0297, -0.0287, -0.0453, -0.0304, -0.0344, -0.0371, -0.0412, -0.0382,\n",
       "                        -0.0267, -0.0369, -0.0294, -0.0915, -0.0539, -0.0508, -0.0256, -0.0442,\n",
       "                        -0.0491, -0.0140, -0.0490, -0.0281, -0.0294, -0.0285, -0.0325, -0.0415,\n",
       "                        -0.0520, -0.0479, -0.0447, -0.0440, -0.0467, -0.0390, -0.0602, -0.0235,\n",
       "                        -0.0229, -0.0350, -0.0423, -0.0762, -0.0676, -0.0266, -0.0261, -0.0306,\n",
       "                        -0.0361, -0.0295, -0.0344, -0.0400, -0.0285, -0.0291, -0.0403, -0.0371,\n",
       "                        -0.0370, -0.0563, -0.0253, -0.0411, -0.0301, -0.0512, -0.0478, -0.0182]), max_val=tensor([0.0454, 0.0556, 0.0353, 0.0326, 0.0298, 0.0391, 0.0440, 0.0418, 0.0434,\n",
       "                        0.0355, 0.0325, 0.0996, 0.0198, 0.0345, 0.0222, 0.0522, 0.0499, 0.0344,\n",
       "                        0.0746, 0.0412, 0.0361, 0.0432, 0.0355, 0.0517, 0.0260, 0.0604, 0.0516,\n",
       "                        0.0333, 0.1028, 0.0274, 0.0402, 0.0484, 0.0336, 0.0348, 0.0604, 0.0406,\n",
       "                        0.0293, 0.0616, 0.0362, 0.0410, 0.0426, 0.0389, 0.0538, 0.0473, 0.0506,\n",
       "                        0.0342, 0.0861, 0.0488, 0.0422, 0.0401, 0.0521, 0.0684, 0.0607, 0.0488,\n",
       "                        0.0462, 0.0380, 0.0322, 0.0409, 0.0607, 0.0508, 0.0736, 0.0460, 0.0808,\n",
       "                        0.0677, 0.0561, 0.0279, 0.0480, 0.1100, 0.0339, 0.0760, 0.0240, 0.1014,\n",
       "                        0.0481, 0.0442, 0.0790, 0.0539, 0.0445, 0.0591, 0.0835, 0.0324, 0.0541,\n",
       "                        0.0534, 0.0700, 0.0649, 0.0438, 0.0669, 0.0550, 0.0279, 0.0319, 0.0428,\n",
       "                        0.0546, 0.0346, 0.0525, 0.0875, 0.0823, 0.0517, 0.0329, 0.0290, 0.0246,\n",
       "                        0.0381, 0.0907, 0.0656, 0.0453, 0.0467, 0.0599, 0.0331, 0.0321, 0.0364,\n",
       "                        0.0382, 0.0658, 0.0474, 0.0263, 0.0564, 0.0461, 0.0351, 0.0444, 0.1449,\n",
       "                        0.0340, 0.0578, 0.0402, 0.0228, 0.0558, 0.0375, 0.0687, 0.0841, 0.0324,\n",
       "                        0.0530, 0.0437, 0.0398, 0.0365, 0.0409, 0.0383, 0.0573, 0.0831, 0.0310,\n",
       "                        0.0329, 0.0426, 0.0460, 0.0276, 0.0524, 0.0506, 0.0585, 0.0505, 0.0305,\n",
       "                        0.0279, 0.0332, 0.0470, 0.0603, 0.0523, 0.0266, 0.0263, 0.0352, 0.0387,\n",
       "                        0.0435, 0.0708, 0.0831, 0.0445, 0.0520, 0.0466, 0.0386, 0.0367, 0.0565,\n",
       "                        0.0329, 0.0727, 0.0488, 0.0879, 0.0412, 0.0369, 0.0436, 0.0682, 0.0452,\n",
       "                        0.0453, 0.0516, 0.0387, 0.0406, 0.0400, 0.0331, 0.0810, 0.0778, 0.0253,\n",
       "                        0.0245, 0.0473, 0.0634, 0.0559, 0.0422, 0.0487, 0.0411, 0.0654, 0.0375,\n",
       "                        0.0805, 0.0505, 0.0420, 0.0612, 0.0705, 0.0513, 0.0213, 0.0378, 0.0240,\n",
       "                        0.0407, 0.0374, 0.0443, 0.0218, 0.0687, 0.0517, 0.0365, 0.0266, 0.0728,\n",
       "                        0.0446, 0.0730, 0.0917, 0.0420, 0.0453, 0.0429, 0.0573, 0.0329, 0.0485,\n",
       "                        0.0406, 0.0318, 0.0518, 0.0279, 0.0343, 0.0227, 0.0430, 0.0491, 0.0794,\n",
       "                        0.0635, 0.0283, 0.0608, 0.0495, 0.0334, 0.0501, 0.0374, 0.0327, 0.0453,\n",
       "                        0.0627, 0.0551, 0.0555, 0.0526, 0.0218, 0.0272, 0.0484, 0.0390, 0.0354,\n",
       "                        0.0979, 0.0328, 0.0649, 0.0441, 0.0966, 0.0852, 0.0882, 0.0279, 0.0387,\n",
       "                        0.0418, 0.0485, 0.0474, 0.0251])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0241]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.063978910446167)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0009, 0.0007, 0.0007, 0.0022, 0.0009, 0.0011, 0.0014, 0.0007, 0.0014,\n",
       "                      0.0008, 0.0008, 0.0009, 0.0011, 0.0011, 0.0011, 0.0008, 0.0010, 0.0024,\n",
       "                      0.0011, 0.0006, 0.0016, 0.0008, 0.0009, 0.0015, 0.0012, 0.0015, 0.0015,\n",
       "                      0.0023, 0.0012, 0.0014, 0.0011, 0.0006, 0.0010, 0.0006, 0.0006, 0.0008,\n",
       "                      0.0027, 0.0015, 0.0004, 0.0018, 0.0011, 0.0009, 0.0009, 0.0008, 0.0012,\n",
       "                      0.0007, 0.0007, 0.0015, 0.0013, 0.0022, 0.0016, 0.0017, 0.0007, 0.0007,\n",
       "                      0.0010, 0.0006, 0.0022, 0.0008, 0.0009, 0.0007, 0.0016, 0.0005, 0.0012,\n",
       "                      0.0008, 0.0010, 0.0012, 0.0014, 0.0019, 0.0016, 0.0015, 0.0011, 0.0005,\n",
       "                      0.0015, 0.0007, 0.0034, 0.0004, 0.0018, 0.0009, 0.0012, 0.0009, 0.0018,\n",
       "                      0.0008, 0.0013, 0.0009, 0.0010, 0.0009, 0.0016, 0.0014, 0.0022, 0.0010,\n",
       "                      0.0009, 0.0026, 0.0030, 0.0011, 0.0006, 0.0013, 0.0010, 0.0008, 0.0007,\n",
       "                      0.0014, 0.0020, 0.0015, 0.0008, 0.0010, 0.0007, 0.0030, 0.0009, 0.0021,\n",
       "                      0.0013, 0.0006, 0.0010, 0.0007, 0.0011, 0.0011, 0.0017, 0.0009, 0.0021,\n",
       "                      0.0041, 0.0026, 0.0008, 0.0030, 0.0012, 0.0013, 0.0021, 0.0016, 0.0005,\n",
       "                      0.0008, 0.0021, 0.0021, 0.0016, 0.0013, 0.0018, 0.0005, 0.0013, 0.0019,\n",
       "                      0.0011, 0.0008, 0.0039, 0.0010, 0.0019, 0.0010, 0.0028, 0.0010, 0.0013,\n",
       "                      0.0006, 0.0021, 0.0010, 0.0011, 0.0036, 0.0014, 0.0022, 0.0009, 0.0015,\n",
       "                      0.0027, 0.0018, 0.0009, 0.0017, 0.0006, 0.0007, 0.0013, 0.0028, 0.0006,\n",
       "                      0.0021, 0.0006, 0.0013, 0.0014, 0.0015, 0.0028, 0.0014, 0.0008, 0.0017,\n",
       "                      0.0010, 0.0007, 0.0040, 0.0012, 0.0016, 0.0011, 0.0010, 0.0011, 0.0011,\n",
       "                      0.0013, 0.0009, 0.0011, 0.0010, 0.0020, 0.0010, 0.0007, 0.0017, 0.0008,\n",
       "                      0.0015, 0.0006, 0.0010, 0.0008, 0.0013, 0.0010, 0.0012, 0.0011, 0.0010,\n",
       "                      0.0013, 0.0020, 0.0010, 0.0029, 0.0009, 0.0013, 0.0029, 0.0021, 0.0006,\n",
       "                      0.0017, 0.0014, 0.0009, 0.0009, 0.0015, 0.0011, 0.0008, 0.0017, 0.0014,\n",
       "                      0.0013, 0.0005, 0.0014, 0.0029, 0.0009, 0.0011, 0.0013, 0.0021, 0.0011,\n",
       "                      0.0018, 0.0025, 0.0023, 0.0013, 0.0014, 0.0007, 0.0011, 0.0011, 0.0009,\n",
       "                      0.0016, 0.0009, 0.0015, 0.0011, 0.0008, 0.0008, 0.0010, 0.0030, 0.0017,\n",
       "                      0.0009, 0.0007, 0.0011, 0.0018, 0.0011, 0.0026, 0.0009, 0.0011, 0.0014,\n",
       "                      0.0009, 0.0013, 0.0011, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0713, -0.0739, -0.0860, -0.1339, -0.0832, -0.1365, -0.1852, -0.0734,\n",
       "                        -0.0953, -0.0829, -0.0993, -0.0798, -0.0790, -0.1181, -0.1382, -0.1013,\n",
       "                        -0.1005, -0.2204, -0.1434, -0.0673, -0.1331, -0.1015, -0.0899, -0.1395,\n",
       "                        -0.1530, -0.1859, -0.1889, -0.1871, -0.1526, -0.1493, -0.0995, -0.0811,\n",
       "                        -0.1124, -0.0503, -0.0652, -0.0905, -0.1748, -0.1889, -0.0477, -0.2216,\n",
       "                        -0.1333, -0.0831, -0.0764, -0.0842, -0.1426, -0.0718, -0.0687, -0.1543,\n",
       "                        -0.0900, -0.1841, -0.1117, -0.2002, -0.0696, -0.0886, -0.0913, -0.0803,\n",
       "                        -0.1186, -0.1062, -0.0819, -0.0781, -0.1093, -0.0570, -0.1107, -0.0971,\n",
       "                        -0.1317, -0.1133, -0.1644, -0.2036, -0.2008, -0.1520, -0.1451, -0.0677,\n",
       "                        -0.1092, -0.0931, -0.2064, -0.0464, -0.1474, -0.0882, -0.1181, -0.1058,\n",
       "                        -0.1011, -0.0750, -0.1314, -0.1036, -0.1285, -0.1125, -0.1385, -0.1366,\n",
       "                        -0.1674, -0.1070, -0.1115, -0.1993, -0.1582, -0.1379, -0.0745, -0.1652,\n",
       "                        -0.1169, -0.1062, -0.0949, -0.1744, -0.2359, -0.1307, -0.0852, -0.1250,\n",
       "                        -0.0913, -0.3794, -0.1191, -0.1628, -0.1569, -0.0762, -0.1319, -0.0949,\n",
       "                        -0.0757, -0.1436, -0.1806, -0.0940, -0.1193, -0.2349, -0.3101, -0.0773,\n",
       "                        -0.2118, -0.1494, -0.1232, -0.1710, -0.1860, -0.0638, -0.1045, -0.1820,\n",
       "                        -0.1830, -0.1995, -0.1617, -0.1598, -0.0580, -0.1137, -0.1273, -0.1471,\n",
       "                        -0.0984, -0.3946, -0.1311, -0.1130, -0.0943, -0.2020, -0.1083, -0.1645,\n",
       "                        -0.0735, -0.2293, -0.0927, -0.0964, -0.4550, -0.1595, -0.1869, -0.1141,\n",
       "                        -0.1840, -0.2667, -0.2332, -0.1001, -0.1428, -0.0779, -0.0753, -0.1536,\n",
       "                        -0.3534, -0.0827, -0.1808, -0.0550, -0.1169, -0.1242, -0.1276, -0.1455,\n",
       "                        -0.1402, -0.1054, -0.2088, -0.0993, -0.0941, -0.2516, -0.1215, -0.2032,\n",
       "                        -0.1284, -0.1084, -0.1424, -0.1100, -0.1074, -0.0929, -0.1285, -0.0968,\n",
       "                        -0.1193, -0.1262, -0.0899, -0.1819, -0.0592, -0.1320, -0.0708, -0.0874,\n",
       "                        -0.0700, -0.1052, -0.1336, -0.1253, -0.0929, -0.0907, -0.1095, -0.2486,\n",
       "                        -0.0980, -0.1531, -0.0867, -0.1226, -0.2335, -0.1740, -0.0678, -0.2201,\n",
       "                        -0.0951, -0.0702, -0.1173, -0.1860, -0.1410, -0.0981, -0.1744, -0.1768,\n",
       "                        -0.1313, -0.0631, -0.1759, -0.3253, -0.1194, -0.0721, -0.1210, -0.2011,\n",
       "                        -0.1326, -0.2055, -0.1902, -0.2909, -0.1252, -0.0645, -0.0939, -0.1199,\n",
       "                        -0.1359, -0.1104, -0.2032, -0.1031, -0.1074, -0.1370, -0.0514, -0.0981,\n",
       "                        -0.0675, -0.2305, -0.2140, -0.0947, -0.0712, -0.1313, -0.2156, -0.1301,\n",
       "                        -0.1907, -0.0977, -0.1407, -0.1378, -0.1140, -0.1003, -0.1205, -0.1041]), max_val=tensor([0.1104, 0.0883, 0.0909, 0.2821, 0.1179, 0.0687, 0.1403, 0.0880, 0.1816,\n",
       "                        0.1006, 0.0709, 0.1080, 0.1350, 0.1458, 0.1135, 0.0923, 0.1208, 0.3072,\n",
       "                        0.1458, 0.0733, 0.2005, 0.0781, 0.1164, 0.1915, 0.1578, 0.1419, 0.1914,\n",
       "                        0.2949, 0.1001, 0.1765, 0.1401, 0.0669, 0.1257, 0.0743, 0.0775, 0.1059,\n",
       "                        0.3407, 0.1095, 0.0571, 0.2299, 0.1358, 0.1088, 0.1095, 0.0979, 0.1488,\n",
       "                        0.0882, 0.0857, 0.1900, 0.1601, 0.2827, 0.2076, 0.2109, 0.0829, 0.0763,\n",
       "                        0.1219, 0.0638, 0.2748, 0.1066, 0.1154, 0.0943, 0.1999, 0.0612, 0.1545,\n",
       "                        0.0790, 0.1118, 0.1489, 0.1782, 0.2432, 0.1916, 0.1963, 0.0884, 0.0489,\n",
       "                        0.1903, 0.0776, 0.4325, 0.0382, 0.2348, 0.1115, 0.1488, 0.1189, 0.2262,\n",
       "                        0.1065, 0.1700, 0.1180, 0.1149, 0.1027, 0.2011, 0.1773, 0.2854, 0.1286,\n",
       "                        0.0777, 0.3355, 0.3833, 0.1180, 0.0673, 0.0910, 0.1313, 0.0700, 0.0772,\n",
       "                        0.1642, 0.2575, 0.1848, 0.1016, 0.0725, 0.0845, 0.2930, 0.0966, 0.2604,\n",
       "                        0.1628, 0.0695, 0.0841, 0.0865, 0.1418, 0.1035, 0.2130, 0.1084, 0.2624,\n",
       "                        0.5156, 0.3268, 0.0976, 0.3769, 0.1355, 0.1679, 0.2659, 0.2008, 0.0652,\n",
       "                        0.1046, 0.2619, 0.2604, 0.0925, 0.1703, 0.2270, 0.0666, 0.1655, 0.2457,\n",
       "                        0.1060, 0.0927, 0.4986, 0.1176, 0.2351, 0.1281, 0.3500, 0.1273, 0.1697,\n",
       "                        0.0796, 0.2642, 0.1292, 0.1374, 0.3701, 0.1817, 0.2771, 0.0762, 0.1925,\n",
       "                        0.3414, 0.1057, 0.1090, 0.2162, 0.0821, 0.0933, 0.1705, 0.3078, 0.0749,\n",
       "                        0.2656, 0.0709, 0.1696, 0.1795, 0.1882, 0.3525, 0.1747, 0.0805, 0.2141,\n",
       "                        0.1309, 0.0876, 0.5103, 0.1569, 0.1629, 0.1386, 0.1272, 0.1075, 0.1435,\n",
       "                        0.1658, 0.1114, 0.1346, 0.1268, 0.2506, 0.1030, 0.0910, 0.2221, 0.0966,\n",
       "                        0.1896, 0.0744, 0.1303, 0.1060, 0.1682, 0.1008, 0.1566, 0.1386, 0.1326,\n",
       "                        0.1706, 0.2534, 0.1302, 0.3681, 0.1158, 0.1707, 0.3717, 0.2645, 0.0727,\n",
       "                        0.1867, 0.1718, 0.1181, 0.0910, 0.1746, 0.0863, 0.0737, 0.2158, 0.1052,\n",
       "                        0.1653, 0.0545, 0.1728, 0.3624, 0.1203, 0.1337, 0.1681, 0.2628, 0.1444,\n",
       "                        0.2315, 0.3123, 0.2822, 0.1613, 0.1785, 0.0819, 0.1405, 0.1063, 0.1194,\n",
       "                        0.1637, 0.1108, 0.1852, 0.1264, 0.0991, 0.0879, 0.1219, 0.3845, 0.2200,\n",
       "                        0.1186, 0.0875, 0.1351, 0.2308, 0.1453, 0.3277, 0.1135, 0.1279, 0.1770,\n",
       "                        0.1185, 0.1598, 0.1382, 0.2008])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0603]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.694704055786133, max_val=3.9669551849365234)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([2.8112e-04, 1.6448e-04, 1.4967e-04, 6.6348e-04, 2.7044e-04, 2.1517e-04,\n",
       "                        4.1155e-04, 3.6378e-04, 1.4579e-04, 1.2716e-04, 2.4949e-04, 3.3637e-04,\n",
       "                        4.2681e-04, 6.1695e-04, 2.9190e-04, 3.6193e-04, 4.7273e-04, 7.9784e-04,\n",
       "                        1.4353e-04, 2.4807e-04, 4.9959e-04, 2.3074e-04, 2.5916e-04, 3.9361e-04,\n",
       "                        7.6518e-04, 5.9661e-04, 8.2003e-04, 2.8859e-04, 2.9780e-04, 3.3863e-04,\n",
       "                        4.3555e-04, 4.4075e-04, 1.6849e-04, 2.1771e-04, 2.7771e-04, 4.7850e-04,\n",
       "                        3.9626e-04, 5.0684e-04, 4.0769e-04, 5.3561e-04, 5.4183e-04, 3.6862e-04,\n",
       "                        3.6296e-04, 1.8706e-04, 8.0330e-04, 1.9366e-04, 1.5587e-04, 9.2838e-04,\n",
       "                        1.8695e-04, 5.6689e-04, 6.6960e-04, 7.9115e-04, 2.2133e-04, 2.5542e-04,\n",
       "                        2.6579e-04, 2.4360e-04, 5.1228e-04, 1.9555e-04, 5.9301e-04, 2.5329e-04,\n",
       "                        4.7687e-04, 4.5768e-04, 4.3727e-04, 4.6252e-04, 4.3154e-04, 7.3847e-04,\n",
       "                        1.9578e-04, 1.1711e-04, 4.8136e-04, 3.0370e-04, 1.4293e-04, 2.8420e-04,\n",
       "                        5.2833e-04, 3.9646e-04, 1.0352e-03, 3.1989e-04, 4.8367e-04, 2.1625e-04,\n",
       "                        2.8967e-04, 2.2393e-04, 5.7644e-04, 2.6538e-04, 4.5870e-04, 4.5471e-04,\n",
       "                        5.8281e-04, 1.6526e-04, 2.6824e-04, 6.7913e-04, 2.2330e-04, 2.8464e-04,\n",
       "                        2.1019e-04, 5.8399e-04, 3.0146e-04, 1.0790e-04, 1.9024e-04, 2.4637e-04,\n",
       "                        2.8949e-04, 3.5756e-04, 1.6491e-04, 1.9237e-04, 3.7214e-04, 6.6527e-04,\n",
       "                        6.8461e-04, 2.0817e-04, 2.8716e-04, 4.7202e-04, 2.0488e-04, 4.2648e-04,\n",
       "                        3.7837e-04, 1.5645e-04, 3.0934e-04, 1.7333e-04, 1.6208e-04, 6.5124e-04,\n",
       "                        4.1257e-04, 1.0537e-04, 4.6985e-04, 1.0935e-03, 2.6611e-04, 2.9077e-04,\n",
       "                        5.8698e-04, 1.0502e-04, 3.5880e-04, 5.2792e-04, 5.1034e-04, 1.4940e-04,\n",
       "                        1.8445e-04, 6.4707e-04, 4.2889e-04, 3.3587e-04, 9.4751e-05, 5.6898e-04,\n",
       "                        2.6581e-04, 5.4027e-04, 3.2288e-04, 3.0408e-04, 1.2955e-04, 1.4180e-03,\n",
       "                        4.9583e-04, 1.3448e-04, 5.7884e-04, 3.2903e-04, 3.4044e-04, 1.9845e-04,\n",
       "                        1.8058e-04, 4.7988e-04, 4.2389e-04, 5.0108e-04, 4.1643e-04, 3.5728e-04,\n",
       "                        7.8468e-04, 1.8234e-04, 3.6708e-04, 4.0569e-04, 6.6262e-04, 2.6545e-04,\n",
       "                        7.0159e-04, 2.0606e-04, 1.0689e-04, 4.0733e-04, 9.8216e-04, 2.7334e-04,\n",
       "                        1.1748e-03, 1.8346e-04, 3.7800e-04, 1.5763e-04, 5.9706e-04, 4.5018e-04,\n",
       "                        4.3990e-04, 2.9985e-04, 3.8552e-04, 1.4854e-04, 4.8308e-04, 6.4369e-04,\n",
       "                        5.9084e-04, 3.7408e-04, 3.9716e-04, 1.4503e-04, 4.9018e-04, 2.1403e-04,\n",
       "                        2.8245e-04, 2.9019e-04, 9.0781e-04, 1.8664e-04, 4.0419e-04, 2.9181e-04,\n",
       "                        2.4391e-04, 4.1124e-04, 1.9072e-04, 4.8048e-04, 2.5185e-04, 1.2172e-04,\n",
       "                        7.9334e-05, 5.5717e-04, 4.1744e-04, 2.6910e-04, 4.2684e-04, 2.5596e-04,\n",
       "                        2.0007e-04, 5.0833e-04, 2.9535e-04, 7.2180e-04, 2.0501e-04, 5.9263e-04,\n",
       "                        4.6479e-04, 4.6001e-04, 9.6188e-05, 3.6886e-04, 2.3429e-04, 5.8409e-04,\n",
       "                        1.4690e-04, 3.5306e-04, 4.1680e-04, 5.5416e-04, 4.3573e-04, 1.2511e-04,\n",
       "                        1.9410e-04, 3.4129e-04, 2.8151e-04, 3.2394e-04, 1.2748e-04, 4.6529e-04,\n",
       "                        6.2394e-04, 2.5640e-04, 9.0339e-05, 3.3031e-04, 7.0689e-04, 7.3891e-04,\n",
       "                        5.0384e-04, 4.0409e-04, 1.4951e-04, 2.8878e-04, 2.0302e-04, 5.4506e-04,\n",
       "                        1.3078e-04, 6.0297e-04, 3.4818e-04, 8.0880e-05, 2.3969e-04, 4.5719e-04,\n",
       "                        2.1394e-04, 8.6246e-04, 4.5213e-04, 6.8470e-04, 1.5501e-04, 5.0750e-05,\n",
       "                        5.0502e-04, 4.1093e-04, 4.8743e-04, 3.0708e-04, 2.9421e-04, 4.1656e-04,\n",
       "                        3.0463e-04, 4.9434e-04, 3.6190e-04, 4.9571e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.0360, -0.0209, -0.0155, -0.0624, -0.0346, -0.0275, -0.0527, -0.0466,\n",
       "                          -0.0187, -0.0138, -0.0319, -0.0337, -0.0422, -0.0790, -0.0366, -0.0296,\n",
       "                          -0.0605, -0.1021, -0.0184, -0.0318, -0.0639, -0.0271, -0.0332, -0.0504,\n",
       "                          -0.0979, -0.0764, -0.0935, -0.0369, -0.0381, -0.0433, -0.0533, -0.0564,\n",
       "                          -0.0203, -0.0279, -0.0349, -0.0378, -0.0288, -0.0649, -0.0266, -0.0686,\n",
       "                          -0.0694, -0.0472, -0.0465, -0.0238, -0.0759, -0.0248, -0.0200, -0.0567,\n",
       "                          -0.0239, -0.0684, -0.0515, -0.0578, -0.0283, -0.0327, -0.0340, -0.0312,\n",
       "                          -0.0605, -0.0244, -0.0759, -0.0324, -0.0610, -0.0586, -0.0560, -0.0592,\n",
       "                          -0.0336, -0.0701, -0.0251, -0.0150, -0.0616, -0.0389, -0.0183, -0.0300,\n",
       "                          -0.0653, -0.0507, -0.0700, -0.0282, -0.0554, -0.0235, -0.0371, -0.0253,\n",
       "                          -0.0713, -0.0340, -0.0587, -0.0554, -0.0746, -0.0192, -0.0282, -0.0869,\n",
       "                          -0.0206, -0.0160, -0.0269, -0.0615, -0.0293, -0.0129, -0.0244, -0.0315,\n",
       "                          -0.0371, -0.0458, -0.0211, -0.0145, -0.0375, -0.0852, -0.0876, -0.0266,\n",
       "                          -0.0368, -0.0412, -0.0262, -0.0273, -0.0432, -0.0200, -0.0396, -0.0222,\n",
       "                          -0.0181, -0.0834, -0.0464, -0.0135, -0.0601, -0.0900, -0.0336, -0.0372,\n",
       "                          -0.0655, -0.0134, -0.0459, -0.0454, -0.0653, -0.0191, -0.0154, -0.0748,\n",
       "                          -0.0417, -0.0292, -0.0107, -0.0570, -0.0340, -0.0377, -0.0323, -0.0374,\n",
       "                          -0.0166, -0.1815, -0.0375, -0.0171, -0.0741, -0.0421, -0.0436, -0.0254,\n",
       "                          -0.0231, -0.0614, -0.0543, -0.0641, -0.0221, -0.0455, -0.0756, -0.0199,\n",
       "                          -0.0470, -0.0519, -0.0848, -0.0326, -0.0898, -0.0200, -0.0135, -0.0521,\n",
       "                          -0.1257, -0.0350, -0.1504, -0.0235, -0.0240, -0.0189, -0.0734, -0.0576,\n",
       "                          -0.0553, -0.0384, -0.0493, -0.0190, -0.0206, -0.0453, -0.0531, -0.0413,\n",
       "                          -0.0508, -0.0186, -0.0627, -0.0274, -0.0362, -0.0325, -0.1162, -0.0239,\n",
       "                          -0.0517, -0.0374, -0.0312, -0.0526, -0.0244, -0.0406, -0.0322, -0.0156,\n",
       "                          -0.0093, -0.0419, -0.0534, -0.0344, -0.0546, -0.0328, -0.0256, -0.0651,\n",
       "                          -0.0378, -0.0924, -0.0244, -0.0759, -0.0595, -0.0394, -0.0112, -0.0321,\n",
       "                          -0.0239, -0.0748, -0.0169, -0.0309, -0.0534, -0.0390, -0.0558, -0.0160,\n",
       "                          -0.0191, -0.0437, -0.0360, -0.0415, -0.0163, -0.0596, -0.0799, -0.0328,\n",
       "                          -0.0065, -0.0423, -0.0696, -0.0698, -0.0645, -0.0517, -0.0191, -0.0337,\n",
       "                          -0.0260, -0.0412, -0.0129, -0.0772, -0.0413, -0.0104, -0.0179, -0.0585,\n",
       "                          -0.0274, -0.1102, -0.0579, -0.0284, -0.0198, -0.0065, -0.0646, -0.0498,\n",
       "                          -0.0424, -0.0282, -0.0377, -0.0458, -0.0246, -0.0420, -0.0463, -0.0635]), max_val=tensor([0.0208, 0.0209, 0.0190, 0.0843, 0.0253, 0.0208, 0.0368, 0.0334, 0.0134,\n",
       "                          0.0161, 0.0232, 0.0427, 0.0542, 0.0376, 0.0371, 0.0460, 0.0324, 0.0929,\n",
       "                          0.0136, 0.0181, 0.0485, 0.0293, 0.0260, 0.0364, 0.0852, 0.0424, 0.1041,\n",
       "                          0.0289, 0.0261, 0.0284, 0.0553, 0.0445, 0.0214, 0.0194, 0.0353, 0.0608,\n",
       "                          0.0503, 0.0256, 0.0518, 0.0523, 0.0498, 0.0379, 0.0187, 0.0238, 0.1020,\n",
       "                          0.0229, 0.0192, 0.1179, 0.0219, 0.0720, 0.0850, 0.1005, 0.0234, 0.0258,\n",
       "                          0.0323, 0.0274, 0.0651, 0.0248, 0.0380, 0.0313, 0.0329, 0.0213, 0.0442,\n",
       "                          0.0383, 0.0548, 0.0938, 0.0245, 0.0079, 0.0275, 0.0361, 0.0106, 0.0361,\n",
       "                          0.0671, 0.0228, 0.1315, 0.0406, 0.0614, 0.0275, 0.0192, 0.0284, 0.0732,\n",
       "                          0.0244, 0.0414, 0.0577, 0.0670, 0.0210, 0.0341, 0.0782, 0.0284, 0.0361,\n",
       "                          0.0164, 0.0742, 0.0383, 0.0137, 0.0162, 0.0234, 0.0265, 0.0301, 0.0169,\n",
       "                          0.0244, 0.0473, 0.0781, 0.0433, 0.0245, 0.0152, 0.0599, 0.0118, 0.0542,\n",
       "                          0.0481, 0.0146, 0.0219, 0.0201, 0.0206, 0.0371, 0.0524, 0.0113, 0.0515,\n",
       "                          0.1389, 0.0338, 0.0330, 0.0745, 0.0120, 0.0448, 0.0670, 0.0577, 0.0162,\n",
       "                          0.0234, 0.0822, 0.0545, 0.0427, 0.0120, 0.0723, 0.0287, 0.0686, 0.0410,\n",
       "                          0.0386, 0.0104, 0.0814, 0.0630, 0.0171, 0.0613, 0.0388, 0.0242, 0.0176,\n",
       "                          0.0143, 0.0271, 0.0247, 0.0395, 0.0529, 0.0454, 0.0997, 0.0232, 0.0195,\n",
       "                          0.0353, 0.0841, 0.0337, 0.0673, 0.0262, 0.0136, 0.0191, 0.1054, 0.0239,\n",
       "                          0.1292, 0.0182, 0.0480, 0.0200, 0.0758, 0.0441, 0.0559, 0.0203, 0.0390,\n",
       "                          0.0143, 0.0614, 0.0817, 0.0750, 0.0475, 0.0368, 0.0132, 0.0472, 0.0202,\n",
       "                          0.0335, 0.0369, 0.0484, 0.0209, 0.0498, 0.0331, 0.0295, 0.0362, 0.0192,\n",
       "                          0.0610, 0.0269, 0.0087, 0.0101, 0.0708, 0.0292, 0.0285, 0.0414, 0.0176,\n",
       "                          0.0217, 0.0470, 0.0362, 0.0802, 0.0260, 0.0736, 0.0569, 0.0584, 0.0122,\n",
       "                          0.0468, 0.0298, 0.0439, 0.0187, 0.0448, 0.0315, 0.0704, 0.0290, 0.0148,\n",
       "                          0.0247, 0.0353, 0.0280, 0.0367, 0.0140, 0.0549, 0.0681, 0.0231, 0.0115,\n",
       "                          0.0354, 0.0898, 0.0938, 0.0600, 0.0436, 0.0144, 0.0367, 0.0229, 0.0692,\n",
       "                          0.0166, 0.0348, 0.0442, 0.0078, 0.0304, 0.0487, 0.0242, 0.1095, 0.0534,\n",
       "                          0.0870, 0.0127, 0.0048, 0.0419, 0.0522, 0.0619, 0.0390, 0.0350, 0.0529,\n",
       "                          0.0387, 0.0628, 0.0300, 0.0554])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0234]), zero_point=tensor([76], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.788132667541504, max_val=1.1860475540161133)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0644]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.953575849533081, max_val=4.226382255554199)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0002, 0.0003, 0.0004, 0.0007, 0.0004, 0.0003, 0.0005, 0.0006, 0.0002,\n",
       "                      0.0004, 0.0004, 0.0003, 0.0004, 0.0003, 0.0004, 0.0007, 0.0004, 0.0004,\n",
       "                      0.0005, 0.0005, 0.0004, 0.0004, 0.0003, 0.0002, 0.0004, 0.0003, 0.0002,\n",
       "                      0.0006, 0.0005, 0.0007, 0.0002, 0.0003, 0.0005, 0.0004, 0.0004, 0.0003,\n",
       "                      0.0005, 0.0003, 0.0003, 0.0003, 0.0005, 0.0008, 0.0004, 0.0003, 0.0004,\n",
       "                      0.0002, 0.0004, 0.0003, 0.0004, 0.0003, 0.0003, 0.0010, 0.0003, 0.0003,\n",
       "                      0.0003, 0.0003, 0.0002, 0.0005, 0.0007, 0.0006, 0.0002, 0.0004, 0.0004,\n",
       "                      0.0007, 0.0003, 0.0005, 0.0006, 0.0003, 0.0007, 0.0008, 0.0004, 0.0003,\n",
       "                      0.0005, 0.0007, 0.0003, 0.0002, 0.0004, 0.0008, 0.0006, 0.0002, 0.0006,\n",
       "                      0.0004, 0.0002, 0.0005, 0.0002, 0.0003, 0.0003, 0.0004, 0.0007, 0.0003,\n",
       "                      0.0003, 0.0004, 0.0004, 0.0005, 0.0005, 0.0003, 0.0005, 0.0003, 0.0003,\n",
       "                      0.0002, 0.0003, 0.0003, 0.0005, 0.0004, 0.0003, 0.0008, 0.0002, 0.0003,\n",
       "                      0.0004, 0.0003, 0.0005, 0.0003, 0.0003, 0.0005, 0.0004, 0.0003, 0.0002,\n",
       "                      0.0005, 0.0005, 0.0004, 0.0004, 0.0006, 0.0002, 0.0003, 0.0004, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0007, 0.0004, 0.0002, 0.0004, 0.0003, 0.0003, 0.0002,\n",
       "                      0.0004, 0.0006, 0.0003, 0.0003, 0.0005, 0.0003, 0.0002, 0.0005, 0.0003,\n",
       "                      0.0005, 0.0003, 0.0004, 0.0007, 0.0004, 0.0003, 0.0005, 0.0003, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0003, 0.0004, 0.0003, 0.0002, 0.0005, 0.0004, 0.0007,\n",
       "                      0.0005, 0.0002, 0.0003, 0.0004, 0.0004, 0.0009, 0.0005, 0.0004, 0.0002,\n",
       "                      0.0004, 0.0004, 0.0003, 0.0004, 0.0008, 0.0005, 0.0004, 0.0005, 0.0002,\n",
       "                      0.0004, 0.0003, 0.0003, 0.0004, 0.0003, 0.0005, 0.0009, 0.0003, 0.0004,\n",
       "                      0.0003, 0.0003, 0.0006, 0.0007, 0.0003, 0.0003, 0.0003, 0.0004, 0.0004,\n",
       "                      0.0005, 0.0007, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0007, 0.0004,\n",
       "                      0.0005, 0.0002, 0.0003, 0.0003, 0.0007, 0.0007, 0.0002, 0.0007, 0.0003,\n",
       "                      0.0005, 0.0004, 0.0003, 0.0012, 0.0003, 0.0005, 0.0004, 0.0003, 0.0004,\n",
       "                      0.0005, 0.0005, 0.0003, 0.0008, 0.0002, 0.0003, 0.0005, 0.0005, 0.0004,\n",
       "                      0.0007, 0.0005, 0.0002, 0.0003, 0.0002, 0.0005, 0.0008, 0.0003, 0.0005,\n",
       "                      0.0003, 0.0006, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0002, 0.0002,\n",
       "                      0.0003, 0.0003, 0.0004, 0.0003]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0246, -0.0438, -0.0264, -0.0878, -0.0373, -0.0335, -0.0453, -0.0808,\n",
       "                        -0.0216, -0.0407, -0.0393, -0.0364, -0.0241, -0.0300, -0.0453, -0.0948,\n",
       "                        -0.0559, -0.0352, -0.0639, -0.0664, -0.0288, -0.0449, -0.0432, -0.0318,\n",
       "                        -0.0426, -0.0374, -0.0318, -0.0317, -0.0362, -0.0574, -0.0276, -0.0416,\n",
       "                        -0.0618, -0.0307, -0.0412, -0.0409, -0.0424, -0.0278, -0.0218, -0.0342,\n",
       "                        -0.0458, -0.0997, -0.0512, -0.0251, -0.0176, -0.0241, -0.0463, -0.0325,\n",
       "                        -0.0459, -0.0403, -0.0337, -0.0633, -0.0198, -0.0444, -0.0382, -0.0404,\n",
       "                        -0.0285, -0.0690, -0.0337, -0.0434, -0.0194, -0.0324, -0.0543, -0.0838,\n",
       "                        -0.0307, -0.0659, -0.0591, -0.0265, -0.0801, -0.0669, -0.0495, -0.0337,\n",
       "                        -0.0526, -0.0744, -0.0348, -0.0297, -0.0379, -0.0624, -0.0560, -0.0298,\n",
       "                        -0.0375, -0.0452, -0.0231, -0.0634, -0.0256, -0.0330, -0.0329, -0.0569,\n",
       "                        -0.0600, -0.0331, -0.0261, -0.0229, -0.0513, -0.0496, -0.0389, -0.0334,\n",
       "                        -0.0516, -0.0404, -0.0435, -0.0241, -0.0382, -0.0325, -0.0627, -0.0360,\n",
       "                        -0.0416, -0.1020, -0.0228, -0.0275, -0.0334, -0.0371, -0.0563, -0.0328,\n",
       "                        -0.0342, -0.0220, -0.0410, -0.0339, -0.0316, -0.0345, -0.0678, -0.0276,\n",
       "                        -0.0366, -0.0634, -0.0288, -0.0374, -0.0489, -0.0265, -0.0310, -0.0252,\n",
       "                        -0.0519, -0.0390, -0.0278, -0.0197, -0.0318, -0.0411, -0.0236, -0.0505,\n",
       "                        -0.0511, -0.0274, -0.0399, -0.0488, -0.0329, -0.0265, -0.0437, -0.0256,\n",
       "                        -0.0451, -0.0323, -0.0476, -0.0894, -0.0358, -0.0366, -0.0619, -0.0315,\n",
       "                        -0.0197, -0.0259, -0.0318, -0.0358, -0.0286, -0.0410, -0.0191, -0.0320,\n",
       "                        -0.0521, -0.0541, -0.0616, -0.0232, -0.0272, -0.0412, -0.0491, -0.0403,\n",
       "                        -0.0612, -0.0215, -0.0240, -0.0235, -0.0278, -0.0351, -0.0161, -0.0274,\n",
       "                        -0.0563, -0.0517, -0.0555, -0.0234, -0.0318, -0.0298, -0.0269, -0.0410,\n",
       "                        -0.0321, -0.0660, -0.0462, -0.0189, -0.0379, -0.0314, -0.0325, -0.0528,\n",
       "                        -0.0672, -0.0422, -0.0326, -0.0336, -0.0429, -0.0250, -0.0348, -0.0713,\n",
       "                        -0.0372, -0.0518, -0.0391, -0.0317, -0.0410, -0.0671, -0.0472, -0.0579,\n",
       "                        -0.0212, -0.0308, -0.0317, -0.0920, -0.0868, -0.0275, -0.0690, -0.0401,\n",
       "                        -0.0696, -0.0346, -0.0231, -0.1484, -0.0357, -0.0591, -0.0492, -0.0326,\n",
       "                        -0.0395, -0.0451, -0.0404, -0.0367, -0.0665, -0.0301, -0.0266, -0.0371,\n",
       "                        -0.0386, -0.0356, -0.0328, -0.0496, -0.0185, -0.0270, -0.0273, -0.0404,\n",
       "                        -0.0709, -0.0300, -0.0643, -0.0231, -0.0426, -0.0417, -0.0419, -0.0340,\n",
       "                        -0.0448, -0.0381, -0.0240, -0.0275, -0.0278, -0.0312, -0.0321, -0.0324]), max_val=tensor([0.0311, 0.0210, 0.0446, 0.0714, 0.0452, 0.0441, 0.0674, 0.0652, 0.0290,\n",
       "                        0.0503, 0.0458, 0.0422, 0.0525, 0.0369, 0.0566, 0.0883, 0.0554, 0.0479,\n",
       "                        0.0532, 0.0595, 0.0472, 0.0409, 0.0423, 0.0317, 0.0482, 0.0331, 0.0226,\n",
       "                        0.0778, 0.0573, 0.0852, 0.0240, 0.0364, 0.0499, 0.0486, 0.0494, 0.0399,\n",
       "                        0.0671, 0.0411, 0.0319, 0.0338, 0.0594, 0.0997, 0.0377, 0.0381, 0.0493,\n",
       "                        0.0218, 0.0533, 0.0303, 0.0363, 0.0395, 0.0331, 0.1294, 0.0404, 0.0404,\n",
       "                        0.0443, 0.0359, 0.0247, 0.0663, 0.0858, 0.0706, 0.0287, 0.0515, 0.0385,\n",
       "                        0.0691, 0.0367, 0.0489, 0.0741, 0.0331, 0.0937, 0.1047, 0.0471, 0.0406,\n",
       "                        0.0664, 0.0932, 0.0368, 0.0276, 0.0461, 0.0988, 0.0819, 0.0243, 0.0709,\n",
       "                        0.0350, 0.0286, 0.0620, 0.0281, 0.0268, 0.0288, 0.0493, 0.0874, 0.0430,\n",
       "                        0.0340, 0.0486, 0.0450, 0.0681, 0.0683, 0.0406, 0.0684, 0.0436, 0.0399,\n",
       "                        0.0192, 0.0421, 0.0358, 0.0620, 0.0533, 0.0374, 0.0704, 0.0276, 0.0382,\n",
       "                        0.0455, 0.0228, 0.0655, 0.0333, 0.0335, 0.0580, 0.0560, 0.0432, 0.0270,\n",
       "                        0.0588, 0.0622, 0.0494, 0.0474, 0.0771, 0.0273, 0.0424, 0.0522, 0.0295,\n",
       "                        0.0235, 0.0436, 0.0877, 0.0473, 0.0244, 0.0545, 0.0430, 0.0396, 0.0299,\n",
       "                        0.0443, 0.0735, 0.0369, 0.0436, 0.0578, 0.0422, 0.0214, 0.0636, 0.0337,\n",
       "                        0.0639, 0.0395, 0.0438, 0.0775, 0.0567, 0.0377, 0.0548, 0.0416, 0.0300,\n",
       "                        0.0363, 0.0292, 0.0386, 0.0521, 0.0357, 0.0245, 0.0606, 0.0570, 0.0931,\n",
       "                        0.0467, 0.0184, 0.0330, 0.0457, 0.0493, 0.1171, 0.0616, 0.0546, 0.0239,\n",
       "                        0.0449, 0.0525, 0.0441, 0.0477, 0.0987, 0.0613, 0.0470, 0.0642, 0.0264,\n",
       "                        0.0497, 0.0420, 0.0428, 0.0505, 0.0329, 0.0662, 0.1174, 0.0367, 0.0471,\n",
       "                        0.0378, 0.0413, 0.0719, 0.0839, 0.0406, 0.0373, 0.0404, 0.0526, 0.0456,\n",
       "                        0.0592, 0.0917, 0.0435, 0.0464, 0.0650, 0.0458, 0.0445, 0.0926, 0.0519,\n",
       "                        0.0386, 0.0280, 0.0341, 0.0429, 0.0694, 0.0630, 0.0300, 0.0879, 0.0330,\n",
       "                        0.0666, 0.0490, 0.0435, 0.1082, 0.0292, 0.0628, 0.0362, 0.0432, 0.0449,\n",
       "                        0.0574, 0.0658, 0.0334, 0.1009, 0.0309, 0.0356, 0.0632, 0.0654, 0.0479,\n",
       "                        0.0863, 0.0576, 0.0220, 0.0428, 0.0307, 0.0618, 0.0955, 0.0417, 0.0513,\n",
       "                        0.0372, 0.0816, 0.0617, 0.0651, 0.0539, 0.0534, 0.0476, 0.0309, 0.0238,\n",
       "                        0.0322, 0.0388, 0.0472, 0.0350])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0197]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.5003714561462402)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0009, 0.0013, 0.0006, 0.0012, 0.0008, 0.0006, 0.0012, 0.0009, 0.0008,\n",
       "                      0.0005, 0.0007, 0.0004, 0.0010, 0.0014, 0.0023, 0.0008, 0.0016, 0.0012,\n",
       "                      0.0014, 0.0030, 0.0018, 0.0021, 0.0033, 0.0017, 0.0012, 0.0018, 0.0007,\n",
       "                      0.0031, 0.0005, 0.0018, 0.0008, 0.0006, 0.0008, 0.0002, 0.0008, 0.0010,\n",
       "                      0.0023, 0.0005, 0.0004, 0.0012, 0.0018, 0.0008, 0.0002, 0.0007, 0.0014,\n",
       "                      0.0003, 0.0007, 0.0019, 0.0015, 0.0025, 0.0009, 0.0018, 0.0012, 0.0005,\n",
       "                      0.0008, 0.0009, 0.0008, 0.0004, 0.0013, 0.0013, 0.0005, 0.0004, 0.0007,\n",
       "                      0.0015, 0.0011, 0.0011, 0.0030, 0.0012, 0.0008, 0.0014, 0.0011, 0.0006,\n",
       "                      0.0005, 0.0009, 0.0015, 0.0011, 0.0011, 0.0014, 0.0005, 0.0007, 0.0015,\n",
       "                      0.0006, 0.0013, 0.0010, 0.0014, 0.0005, 0.0021, 0.0011, 0.0021, 0.0014,\n",
       "                      0.0005, 0.0019, 0.0017, 0.0007, 0.0006, 0.0006, 0.0010, 0.0009, 0.0005,\n",
       "                      0.0010, 0.0029, 0.0016, 0.0008, 0.0006, 0.0003, 0.0024, 0.0006, 0.0015,\n",
       "                      0.0032, 0.0005, 0.0007, 0.0001, 0.0020, 0.0016, 0.0019, 0.0004, 0.0015,\n",
       "                      0.0019, 0.0017, 0.0008, 0.0021, 0.0015, 0.0020, 0.0008, 0.0013, 0.0005,\n",
       "                      0.0016, 0.0017, 0.0046, 0.0003, 0.0007, 0.0012, 0.0006, 0.0011, 0.0009,\n",
       "                      0.0017, 0.0004, 0.0029, 0.0009, 0.0010, 0.0005, 0.0036, 0.0008, 0.0008,\n",
       "                      0.0011, 0.0023, 0.0010, 0.0007, 0.0015, 0.0029, 0.0032, 0.0003, 0.0009,\n",
       "                      0.0017, 0.0018, 0.0015, 0.0012, 0.0004, 0.0016, 0.0008, 0.0036, 0.0009,\n",
       "                      0.0011, 0.0020, 0.0016, 0.0015, 0.0012, 0.0031, 0.0017, 0.0013, 0.0042,\n",
       "                      0.0002, 0.0013, 0.0015, 0.0010, 0.0010, 0.0011, 0.0009, 0.0016, 0.0010,\n",
       "                      0.0014, 0.0022, 0.0015, 0.0020, 0.0011, 0.0009, 0.0012, 0.0041, 0.0016,\n",
       "                      0.0014, 0.0008, 0.0011, 0.0011, 0.0015, 0.0007, 0.0010, 0.0009, 0.0006,\n",
       "                      0.0015, 0.0031, 0.0019, 0.0012, 0.0007, 0.0019, 0.0024, 0.0042, 0.0009,\n",
       "                      0.0016, 0.0003, 0.0010, 0.0006, 0.0016, 0.0004, 0.0013, 0.0033, 0.0014,\n",
       "                      0.0020, 0.0006, 0.0013, 0.0031, 0.0010, 0.0013, 0.0008, 0.0017, 0.0015,\n",
       "                      0.0019, 0.0023, 0.0022, 0.0007, 0.0044, 0.0006, 0.0006, 0.0024, 0.0009,\n",
       "                      0.0015, 0.0005, 0.0010, 0.0007, 0.0003, 0.0008, 0.0007, 0.0014, 0.0014,\n",
       "                      0.0004, 0.0006, 0.0006, 0.0028, 0.0012, 0.0028, 0.0005, 0.0014, 0.0012,\n",
       "                      0.0013, 0.0011, 0.0007, 0.0010]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0780, -0.1614, -0.0611, -0.1483, -0.1014, -0.0832, -0.1351, -0.1061,\n",
       "                        -0.0626, -0.0683, -0.0878, -0.0532, -0.1108, -0.1817, -0.2901, -0.1068,\n",
       "                        -0.2091, -0.1520, -0.1396, -0.2002, -0.2251, -0.2696, -0.4258, -0.2182,\n",
       "                        -0.1531, -0.1950, -0.0866, -0.3992, -0.0388, -0.2354, -0.0767, -0.0689,\n",
       "                        -0.0843, -0.0208, -0.0827, -0.1132, -0.2033, -0.0351, -0.0476, -0.1543,\n",
       "                        -0.2249, -0.1012, -0.0316, -0.0799, -0.1061, -0.0443, -0.0569, -0.2429,\n",
       "                        -0.1327, -0.2586, -0.1184, -0.2067, -0.1477, -0.0483, -0.1048, -0.0987,\n",
       "                        -0.0797, -0.0566, -0.1719, -0.0719, -0.0585, -0.0426, -0.0881, -0.1437,\n",
       "                        -0.1379, -0.1377, -0.3780, -0.1443, -0.1026, -0.1355, -0.1383, -0.0786,\n",
       "                        -0.0649, -0.1169, -0.1670, -0.1347, -0.1228, -0.1249, -0.0646, -0.0930,\n",
       "                        -0.0999, -0.0648, -0.1500, -0.1311, -0.1777, -0.0380, -0.2187, -0.1424,\n",
       "                        -0.2377, -0.0631, -0.0582, -0.2437, -0.2132, -0.0687, -0.0780, -0.0743,\n",
       "                        -0.1333, -0.1096, -0.0539, -0.1227, -0.3688, -0.1725, -0.0763, -0.0798,\n",
       "                        -0.0444, -0.2474, -0.0722, -0.1541, -0.4079, -0.0658, -0.0940, -0.0191,\n",
       "                        -0.2517, -0.1456, -0.1833, -0.0510, -0.0959, -0.2341, -0.1684, -0.1023,\n",
       "                        -0.2311, -0.1792, -0.2557, -0.1060, -0.1683, -0.0581, -0.2051, -0.2165,\n",
       "                        -0.4384, -0.0371, -0.0922, -0.1251, -0.0790, -0.1117, -0.1094, -0.2025,\n",
       "                        -0.0514, -0.3774, -0.1132, -0.1299, -0.0435, -0.3289, -0.1050, -0.1020,\n",
       "                        -0.1379, -0.2472, -0.0850, -0.0879, -0.1318, -0.2225, -0.4123, -0.0391,\n",
       "                        -0.1111, -0.2117, -0.2363, -0.1869, -0.1524, -0.0415, -0.1793, -0.1061,\n",
       "                        -0.2415, -0.0960, -0.0900, -0.2593, -0.1275, -0.1558, -0.1292, -0.3914,\n",
       "                        -0.1667, -0.1644, -0.3157, -0.0315, -0.1669, -0.1576, -0.1185, -0.1155,\n",
       "                        -0.1362, -0.1169, -0.2043, -0.1273, -0.1735, -0.2862, -0.1899, -0.2016,\n",
       "                        -0.1471, -0.1093, -0.1473, -0.2746, -0.1791, -0.1379, -0.0776, -0.1425,\n",
       "                        -0.1312, -0.1874, -0.0834, -0.0678, -0.1116, -0.0830, -0.1731, -0.3958,\n",
       "                        -0.2404, -0.1593, -0.0851, -0.2447, -0.2395, -0.5353, -0.0961, -0.2055,\n",
       "                        -0.0434, -0.1290, -0.0706, -0.2092, -0.0452, -0.1589, -0.4284, -0.1699,\n",
       "                        -0.2611, -0.0631, -0.1604, -0.2225, -0.1325, -0.1721, -0.0986, -0.1528,\n",
       "                        -0.1718, -0.2329, -0.2975, -0.2627, -0.0739, -0.2011, -0.0763, -0.0689,\n",
       "                        -0.3110, -0.0832, -0.1650, -0.0611, -0.0815, -0.0791, -0.0331, -0.0993,\n",
       "                        -0.0635, -0.1352, -0.1805, -0.0450, -0.0696, -0.0765, -0.2849, -0.1569,\n",
       "                        -0.3540, -0.0436, -0.1509, -0.1329, -0.1701, -0.1155, -0.0902, -0.1226]), max_val=tensor([0.1159, 0.1095, 0.0741, 0.1160, 0.1057, 0.0422, 0.1565, 0.1135, 0.0976,\n",
       "                        0.0601, 0.0690, 0.0551, 0.1315, 0.1551, 0.2204, 0.0934, 0.1729, 0.1242,\n",
       "                        0.1815, 0.3749, 0.1800, 0.1172, 0.3734, 0.1168, 0.1166, 0.2237, 0.0848,\n",
       "                        0.3100, 0.0575, 0.1629, 0.1000, 0.0753, 0.1041, 0.0177, 0.1000, 0.1298,\n",
       "                        0.2935, 0.0587, 0.0511, 0.1121, 0.1525, 0.0938, 0.0303, 0.0845, 0.1724,\n",
       "                        0.0275, 0.0834, 0.2340, 0.1859, 0.3161, 0.1022, 0.2244, 0.0828, 0.0578,\n",
       "                        0.0824, 0.1100, 0.1002, 0.0460, 0.1527, 0.1650, 0.0672, 0.0561, 0.0903,\n",
       "                        0.1895, 0.1414, 0.1236, 0.2726, 0.1583, 0.0423, 0.1820, 0.0992, 0.0545,\n",
       "                        0.0641, 0.0941, 0.1943, 0.0966, 0.1366, 0.1717, 0.0641, 0.0897, 0.1894,\n",
       "                        0.0702, 0.1637, 0.1061, 0.1569, 0.0669, 0.2672, 0.1286, 0.2659, 0.1813,\n",
       "                        0.0543, 0.1961, 0.2213, 0.0878, 0.0809, 0.0692, 0.1293, 0.1107, 0.0629,\n",
       "                        0.1247, 0.3083, 0.1983, 0.0964, 0.0627, 0.0429, 0.3004, 0.0680, 0.1870,\n",
       "                        0.2874, 0.0455, 0.0607, 0.0174, 0.2289, 0.1986, 0.2395, 0.0466, 0.1878,\n",
       "                        0.2379, 0.2196, 0.1019, 0.2717, 0.1911, 0.2043, 0.0819, 0.1574, 0.0485,\n",
       "                        0.1482, 0.2169, 0.5826, 0.0367, 0.0836, 0.1482, 0.0592, 0.1337, 0.1065,\n",
       "                        0.2199, 0.0446, 0.2776, 0.0903, 0.1077, 0.0573, 0.4586, 0.1028, 0.0956,\n",
       "                        0.1196, 0.2903, 0.1267, 0.0896, 0.1926, 0.3684, 0.2860, 0.0348, 0.0532,\n",
       "                        0.2212, 0.1583, 0.1721, 0.0938, 0.0571, 0.2021, 0.0813, 0.4523, 0.1157,\n",
       "                        0.1347, 0.1147, 0.1983, 0.1949, 0.1509, 0.3189, 0.2132, 0.1409, 0.5340,\n",
       "                        0.0309, 0.1711, 0.1876, 0.1233, 0.1312, 0.1036, 0.1074, 0.1817, 0.1293,\n",
       "                        0.1065, 0.1505, 0.1875, 0.2520, 0.1333, 0.1174, 0.0860, 0.5206, 0.1982,\n",
       "                        0.1761, 0.0975, 0.0810, 0.1410, 0.1007, 0.0783, 0.1259, 0.1056, 0.0564,\n",
       "                        0.1895, 0.3874, 0.1424, 0.1225, 0.0603, 0.2379, 0.3028, 0.2622, 0.1131,\n",
       "                        0.2086, 0.0395, 0.1212, 0.0687, 0.1810, 0.0356, 0.1605, 0.3166, 0.1776,\n",
       "                        0.2537, 0.0825, 0.1403, 0.3908, 0.1128, 0.0998, 0.0841, 0.2193, 0.1847,\n",
       "                        0.2448, 0.2290, 0.2841, 0.0873, 0.5616, 0.0741, 0.0811, 0.2608, 0.1115,\n",
       "                        0.1851, 0.0423, 0.1300, 0.0922, 0.0298, 0.0856, 0.0862, 0.1778, 0.1658,\n",
       "                        0.0513, 0.0722, 0.0762, 0.3569, 0.1459, 0.3143, 0.0603, 0.1765, 0.1495,\n",
       "                        0.0943, 0.1355, 0.0781, 0.1238])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0714]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.0119948387146, max_val=4.052196502685547)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0908]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.378376007080078, max_val=5.150312423706055)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0002, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0004, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0003, 0.0004, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0001, 0.0002, 0.0004, 0.0003, 0.0001, 0.0002, 0.0003,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0001,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0004, 0.0002, 0.0004, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0004, 0.0003,\n",
       "                      0.0005, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0004, 0.0001, 0.0003, 0.0003, 0.0005, 0.0002, 0.0003, 0.0002, 0.0003,\n",
       "                      0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0004, 0.0003, 0.0002,\n",
       "                      0.0003, 0.0005, 0.0003, 0.0002, 0.0001, 0.0003, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0006, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0001, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0001, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0004, 0.0003, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0001, 0.0003, 0.0003, 0.0004, 0.0002, 0.0002, 0.0001,\n",
       "                      0.0003, 0.0005, 0.0003, 0.0004, 0.0002, 0.0002, 0.0003, 0.0001, 0.0003,\n",
       "                      0.0003, 0.0001, 0.0002, 0.0002, 0.0004, 0.0003, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0003, 0.0002, 0.0001, 0.0001, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0003, 0.0004, 0.0001, 0.0003, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0005, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0004, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0003, 0.0002, 0.0001, 0.0002, 0.0002, 0.0001, 0.0002, 0.0003, 0.0003,\n",
       "                      0.0003, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0002, 0.0003, 0.0005,\n",
       "                      0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0001, 0.0002,\n",
       "                      0.0010, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0001, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0004, 0.0002, 0.0002, 0.0005,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0001,\n",
       "                      0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0001, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0006, 0.0002, 0.0002, 0.0001,\n",
       "                      0.0001, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0009, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002, 0.0001, 0.0002,\n",
       "                      0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0001, 0.0002, 0.0002, 0.0003, 0.0004, 0.0002, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0002, 0.0003, 0.0002, 0.0003, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0004, 0.0003, 0.0002, 0.0001, 0.0003, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0004, 0.0002,\n",
       "                      0.0001, 0.0004, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0001,\n",
       "                      0.0002, 0.0004, 0.0005, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0004,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0004, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0006, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0004,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0003, 0.0002, 0.0002, 0.0004, 0.0002, 0.0001, 0.0003, 0.0004, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0003, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002,\n",
       "                      0.0003, 0.0004, 0.0004, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002,\n",
       "                      0.0002, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0004, 0.0003, 0.0003, 0.0001, 0.0002, 0.0003, 0.0003, 0.0002, 0.0002,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0003, 0.0004,\n",
       "                      0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0003,\n",
       "                      0.0003, 0.0003, 0.0002, 0.0004, 0.0004, 0.0002, 0.0002, 0.0004, 0.0003,\n",
       "                      0.0004, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0001, 0.0002, 0.0004,\n",
       "                      0.0003, 0.0001, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0005, 0.0002,\n",
       "                      0.0003, 0.0002, 0.0001, 0.0001, 0.0001, 0.0003, 0.0001, 0.0001, 0.0002,\n",
       "                      0.0002, 0.0004, 0.0004, 0.0002, 0.0002, 0.0003, 0.0004, 0.0002, 0.0002,\n",
       "                      0.0001, 0.0004, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0004]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0208, -0.0172, -0.0247, -0.0195, -0.0232, -0.0252, -0.0194, -0.0251,\n",
       "                        -0.0152, -0.0327, -0.0192, -0.0173, -0.0354, -0.0230, -0.0243, -0.0242,\n",
       "                        -0.0383, -0.0170, -0.0195, -0.0248, -0.0163, -0.0215, -0.0336, -0.0367,\n",
       "                        -0.0135, -0.0186, -0.0232, -0.0176, -0.0160, -0.0215, -0.0267, -0.0266,\n",
       "                        -0.0190, -0.0249, -0.0208, -0.0151, -0.0240, -0.0268, -0.0199, -0.0244,\n",
       "                        -0.0237, -0.0459, -0.0284, -0.0205, -0.0198, -0.0184, -0.0211, -0.0246,\n",
       "                        -0.0174, -0.0139, -0.0170, -0.0171, -0.0347, -0.0267, -0.0557, -0.0168,\n",
       "                        -0.0224, -0.0278, -0.0201, -0.0327, -0.0204, -0.0190, -0.0140, -0.0267,\n",
       "                        -0.0157, -0.0203, -0.0351, -0.0370, -0.0262, -0.0306, -0.0223, -0.0271,\n",
       "                        -0.0177, -0.0233, -0.0278, -0.0172, -0.0188, -0.0213, -0.0292, -0.0359,\n",
       "                        -0.0249, -0.0254, -0.0314, -0.0199, -0.0204, -0.0171, -0.0344, -0.0209,\n",
       "                        -0.0287, -0.0258, -0.0239, -0.0152, -0.0500, -0.0506, -0.0207, -0.0244,\n",
       "                        -0.0197, -0.0193, -0.0215, -0.0146, -0.0254, -0.0198, -0.0165, -0.0180,\n",
       "                        -0.0207, -0.0266, -0.0165, -0.0296, -0.0206, -0.0286, -0.0218, -0.0251,\n",
       "                        -0.0154, -0.0178, -0.0293, -0.0215, -0.0161, -0.0223, -0.0220, -0.0149,\n",
       "                        -0.0390, -0.0185, -0.0463, -0.0199, -0.0202, -0.0188, -0.0252, -0.0362,\n",
       "                        -0.0221, -0.0292, -0.0211, -0.0258, -0.0246, -0.0147, -0.0297, -0.0233,\n",
       "                        -0.0128, -0.0224, -0.0316, -0.0434, -0.0367, -0.0307, -0.0181, -0.0208,\n",
       "                        -0.0170, -0.0134, -0.0286, -0.0241, -0.0140, -0.0153, -0.0182, -0.0252,\n",
       "                        -0.0198, -0.0146, -0.0248, -0.0219, -0.0224, -0.0142, -0.0361, -0.0279,\n",
       "                        -0.0354, -0.0252, -0.0212, -0.0164, -0.0243, -0.0210, -0.0156, -0.0264,\n",
       "                        -0.0205, -0.0246, -0.0163, -0.0239, -0.0199, -0.0325, -0.0318, -0.0302,\n",
       "                        -0.0227, -0.0174, -0.0183, -0.0216, -0.0289, -0.0368, -0.0195, -0.0216,\n",
       "                        -0.0232, -0.0198, -0.0196, -0.0258, -0.0300, -0.0339, -0.0252, -0.0171,\n",
       "                        -0.0257, -0.0176, -0.0157, -0.0220, -0.0203, -0.0265, -0.0358, -0.0154,\n",
       "                        -0.0138, -0.0182, -0.0168, -0.0287, -0.0223, -0.0228, -0.0240, -0.0198,\n",
       "                        -0.0283, -0.0190, -0.0224, -0.0428, -0.0270, -0.0256, -0.0180, -0.0243,\n",
       "                        -0.0649, -0.0253, -0.0177, -0.0154, -0.0153, -0.0171, -0.0254, -0.0275,\n",
       "                        -0.0187, -0.0286, -0.0218, -0.0235, -0.0183, -0.0228, -0.0141, -0.0211,\n",
       "                        -0.0218, -0.0169, -0.0237, -0.0212, -0.0206, -0.0222, -0.0200, -0.0199,\n",
       "                        -0.0206, -0.0252, -0.0335, -0.0298, -0.0395, -0.0204, -0.0215, -0.0256,\n",
       "                        -0.0175, -0.0262, -0.0146, -0.0213, -0.0238, -0.0206, -0.0242, -0.0209,\n",
       "                        -0.0202, -0.0226, -0.0277, -0.0301, -0.0169, -0.0326, -0.0251, -0.0220,\n",
       "                        -0.0220, -0.0276, -0.0198, -0.0303, -0.0093, -0.0170, -0.0249, -0.0211,\n",
       "                        -0.0346, -0.0195, -0.0249, -0.0269, -0.0268, -0.0240, -0.0163, -0.0135,\n",
       "                        -0.0152, -0.0449, -0.0247, -0.0219, -0.0151, -0.0195, -0.0437, -0.0295,\n",
       "                        -0.0307, -0.0296, -0.0198, -0.0204, -0.0332, -0.0301, -0.0282, -0.0308,\n",
       "                        -0.0246, -0.0196, -0.0289, -0.0202, -0.0189, -0.0251, -0.0199, -0.0197,\n",
       "                        -0.0121, -0.0193, -0.0493, -0.0149, -0.0231, -0.0200, -0.0294, -0.0331,\n",
       "                        -0.0164, -0.0218, -0.0258, -0.0114, -0.0238, -0.0316, -0.0277, -0.0404,\n",
       "                        -0.0246, -0.0203, -0.0192, -0.0289, -0.0257, -0.0250, -0.0236, -0.0204,\n",
       "                        -0.0246, -0.0240, -0.0195, -0.0258, -0.0156, -0.0305, -0.0416, -0.0191,\n",
       "                        -0.0169, -0.0187, -0.0338, -0.0220, -0.0251, -0.0193, -0.0295, -0.0194,\n",
       "                        -0.0221, -0.0251, -0.0219, -0.0360, -0.0242, -0.0328, -0.0228, -0.0139,\n",
       "                        -0.0319, -0.0141, -0.0161, -0.0351, -0.0246, -0.0183, -0.0127, -0.0167,\n",
       "                        -0.0179, -0.0269, -0.0353, -0.0335, -0.0210, -0.0250, -0.0368, -0.0164,\n",
       "                        -0.0459, -0.0252, -0.0220, -0.0155, -0.0143, -0.0254, -0.0204, -0.0238,\n",
       "                        -0.0361, -0.0203, -0.0466, -0.0290, -0.0209, -0.0248, -0.0227, -0.0229,\n",
       "                        -0.0369, -0.0145, -0.0482, -0.0289, -0.0254, -0.0176, -0.0339, -0.0263,\n",
       "                        -0.0195, -0.0298, -0.0187, -0.0319, -0.0220, -0.0230, -0.0179, -0.0361,\n",
       "                        -0.0153, -0.0147, -0.0322, -0.0503, -0.0208, -0.0319, -0.0224, -0.0247,\n",
       "                        -0.0130, -0.0259, -0.0202, -0.0185, -0.0182, -0.0210, -0.0377, -0.0248,\n",
       "                        -0.0307, -0.0159, -0.0277, -0.0285, -0.0181, -0.0368, -0.0253, -0.0263,\n",
       "                        -0.0159, -0.0355, -0.0243, -0.0181, -0.0217, -0.0249, -0.0171, -0.0217,\n",
       "                        -0.0350, -0.0177, -0.0258, -0.0124, -0.0240, -0.0226, -0.0296, -0.0204,\n",
       "                        -0.0139, -0.0306, -0.0259, -0.0166, -0.0282, -0.0176, -0.0236, -0.0215,\n",
       "                        -0.0184, -0.0390, -0.0178, -0.0201, -0.0227, -0.0219, -0.0170, -0.0215,\n",
       "                        -0.0236, -0.0247, -0.0251, -0.0193, -0.0255, -0.0280, -0.0568, -0.0189,\n",
       "                        -0.0166, -0.0196, -0.0225, -0.0299, -0.0319, -0.0157, -0.0257, -0.0272,\n",
       "                        -0.0390, -0.0161, -0.0176, -0.0242, -0.0456, -0.0355, -0.0174, -0.0226,\n",
       "                        -0.0268, -0.0163, -0.0173, -0.0294, -0.0254, -0.0142, -0.0198, -0.0196,\n",
       "                        -0.0185, -0.0162, -0.0160, -0.0229, -0.0124, -0.0174, -0.0219, -0.0199,\n",
       "                        -0.0317, -0.0200, -0.0192, -0.0183, -0.0224, -0.0294, -0.0128, -0.0183,\n",
       "                        -0.0134, -0.0278, -0.0158, -0.0275, -0.0273, -0.0197, -0.0188, -0.0456]), max_val=tensor([0.0227, 0.0180, 0.0348, 0.0244, 0.0220, 0.0260, 0.0218, 0.0437, 0.0251,\n",
       "                        0.0467, 0.0216, 0.0250, 0.0537, 0.0307, 0.0222, 0.0341, 0.0487, 0.0237,\n",
       "                        0.0259, 0.0231, 0.0166, 0.0156, 0.0489, 0.0319, 0.0151, 0.0244, 0.0339,\n",
       "                        0.0260, 0.0235, 0.0301, 0.0244, 0.0253, 0.0325, 0.0376, 0.0264, 0.0176,\n",
       "                        0.0294, 0.0388, 0.0210, 0.0494, 0.0270, 0.0357, 0.0323, 0.0214, 0.0262,\n",
       "                        0.0222, 0.0207, 0.0269, 0.0311, 0.0205, 0.0290, 0.0173, 0.0535, 0.0376,\n",
       "                        0.0689, 0.0211, 0.0405, 0.0344, 0.0219, 0.0414, 0.0273, 0.0316, 0.0204,\n",
       "                        0.0503, 0.0166, 0.0367, 0.0408, 0.0582, 0.0288, 0.0422, 0.0261, 0.0432,\n",
       "                        0.0208, 0.0302, 0.0499, 0.0289, 0.0203, 0.0226, 0.0463, 0.0309, 0.0260,\n",
       "                        0.0347, 0.0572, 0.0338, 0.0196, 0.0176, 0.0424, 0.0211, 0.0398, 0.0290,\n",
       "                        0.0253, 0.0220, 0.0804, 0.0523, 0.0250, 0.0233, 0.0220, 0.0298, 0.0294,\n",
       "                        0.0175, 0.0327, 0.0211, 0.0217, 0.0344, 0.0249, 0.0305, 0.0173, 0.0300,\n",
       "                        0.0224, 0.0377, 0.0279, 0.0154, 0.0205, 0.0285, 0.0551, 0.0416, 0.0242,\n",
       "                        0.0202, 0.0202, 0.0169, 0.0324, 0.0397, 0.0435, 0.0286, 0.0253, 0.0164,\n",
       "                        0.0340, 0.0602, 0.0394, 0.0449, 0.0174, 0.0215, 0.0344, 0.0152, 0.0362,\n",
       "                        0.0318, 0.0155, 0.0301, 0.0299, 0.0449, 0.0387, 0.0376, 0.0201, 0.0213,\n",
       "                        0.0340, 0.0201, 0.0435, 0.0277, 0.0181, 0.0177, 0.0226, 0.0323, 0.0260,\n",
       "                        0.0234, 0.0385, 0.0398, 0.0461, 0.0140, 0.0255, 0.0199, 0.0339, 0.0226,\n",
       "                        0.0212, 0.0259, 0.0219, 0.0273, 0.0214, 0.0596, 0.0196, 0.0428, 0.0206,\n",
       "                        0.0323, 0.0221, 0.0384, 0.0269, 0.0351, 0.0479, 0.0332, 0.0202, 0.0214,\n",
       "                        0.0269, 0.0376, 0.0248, 0.0265, 0.0322, 0.0334, 0.0273, 0.0286, 0.0422,\n",
       "                        0.0233, 0.0220, 0.0189, 0.0190, 0.0254, 0.0145, 0.0206, 0.0418, 0.0379,\n",
       "                        0.0301, 0.0167, 0.0161, 0.0172, 0.0163, 0.0232, 0.0312, 0.0333, 0.0604,\n",
       "                        0.0342, 0.0269, 0.0319, 0.0300, 0.0344, 0.0294, 0.0335, 0.0172, 0.0267,\n",
       "                        0.1290, 0.0274, 0.0227, 0.0201, 0.0170, 0.0198, 0.0240, 0.0254, 0.0208,\n",
       "                        0.0226, 0.0197, 0.0284, 0.0179, 0.0508, 0.0249, 0.0190, 0.0237, 0.0311,\n",
       "                        0.0232, 0.0224, 0.0246, 0.0337, 0.0348, 0.0489, 0.0204, 0.0300, 0.0633,\n",
       "                        0.0281, 0.0347, 0.0252, 0.0221, 0.0191, 0.0278, 0.0285, 0.0207, 0.0239,\n",
       "                        0.0197, 0.0287, 0.0339, 0.0192, 0.0316, 0.0299, 0.0345, 0.0260, 0.0162,\n",
       "                        0.0369, 0.0423, 0.0319, 0.0216, 0.0430, 0.0208, 0.0332, 0.0170, 0.0265,\n",
       "                        0.0323, 0.0164, 0.0265, 0.0305, 0.0271, 0.0730, 0.0311, 0.0284, 0.0151,\n",
       "                        0.0159, 0.0212, 0.0434, 0.0289, 0.0239, 0.0204, 0.0213, 0.1097, 0.0270,\n",
       "                        0.0234, 0.0373, 0.0209, 0.0304, 0.0346, 0.0282, 0.0422, 0.0308, 0.0267,\n",
       "                        0.0292, 0.0428, 0.0198, 0.0359, 0.0321, 0.0306, 0.0280, 0.0176, 0.0278,\n",
       "                        0.0253, 0.0202, 0.0208, 0.0204, 0.0298, 0.0213, 0.0198, 0.0287, 0.0386,\n",
       "                        0.0187, 0.0220, 0.0238, 0.0321, 0.0533, 0.0274, 0.0188, 0.0250, 0.0403,\n",
       "                        0.0205, 0.0417, 0.0311, 0.0328, 0.0463, 0.0310, 0.0236, 0.0314, 0.0251,\n",
       "                        0.0284, 0.0462, 0.0404, 0.0228, 0.0183, 0.0395, 0.0354, 0.0279, 0.0254,\n",
       "                        0.0412, 0.0297, 0.0253, 0.0316, 0.0218, 0.0442, 0.0412, 0.0516, 0.0244,\n",
       "                        0.0179, 0.0522, 0.0202, 0.0231, 0.0376, 0.0247, 0.0220, 0.0212, 0.0147,\n",
       "                        0.0266, 0.0473, 0.0602, 0.0338, 0.0354, 0.0160, 0.0357, 0.0205, 0.0354,\n",
       "                        0.0317, 0.0307, 0.0287, 0.0178, 0.0257, 0.0495, 0.0291, 0.0395, 0.0273,\n",
       "                        0.0719, 0.0236, 0.0329, 0.0336, 0.0338, 0.0346, 0.0368, 0.0210, 0.0293,\n",
       "                        0.0316, 0.0234, 0.0261, 0.0259, 0.0226, 0.0302, 0.0251, 0.0253, 0.0355,\n",
       "                        0.0335, 0.0222, 0.0205, 0.0516, 0.0279, 0.0190, 0.0235, 0.0349, 0.0210,\n",
       "                        0.0228, 0.0301, 0.0340, 0.0135, 0.0289, 0.0198, 0.0255, 0.0182, 0.0240,\n",
       "                        0.0339, 0.0445, 0.0497, 0.0240, 0.0268, 0.0336, 0.0236, 0.0243, 0.0316,\n",
       "                        0.0271, 0.0183, 0.0290, 0.0306, 0.0247, 0.0192, 0.0200, 0.0240, 0.0387,\n",
       "                        0.0516, 0.0377, 0.0348, 0.0145, 0.0189, 0.0379, 0.0324, 0.0291, 0.0212,\n",
       "                        0.0302, 0.0278, 0.0204, 0.0309, 0.0186, 0.0295, 0.0259, 0.0327, 0.0505,\n",
       "                        0.0263, 0.0210, 0.0316, 0.0214, 0.0172, 0.0191, 0.0216, 0.0268, 0.0328,\n",
       "                        0.0368, 0.0423, 0.0198, 0.0532, 0.0571, 0.0298, 0.0225, 0.0524, 0.0385,\n",
       "                        0.0479, 0.0208, 0.0388, 0.0267, 0.0260, 0.0337, 0.0189, 0.0284, 0.0529,\n",
       "                        0.0391, 0.0160, 0.0300, 0.0266, 0.0187, 0.0195, 0.0252, 0.0671, 0.0307,\n",
       "                        0.0351, 0.0202, 0.0166, 0.0181, 0.0183, 0.0377, 0.0173, 0.0186, 0.0161,\n",
       "                        0.0293, 0.0527, 0.0449, 0.0268, 0.0217, 0.0396, 0.0529, 0.0195, 0.0258,\n",
       "                        0.0131, 0.0446, 0.0223, 0.0255, 0.0383, 0.0325, 0.0275, 0.0284])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0173]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.191744804382324)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0014, 0.0021, 0.0019, 0.0017, 0.0018, 0.0011, 0.0022, 0.0025, 0.0009,\n",
       "                      0.0026, 0.0019, 0.0015, 0.0019, 0.0037, 0.0012, 0.0017, 0.0013, 0.0022,\n",
       "                      0.0016, 0.0017, 0.0019, 0.0030, 0.0013, 0.0023, 0.0014, 0.0018, 0.0013,\n",
       "                      0.0014, 0.0013, 0.0035, 0.0025, 0.0026, 0.0012, 0.0026, 0.0023, 0.0030,\n",
       "                      0.0010, 0.0019, 0.0023, 0.0015, 0.0012, 0.0011, 0.0022, 0.0015, 0.0018,\n",
       "                      0.0027, 0.0015, 0.0018, 0.0016, 0.0015, 0.0011, 0.0026, 0.0031, 0.0020,\n",
       "                      0.0015, 0.0023, 0.0023, 0.0013, 0.0011, 0.0011, 0.0016, 0.0029, 0.0009,\n",
       "                      0.0029, 0.0015, 0.0016, 0.0010, 0.0028, 0.0056, 0.0013, 0.0023, 0.0014,\n",
       "                      0.0013, 0.0030, 0.0022, 0.0022, 0.0027, 0.0013, 0.0014, 0.0023, 0.0024,\n",
       "                      0.0015, 0.0025, 0.0021, 0.0026, 0.0011, 0.0012, 0.0014, 0.0015, 0.0019,\n",
       "                      0.0012, 0.0012, 0.0013, 0.0017, 0.0011, 0.0014, 0.0029, 0.0010, 0.0023,\n",
       "                      0.0018, 0.0014, 0.0014, 0.0021, 0.0017, 0.0017, 0.0026, 0.0015, 0.0018,\n",
       "                      0.0023, 0.0022, 0.0016, 0.0018, 0.0020, 0.0031, 0.0021, 0.0019, 0.0014,\n",
       "                      0.0020, 0.0016, 0.0014, 0.0013, 0.0017, 0.0020, 0.0014, 0.0015, 0.0017,\n",
       "                      0.0023, 0.0019, 0.0012, 0.0015, 0.0026, 0.0017, 0.0011, 0.0018, 0.0034,\n",
       "                      0.0015, 0.0020, 0.0023, 0.0013, 0.0013, 0.0012, 0.0040, 0.0010, 0.0016,\n",
       "                      0.0025, 0.0024, 0.0021, 0.0016, 0.0019, 0.0017, 0.0015, 0.0014, 0.0019,\n",
       "                      0.0034, 0.0033, 0.0028, 0.0009, 0.0016, 0.0010, 0.0018, 0.0033, 0.0032,\n",
       "                      0.0026, 0.0005, 0.0054, 0.0014, 0.0017, 0.0020, 0.0020, 0.0024, 0.0016,\n",
       "                      0.0016, 0.0019, 0.0032, 0.0017, 0.0011, 0.0016, 0.0016, 0.0012, 0.0029,\n",
       "                      0.0025, 0.0017, 0.0012, 0.0014, 0.0016, 0.0010, 0.0011, 0.0025, 0.0009,\n",
       "                      0.0028, 0.0013, 0.0038, 0.0029, 0.0019, 0.0013, 0.0016, 0.0011, 0.0031,\n",
       "                      0.0014, 0.0017, 0.0022, 0.0027, 0.0025, 0.0016, 0.0022, 0.0025, 0.0017,\n",
       "                      0.0016, 0.0012, 0.0016, 0.0016, 0.0014, 0.0014, 0.0012, 0.0017, 0.0014,\n",
       "                      0.0014, 0.0025, 0.0014, 0.0023, 0.0019, 0.0024, 0.0023, 0.0024, 0.0018,\n",
       "                      0.0016, 0.0017, 0.0022, 0.0014, 0.0016, 0.0014, 0.0029, 0.0016, 0.0013,\n",
       "                      0.0016, 0.0019, 0.0016, 0.0012, 0.0013, 0.0020, 0.0017, 0.0026, 0.0039,\n",
       "                      0.0014, 0.0042, 0.0022, 0.0028, 0.0013, 0.0020, 0.0016, 0.0021, 0.0016,\n",
       "                      0.0024, 0.0033, 0.0008, 0.0010, 0.0018, 0.0015, 0.0014, 0.0020, 0.0018,\n",
       "                      0.0027, 0.0011, 0.0014, 0.0013, 0.0013, 0.0051, 0.0040, 0.0012, 0.0013,\n",
       "                      0.0008, 0.0035, 0.0028, 0.0034, 0.0060, 0.0015, 0.0014, 0.0020, 0.0016,\n",
       "                      0.0014, 0.0012, 0.0013, 0.0024, 0.0015, 0.0024, 0.0014, 0.0020, 0.0028,\n",
       "                      0.0007, 0.0021, 0.0010, 0.0023, 0.0010, 0.0018, 0.0018, 0.0030, 0.0027,\n",
       "                      0.0025, 0.0020, 0.0008, 0.0024, 0.0018, 0.0017, 0.0017, 0.0023, 0.0009,\n",
       "                      0.0017, 0.0013, 0.0019, 0.0011, 0.0015, 0.0012, 0.0020, 0.0017, 0.0020,\n",
       "                      0.0017, 0.0014, 0.0024, 0.0015, 0.0034, 0.0014, 0.0028, 0.0024, 0.0033,\n",
       "                      0.0022, 0.0020, 0.0024, 0.0020, 0.0014, 0.0026, 0.0013, 0.0015, 0.0022,\n",
       "                      0.0019, 0.0024, 0.0007, 0.0014, 0.0026, 0.0023, 0.0012, 0.0017, 0.0045,\n",
       "                      0.0014, 0.0016, 0.0013, 0.0026, 0.0019, 0.0017, 0.0010, 0.0046, 0.0015,\n",
       "                      0.0015, 0.0012, 0.0019, 0.0017, 0.0019, 0.0019, 0.0015, 0.0016, 0.0043,\n",
       "                      0.0008, 0.0041, 0.0011, 0.0017, 0.0015, 0.0017, 0.0029, 0.0018, 0.0023,\n",
       "                      0.0018, 0.0015, 0.0018, 0.0068, 0.0023, 0.0015, 0.0015, 0.0018, 0.0020,\n",
       "                      0.0023, 0.0028, 0.0011, 0.0017, 0.0021, 0.0026, 0.0015, 0.0018, 0.0009,\n",
       "                      0.0055, 0.0008, 0.0023, 0.0017, 0.0019, 0.0040, 0.0019, 0.0023, 0.0020,\n",
       "                      0.0015, 0.0013, 0.0013, 0.0013, 0.0036, 0.0017, 0.0018, 0.0012, 0.0022,\n",
       "                      0.0012, 0.0023, 0.0029, 0.0013, 0.0015, 0.0015, 0.0022, 0.0025, 0.0019,\n",
       "                      0.0016, 0.0022, 0.0011, 0.0024, 0.0030, 0.0047, 0.0012, 0.0014, 0.0013,\n",
       "                      0.0023, 0.0015, 0.0027, 0.0021, 0.0015, 0.0014, 0.0028, 0.0020, 0.0019,\n",
       "                      0.0022, 0.0017, 0.0026, 0.0019, 0.0015, 0.0027, 0.0014, 0.0017, 0.0031,\n",
       "                      0.0025, 0.0011, 0.0018, 0.0019, 0.0026, 0.0014, 0.0018, 0.0098, 0.0042,\n",
       "                      0.0025, 0.0024, 0.0021, 0.0020, 0.0017, 0.0012, 0.0012, 0.0016, 0.0015,\n",
       "                      0.0014, 0.0022, 0.0012, 0.0019, 0.0017, 0.0023, 0.0009, 0.0020, 0.0017,\n",
       "                      0.0024, 0.0030, 0.0016, 0.0013, 0.0013, 0.0020, 0.0015, 0.0023, 0.0022,\n",
       "                      0.0018, 0.0023, 0.0019, 0.0012, 0.0023, 0.0013, 0.0012, 0.0021, 0.0018,\n",
       "                      0.0012, 0.0013, 0.0017, 0.0017, 0.0029, 0.0010, 0.0013, 0.0021, 0.0021,\n",
       "                      0.0014, 0.0029, 0.0025, 0.0013, 0.0033, 0.0010, 0.0012, 0.0018, 0.0013,\n",
       "                      0.0018, 0.0015, 0.0023, 0.0024, 0.0012, 0.0013, 0.0032, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1295, -0.1740, -0.2070, -0.2125, -0.1516, -0.1326, -0.2009, -0.1766,\n",
       "                        -0.1139, -0.2510, -0.1974, -0.1222, -0.2383, -0.4743, -0.1534, -0.1515,\n",
       "                        -0.1097, -0.1447, -0.1474, -0.1487, -0.1455, -0.1545, -0.1469, -0.1452,\n",
       "                        -0.1781, -0.2114, -0.1719, -0.1620, -0.1327, -0.2266, -0.1876, -0.3019,\n",
       "                        -0.1540, -0.3318, -0.2596, -0.2604, -0.1210, -0.2069, -0.2335, -0.1553,\n",
       "                        -0.1174, -0.0903, -0.2821, -0.1757, -0.1772, -0.2125, -0.1791, -0.2365,\n",
       "                        -0.1401, -0.1956, -0.1220, -0.1752, -0.2448, -0.1941, -0.1841, -0.2961,\n",
       "                        -0.2922, -0.1454, -0.1453, -0.1381, -0.1278, -0.2753, -0.1108, -0.2753,\n",
       "                        -0.1686, -0.1775, -0.1295, -0.2710, -0.4309, -0.1701, -0.1590, -0.1459,\n",
       "                        -0.1633, -0.2826, -0.2868, -0.2149, -0.1988, -0.1468, -0.1201, -0.2044,\n",
       "                        -0.1731, -0.1728, -0.1505, -0.1977, -0.2040, -0.1196, -0.1028, -0.1612,\n",
       "                        -0.1270, -0.2427, -0.1392, -0.1584, -0.1356, -0.2206, -0.1161, -0.1343,\n",
       "                        -0.1712, -0.1253, -0.2276, -0.1778, -0.1602, -0.1218, -0.1621, -0.2164,\n",
       "                        -0.1460, -0.2396, -0.1840, -0.1855, -0.2754, -0.2805, -0.1985, -0.1719,\n",
       "                        -0.2093, -0.2749, -0.1578, -0.1934, -0.1449, -0.2589, -0.2100, -0.1581,\n",
       "                        -0.1507, -0.2185, -0.1951, -0.1102, -0.1688, -0.1895, -0.2984, -0.1667,\n",
       "                        -0.1573, -0.1394, -0.3316, -0.1312, -0.1110, -0.1622, -0.2930, -0.1457,\n",
       "                        -0.1698, -0.1662, -0.1458, -0.1703, -0.1175, -0.2372, -0.1047, -0.2035,\n",
       "                        -0.1945, -0.1684, -0.2740, -0.1880, -0.1766, -0.1755, -0.1509, -0.1693,\n",
       "                        -0.2039, -0.2402, -0.1868, -0.1675, -0.1110, -0.1830, -0.1080, -0.1908,\n",
       "                        -0.3339, -0.2395, -0.2670, -0.0674, -0.3651, -0.1842, -0.1591, -0.1850,\n",
       "                        -0.1679, -0.1603, -0.1621, -0.1609, -0.2292, -0.3098, -0.1882, -0.1364,\n",
       "                        -0.2007, -0.1520, -0.1430, -0.2293, -0.2226, -0.1826, -0.1177, -0.1510,\n",
       "                        -0.1395, -0.1317, -0.1000, -0.2438, -0.1003, -0.2031, -0.1642, -0.2025,\n",
       "                        -0.3649, -0.1860, -0.1164, -0.1999, -0.0998, -0.3972, -0.1534, -0.1633,\n",
       "                        -0.2831, -0.2038, -0.2508, -0.1734, -0.2625, -0.1754, -0.2205, -0.2038,\n",
       "                        -0.1523, -0.1389, -0.1666, -0.1453, -0.1557, -0.1239, -0.1582, -0.1353,\n",
       "                        -0.1374, -0.2230, -0.1231, -0.2060, -0.1826, -0.2776, -0.1884, -0.2384,\n",
       "                        -0.1487, -0.1356, -0.1651, -0.2135, -0.1551, -0.1539, -0.1533, -0.2188,\n",
       "                        -0.1405, -0.1131, -0.1218, -0.1634, -0.2038, -0.1070, -0.1702, -0.2516,\n",
       "                        -0.1967, -0.3357, -0.4936, -0.1322, -0.2517, -0.2288, -0.2017, -0.1570,\n",
       "                        -0.1734, -0.2011, -0.2445, -0.1653, -0.2707, -0.2236, -0.0886, -0.1306,\n",
       "                        -0.2274, -0.1698, -0.1601, -0.1386, -0.2329, -0.2092, -0.1377, -0.1301,\n",
       "                        -0.1520, -0.1680, -0.3574, -0.3751, -0.1564, -0.1610, -0.1082, -0.2335,\n",
       "                        -0.3565, -0.2710, -0.3351, -0.1418, -0.1384, -0.2117, -0.1563, -0.1518,\n",
       "                        -0.1146, -0.1656, -0.2351, -0.1749, -0.3130, -0.1727, -0.1746, -0.3542,\n",
       "                        -0.0733, -0.2177, -0.1169, -0.2934, -0.1238, -0.1927, -0.1389, -0.3798,\n",
       "                        -0.3508, -0.1612, -0.1676, -0.0875, -0.2806, -0.2103, -0.1799, -0.1902,\n",
       "                        -0.2947, -0.1210, -0.2200, -0.1288, -0.2165, -0.1294, -0.1255, -0.1302,\n",
       "                        -0.1839, -0.1424, -0.2047, -0.1813, -0.1373, -0.2543, -0.1459, -0.4333,\n",
       "                        -0.1754, -0.2001, -0.1904, -0.2470, -0.2296, -0.2196, -0.2254, -0.1970,\n",
       "                        -0.1639, -0.2628, -0.1460, -0.1705, -0.1526, -0.1949, -0.3038, -0.0911,\n",
       "                        -0.1741, -0.1775, -0.1771, -0.1038, -0.1480, -0.2953, -0.1323, -0.2052,\n",
       "                        -0.1468, -0.3275, -0.1947, -0.2120, -0.1149, -0.5592, -0.1553, -0.1519,\n",
       "                        -0.1582, -0.1570, -0.1993, -0.2027, -0.2372, -0.1631, -0.1586, -0.3087,\n",
       "                        -0.1033, -0.2849, -0.1378, -0.1591, -0.1936, -0.2139, -0.2384, -0.2275,\n",
       "                        -0.2881, -0.1579, -0.1752, -0.1575, -0.8694, -0.2145, -0.1764, -0.1850,\n",
       "                        -0.2364, -0.1907, -0.2262, -0.2075, -0.1315, -0.1620, -0.2331, -0.2078,\n",
       "                        -0.1916, -0.2300, -0.1088, -0.3497, -0.0866, -0.1839, -0.1730, -0.1601,\n",
       "                        -0.3504, -0.1507, -0.2917, -0.2123, -0.1595, -0.1354, -0.1554, -0.1724,\n",
       "                        -0.1984, -0.1266, -0.1600, -0.1572, -0.2843, -0.1277, -0.1921, -0.2322,\n",
       "                        -0.1418, -0.1809, -0.1953, -0.2818, -0.2058, -0.1801, -0.1524, -0.2000,\n",
       "                        -0.1216, -0.3111, -0.3232, -0.3102, -0.1056, -0.1549, -0.1514, -0.1912,\n",
       "                        -0.1956, -0.2384, -0.1957, -0.1684, -0.1557, -0.3614, -0.2602, -0.2144,\n",
       "                        -0.2253, -0.1653, -0.2562, -0.1796, -0.1366, -0.2212, -0.1736, -0.1976,\n",
       "                        -0.3410, -0.1930, -0.1286, -0.1881, -0.1843, -0.2042, -0.1459, -0.1466,\n",
       "                        -0.5289, -0.2118, -0.3228, -0.2714, -0.2002, -0.2489, -0.2221, -0.1231,\n",
       "                        -0.1557, -0.1402, -0.1374, -0.1261, -0.2457, -0.1547, -0.1699, -0.2028,\n",
       "                        -0.2103, -0.1204, -0.1982, -0.1774, -0.1785, -0.1768, -0.1844, -0.1172,\n",
       "                        -0.1557, -0.2271, -0.1916, -0.2129, -0.1425, -0.1674, -0.2015, -0.1768,\n",
       "                        -0.1593, -0.2103, -0.1602, -0.1105, -0.1674, -0.2353, -0.1501, -0.1347,\n",
       "                        -0.1543, -0.2227, -0.2607, -0.1062, -0.1486, -0.2745, -0.1641, -0.1834,\n",
       "                        -0.2576, -0.2805, -0.1494, -0.2543, -0.1285, -0.1340, -0.1570, -0.1349,\n",
       "                        -0.1767, -0.1625, -0.1971, -0.1736, -0.1389, -0.1488, -0.3392, -0.1894]), max_val=tensor([0.1822, 0.2682, 0.2389, 0.1803, 0.2246, 0.1346, 0.2767, 0.3125, 0.1007,\n",
       "                        0.3291, 0.2386, 0.1898, 0.2326, 0.3913, 0.1509, 0.2134, 0.1657, 0.2841,\n",
       "                        0.2007, 0.2203, 0.2420, 0.3778, 0.1689, 0.2872, 0.1828, 0.2247, 0.1512,\n",
       "                        0.1730, 0.1714, 0.4497, 0.3230, 0.3265, 0.1428, 0.3308, 0.2918, 0.3778,\n",
       "                        0.1298, 0.2387, 0.2864, 0.1927, 0.1552, 0.1400, 0.1649, 0.1855, 0.2266,\n",
       "                        0.3441, 0.1901, 0.1940, 0.2006, 0.1785, 0.1386, 0.3243, 0.3953, 0.2496,\n",
       "                        0.1916, 0.2819, 0.2919, 0.1641, 0.1196, 0.1061, 0.1996, 0.3641, 0.0919,\n",
       "                        0.3636, 0.1940, 0.2074, 0.1246, 0.3523, 0.7056, 0.1336, 0.2950, 0.1739,\n",
       "                        0.1615, 0.3833, 0.1806, 0.2852, 0.3379, 0.1711, 0.1771, 0.2900, 0.3025,\n",
       "                        0.1892, 0.3122, 0.2681, 0.3323, 0.1383, 0.1557, 0.1778, 0.1895, 0.2327,\n",
       "                        0.1576, 0.1584, 0.1710, 0.1895, 0.1374, 0.1771, 0.3683, 0.1312, 0.2878,\n",
       "                        0.2260, 0.1736, 0.1770, 0.2666, 0.2028, 0.2215, 0.3268, 0.1870, 0.2232,\n",
       "                        0.2912, 0.2200, 0.2037, 0.2334, 0.2585, 0.3875, 0.2657, 0.2394, 0.1793,\n",
       "                        0.1464, 0.1640, 0.1808, 0.1680, 0.1871, 0.2493, 0.1780, 0.1950, 0.2134,\n",
       "                        0.2257, 0.2366, 0.1526, 0.1878, 0.2841, 0.2116, 0.1380, 0.2257, 0.4280,\n",
       "                        0.1879, 0.2561, 0.2983, 0.1630, 0.1498, 0.1461, 0.5031, 0.1298, 0.2029,\n",
       "                        0.3207, 0.3108, 0.2347, 0.1976, 0.2378, 0.2163, 0.1946, 0.1718, 0.2352,\n",
       "                        0.4361, 0.4199, 0.3519, 0.1178, 0.2027, 0.1323, 0.2229, 0.4181, 0.4028,\n",
       "                        0.3295, 0.0543, 0.6823, 0.1652, 0.2131, 0.2508, 0.2556, 0.3077, 0.1982,\n",
       "                        0.2072, 0.2367, 0.4079, 0.2203, 0.1221, 0.2031, 0.2030, 0.1490, 0.3704,\n",
       "                        0.3141, 0.2132, 0.1475, 0.1759, 0.2018, 0.1067, 0.1367, 0.3175, 0.1159,\n",
       "                        0.3530, 0.1447, 0.4829, 0.3273, 0.2430, 0.1636, 0.1812, 0.1386, 0.2660,\n",
       "                        0.1782, 0.2189, 0.2824, 0.3432, 0.3126, 0.2018, 0.2829, 0.3167, 0.1977,\n",
       "                        0.1849, 0.1310, 0.2006, 0.2055, 0.1773, 0.1746, 0.1481, 0.2173, 0.1727,\n",
       "                        0.1814, 0.3120, 0.1806, 0.2946, 0.2457, 0.3075, 0.2902, 0.3068, 0.2271,\n",
       "                        0.1977, 0.2201, 0.2846, 0.1733, 0.2024, 0.1831, 0.3682, 0.1992, 0.1596,\n",
       "                        0.2019, 0.2372, 0.1980, 0.1537, 0.1633, 0.2102, 0.2174, 0.2946, 0.2690,\n",
       "                        0.1764, 0.5310, 0.2841, 0.3555, 0.1684, 0.2513, 0.1898, 0.2642, 0.2065,\n",
       "                        0.3040, 0.4155, 0.0978, 0.1139, 0.1899, 0.1849, 0.1738, 0.2534, 0.2012,\n",
       "                        0.3465, 0.1442, 0.1761, 0.1705, 0.1471, 0.6518, 0.5026, 0.0988, 0.1644,\n",
       "                        0.0901, 0.4394, 0.3229, 0.4290, 0.7591, 0.1947, 0.1765, 0.2554, 0.2001,\n",
       "                        0.1804, 0.1513, 0.1570, 0.3007, 0.1919, 0.2910, 0.1816, 0.2522, 0.2554,\n",
       "                        0.0886, 0.2614, 0.1249, 0.2700, 0.1243, 0.2270, 0.2245, 0.3622, 0.3140,\n",
       "                        0.3172, 0.2574, 0.0966, 0.3063, 0.2284, 0.2182, 0.2197, 0.2514, 0.1157,\n",
       "                        0.1857, 0.1605, 0.2443, 0.1427, 0.1938, 0.1489, 0.2565, 0.2101, 0.2520,\n",
       "                        0.2106, 0.1831, 0.3031, 0.1888, 0.4220, 0.1753, 0.3530, 0.3066, 0.4142,\n",
       "                        0.2768, 0.2573, 0.3089, 0.2567, 0.1770, 0.3291, 0.1694, 0.1895, 0.2738,\n",
       "                        0.2378, 0.2963, 0.0865, 0.1347, 0.3244, 0.2968, 0.1507, 0.2159, 0.5767,\n",
       "                        0.1770, 0.1706, 0.1645, 0.2986, 0.2398, 0.2068, 0.1270, 0.5877, 0.1896,\n",
       "                        0.1863, 0.1540, 0.2387, 0.2195, 0.2452, 0.1832, 0.1919, 0.1994, 0.5404,\n",
       "                        0.1032, 0.5190, 0.1238, 0.2104, 0.1968, 0.1904, 0.3643, 0.1542, 0.2829,\n",
       "                        0.2302, 0.1951, 0.2298, 0.7197, 0.2938, 0.1848, 0.1936, 0.2156, 0.2546,\n",
       "                        0.2956, 0.3558, 0.1412, 0.2146, 0.2710, 0.3343, 0.1805, 0.2073, 0.0995,\n",
       "                        0.7043, 0.1040, 0.2882, 0.2158, 0.2469, 0.5100, 0.2419, 0.2004, 0.2528,\n",
       "                        0.1843, 0.1668, 0.1629, 0.1478, 0.4573, 0.2150, 0.2299, 0.1537, 0.2533,\n",
       "                        0.1554, 0.2926, 0.3741, 0.1696, 0.1935, 0.1530, 0.2203, 0.3197, 0.2454,\n",
       "                        0.2044, 0.2731, 0.1395, 0.2826, 0.3764, 0.6016, 0.1541, 0.1759, 0.1712,\n",
       "                        0.2925, 0.1410, 0.3393, 0.2619, 0.1957, 0.1783, 0.3034, 0.2145, 0.2419,\n",
       "                        0.2754, 0.2206, 0.3290, 0.2419, 0.1908, 0.3448, 0.1529, 0.2167, 0.3933,\n",
       "                        0.3201, 0.1401, 0.2267, 0.2356, 0.3354, 0.1769, 0.2273, 1.2499, 0.5297,\n",
       "                        0.2762, 0.3034, 0.2651, 0.2597, 0.1746, 0.1491, 0.1373, 0.2087, 0.1943,\n",
       "                        0.1725, 0.2746, 0.1377, 0.2433, 0.2167, 0.2980, 0.1196, 0.2510, 0.2158,\n",
       "                        0.3046, 0.3805, 0.2005, 0.1657, 0.1637, 0.2546, 0.1784, 0.2892, 0.2801,\n",
       "                        0.2267, 0.2871, 0.2435, 0.1392, 0.2913, 0.1708, 0.1479, 0.2641, 0.1763,\n",
       "                        0.1419, 0.1594, 0.2116, 0.2125, 0.3713, 0.1230, 0.1712, 0.2564, 0.2706,\n",
       "                        0.1603, 0.3716, 0.3161, 0.1615, 0.4190, 0.1330, 0.1467, 0.2235, 0.1691,\n",
       "                        0.2231, 0.1854, 0.2974, 0.3036, 0.1524, 0.1643, 0.4051, 0.2009])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0690]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.805248260498047, max_val=3.958695888519287)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([4.5951e-04, 1.2551e-03, 8.0677e-04, 2.6224e-03, 5.8778e-04, 5.9013e-04,\n",
       "                        1.1015e-03, 2.1210e-03, 4.0332e-04, 7.3590e-04, 3.1243e-04, 7.0479e-04,\n",
       "                        1.5580e-03, 2.1822e-04, 7.7000e-04, 9.0661e-04, 1.6836e-03, 1.4277e-03,\n",
       "                        1.1616e-03, 1.1202e-03, 5.6113e-04, 1.3802e-03, 1.4253e-03, 1.4293e-03,\n",
       "                        7.4588e-04, 4.4554e-04, 1.6031e-03, 7.3080e-04, 1.0991e-03, 7.3561e-04,\n",
       "                        1.1117e-03, 1.8361e-03, 7.3739e-04, 1.0908e-03, 8.1289e-04, 1.4180e-03,\n",
       "                        5.5586e-04, 8.2532e-04, 1.0634e-03, 1.9223e-03, 6.8505e-04, 7.8094e-04,\n",
       "                        1.4797e-03, 1.8879e-03, 7.5397e-04, 4.4565e-04, 9.8063e-04, 2.0996e-03,\n",
       "                        6.3557e-04, 8.3531e-04, 6.3055e-04, 1.5822e-03, 8.4802e-04, 5.8026e-04,\n",
       "                        1.1892e-03, 2.3574e-03, 1.3405e-03, 9.9700e-04, 1.0129e-03, 3.5231e-04,\n",
       "                        1.9330e-03, 1.7863e-03, 3.7456e-04, 1.0946e-03, 5.2276e-04, 1.8967e-03,\n",
       "                        1.1651e-03, 6.2083e-04, 2.9197e-03, 1.0500e-03, 5.6996e-04, 5.4109e-04,\n",
       "                        7.8266e-04, 2.7964e-04, 1.1422e-03, 6.4920e-04, 2.0254e-03, 1.1690e-03,\n",
       "                        7.8742e-04, 1.2540e-03, 1.2195e-03, 1.2951e-03, 8.9836e-04, 4.4353e-04,\n",
       "                        9.7374e-04, 5.9538e-04, 1.3784e-03, 1.8448e-03, 7.4706e-04, 7.9993e-04,\n",
       "                        5.0206e-04, 5.4376e-04, 7.8182e-04, 3.3289e-03, 6.8296e-04, 7.9510e-04,\n",
       "                        1.6473e-03, 9.5832e-04, 1.2643e-03, 7.3972e-04, 8.6545e-04, 5.9056e-04,\n",
       "                        4.2936e-04, 1.1843e-03, 1.2971e-03, 1.9871e-03, 4.5494e-04, 1.0136e-03,\n",
       "                        7.1003e-04, 1.5977e-03, 2.1664e-03, 9.6760e-04, 1.2534e-03, 2.5172e-03,\n",
       "                        7.1025e-04, 1.3251e-03, 4.0344e-04, 5.6479e-03, 1.1027e-03, 8.7217e-04,\n",
       "                        4.4407e-04, 7.0094e-04, 1.5140e-04, 1.0795e-03, 9.9232e-04, 7.7183e-04,\n",
       "                        5.7368e-04, 5.2740e-04, 9.9401e-04, 4.6802e-04, 1.1931e-03, 5.6579e-04,\n",
       "                        5.5722e-04, 1.1899e-03, 2.2941e-03, 1.5902e-03, 2.8413e-04, 7.4247e-04,\n",
       "                        5.6260e-04, 1.1110e-03, 8.9651e-04, 3.6557e-03, 7.0592e-04, 5.6751e-04,\n",
       "                        7.0634e-04, 1.0379e-03, 1.1503e-03, 2.0695e-03, 4.4100e-04, 1.3760e-03,\n",
       "                        5.3795e-04, 4.3296e-04, 1.1647e-03, 9.7295e-04, 2.2709e-03, 6.4227e-04,\n",
       "                        7.3259e-04, 8.4482e-04, 7.1139e-04, 7.8338e-04, 1.3093e-03, 2.8735e-03,\n",
       "                        1.2907e-03, 4.3249e-03, 1.5761e-03, 1.8267e-04, 1.7234e-03, 7.1015e-04,\n",
       "                        1.5335e-03, 9.2352e-04, 8.1013e-04, 1.1341e-03, 9.7554e-04, 1.5293e-03,\n",
       "                        1.1553e-03, 1.0173e-03, 1.5242e-03, 8.2838e-04, 6.6558e-04, 9.0536e-04,\n",
       "                        3.5304e-03, 1.6211e-03, 4.7604e-04, 6.4748e-04, 1.8994e-03, 1.7183e-03,\n",
       "                        6.6532e-04, 1.0495e-03, 5.9393e-04, 9.3485e-04, 1.0973e-03, 2.7603e-03,\n",
       "                        1.8762e-03, 2.5304e-03, 4.5359e-04, 1.3821e-03, 7.6860e-04, 1.1142e-03,\n",
       "                        1.4368e-03, 5.6109e-04, 3.9470e-04, 1.0462e-03, 1.1677e-03, 6.0902e-04,\n",
       "                        1.6195e-03, 1.0981e-03, 6.7133e-04, 3.6574e-03, 1.0233e-03, 1.2439e-03,\n",
       "                        9.0585e-04, 8.0591e-04, 6.1953e-04, 1.1290e-03, 7.0027e-04, 6.1231e-04,\n",
       "                        8.6271e-04, 1.1923e-03, 7.1672e-04, 6.0801e-04, 1.6881e-03, 4.0787e-03,\n",
       "                        1.0789e-03, 9.3529e-04, 1.3076e-03, 7.2346e-04, 7.7645e-04, 6.8193e-04,\n",
       "                        1.0187e-03, 7.5697e-04, 4.4188e-04, 9.8578e-04, 6.4868e-04, 9.6556e-04,\n",
       "                        7.5723e-04, 2.3275e-03, 4.6509e-04, 1.1802e-03, 9.4208e-04, 2.7612e-04,\n",
       "                        6.0831e-04, 7.8526e-04, 1.8378e-03, 8.8305e-04, 8.3965e-04, 9.4184e-04,\n",
       "                        9.9841e-04, 1.0879e-03, 7.2963e-04, 4.9472e-04, 1.3986e-03, 2.0961e-03,\n",
       "                        1.4380e-03, 1.1013e-03, 6.4586e-04, 6.7872e-04, 2.0706e-03, 1.1658e-03,\n",
       "                        4.1723e-04, 1.8317e-03, 6.6988e-04, 8.8609e-04, 2.7045e-04, 7.9854e-04,\n",
       "                        1.4916e-03, 5.6800e-04, 1.4459e-03, 1.4061e-03, 3.0271e-03, 1.0585e-03,\n",
       "                        5.0422e-04, 1.0110e-03, 5.4578e-04, 1.4805e-03, 2.1989e-03, 6.5633e-04,\n",
       "                        7.5871e-04, 5.6341e-04, 7.8541e-04, 8.5558e-04, 5.6659e-04, 1.1047e-03,\n",
       "                        6.6894e-04, 1.5002e-03, 1.1657e-04, 1.1302e-03, 1.1143e-03, 1.8173e-03,\n",
       "                        3.8966e-04, 1.1293e-03, 7.2951e-04, 1.5401e-03, 1.7795e-03, 9.3058e-04,\n",
       "                        1.0006e-03, 1.9289e-03, 1.1688e-03, 1.9623e-03, 8.3519e-04, 5.4731e-04,\n",
       "                        3.1648e-03, 8.9768e-04, 5.4123e-04, 1.4354e-03, 1.4978e-03, 8.4420e-04,\n",
       "                        2.3931e-04, 1.2483e-03, 9.3094e-04, 5.8719e-04, 1.2381e-03, 6.0942e-04,\n",
       "                        1.3257e-03, 1.4857e-03, 5.5338e-04, 6.6773e-04, 6.5246e-04, 1.6626e-03,\n",
       "                        1.3136e-03, 8.0835e-05, 6.5640e-04, 7.0137e-04, 1.5875e-03, 1.5916e-03,\n",
       "                        1.0570e-03, 1.2030e-03, 3.6397e-04, 1.1174e-03, 7.2843e-04, 1.5293e-03,\n",
       "                        4.5752e-04, 9.1305e-04, 6.9568e-04, 8.6320e-04, 1.5415e-03, 9.0120e-04,\n",
       "                        2.6229e-04, 1.3913e-03, 1.0131e-03, 5.6567e-04, 3.1728e-04, 1.2713e-04,\n",
       "                        1.3173e-03, 7.8768e-04, 5.8275e-04, 5.4038e-04, 8.4014e-04, 1.1299e-03,\n",
       "                        4.9563e-04, 2.9450e-04, 9.8313e-04, 1.3641e-03, 2.0666e-03, 1.4362e-03,\n",
       "                        7.7850e-04, 1.8305e-03, 1.1653e-03, 1.4513e-03, 1.1400e-03, 1.0702e-03,\n",
       "                        4.6965e-04, 1.3465e-03, 3.9559e-04, 5.1210e-04, 1.2839e-03, 9.9451e-04,\n",
       "                        8.5512e-04, 1.5276e-03, 1.5674e-03, 2.6142e-04, 9.0049e-04, 6.4824e-04,\n",
       "                        4.5682e-04, 6.1075e-04, 7.1975e-04, 7.0668e-04, 9.9177e-04, 1.3241e-03,\n",
       "                        1.4651e-03, 1.9478e-03, 1.1321e-03, 2.3611e-03, 1.0049e-03, 1.4466e-03,\n",
       "                        7.5717e-04, 1.0738e-03, 7.8344e-04, 2.2197e-03, 1.0573e-03, 9.6642e-04,\n",
       "                        8.1168e-04, 1.2355e-03, 1.3257e-03, 6.3680e-04, 2.1017e-03, 1.1869e-03,\n",
       "                        1.4789e-03, 8.6482e-04, 5.3776e-04, 1.0723e-03, 1.5769e-03, 8.8864e-04,\n",
       "                        8.0053e-04, 9.9603e-04, 1.4205e-03, 7.7899e-04, 1.0413e-03, 8.7717e-04,\n",
       "                        1.1566e-03, 6.2939e-04, 6.9436e-04, 7.9878e-04, 1.8545e-03, 5.7042e-04,\n",
       "                        4.8853e-04, 1.0573e-03, 1.4220e-03, 5.8209e-04, 1.1879e-03, 8.7547e-04,\n",
       "                        3.5024e-04, 6.7079e-04, 8.3632e-04, 1.0950e-03, 3.4642e-04, 1.3197e-03,\n",
       "                        1.4912e-03, 6.7116e-04, 2.3370e-04, 1.0888e-03, 1.3804e-03, 3.6214e-04,\n",
       "                        1.5290e-03, 3.3359e-04, 1.3001e-03, 1.4928e-03, 4.5991e-04, 1.5297e-03,\n",
       "                        7.1142e-04, 3.1584e-04, 4.7830e-04, 1.0824e-03, 9.5937e-04, 1.6297e-03,\n",
       "                        6.9120e-04, 7.7815e-04, 6.7398e-04, 6.3750e-04, 3.5914e-03, 2.9225e-03,\n",
       "                        1.4156e-03, 1.8550e-03, 6.8799e-04, 1.1738e-03, 1.0193e-03, 3.0926e-04,\n",
       "                        1.0588e-03, 9.2254e-04, 1.1384e-03, 1.4105e-03, 2.1617e-03, 3.3012e-04,\n",
       "                        5.4861e-04, 6.5330e-04, 2.3199e-03, 9.2694e-04, 1.0257e-03, 1.2955e-03,\n",
       "                        3.1621e-04, 1.1928e-03, 5.6145e-04, 7.0870e-04, 5.2037e-04, 1.1981e-03,\n",
       "                        1.4563e-03, 7.4668e-04, 8.8726e-04, 6.3569e-04, 1.3989e-03, 7.0434e-04,\n",
       "                        1.2548e-03, 6.1368e-04, 9.0267e-04, 3.3028e-04, 1.2651e-03, 3.0858e-04,\n",
       "                        7.9945e-04, 1.7521e-03, 2.7151e-03, 9.6740e-04, 1.8577e-03, 2.0122e-03,\n",
       "                        5.3370e-04, 1.0178e-03, 6.1324e-04, 1.0942e-03, 1.4172e-03, 7.8100e-04,\n",
       "                        6.9680e-04, 8.6721e-04, 5.7299e-04, 1.1881e-03, 8.5610e-04, 6.1315e-04,\n",
       "                        7.4645e-04, 1.8434e-03, 1.1046e-03, 1.9323e-03, 6.6190e-04, 1.6257e-03,\n",
       "                        1.8043e-03, 8.5352e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.0573, -0.1338, -0.0844, -0.1194, -0.0601, -0.0755, -0.1373, -0.1478,\n",
       "                          -0.0516, -0.0645, -0.0400, -0.0902, -0.1443, -0.0279, -0.0986, -0.1160,\n",
       "                          -0.1477, -0.1690, -0.1487, -0.0973, -0.0567, -0.1644, -0.1151, -0.1829,\n",
       "                          -0.0782, -0.0462, -0.2052, -0.0707, -0.1279, -0.0805, -0.0813, -0.2350,\n",
       "                          -0.0944, -0.1396, -0.0822, -0.1388, -0.0563, -0.0965, -0.0924, -0.1456,\n",
       "                          -0.0877, -0.0880, -0.1827, -0.2417, -0.0965, -0.0570, -0.0750, -0.1543,\n",
       "                          -0.0614, -0.0908, -0.0552, -0.2025, -0.0814, -0.0743, -0.1393, -0.1919,\n",
       "                          -0.1716, -0.1254, -0.0986, -0.0447, -0.1292, -0.2039, -0.0381, -0.0745,\n",
       "                          -0.0455, -0.2428, -0.1084, -0.0564, -0.2705, -0.1210, -0.0726, -0.0640,\n",
       "                          -0.0699, -0.0293, -0.1462, -0.0831, -0.1488, -0.1224, -0.1008, -0.1204,\n",
       "                          -0.1534, -0.1181, -0.1031, -0.0568, -0.0904, -0.0762, -0.1764, -0.2361,\n",
       "                          -0.0956, -0.0985, -0.0507, -0.0672, -0.1001, -0.2907, -0.0682, -0.0931,\n",
       "                          -0.1186, -0.1227, -0.1618, -0.0662, -0.1108, -0.0703, -0.0550, -0.1489,\n",
       "                          -0.1151, -0.1586, -0.0527, -0.0907, -0.0909, -0.1294, -0.2773, -0.1239,\n",
       "                          -0.0743, -0.3222, -0.0709, -0.1544, -0.0478, -0.2253, -0.1135, -0.1116,\n",
       "                          -0.0482, -0.0551, -0.0176, -0.0888, -0.0604, -0.0988, -0.0734, -0.0545,\n",
       "                          -0.0766, -0.0567, -0.1231, -0.0724, -0.0713, -0.1482, -0.2936, -0.1921,\n",
       "                          -0.0299, -0.0950, -0.0670, -0.0885, -0.0937, -0.3525, -0.0693, -0.0510,\n",
       "                          -0.0683, -0.0890, -0.1049, -0.2122, -0.0475, -0.1443, -0.0536, -0.0454,\n",
       "                          -0.1491, -0.1119, -0.2376, -0.0711, -0.0771, -0.0896, -0.0685, -0.0789,\n",
       "                          -0.1439, -0.3512, -0.1020, -0.4624, -0.1362, -0.0234, -0.2206, -0.0905,\n",
       "                          -0.1963, -0.0949, -0.1037, -0.0697, -0.1217, -0.1378, -0.0797, -0.1057,\n",
       "                          -0.1951, -0.1060, -0.0838, -0.0520, -0.4519, -0.1246, -0.0609, -0.0829,\n",
       "                          -0.1748, -0.1144, -0.0716, -0.0682, -0.0760, -0.0724, -0.1043, -0.3533,\n",
       "                          -0.1043, -0.3239, -0.0581, -0.1769, -0.0778, -0.1426, -0.1839, -0.0579,\n",
       "                          -0.0505, -0.1339, -0.1495, -0.0534, -0.1930, -0.1356, -0.0859, -0.4682,\n",
       "                          -0.0518, -0.1592, -0.1150, -0.0721, -0.0435, -0.0987, -0.0800, -0.0679,\n",
       "                          -0.0910, -0.1152, -0.0748, -0.0519, -0.2161, -0.5221, -0.0788, -0.0917,\n",
       "                          -0.1174, -0.0493, -0.0561, -0.0655, -0.1304, -0.0800, -0.0566, -0.0675,\n",
       "                          -0.0771, -0.1236, -0.0793, -0.2222, -0.0566, -0.1511, -0.1206, -0.0353,\n",
       "                          -0.0522, -0.0720, -0.2352, -0.1008, -0.0755, -0.1205, -0.0662, -0.1357,\n",
       "                          -0.0610, -0.0633, -0.1318, -0.2683, -0.1841, -0.1410, -0.0644, -0.0832,\n",
       "                          -0.2650, -0.1044, -0.0389, -0.2345, -0.0837, -0.0992, -0.0346, -0.0909,\n",
       "                          -0.1251, -0.0727, -0.1851, -0.1800, -0.0928, -0.0843, -0.0645, -0.0824,\n",
       "                          -0.0687, -0.1723, -0.2268, -0.0840, -0.0760, -0.0622, -0.1005, -0.0965,\n",
       "                          -0.0725, -0.1179, -0.0752, -0.1551, -0.0149, -0.1261, -0.0827, -0.1281,\n",
       "                          -0.0499, -0.0884, -0.0779, -0.1623, -0.2278, -0.1072, -0.1224, -0.1381,\n",
       "                          -0.1496, -0.1929, -0.1069, -0.0589, -0.2471, -0.1149, -0.0669, -0.1205,\n",
       "                          -0.1536, -0.1036, -0.0306, -0.1334, -0.1192, -0.0707, -0.1225, -0.0714,\n",
       "                          -0.1697, -0.1750, -0.0519, -0.0496, -0.0835, -0.1773, -0.1681, -0.0103,\n",
       "                          -0.0617, -0.0691, -0.1003, -0.2037, -0.0967, -0.1540, -0.0371, -0.1430,\n",
       "                          -0.0720, -0.1247, -0.0586, -0.0886, -0.0848, -0.0925, -0.1007, -0.0871,\n",
       "                          -0.0336, -0.1460, -0.1192, -0.0702, -0.0341, -0.0163, -0.1417, -0.0822,\n",
       "                          -0.0746, -0.0510, -0.0898, -0.1446, -0.0467, -0.0377, -0.1016, -0.1746,\n",
       "                          -0.1735, -0.1048, -0.0734, -0.2343, -0.0964, -0.1185, -0.1378, -0.1079,\n",
       "                          -0.0596, -0.1723, -0.0506, -0.0503, -0.0801, -0.1257, -0.0904, -0.1353,\n",
       "                          -0.0983, -0.0263, -0.0883, -0.0830, -0.0585, -0.0782, -0.0834, -0.0905,\n",
       "                          -0.1269, -0.1652, -0.1635, -0.2493, -0.1449, -0.3022, -0.1116, -0.1852,\n",
       "                          -0.0969, -0.1375, -0.0621, -0.2688, -0.1353, -0.0946, -0.1039, -0.1094,\n",
       "                          -0.1416, -0.0815, -0.2690, -0.1441, -0.1075, -0.0943, -0.0538, -0.1079,\n",
       "                          -0.1567, -0.0883, -0.0664, -0.1275, -0.1007, -0.0689, -0.1333, -0.1020,\n",
       "                          -0.1480, -0.0806, -0.0666, -0.0915, -0.1289, -0.0522, -0.0625, -0.1172,\n",
       "                          -0.1820, -0.0563, -0.1007, -0.0890, -0.0341, -0.0766, -0.0896, -0.1303,\n",
       "                          -0.0443, -0.1400, -0.0801, -0.0598, -0.0299, -0.0994, -0.1148, -0.0379,\n",
       "                          -0.1957, -0.0427, -0.1146, -0.1348, -0.0543, -0.1747, -0.0774, -0.0396,\n",
       "                          -0.0612, -0.1362, -0.1228, -0.2086, -0.0564, -0.0996, -0.0767, -0.0816,\n",
       "                          -0.2375, -0.1783, -0.1373, -0.2374, -0.0833, -0.1023, -0.1305, -0.0396,\n",
       "                          -0.1320, -0.0909, -0.1374, -0.1770, -0.2767, -0.0423, -0.0615, -0.0828,\n",
       "                          -0.2639, -0.1186, -0.1313, -0.1205, -0.0380, -0.1479, -0.0719, -0.0650,\n",
       "                          -0.0602, -0.0881, -0.1864, -0.0956, -0.1001, -0.0814, -0.1791, -0.0902,\n",
       "                          -0.0950, -0.0786, -0.0505, -0.0347, -0.1135, -0.0360, -0.1023, -0.2243,\n",
       "                          -0.1694, -0.0600, -0.2378, -0.1473, -0.0670, -0.0874, -0.0656, -0.1078,\n",
       "                          -0.1179, -0.0871, -0.0790, -0.0776, -0.0680, -0.0872, -0.0721, -0.0656,\n",
       "                          -0.0738, -0.2218, -0.1364, -0.1551, -0.0600, -0.1317, -0.1388, -0.0504]), max_val=tensor([0.0584, 0.1594, 0.1025, 0.3330, 0.0746, 0.0566, 0.1399, 0.2694, 0.0449,\n",
       "                          0.0935, 0.0384, 0.0831, 0.1979, 0.0241, 0.0956, 0.0969, 0.2138, 0.1813,\n",
       "                          0.1024, 0.1423, 0.0713, 0.1753, 0.1810, 0.1373, 0.0947, 0.0566, 0.1572,\n",
       "                          0.0928, 0.1396, 0.0934, 0.1412, 0.2302, 0.0872, 0.0864, 0.1032, 0.1801,\n",
       "                          0.0706, 0.1048, 0.1350, 0.2441, 0.0844, 0.0992, 0.1879, 0.1979, 0.0855,\n",
       "                          0.0488, 0.1245, 0.2667, 0.0807, 0.1061, 0.0801, 0.1991, 0.1077, 0.0717,\n",
       "                          0.1510, 0.2994, 0.1473, 0.1266, 0.1286, 0.0447, 0.2455, 0.2269, 0.0476,\n",
       "                          0.1390, 0.0664, 0.1597, 0.1480, 0.0788, 0.3708, 0.1334, 0.0724, 0.0687,\n",
       "                          0.0994, 0.0355, 0.0607, 0.0750, 0.2572, 0.1485, 0.0875, 0.1593, 0.1549,\n",
       "                          0.1645, 0.1141, 0.0416, 0.1237, 0.0604, 0.1178, 0.2281, 0.0688, 0.1016,\n",
       "                          0.0638, 0.0691, 0.0938, 0.4228, 0.0867, 0.1010, 0.2092, 0.0886, 0.1503,\n",
       "                          0.0939, 0.0893, 0.0750, 0.0545, 0.1504, 0.1647, 0.2524, 0.0578, 0.1287,\n",
       "                          0.0847, 0.2029, 0.1529, 0.1035, 0.1592, 0.2767, 0.0902, 0.1683, 0.0512,\n",
       "                          0.7173, 0.1400, 0.0990, 0.0564, 0.0890, 0.0192, 0.1371, 0.1260, 0.0935,\n",
       "                          0.0678, 0.0670, 0.1262, 0.0594, 0.1515, 0.0621, 0.0591, 0.1511, 0.2433,\n",
       "                          0.2020, 0.0361, 0.0770, 0.0715, 0.1411, 0.1139, 0.4643, 0.0897, 0.0721,\n",
       "                          0.0897, 0.1318, 0.1461, 0.2628, 0.0560, 0.1747, 0.0683, 0.0550, 0.0937,\n",
       "                          0.1236, 0.2884, 0.0816, 0.0930, 0.1073, 0.0903, 0.0995, 0.1663, 0.3649,\n",
       "                          0.1639, 0.5493, 0.2002, 0.0211, 0.2028, 0.0902, 0.1867, 0.1173, 0.0899,\n",
       "                          0.1440, 0.1239, 0.1942, 0.1467, 0.1292, 0.1629, 0.0968, 0.0845, 0.1150,\n",
       "                          0.3298, 0.2059, 0.0433, 0.0798, 0.2412, 0.2182, 0.0845, 0.1333, 0.0684,\n",
       "                          0.1187, 0.1394, 0.2964, 0.2383, 0.2714, 0.0496, 0.1309, 0.0976, 0.0982,\n",
       "                          0.1825, 0.0713, 0.0396, 0.1306, 0.1418, 0.0773, 0.2057, 0.1395, 0.0814,\n",
       "                          0.3094, 0.1300, 0.1280, 0.1150, 0.1024, 0.0787, 0.1434, 0.0889, 0.0778,\n",
       "                          0.1096, 0.1514, 0.0910, 0.0772, 0.1579, 0.3544, 0.1370, 0.1188, 0.1661,\n",
       "                          0.0919, 0.0986, 0.0866, 0.1045, 0.0961, 0.0495, 0.1252, 0.0824, 0.0727,\n",
       "                          0.0962, 0.2956, 0.0591, 0.0898, 0.0985, 0.0211, 0.0773, 0.0997, 0.1963,\n",
       "                          0.1121, 0.1066, 0.1196, 0.1268, 0.1382, 0.0927, 0.0455, 0.1776, 0.1540,\n",
       "                          0.1367, 0.1177, 0.0820, 0.0862, 0.1437, 0.1481, 0.0530, 0.1808, 0.0851,\n",
       "                          0.1125, 0.0319, 0.1014, 0.1894, 0.0714, 0.1389, 0.0885, 0.3844, 0.1344,\n",
       "                          0.0537, 0.1284, 0.0693, 0.1880, 0.2793, 0.0677, 0.0964, 0.0716, 0.0630,\n",
       "                          0.1087, 0.0678, 0.1403, 0.0850, 0.1905, 0.0093, 0.1435, 0.1415, 0.2308,\n",
       "                          0.0309, 0.1434, 0.0926, 0.1956, 0.2009, 0.1182, 0.1271, 0.2450, 0.1155,\n",
       "                          0.2492, 0.0997, 0.0695, 0.4019, 0.1030, 0.0687, 0.1823, 0.1902, 0.1072,\n",
       "                          0.0226, 0.1585, 0.0849, 0.0746, 0.1572, 0.0774, 0.1521, 0.1887, 0.0703,\n",
       "                          0.0848, 0.0565, 0.2111, 0.1380, 0.0071, 0.0834, 0.0891, 0.2016, 0.1990,\n",
       "                          0.1342, 0.1341, 0.0462, 0.0848, 0.0925, 0.1942, 0.0536, 0.1160, 0.0884,\n",
       "                          0.1096, 0.1958, 0.1145, 0.0262, 0.1767, 0.1287, 0.0718, 0.0403, 0.0152,\n",
       "                          0.1673, 0.1000, 0.0695, 0.0686, 0.1067, 0.1356, 0.0629, 0.0233, 0.1249,\n",
       "                          0.1381, 0.2625, 0.1824, 0.0989, 0.1755, 0.1480, 0.1843, 0.1448, 0.1359,\n",
       "                          0.0596, 0.1701, 0.0494, 0.0650, 0.1631, 0.1263, 0.1086, 0.1940, 0.1991,\n",
       "                          0.0332, 0.1144, 0.0698, 0.0555, 0.0507, 0.0914, 0.0833, 0.1207, 0.1682,\n",
       "                          0.1861, 0.1427, 0.0811, 0.1794, 0.1276, 0.1105, 0.0876, 0.1258, 0.0995,\n",
       "                          0.2819, 0.1262, 0.1227, 0.1008, 0.1569, 0.1684, 0.0790, 0.1918, 0.1507,\n",
       "                          0.1878, 0.1098, 0.0683, 0.1362, 0.2003, 0.1129, 0.1017, 0.1142, 0.1804,\n",
       "                          0.0989, 0.1162, 0.1114, 0.0948, 0.0590, 0.0882, 0.1014, 0.2355, 0.0724,\n",
       "                          0.0617, 0.1343, 0.1421, 0.0739, 0.1509, 0.1112, 0.0445, 0.0852, 0.1062,\n",
       "                          0.1391, 0.0329, 0.1676, 0.1894, 0.0852, 0.0214, 0.1383, 0.1753, 0.0460,\n",
       "                          0.1528, 0.0412, 0.1651, 0.1896, 0.0584, 0.1943, 0.0904, 0.0401, 0.0347,\n",
       "                          0.1375, 0.1158, 0.1400, 0.0878, 0.0810, 0.0856, 0.0644, 0.4561, 0.3712,\n",
       "                          0.1798, 0.2170, 0.0874, 0.1491, 0.1265, 0.0375, 0.1345, 0.1172, 0.1446,\n",
       "                          0.1791, 0.2412, 0.0391, 0.0697, 0.0830, 0.2946, 0.1115, 0.0904, 0.1645,\n",
       "                          0.0402, 0.1515, 0.0499, 0.0900, 0.0661, 0.1522, 0.1818, 0.0941, 0.1127,\n",
       "                          0.0579, 0.1548, 0.0756, 0.1594, 0.0757, 0.1146, 0.0419, 0.1607, 0.0392,\n",
       "                          0.0912, 0.1794, 0.3448, 0.1229, 0.2301, 0.2556, 0.0678, 0.1293, 0.0779,\n",
       "                          0.1390, 0.1800, 0.0992, 0.0885, 0.1101, 0.0728, 0.1509, 0.1087, 0.0779,\n",
       "                          0.0948, 0.2341, 0.1403, 0.2454, 0.0841, 0.2065, 0.2291, 0.1084])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0467]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1740987300872803, max_val=2.7555463314056396)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0797]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.50443696975708, max_val=4.621884822845459)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([1.1269e-04, 1.3439e-04, 1.1692e-04, 9.1693e-05, 6.7513e-05, 9.4066e-05,\n",
       "                      9.3009e-05, 1.2457e-04, 1.1405e-04, 1.0038e-04, 1.1612e-04, 1.8396e-04,\n",
       "                      1.0054e-04, 1.0739e-04, 1.7200e-04, 1.9309e-04, 1.3836e-04, 2.7441e-04,\n",
       "                      7.1195e-05, 1.2975e-04, 1.5259e-04, 1.7333e-04, 1.3581e-04, 1.2048e-04,\n",
       "                      1.6747e-04, 9.9422e-05, 2.8946e-04, 1.3298e-04, 2.2794e-04, 1.8959e-04,\n",
       "                      1.3294e-04, 1.3472e-04, 8.4643e-05, 1.9843e-04, 1.2843e-04, 1.1009e-04,\n",
       "                      1.4770e-04, 9.6474e-05, 1.1286e-04, 1.0337e-04, 9.0055e-05, 1.1123e-04,\n",
       "                      9.2287e-05, 8.1003e-05, 1.1737e-04, 9.2508e-05, 1.0258e-04, 1.6394e-04,\n",
       "                      1.1446e-04, 9.7852e-05, 8.8458e-05, 5.6539e-05, 1.3614e-04, 1.6734e-04,\n",
       "                      1.3939e-04, 8.5751e-05, 1.5907e-04, 1.3160e-04, 1.5095e-04, 9.4699e-05,\n",
       "                      1.8071e-04, 2.3058e-04, 1.0392e-04, 3.4084e-04, 5.1580e-04, 1.0231e-04,\n",
       "                      7.2869e-05, 1.6024e-04, 1.0695e-04, 1.8175e-04, 1.2808e-04, 1.4804e-04,\n",
       "                      1.5920e-04, 1.3481e-04, 1.1630e-04, 9.8439e-05, 1.7527e-04, 9.2293e-05,\n",
       "                      1.3655e-04, 1.4024e-04, 1.4790e-04, 7.1770e-05, 2.1442e-04, 1.1391e-04,\n",
       "                      1.2050e-04, 1.6162e-04, 1.5750e-04, 3.1888e-04, 7.7617e-05, 8.4450e-05,\n",
       "                      1.3349e-04, 2.9700e-04, 1.1350e-04, 1.0566e-04, 1.6720e-04, 1.1807e-04,\n",
       "                      1.7126e-04, 1.4168e-04, 2.4858e-04, 1.2315e-04, 8.2243e-05, 7.5149e-05,\n",
       "                      2.1184e-04, 1.2134e-04, 1.2962e-04, 7.4318e-05, 1.5376e-04, 1.7012e-04,\n",
       "                      1.7937e-04, 8.0331e-05, 4.5506e-04, 8.9823e-05, 1.2378e-04, 1.1497e-04,\n",
       "                      1.0356e-04, 7.6213e-05, 1.6540e-04, 8.5743e-05, 1.7275e-04, 7.8586e-05,\n",
       "                      1.3567e-04, 8.3665e-05, 2.3852e-04, 1.4957e-04, 1.3102e-04, 1.2552e-04,\n",
       "                      1.1874e-04, 2.5643e-04, 1.1740e-04, 1.2988e-04, 8.0771e-05, 1.1095e-04,\n",
       "                      7.6977e-05, 1.1842e-04, 1.6766e-04, 3.0357e-04, 1.8584e-04, 1.0774e-04,\n",
       "                      2.3284e-04, 1.0108e-04, 1.1668e-04, 1.4423e-04, 2.6206e-04, 1.2269e-04,\n",
       "                      1.1222e-04, 1.1947e-04, 1.1687e-04, 1.1868e-04, 1.1084e-04, 7.2992e-05,\n",
       "                      5.4025e-05, 9.1392e-05, 1.4271e-04, 1.6886e-04, 9.8279e-05, 1.5525e-04,\n",
       "                      1.5091e-04, 2.2462e-04, 2.4290e-04, 1.4376e-04, 7.7204e-05, 1.1226e-04,\n",
       "                      1.8949e-04, 1.2716e-04, 1.0138e-04, 9.8661e-05, 8.6557e-05, 1.4762e-04,\n",
       "                      1.6251e-04, 1.0573e-04, 7.6455e-05, 1.1066e-04, 9.3971e-05, 1.6164e-04,\n",
       "                      1.1617e-04, 1.2445e-04, 1.5361e-04, 1.9475e-04, 5.9328e-05, 1.4908e-04,\n",
       "                      1.7619e-04, 1.0901e-04, 1.5954e-04, 1.7417e-04, 2.7809e-04, 1.0357e-04,\n",
       "                      9.5450e-05, 9.5505e-05, 7.7958e-05, 3.1860e-04, 1.2247e-04, 1.8196e-04,\n",
       "                      1.9253e-04, 1.2901e-04, 9.6735e-05, 1.4708e-04, 3.2662e-04, 5.8824e-05,\n",
       "                      1.6229e-04, 2.3497e-04, 2.1740e-04, 8.8012e-05, 1.1648e-04, 1.0096e-04,\n",
       "                      1.0916e-04, 1.2982e-04, 1.4861e-04, 8.8200e-05, 1.5503e-04, 1.6460e-04,\n",
       "                      1.1512e-04, 1.1325e-04, 1.2098e-04, 1.0817e-04, 1.6910e-04, 1.4510e-04,\n",
       "                      1.7648e-04, 1.3269e-04, 1.0448e-04, 9.8384e-05, 1.6397e-04, 1.7347e-04,\n",
       "                      9.4523e-05, 1.1475e-04, 1.0508e-04, 2.0100e-04, 1.0020e-04, 1.0061e-04,\n",
       "                      2.0610e-04, 1.2578e-04, 6.9471e-05, 1.0777e-04, 1.0089e-04, 6.1916e-05,\n",
       "                      1.4771e-04, 1.5628e-04, 1.3297e-04, 1.1360e-04, 1.3466e-04, 9.6987e-05,\n",
       "                      1.2570e-04, 1.3002e-04, 8.1191e-05, 1.5716e-04, 1.5797e-04, 8.6372e-05,\n",
       "                      1.3760e-04, 1.8496e-04, 1.3502e-04, 1.0382e-04, 1.7389e-04, 2.9928e-04,\n",
       "                      1.5179e-04, 1.3589e-04, 6.4674e-05, 1.1571e-04, 1.0880e-04, 2.0067e-04,\n",
       "                      9.9577e-05, 1.6603e-04, 1.2191e-04, 9.8856e-05, 1.7473e-04, 1.5511e-04,\n",
       "                      2.0126e-04, 1.1608e-04, 2.0521e-04, 1.4382e-04, 2.2514e-04, 1.2137e-04,\n",
       "                      1.1452e-04, 4.2380e-05, 1.4132e-04, 1.5035e-04, 9.5244e-05, 9.9424e-05,\n",
       "                      1.3105e-04, 1.8518e-04, 7.8481e-05, 8.0296e-05, 3.2441e-04, 1.2833e-04,\n",
       "                      2.3465e-04, 1.9183e-04, 1.2179e-04, 1.2297e-04, 9.8665e-05, 1.0355e-04,\n",
       "                      9.9862e-05, 8.0949e-05, 1.5836e-04, 6.1172e-05, 1.1113e-04, 2.2953e-04,\n",
       "                      1.3904e-04, 1.1164e-04, 7.0630e-05, 1.3063e-04, 1.1080e-04, 1.1100e-04,\n",
       "                      8.0249e-05, 1.7671e-04, 8.0187e-05, 3.7337e-05, 1.5955e-04, 8.6475e-05,\n",
       "                      1.2169e-04, 1.4897e-04, 1.0572e-04, 1.7547e-04, 2.8815e-04, 1.1629e-04,\n",
       "                      6.4436e-05, 9.7228e-05, 8.2991e-05, 1.0821e-04, 1.1990e-04, 1.4149e-04,\n",
       "                      9.5920e-05, 2.3477e-04, 1.1410e-04, 1.2142e-04, 1.7450e-04, 9.1071e-05,\n",
       "                      1.0752e-04, 2.2949e-04, 3.2567e-04, 9.2887e-05, 1.4762e-04, 1.5440e-04,\n",
       "                      2.1974e-04, 1.2546e-04, 8.3341e-05, 7.0923e-05, 7.9998e-05, 3.4535e-04,\n",
       "                      1.7552e-04, 1.4292e-04, 2.3847e-04, 6.7544e-05, 1.1153e-04, 1.4911e-04,\n",
       "                      1.7996e-04, 1.3234e-04, 1.2218e-04, 1.9745e-04, 1.1888e-04, 1.7036e-04,\n",
       "                      8.3508e-05, 1.6742e-04, 1.9737e-04, 1.2535e-04, 1.0056e-04, 1.2986e-04,\n",
       "                      1.3668e-04, 6.3286e-05, 1.2946e-04, 1.7703e-04, 9.6303e-05, 1.5596e-04,\n",
       "                      1.0142e-04, 1.0430e-04, 1.3180e-04, 9.8893e-05, 1.3502e-04, 7.9259e-05,\n",
       "                      1.0247e-04, 1.2908e-04, 2.0321e-04, 1.0825e-04, 9.6152e-05, 1.2691e-04,\n",
       "                      9.3106e-05, 1.2700e-04, 1.2868e-04, 1.6204e-04, 1.4064e-04, 6.7225e-05,\n",
       "                      1.1676e-04, 8.4788e-05, 7.5099e-05, 1.2850e-04, 1.1139e-04, 1.8165e-04,\n",
       "                      8.4195e-05, 9.4671e-05, 1.2625e-04, 1.4027e-04, 2.0418e-04, 2.1467e-04,\n",
       "                      1.1943e-04, 1.1631e-04, 1.4058e-04, 1.0649e-04, 1.6514e-04, 1.2598e-04,\n",
       "                      4.9732e-05, 9.5089e-05, 1.9492e-04, 9.8125e-05, 3.8425e-04, 1.1298e-04,\n",
       "                      1.0170e-04, 9.6945e-05, 1.2681e-04, 1.3514e-04, 2.1197e-04, 2.0607e-04,\n",
       "                      3.8859e-04, 1.0050e-04, 7.7825e-05, 1.7920e-04, 1.0970e-04, 1.0382e-04,\n",
       "                      1.2540e-04, 1.4217e-04, 9.2451e-05, 1.4167e-04, 1.1599e-04, 1.4123e-04,\n",
       "                      1.5485e-04, 1.1997e-04, 9.0672e-05, 1.8441e-04, 1.2205e-04, 9.3797e-05,\n",
       "                      1.4765e-04, 1.2305e-04, 9.8349e-05, 1.0039e-04, 1.6266e-04, 2.3599e-04,\n",
       "                      1.1610e-04, 1.2088e-04, 9.3733e-05, 7.9777e-05, 1.3184e-04, 1.7027e-04,\n",
       "                      1.0856e-04, 8.2744e-05, 1.6324e-04, 9.8466e-05, 1.0564e-04, 9.8775e-05,\n",
       "                      1.3175e-04, 1.4331e-04, 1.1178e-04, 1.0152e-04, 2.5324e-04, 8.2288e-05,\n",
       "                      9.3751e-05, 1.3173e-04, 1.3831e-04, 1.0659e-04, 1.2725e-04, 5.8583e-05,\n",
       "                      1.4327e-04, 1.7490e-04, 1.4009e-04, 1.3396e-04, 2.6581e-04, 7.9914e-05,\n",
       "                      1.7657e-04, 7.4473e-05, 7.6986e-05, 1.0419e-04, 9.5564e-05, 1.2104e-04,\n",
       "                      1.1277e-04, 6.6118e-05, 1.0340e-04, 9.4737e-05, 1.0416e-04, 1.3048e-04,\n",
       "                      1.2679e-04, 1.5646e-04, 1.1790e-04, 8.1153e-05, 2.1260e-04, 1.2181e-04,\n",
       "                      1.3739e-04, 1.0032e-04, 9.7734e-05, 1.4966e-04, 1.2781e-04, 1.7974e-04,\n",
       "                      1.2699e-04, 1.7389e-04, 2.2486e-04, 1.5990e-04, 2.2447e-04, 1.8383e-04,\n",
       "                      8.1830e-05, 1.0457e-04, 1.3905e-04, 1.2998e-04, 8.7852e-05, 1.9788e-04,\n",
       "                      2.0388e-04, 8.0186e-05, 1.2909e-04, 1.4934e-04, 1.3032e-04, 2.2933e-04,\n",
       "                      1.0588e-04, 1.0277e-04, 1.3635e-04, 2.3204e-04, 9.6300e-05, 8.3086e-05,\n",
       "                      8.6979e-05, 9.9763e-05]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0116, -0.0124, -0.0134, -0.0113, -0.0069, -0.0116, -0.0114, -0.0103,\n",
       "                        -0.0146, -0.0128, -0.0099, -0.0166, -0.0106, -0.0087, -0.0149, -0.0228,\n",
       "                        -0.0147, -0.0233, -0.0073, -0.0115, -0.0136, -0.0106, -0.0119, -0.0112,\n",
       "                        -0.0213, -0.0110, -0.0218, -0.0170, -0.0220, -0.0181, -0.0128, -0.0146,\n",
       "                        -0.0108, -0.0248, -0.0127, -0.0126, -0.0143, -0.0094, -0.0137, -0.0132,\n",
       "                        -0.0100, -0.0102, -0.0097, -0.0104, -0.0150, -0.0100, -0.0131, -0.0210,\n",
       "                        -0.0147, -0.0119, -0.0084, -0.0072, -0.0164, -0.0145, -0.0129, -0.0097,\n",
       "                        -0.0162, -0.0107, -0.0179, -0.0101, -0.0231, -0.0174, -0.0105, -0.0220,\n",
       "                        -0.0152, -0.0131, -0.0090, -0.0205, -0.0106, -0.0124, -0.0116, -0.0130,\n",
       "                        -0.0165, -0.0157, -0.0149, -0.0082, -0.0138, -0.0093, -0.0124, -0.0156,\n",
       "                        -0.0189, -0.0090, -0.0156, -0.0135, -0.0154, -0.0133, -0.0175, -0.0399,\n",
       "                        -0.0089, -0.0108, -0.0125, -0.0197, -0.0140, -0.0135, -0.0136, -0.0137,\n",
       "                        -0.0151, -0.0132, -0.0206, -0.0106, -0.0105, -0.0087, -0.0139, -0.0128,\n",
       "                        -0.0166, -0.0095, -0.0197, -0.0201, -0.0140, -0.0091, -0.0198, -0.0085,\n",
       "                        -0.0130, -0.0147, -0.0128, -0.0063, -0.0205, -0.0110, -0.0169, -0.0088,\n",
       "                        -0.0174, -0.0107, -0.0152, -0.0124, -0.0168, -0.0161, -0.0135, -0.0268,\n",
       "                        -0.0142, -0.0118, -0.0103, -0.0099, -0.0075, -0.0122, -0.0189, -0.0223,\n",
       "                        -0.0138, -0.0122, -0.0210, -0.0129, -0.0123, -0.0146, -0.0165, -0.0128,\n",
       "                        -0.0144, -0.0137, -0.0144, -0.0119, -0.0135, -0.0092, -0.0069, -0.0114,\n",
       "                        -0.0134, -0.0130, -0.0101, -0.0134, -0.0175, -0.0288, -0.0234, -0.0184,\n",
       "                        -0.0085, -0.0096, -0.0243, -0.0145, -0.0104, -0.0101, -0.0103, -0.0116,\n",
       "                        -0.0194, -0.0108, -0.0095, -0.0132, -0.0096, -0.0207, -0.0118, -0.0153,\n",
       "                        -0.0131, -0.0154, -0.0065, -0.0104, -0.0150, -0.0112, -0.0127, -0.0185,\n",
       "                        -0.0195, -0.0097, -0.0116, -0.0122, -0.0100, -0.0212, -0.0134, -0.0201,\n",
       "                        -0.0194, -0.0101, -0.0099, -0.0176, -0.0117, -0.0075, -0.0208, -0.0301,\n",
       "                        -0.0125, -0.0113, -0.0129, -0.0129, -0.0111, -0.0152, -0.0125, -0.0113,\n",
       "                        -0.0147, -0.0171, -0.0147, -0.0108, -0.0149, -0.0128, -0.0181, -0.0170,\n",
       "                        -0.0132, -0.0170, -0.0129, -0.0096, -0.0187, -0.0114, -0.0110, -0.0137,\n",
       "                        -0.0095, -0.0181, -0.0116, -0.0094, -0.0165, -0.0122, -0.0084, -0.0138,\n",
       "                        -0.0129, -0.0075, -0.0178, -0.0161, -0.0120, -0.0115, -0.0094, -0.0123,\n",
       "                        -0.0125, -0.0098, -0.0104, -0.0121, -0.0167, -0.0111, -0.0126, -0.0137,\n",
       "                        -0.0108, -0.0095, -0.0132, -0.0157, -0.0194, -0.0116, -0.0075, -0.0139,\n",
       "                        -0.0139, -0.0163, -0.0127, -0.0213, -0.0100, -0.0111, -0.0124, -0.0156,\n",
       "                        -0.0182, -0.0128, -0.0252, -0.0121, -0.0288, -0.0124, -0.0097, -0.0054,\n",
       "                        -0.0136, -0.0170, -0.0122, -0.0121, -0.0111, -0.0169, -0.0091, -0.0093,\n",
       "                        -0.0170, -0.0151, -0.0170, -0.0153, -0.0147, -0.0117, -0.0126, -0.0112,\n",
       "                        -0.0111, -0.0081, -0.0109, -0.0078, -0.0142, -0.0136, -0.0168, -0.0117,\n",
       "                        -0.0083, -0.0141, -0.0142, -0.0142, -0.0103, -0.0226, -0.0103, -0.0048,\n",
       "                        -0.0120, -0.0111, -0.0156, -0.0158, -0.0124, -0.0215, -0.0200, -0.0112,\n",
       "                        -0.0082, -0.0100, -0.0106, -0.0119, -0.0153, -0.0116, -0.0123, -0.0243,\n",
       "                        -0.0146, -0.0128, -0.0117, -0.0104, -0.0124, -0.0167, -0.0417, -0.0078,\n",
       "                        -0.0136, -0.0135, -0.0210, -0.0146, -0.0107, -0.0086, -0.0102, -0.0263,\n",
       "                        -0.0147, -0.0183, -0.0155, -0.0061, -0.0143, -0.0140, -0.0176, -0.0140,\n",
       "                        -0.0138, -0.0160, -0.0115, -0.0145, -0.0107, -0.0129, -0.0134, -0.0157,\n",
       "                        -0.0113, -0.0166, -0.0175, -0.0070, -0.0127, -0.0119, -0.0123, -0.0150,\n",
       "                        -0.0116, -0.0134, -0.0148, -0.0109, -0.0136, -0.0086, -0.0119, -0.0135,\n",
       "                        -0.0163, -0.0112, -0.0107, -0.0134, -0.0119, -0.0163, -0.0165, -0.0207,\n",
       "                        -0.0168, -0.0086, -0.0103, -0.0091, -0.0086, -0.0135, -0.0106, -0.0198,\n",
       "                        -0.0108, -0.0100, -0.0117, -0.0144, -0.0179, -0.0201, -0.0091, -0.0122,\n",
       "                        -0.0155, -0.0113, -0.0211, -0.0151, -0.0064, -0.0085, -0.0169, -0.0126,\n",
       "                        -0.0304, -0.0145, -0.0109, -0.0104, -0.0162, -0.0134, -0.0271, -0.0264,\n",
       "                        -0.0260, -0.0110, -0.0100, -0.0114, -0.0137, -0.0103, -0.0125, -0.0182,\n",
       "                        -0.0100, -0.0129, -0.0148, -0.0181, -0.0115, -0.0144, -0.0093, -0.0196,\n",
       "                        -0.0121, -0.0106, -0.0140, -0.0129, -0.0122, -0.0108, -0.0130, -0.0180,\n",
       "                        -0.0138, -0.0124, -0.0120, -0.0102, -0.0124, -0.0161, -0.0126, -0.0106,\n",
       "                        -0.0145, -0.0126, -0.0130, -0.0123, -0.0162, -0.0139, -0.0117, -0.0130,\n",
       "                        -0.0228, -0.0105, -0.0092, -0.0134, -0.0124, -0.0125, -0.0126, -0.0075,\n",
       "                        -0.0117, -0.0099, -0.0138, -0.0171, -0.0181, -0.0090, -0.0188, -0.0088,\n",
       "                        -0.0099, -0.0104, -0.0122, -0.0125, -0.0094, -0.0085, -0.0117, -0.0121,\n",
       "                        -0.0132, -0.0103, -0.0120, -0.0164, -0.0121, -0.0104, -0.0117, -0.0126,\n",
       "                        -0.0100, -0.0128, -0.0084, -0.0125, -0.0164, -0.0230, -0.0123, -0.0179,\n",
       "                        -0.0153, -0.0205, -0.0287, -0.0196, -0.0105, -0.0107, -0.0178, -0.0131,\n",
       "                        -0.0109, -0.0142, -0.0169, -0.0103, -0.0102, -0.0174, -0.0116, -0.0241,\n",
       "                        -0.0110, -0.0124, -0.0175, -0.0205, -0.0117, -0.0105, -0.0095, -0.0128]), max_val=tensor([0.0143, 0.0171, 0.0148, 0.0116, 0.0086, 0.0119, 0.0118, 0.0158, 0.0139,\n",
       "                        0.0112, 0.0147, 0.0234, 0.0128, 0.0136, 0.0218, 0.0245, 0.0176, 0.0349,\n",
       "                        0.0090, 0.0165, 0.0194, 0.0220, 0.0172, 0.0153, 0.0213, 0.0126, 0.0368,\n",
       "                        0.0155, 0.0289, 0.0241, 0.0169, 0.0171, 0.0098, 0.0252, 0.0163, 0.0140,\n",
       "                        0.0188, 0.0123, 0.0143, 0.0125, 0.0114, 0.0141, 0.0117, 0.0094, 0.0124,\n",
       "                        0.0117, 0.0128, 0.0200, 0.0141, 0.0124, 0.0112, 0.0068, 0.0173, 0.0213,\n",
       "                        0.0177, 0.0109, 0.0202, 0.0167, 0.0192, 0.0120, 0.0222, 0.0293, 0.0132,\n",
       "                        0.0433, 0.0655, 0.0117, 0.0093, 0.0148, 0.0136, 0.0231, 0.0163, 0.0188,\n",
       "                        0.0202, 0.0171, 0.0135, 0.0125, 0.0223, 0.0117, 0.0173, 0.0178, 0.0149,\n",
       "                        0.0091, 0.0272, 0.0145, 0.0153, 0.0205, 0.0200, 0.0405, 0.0099, 0.0087,\n",
       "                        0.0170, 0.0377, 0.0144, 0.0117, 0.0212, 0.0150, 0.0217, 0.0180, 0.0316,\n",
       "                        0.0156, 0.0103, 0.0095, 0.0269, 0.0154, 0.0151, 0.0081, 0.0120, 0.0216,\n",
       "                        0.0228, 0.0102, 0.0578, 0.0114, 0.0157, 0.0111, 0.0132, 0.0097, 0.0210,\n",
       "                        0.0099, 0.0219, 0.0100, 0.0114, 0.0074, 0.0303, 0.0190, 0.0122, 0.0126,\n",
       "                        0.0151, 0.0326, 0.0149, 0.0165, 0.0100, 0.0141, 0.0098, 0.0150, 0.0213,\n",
       "                        0.0386, 0.0236, 0.0137, 0.0296, 0.0114, 0.0148, 0.0183, 0.0333, 0.0156,\n",
       "                        0.0142, 0.0152, 0.0148, 0.0151, 0.0141, 0.0093, 0.0066, 0.0116, 0.0181,\n",
       "                        0.0214, 0.0125, 0.0197, 0.0192, 0.0188, 0.0308, 0.0169, 0.0098, 0.0143,\n",
       "                        0.0178, 0.0161, 0.0129, 0.0125, 0.0110, 0.0187, 0.0206, 0.0134, 0.0097,\n",
       "                        0.0141, 0.0119, 0.0202, 0.0148, 0.0158, 0.0195, 0.0247, 0.0075, 0.0189,\n",
       "                        0.0224, 0.0138, 0.0203, 0.0221, 0.0353, 0.0132, 0.0121, 0.0119, 0.0087,\n",
       "                        0.0405, 0.0156, 0.0231, 0.0245, 0.0164, 0.0123, 0.0187, 0.0415, 0.0073,\n",
       "                        0.0107, 0.0221, 0.0276, 0.0111, 0.0148, 0.0124, 0.0139, 0.0165, 0.0189,\n",
       "                        0.0087, 0.0197, 0.0209, 0.0139, 0.0144, 0.0154, 0.0137, 0.0215, 0.0184,\n",
       "                        0.0224, 0.0088, 0.0133, 0.0125, 0.0208, 0.0220, 0.0120, 0.0146, 0.0133,\n",
       "                        0.0255, 0.0127, 0.0128, 0.0262, 0.0160, 0.0088, 0.0128, 0.0114, 0.0079,\n",
       "                        0.0188, 0.0198, 0.0169, 0.0144, 0.0171, 0.0123, 0.0160, 0.0165, 0.0098,\n",
       "                        0.0200, 0.0201, 0.0109, 0.0175, 0.0235, 0.0171, 0.0132, 0.0221, 0.0380,\n",
       "                        0.0176, 0.0173, 0.0082, 0.0147, 0.0133, 0.0255, 0.0126, 0.0190, 0.0155,\n",
       "                        0.0126, 0.0222, 0.0197, 0.0256, 0.0147, 0.0261, 0.0183, 0.0284, 0.0154,\n",
       "                        0.0145, 0.0050, 0.0179, 0.0191, 0.0108, 0.0126, 0.0166, 0.0235, 0.0100,\n",
       "                        0.0102, 0.0412, 0.0163, 0.0298, 0.0244, 0.0155, 0.0156, 0.0092, 0.0132,\n",
       "                        0.0127, 0.0103, 0.0201, 0.0073, 0.0140, 0.0292, 0.0177, 0.0142, 0.0090,\n",
       "                        0.0166, 0.0138, 0.0136, 0.0078, 0.0150, 0.0094, 0.0043, 0.0203, 0.0106,\n",
       "                        0.0154, 0.0189, 0.0134, 0.0223, 0.0366, 0.0148, 0.0072, 0.0123, 0.0094,\n",
       "                        0.0137, 0.0140, 0.0180, 0.0120, 0.0298, 0.0140, 0.0154, 0.0222, 0.0116,\n",
       "                        0.0137, 0.0291, 0.0157, 0.0118, 0.0187, 0.0196, 0.0279, 0.0159, 0.0083,\n",
       "                        0.0090, 0.0100, 0.0439, 0.0223, 0.0137, 0.0303, 0.0086, 0.0138, 0.0189,\n",
       "                        0.0229, 0.0168, 0.0155, 0.0251, 0.0151, 0.0216, 0.0099, 0.0213, 0.0251,\n",
       "                        0.0159, 0.0128, 0.0098, 0.0159, 0.0080, 0.0164, 0.0225, 0.0108, 0.0198,\n",
       "                        0.0129, 0.0095, 0.0167, 0.0126, 0.0171, 0.0101, 0.0130, 0.0164, 0.0258,\n",
       "                        0.0137, 0.0122, 0.0161, 0.0117, 0.0152, 0.0109, 0.0167, 0.0179, 0.0085,\n",
       "                        0.0148, 0.0108, 0.0095, 0.0163, 0.0141, 0.0231, 0.0098, 0.0120, 0.0160,\n",
       "                        0.0178, 0.0259, 0.0273, 0.0152, 0.0148, 0.0179, 0.0135, 0.0208, 0.0160,\n",
       "                        0.0056, 0.0121, 0.0248, 0.0114, 0.0488, 0.0134, 0.0129, 0.0123, 0.0141,\n",
       "                        0.0172, 0.0225, 0.0108, 0.0494, 0.0128, 0.0084, 0.0228, 0.0139, 0.0132,\n",
       "                        0.0159, 0.0159, 0.0117, 0.0180, 0.0123, 0.0116, 0.0197, 0.0152, 0.0115,\n",
       "                        0.0234, 0.0155, 0.0119, 0.0188, 0.0156, 0.0125, 0.0127, 0.0207, 0.0300,\n",
       "                        0.0147, 0.0154, 0.0098, 0.0069, 0.0167, 0.0216, 0.0138, 0.0102, 0.0207,\n",
       "                        0.0117, 0.0134, 0.0125, 0.0167, 0.0182, 0.0142, 0.0105, 0.0322, 0.0104,\n",
       "                        0.0119, 0.0167, 0.0176, 0.0135, 0.0162, 0.0056, 0.0182, 0.0222, 0.0178,\n",
       "                        0.0147, 0.0338, 0.0101, 0.0224, 0.0095, 0.0088, 0.0132, 0.0107, 0.0154,\n",
       "                        0.0143, 0.0084, 0.0131, 0.0120, 0.0132, 0.0166, 0.0161, 0.0199, 0.0150,\n",
       "                        0.0084, 0.0270, 0.0155, 0.0174, 0.0115, 0.0124, 0.0190, 0.0144, 0.0155,\n",
       "                        0.0161, 0.0221, 0.0286, 0.0184, 0.0177, 0.0233, 0.0101, 0.0133, 0.0125,\n",
       "                        0.0165, 0.0112, 0.0251, 0.0259, 0.0090, 0.0164, 0.0190, 0.0166, 0.0291,\n",
       "                        0.0134, 0.0131, 0.0127, 0.0295, 0.0122, 0.0106, 0.0110, 0.0114])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0168]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.138709783554077)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0084, 0.0068, 0.0072, 0.0093, 0.0064, 0.0084, 0.0108, 0.0130, 0.0073,\n",
       "                      0.0103, 0.0095, 0.0070, 0.0060, 0.0160, 0.0084, 0.0070, 0.0041, 0.0095,\n",
       "                      0.0043, 0.0060, 0.0093, 0.0085, 0.0075, 0.0073, 0.0101, 0.0096, 0.0094,\n",
       "                      0.0055, 0.0134, 0.0097, 0.0060, 0.0128, 0.0130, 0.0045, 0.0089, 0.0121,\n",
       "                      0.0040, 0.0048, 0.0094, 0.0233, 0.0060, 0.0041, 0.0042, 0.0054, 0.0045,\n",
       "                      0.0101, 0.0076, 0.0078, 0.0113, 0.0075, 0.0067, 0.0089, 0.0075, 0.0053,\n",
       "                      0.0083, 0.0081, 0.0057, 0.0070, 0.0106, 0.0093, 0.0054, 0.0159, 0.0091,\n",
       "                      0.0068, 0.0106, 0.0069, 0.0054, 0.0118, 0.0163, 0.0089, 0.0071, 0.0062,\n",
       "                      0.0102, 0.0122, 0.0115, 0.0105, 0.0056, 0.0110, 0.0052, 0.0110, 0.0107,\n",
       "                      0.0048, 0.0091, 0.0065, 0.0107, 0.0088, 0.0084, 0.0069, 0.0071, 0.0105,\n",
       "                      0.0089, 0.0097, 0.0091, 0.0057, 0.0066, 0.0051, 0.0043, 0.0033, 0.0068,\n",
       "                      0.0071, 0.0082, 0.0073, 0.0091, 0.0047, 0.0058, 0.0068, 0.0073, 0.0067,\n",
       "                      0.0058, 0.0064, 0.0071, 0.0086, 0.0056, 0.0045, 0.0055, 0.0187, 0.0059,\n",
       "                      0.0116, 0.0162, 0.0069, 0.0089, 0.0057, 0.0074, 0.0051, 0.0105, 0.0093,\n",
       "                      0.0101, 0.0154, 0.0068, 0.0110, 0.0071, 0.0055, 0.0102, 0.0061, 0.0120,\n",
       "                      0.0083, 0.0054, 0.0081, 0.0104, 0.0119, 0.0043, 0.0075, 0.0062, 0.0040,\n",
       "                      0.0058, 0.0073, 0.0064, 0.0062, 0.0071, 0.0061, 0.0047, 0.0062, 0.0064,\n",
       "                      0.0059, 0.0126, 0.0068, 0.0079, 0.0075, 0.0046, 0.0053, 0.0236, 0.0101,\n",
       "                      0.0128, 0.0235, 0.0099, 0.0068, 0.0179, 0.0078, 0.0132, 0.0140, 0.0066,\n",
       "                      0.0055, 0.0067, 0.0084, 0.0133, 0.0073, 0.0090, 0.0108, 0.0116, 0.0121,\n",
       "                      0.0087, 0.0049, 0.0123, 0.0058, 0.0068, 0.0058, 0.0086, 0.0078, 0.0107,\n",
       "                      0.0066, 0.0049, 0.0085, 0.0042, 0.0063, 0.0097, 0.0035, 0.0086, 0.0103,\n",
       "                      0.0044, 0.0051, 0.0060, 0.0124, 0.0117, 0.0088, 0.0160, 0.0053, 0.0047,\n",
       "                      0.0061, 0.0084, 0.0085, 0.0082, 0.0076, 0.0094, 0.0132, 0.0172, 0.0053,\n",
       "                      0.0074, 0.0051, 0.0084, 0.0090, 0.0078, 0.0094, 0.0044, 0.0065, 0.0053,\n",
       "                      0.0050, 0.0145, 0.0075, 0.0121, 0.0050, 0.0090, 0.0080, 0.0096, 0.0093,\n",
       "                      0.0093, 0.0058, 0.0072, 0.0085, 0.0049, 0.0090, 0.0109, 0.0112, 0.0118,\n",
       "                      0.0059, 0.0086, 0.0107, 0.0056, 0.0063, 0.0080, 0.0082, 0.0071, 0.0089,\n",
       "                      0.0084, 0.0089, 0.0054, 0.0066, 0.0053, 0.0043, 0.0117, 0.0044, 0.0041,\n",
       "                      0.0058, 0.0098, 0.0063, 0.0056, 0.0075, 0.0235, 0.0081, 0.0127, 0.0086,\n",
       "                      0.0096, 0.0073, 0.0074, 0.0089, 0.0068, 0.0053, 0.0046, 0.0126, 0.0077,\n",
       "                      0.0219, 0.0140, 0.0036, 0.0097, 0.0117, 0.0047, 0.0082, 0.0047, 0.0052,\n",
       "                      0.0109, 0.0100, 0.0047, 0.0086, 0.0050, 0.0188, 0.0063, 0.0046, 0.0057,\n",
       "                      0.0076, 0.0091, 0.0067, 0.0077, 0.0120, 0.0075, 0.0112, 0.0060, 0.0090,\n",
       "                      0.0100, 0.0101, 0.0063, 0.0048, 0.0055, 0.0064, 0.0079, 0.0042, 0.0066,\n",
       "                      0.0108, 0.0096, 0.0078, 0.0087, 0.0066, 0.0080, 0.0058, 0.0067, 0.0112,\n",
       "                      0.0044, 0.0090, 0.0052, 0.0163, 0.0066, 0.0108, 0.0118, 0.0105, 0.0062,\n",
       "                      0.0081, 0.0060, 0.0051, 0.0054, 0.0121, 0.0075, 0.0065, 0.0116, 0.0107,\n",
       "                      0.0057, 0.0138, 0.0052, 0.0081, 0.0090, 0.0071, 0.0059, 0.0154, 0.0116,\n",
       "                      0.0074, 0.0080, 0.0117, 0.0084, 0.0086, 0.0059, 0.0055, 0.0150, 0.0063,\n",
       "                      0.0055, 0.0061, 0.0192, 0.0077, 0.0061, 0.0071, 0.0067, 0.0074, 0.0075,\n",
       "                      0.0130, 0.0101, 0.0121, 0.0097, 0.0086, 0.0076, 0.0074, 0.0070, 0.0048,\n",
       "                      0.0069, 0.0125, 0.0081, 0.0053, 0.0084, 0.0104, 0.0071, 0.0057, 0.0066,\n",
       "                      0.0220, 0.0044, 0.0095, 0.0086, 0.0051, 0.0072, 0.0135, 0.0074, 0.0075,\n",
       "                      0.0049, 0.0072, 0.0166, 0.0097, 0.0075, 0.0067, 0.0067, 0.0056, 0.0072,\n",
       "                      0.0056, 0.0057, 0.0144, 0.0086, 0.0056, 0.0124, 0.0104, 0.0097, 0.0058,\n",
       "                      0.0092, 0.0127, 0.0070, 0.0101, 0.0040, 0.0055, 0.0064, 0.0059, 0.0081,\n",
       "                      0.0091, 0.0057, 0.0108, 0.0116, 0.0073, 0.0078, 0.0098, 0.0061, 0.0107,\n",
       "                      0.0094, 0.0193, 0.0051, 0.0079, 0.0120, 0.0041, 0.0059, 0.0070, 0.0049,\n",
       "                      0.0087, 0.0100, 0.0053, 0.0052, 0.0107, 0.0059, 0.0079, 0.0098, 0.0072,\n",
       "                      0.0082, 0.0088, 0.0084, 0.0095, 0.0067, 0.0079, 0.0062, 0.0088, 0.0088,\n",
       "                      0.0075, 0.0059, 0.0105, 0.0072, 0.0091, 0.0071, 0.0050, 0.0096, 0.0077,\n",
       "                      0.0163, 0.0091, 0.0057, 0.0071, 0.0160, 0.0073, 0.0065, 0.0126, 0.0103,\n",
       "                      0.0072, 0.0074, 0.0051, 0.0050, 0.0067, 0.0039, 0.0074, 0.0080, 0.0114,\n",
       "                      0.0056, 0.0056, 0.0118, 0.0060, 0.0081, 0.0070, 0.0066, 0.0069, 0.0080,\n",
       "                      0.0090, 0.0077, 0.0049, 0.0082, 0.0118, 0.0070, 0.0047, 0.0078, 0.0076,\n",
       "                      0.0044, 0.0043, 0.0081, 0.0062, 0.0069, 0.0092, 0.0177, 0.0167]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.6374, -0.8143, -0.6701, -0.7485, -0.4576, -0.5355, -0.5164, -0.8796,\n",
       "                        -0.5910, -0.8504, -0.6749, -0.5777, -0.3998, -0.4761, -0.7571, -0.5152,\n",
       "                        -0.5135, -0.9138, -0.4237, -0.5231, -0.7970, -1.0454, -0.6234, -0.7131,\n",
       "                        -1.1146, -0.5850, -0.6561, -0.3652, -0.9313, -0.3676, -0.4572, -0.5492,\n",
       "                        -0.6781, -0.4220, -0.4643, -0.6447, -0.3985, -0.4380, -0.7382, -0.6280,\n",
       "                        -0.5101, -0.3144, -0.4147, -0.4719, -0.3012, -0.5992, -0.6766, -0.9960,\n",
       "                        -0.7221, -0.6409, -0.5823, -0.4431, -0.5770, -0.3779, -0.7979, -0.8966,\n",
       "                        -0.7072, -0.8945, -0.7157, -0.7960, -0.6099, -0.7971, -0.6847, -0.5016,\n",
       "                        -0.5298, -0.7594, -0.4959, -0.7982, -0.5637, -1.1444, -0.5417, -0.5045,\n",
       "                        -0.7013, -0.7507, -0.8394, -0.6609, -0.3092, -0.5404, -0.4867, -0.7493,\n",
       "                        -0.5449, -0.4384, -0.5258, -0.5815, -0.4717, -0.6531, -0.7254, -0.6549,\n",
       "                        -0.6327, -0.5238, -0.6066, -0.6633, -0.6456, -0.7263, -0.5280, -0.4872,\n",
       "                        -0.5532, -0.3921, -0.5422, -0.5729, -0.6615, -0.8145, -0.5308, -0.4199,\n",
       "                        -0.3618, -0.6118, -0.7896, -0.4701, -0.4877, -0.6655, -0.6117, -0.5083,\n",
       "                        -0.5328, -0.3629, -0.5545, -0.7747, -0.5490, -0.6282, -0.7717, -0.5544,\n",
       "                        -0.8363, -0.5738, -0.6434, -0.4374, -0.5685, -0.9110, -0.7561, -0.8442,\n",
       "                        -0.5850, -0.9102, -0.5654, -0.5741, -0.5004, -0.5496, -0.6503, -1.0675,\n",
       "                        -0.5210, -0.6576, -0.6684, -0.5721, -0.3609, -0.7122, -0.4581, -0.3390,\n",
       "                        -0.4831, -0.4852, -0.5874, -0.4857, -0.4572, -0.4818, -0.4593, -0.5296,\n",
       "                        -0.4988, -0.4698, -0.3256, -0.5528, -0.6488, -0.4084, -0.3844, -0.3933,\n",
       "                        -0.5966, -1.2985, -0.6547, -1.8815, -0.8023, -0.4719, -0.9836, -0.6034,\n",
       "                        -0.5470, -0.8359, -0.5147, -0.4674, -0.5086, -0.4925, -0.6243, -0.5101,\n",
       "                        -0.5974, -0.6054, -0.6572, -0.6985, -0.7384, -0.4009, -1.1896, -0.3170,\n",
       "                        -0.5594, -0.5222, -0.7265, -0.7876, -0.6705, -0.5712, -0.3814, -0.6457,\n",
       "                        -0.4179, -0.8068, -0.4302, -0.3633, -0.4663, -0.6868, -0.4179, -0.4200,\n",
       "                        -0.5292, -0.5069, -0.9042, -0.5795, -1.2224, -0.4478, -0.5264, -0.5959,\n",
       "                        -0.6717, -0.7455, -0.6013, -0.4671, -0.4917, -0.5582, -0.8803, -0.5611,\n",
       "                        -0.5283, -0.4624, -0.7830, -0.7260, -0.5171, -0.7089, -0.3758, -0.3395,\n",
       "                        -0.5481, -0.4867, -0.6725, -0.5013, -0.6880, -0.4708, -0.5860, -0.5188,\n",
       "                        -0.8958, -0.7743, -0.8094, -0.4737, -0.4356, -0.7674, -0.4401, -0.6260,\n",
       "                        -0.5815, -0.9787, -1.3065, -0.3511, -0.6030, -0.9130, -0.4659, -0.6699,\n",
       "                        -0.6082, -0.7209, -0.6884, -0.6938, -0.4679, -0.6646, -0.5887, -0.8458,\n",
       "                        -0.4484, -0.3902, -0.5536, -0.5285, -0.4669, -0.5552, -0.6365, -0.4624,\n",
       "                        -0.3976, -0.5954, -1.0478, -0.8237, -0.4591, -0.7359, -0.2843, -0.7764,\n",
       "                        -0.6674, -0.5914, -0.8682, -0.3777, -0.4402, -0.5321, -0.8334, -0.5249,\n",
       "                        -1.0531, -0.4520, -0.7370, -0.5318, -0.4152, -0.7140, -0.4622, -0.4064,\n",
       "                        -0.6386, -0.6368, -0.4589, -0.5777, -0.4492, -0.9239, -0.5732, -0.3521,\n",
       "                        -0.5945, -0.7299, -0.5055, -0.5810, -0.9797, -0.6544, -0.4396, -0.6290,\n",
       "                        -0.4569, -0.5449, -0.5609, -0.8930, -0.5168, -0.3986, -0.5537, -0.6494,\n",
       "                        -0.5818, -0.3301, -0.5763, -0.7743, -0.6611, -0.9998, -0.9375, -0.4290,\n",
       "                        -0.6603, -0.4438, -0.6801, -0.6143, -0.4770, -0.7706, -0.4767, -0.4869,\n",
       "                        -0.5570, -0.8345, -0.7395, -0.5588, -0.5415, -0.9249, -0.5550, -0.4276,\n",
       "                        -0.6205, -0.7071, -0.6130, -0.4600, -0.6738, -0.6315, -0.6678, -0.9492,\n",
       "                        -0.4593, -0.3931, -0.4315, -0.5856, -0.5295, -0.5542, -0.6181, -0.6255,\n",
       "                        -0.5641, -0.7536, -0.6956, -0.7216, -0.5206, -0.5069, -0.5196, -0.5385,\n",
       "                        -0.4868, -0.4466, -0.8061, -0.5788, -0.6826, -0.5250, -0.5476, -0.6304,\n",
       "                        -0.5421, -0.7622, -0.5493, -0.5546, -0.8884, -0.6007, -0.6943, -0.4732,\n",
       "                        -0.6025, -0.3892, -0.5830, -0.4388, -0.5181, -0.6246, -0.7673, -0.6149,\n",
       "                        -0.6063, -0.3798, -0.4580, -0.8816, -0.3804, -0.9603, -0.5308, -0.3947,\n",
       "                        -0.6165, -0.6991, -0.6035, -0.5955, -0.4691, -0.8003, -0.5741, -0.7707,\n",
       "                        -0.9569, -0.6758, -0.5509, -0.5000, -0.6283, -0.4144, -0.5115, -0.8301,\n",
       "                        -0.6366, -0.7063, -0.8004, -0.6370, -0.5641, -0.3761, -0.5251, -0.6338,\n",
       "                        -0.5636, -1.0552, -0.3536, -0.4227, -0.4645, -0.3574, -0.6094, -0.5207,\n",
       "                        -0.5104, -0.4976, -0.9119, -0.4249, -0.5549, -0.9640, -0.6640, -0.7046,\n",
       "                        -0.5150, -0.5144, -0.3991, -0.7910, -0.6785, -0.3532, -0.4743, -0.5975,\n",
       "                        -0.3579, -0.5063, -0.5403, -0.4411, -0.6701, -0.7078, -0.7502, -0.6433,\n",
       "                        -0.6215, -0.5654, -0.6154, -0.5659, -0.7205, -0.7050, -0.4746, -0.4992,\n",
       "                        -0.5351, -0.6494, -0.5499, -0.4884, -0.7530, -0.4812, -0.5961, -0.4920,\n",
       "                        -0.5188, -0.4289, -0.7960, -0.5280, -0.7357, -0.4851, -0.3801, -0.5895,\n",
       "                        -0.5439, -0.7712, -0.6454, -0.7683, -0.7734, -0.5912, -0.6432, -0.5879,\n",
       "                        -0.5023, -0.4806, -0.3882, -0.4551, -0.6349, -0.8671, -0.5621, -0.4380,\n",
       "                        -0.8132, -0.5427, -0.5768, -0.4138, -0.5052, -0.8779, -0.7413, -0.6477,\n",
       "                        -0.6324, -0.5777, -0.7200, -0.7444, -0.5151, -0.5000, -0.5113, -0.5284,\n",
       "                        -0.3966, -0.3444, -0.8087, -0.6449, -0.4838, -0.5770, -0.7398, -0.9311]), max_val=tensor([1.0616, 0.8605, 0.9101, 1.1839, 0.8174, 1.0681, 1.3742, 1.6491, 0.9279,\n",
       "                        1.3104, 1.2084, 0.8927, 0.7582, 2.0360, 1.0730, 0.8900, 0.5153, 1.2088,\n",
       "                        0.5440, 0.7564, 1.1781, 1.0784, 0.9476, 0.9209, 1.2883, 1.2134, 1.1974,\n",
       "                        0.6980, 1.6990, 1.2257, 0.7674, 1.6277, 1.6548, 0.5692, 1.1282, 1.5329,\n",
       "                        0.5141, 0.6149, 1.1984, 2.9542, 0.7619, 0.5226, 0.5313, 0.6795, 0.5702,\n",
       "                        1.2788, 0.9710, 0.8895, 1.4399, 0.9545, 0.8499, 1.1249, 0.9583, 0.6714,\n",
       "                        1.0505, 1.0281, 0.7297, 0.6566, 1.3468, 1.1802, 0.6803, 2.0214, 1.1530,\n",
       "                        0.8680, 1.3516, 0.8751, 0.6835, 1.4977, 2.0741, 0.7854, 0.9005, 0.7930,\n",
       "                        1.2926, 1.5465, 1.4544, 1.3330, 0.7165, 1.3989, 0.6663, 1.3967, 1.3627,\n",
       "                        0.6096, 1.1549, 0.8205, 1.3602, 1.1177, 1.0616, 0.8735, 0.9064, 1.3321,\n",
       "                        1.1291, 1.2281, 1.1539, 0.7299, 0.8403, 0.6473, 0.5317, 0.4196, 0.8647,\n",
       "                        0.9039, 1.0407, 0.9328, 1.1515, 0.5943, 0.7423, 0.8665, 0.9208, 0.8561,\n",
       "                        0.7396, 0.8145, 0.9053, 1.0895, 0.7052, 0.5688, 0.6944, 2.3798, 0.7543,\n",
       "                        1.4745, 2.0567, 0.8826, 1.1338, 0.7261, 0.9356, 0.6414, 1.3358, 1.1847,\n",
       "                        1.2808, 1.9532, 0.8661, 1.3914, 0.9068, 0.6974, 1.2919, 0.7751, 1.5218,\n",
       "                        1.0288, 0.6826, 1.0306, 1.3210, 1.5112, 0.5478, 0.9467, 0.7845, 0.5103,\n",
       "                        0.7326, 0.9218, 0.8088, 0.7830, 0.9080, 0.7761, 0.5923, 0.7869, 0.8146,\n",
       "                        0.7438, 1.5952, 0.8659, 1.0060, 0.9540, 0.5831, 0.6686, 2.9918, 1.0250,\n",
       "                        1.6196, 2.9794, 1.2585, 0.8690, 2.2718, 0.9875, 1.6786, 1.7804, 0.8349,\n",
       "                        0.7003, 0.8555, 1.0678, 1.6879, 0.9260, 1.1367, 1.3700, 1.4723, 1.5373,\n",
       "                        1.1046, 0.6193, 1.5619, 0.7420, 0.8601, 0.7357, 1.0861, 0.9913, 1.3586,\n",
       "                        0.8336, 0.6201, 1.0790, 0.5353, 0.5539, 1.2314, 0.4417, 1.0877, 1.3044,\n",
       "                        0.5641, 0.6531, 0.7607, 1.5742, 1.4895, 1.1117, 2.0370, 0.6678, 0.6027,\n",
       "                        0.7792, 1.0618, 1.0750, 1.0385, 0.9669, 1.1956, 1.6728, 2.1799, 0.6737,\n",
       "                        0.9354, 0.6486, 1.0701, 1.1483, 0.9844, 1.1927, 0.5616, 0.8297, 0.6792,\n",
       "                        0.6377, 1.8428, 0.9551, 1.5340, 0.6370, 1.1490, 1.0118, 1.2248, 1.1840,\n",
       "                        1.1810, 0.7334, 0.9176, 1.0787, 0.6188, 1.1371, 1.3901, 1.4162, 1.5040,\n",
       "                        0.7473, 1.0911, 1.3589, 0.7096, 0.8050, 1.0162, 1.0416, 0.9025, 1.1261,\n",
       "                        1.0628, 1.1274, 0.6848, 0.7626, 0.6783, 0.5404, 1.4888, 0.5645, 0.5171,\n",
       "                        0.7367, 1.2404, 0.8005, 0.7157, 0.9587, 2.9907, 1.0250, 1.6086, 1.0901,\n",
       "                        1.2131, 0.9257, 0.9412, 1.1348, 0.6058, 0.6711, 0.5895, 1.5962, 0.9811,\n",
       "                        2.7860, 1.7801, 0.4627, 1.2289, 1.4887, 0.5949, 1.0375, 0.6018, 0.6544,\n",
       "                        1.3811, 1.2643, 0.5989, 1.0879, 0.6287, 2.3893, 0.8037, 0.5808, 0.7295,\n",
       "                        0.9710, 1.1504, 0.8572, 0.6274, 1.5195, 0.9536, 1.4166, 0.7564, 1.1468,\n",
       "                        1.2651, 1.2770, 0.7940, 0.6095, 0.6984, 0.8185, 0.9971, 0.5390, 0.8428,\n",
       "                        1.3757, 1.2197, 0.9135, 1.1058, 0.8385, 1.0201, 0.7328, 0.8470, 1.4275,\n",
       "                        0.5651, 1.1437, 0.6573, 2.0712, 0.8373, 1.3668, 1.4923, 1.3273, 0.7813,\n",
       "                        1.0319, 0.7627, 0.6498, 0.6868, 1.5421, 0.9545, 0.8242, 1.4771, 1.3540,\n",
       "                        0.7299, 1.7523, 0.6600, 1.0319, 1.1398, 0.9020, 0.7452, 1.9520, 1.4696,\n",
       "                        0.9380, 1.0097, 1.4892, 1.0606, 1.0886, 0.7537, 0.6974, 1.9085, 0.8056,\n",
       "                        0.7023, 0.7712, 2.4409, 0.9785, 0.7772, 0.9062, 0.8540, 0.9424, 0.9588,\n",
       "                        1.6562, 1.2827, 1.5328, 1.2344, 1.0925, 0.9592, 0.9358, 0.8846, 0.6123,\n",
       "                        0.8762, 1.5848, 1.0330, 0.6789, 1.0683, 1.3211, 0.9056, 0.7270, 0.8423,\n",
       "                        2.7893, 0.5526, 1.2127, 1.0897, 0.6517, 0.9134, 1.7172, 0.9407, 0.9482,\n",
       "                        0.6229, 0.9093, 2.1054, 1.2293, 0.8766, 0.8528, 0.8502, 0.7059, 0.9162,\n",
       "                        0.7118, 0.7202, 1.8257, 1.0868, 0.7074, 1.5762, 1.3218, 1.2281, 0.7321,\n",
       "                        1.1670, 1.6116, 0.8914, 1.2776, 0.5034, 0.6986, 0.8104, 0.7449, 1.0253,\n",
       "                        1.1499, 0.7249, 1.3735, 1.4688, 0.9238, 0.9962, 1.2492, 0.7754, 1.3628,\n",
       "                        1.1907, 2.4497, 0.6530, 1.0059, 1.5197, 0.5243, 0.7522, 0.8922, 0.6174,\n",
       "                        1.1056, 1.2673, 0.6680, 0.6109, 1.3539, 0.5992, 0.9976, 1.2495, 0.9126,\n",
       "                        1.0453, 1.1212, 1.0652, 1.2061, 0.8525, 1.0084, 0.7936, 1.1152, 1.1188,\n",
       "                        0.9513, 0.7128, 1.3370, 0.9083, 1.1613, 0.8980, 0.6328, 1.2177, 0.9810,\n",
       "                        2.0730, 1.1513, 0.7219, 0.9013, 2.0277, 0.9271, 0.8310, 1.6037, 1.3098,\n",
       "                        0.9128, 0.9343, 0.6464, 0.6299, 0.8552, 0.4907, 0.9399, 1.0210, 1.4514,\n",
       "                        0.7080, 0.7156, 1.4931, 0.7667, 1.0228, 0.8836, 0.8444, 0.6702, 1.0193,\n",
       "                        1.1479, 0.9743, 0.6219, 1.0441, 1.5049, 0.8931, 0.5979, 0.9964, 0.9709,\n",
       "                        0.5650, 0.5427, 1.0339, 0.7933, 0.8792, 1.1671, 2.2500, 2.1201])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.2851]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.20299530029297, max_val=19.010265350341797)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.3029]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.372282028198242, max_val=19.093599319458008)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(\n",
       "        in_features=512, out_features=10, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "                  0.0005]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0644, -0.0656, -0.0655, -0.0580, -0.0635, -0.0543, -0.0531, -0.0625,\n",
       "                    -0.0720, -0.0669]), max_val=tensor([0.0615, 0.0506, 0.0511, 0.0625, 0.0625, 0.0629, 0.0585, 0.0654, 0.0549,\n",
       "                    0.0676])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.1384]), zero_point=tensor([42], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.805233955383301, max_val=11.777641296386719)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([0]), scale=tensor([0.0408]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.429065704345703, max_val=2.7537076473236084)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.convert(prepared_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8944\n"
     ]
    }
   ],
   "source": [
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, 'cpu')\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6832,\n",
       " 0.7748,\n",
       " 0.8076,\n",
       " 0.8304,\n",
       " 0.8376,\n",
       " 0.8632,\n",
       " 0.8536,\n",
       " 0.8672,\n",
       " 0.8608,\n",
       " 0.8584,\n",
       " 0.8532,\n",
       " 0.8724,\n",
       " 0.8612,\n",
       " 0.872,\n",
       " 0.8856,\n",
       " 0.8892,\n",
       " 0.882,\n",
       " 0.8892,\n",
       " 0.888,\n",
       " 0.888,\n",
       " 0.8824,\n",
       " 0.8916,\n",
       " 0.89,\n",
       " 0.8972,\n",
       " 0.8976,\n",
       " 0.9056,\n",
       " 0.8964,\n",
       " 0.9048,\n",
       " 0.9,\n",
       " 0.8972,\n",
       " 0.9104,\n",
       " 0.9028,\n",
       " 0.9004,\n",
       " 0.9008,\n",
       " 0.9076,\n",
       " 0.914,\n",
       " 0.9052,\n",
       " 0.9176,\n",
       " 0.9208,\n",
       " 0.9052,\n",
       " 0.9028,\n",
       " 0.9184,\n",
       " 0.9116,\n",
       " 0.914,\n",
       " 0.9164,\n",
       " 0.9124,\n",
       " 0.9128,\n",
       " 0.9132,\n",
       " 0.9148,\n",
       " 0.9076,\n",
       " 0.9196,\n",
       " 0.9188,\n",
       " 0.918,\n",
       " 0.9228,\n",
       " 0.9244,\n",
       " 0.9144,\n",
       " 0.9188,\n",
       " 0.9212,\n",
       " 0.9196,\n",
       " 0.9244,\n",
       " 0.918,\n",
       " 0.924,\n",
       " 0.9248,\n",
       " 0.9176,\n",
       " 0.9244,\n",
       " 0.9248,\n",
       " 0.916,\n",
       " 0.9308,\n",
       " 0.9344,\n",
       " 0.9216,\n",
       " 0.9268,\n",
       " 0.9304,\n",
       " 0.9284,\n",
       " 0.928,\n",
       " 0.928,\n",
       " 0.9316,\n",
       " 0.9256,\n",
       " 0.9308,\n",
       " 0.9352,\n",
       " 0.9232,\n",
       " 0.9284,\n",
       " 0.932,\n",
       " 0.9328,\n",
       " 0.9232,\n",
       " 0.932,\n",
       " 0.9292,\n",
       " 0.9228,\n",
       " 0.9264,\n",
       " 0.9344,\n",
       " 0.9292,\n",
       " 0.9356,\n",
       " 0.9328,\n",
       " 0.9272,\n",
       " 0.932,\n",
       " 0.9344,\n",
       " 0.932,\n",
       " 0.9252,\n",
       " 0.9284,\n",
       " 0.9368,\n",
       " 0.9336,\n",
       " 0.9224,\n",
       " 0.9348,\n",
       " 0.9328,\n",
       " 0.9284,\n",
       " 0.9348,\n",
       " 0.9336,\n",
       " 0.926,\n",
       " 0.93,\n",
       " 0.9348,\n",
       " 0.932,\n",
       " 0.936,\n",
       " 0.932,\n",
       " 0.9316,\n",
       " 0.9324,\n",
       " 0.9308,\n",
       " 0.9364,\n",
       " 0.9228,\n",
       " 0.9332,\n",
       " 0.9364,\n",
       " 0.938,\n",
       " 0.9368,\n",
       " 0.9384,\n",
       " 0.9428,\n",
       " 0.9376,\n",
       " 0.936,\n",
       " 0.9388,\n",
       " 0.9272,\n",
       " 0.9328,\n",
       " 0.9376,\n",
       " 0.9372,\n",
       " 0.934,\n",
       " 0.9404,\n",
       " 0.9436,\n",
       " 0.9328,\n",
       " 0.934,\n",
       " 0.9388,\n",
       " 0.94,\n",
       " 0.9404,\n",
       " 0.934,\n",
       " 0.9384,\n",
       " 0.932,\n",
       " 0.9372,\n",
       " 0.936,\n",
       " 0.94,\n",
       " 0.938,\n",
       " 0.9364,\n",
       " 0.9384,\n",
       " 0.9392,\n",
       " 0.9392,\n",
       " 0.942,\n",
       " 0.9368,\n",
       " 0.9388,\n",
       " 0.9396,\n",
       " 0.9392,\n",
       " 0.9396,\n",
       " 0.9412,\n",
       " 0.9428,\n",
       " 0.9356,\n",
       " 0.9416,\n",
       " 0.946,\n",
       " 0.9352,\n",
       " 0.9372,\n",
       " 0.938,\n",
       " 0.9364,\n",
       " 0.9376,\n",
       " 0.9412,\n",
       " 0.9368,\n",
       " 0.9452,\n",
       " 0.94,\n",
       " 0.9404,\n",
       " 0.946,\n",
       " 0.944,\n",
       " 0.9452,\n",
       " 0.9412,\n",
       " 0.9408,\n",
       " 0.9428,\n",
       " 0.9424,\n",
       " 0.9404,\n",
       " 0.9424,\n",
       " 0.942,\n",
       " 0.9424,\n",
       " 0.9448,\n",
       " 0.9396,\n",
       " 0.9444,\n",
       " 0.9428,\n",
       " 0.936,\n",
       " 0.9436,\n",
       " 0.9452,\n",
       " 0.9408,\n",
       " 0.9364,\n",
       " 0.9412,\n",
       " 0.9368,\n",
       " 0.9428,\n",
       " 0.94,\n",
       " 0.9452,\n",
       " 0.9452,\n",
       " 0.9444,\n",
       " 0.9452,\n",
       " 0.9428,\n",
       " 0.9388]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.14247293770313263, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06992616504430771, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3518623411655426, zero_point=44, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.37295427918434143, zero_point=41\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.15230172872543335, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3299316167831421, zero_point=63, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.40514421463012695, zero_point=50\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.3343055546283722, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.19264449179172516, zero_point=78, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.2784789204597473, zero_point=56)\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.2751938998699188, zero_point=56\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.05432270094752312, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.12099578976631165, zero_point=69, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.268587201833725, zero_point=57\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.046501461416482925, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07713326811790466, zero_point=61, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.14157457649707794, zero_point=52)\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.13968956470489502, zero_point=57\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02699979767203331, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.09255240857601166, zero_point=63, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.13072077929973602, zero_point=57\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.019709620624780655, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.09433502703905106, zero_point=69, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.04527599364519119, zero_point=71)\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.092128224670887, zero_point=64\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.05211249738931656, zero_point=0, padding=(1, 1))\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.19190114736557007, zero_point=65, padding=(1, 1))\n",
       "          (bn2): Identity()\n",
       "          (add_func): QFunctional(\n",
       "            scale=0.20976170897483826, zero_point=65\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.4769597053527832, zero_point=48, qscheme=torch.per_channel_affine)\n",
       "    )\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized weights for model.model.conv1: torch.Size([64, 3, 3, 3])\n",
      "Quantized weights for model.model.layer1.0.conv1: torch.Size([64, 64, 3, 3])\n",
      "Quantized weights for model.model.layer1.0.conv2: torch.Size([64, 64, 3, 3])\n",
      "Quantized weights for model.model.layer1.1.conv1: torch.Size([64, 64, 3, 3])\n",
      "Quantized weights for model.model.layer1.1.conv2: torch.Size([64, 64, 3, 3])\n",
      "Quantized weights for model.model.layer2.0.conv1: torch.Size([128, 64, 3, 3])\n",
      "Quantized weights for model.model.layer2.0.conv2: torch.Size([128, 128, 3, 3])\n",
      "Quantized weights for model.model.layer2.0.downsample.0: torch.Size([128, 64, 1, 1])\n",
      "Quantized weights for model.model.layer2.1.conv1: torch.Size([128, 128, 3, 3])\n",
      "Quantized weights for model.model.layer2.1.conv2: torch.Size([128, 128, 3, 3])\n",
      "Quantized weights for model.model.layer3.0.conv1: torch.Size([256, 128, 3, 3])\n",
      "Quantized weights for model.model.layer3.0.conv2: torch.Size([256, 256, 3, 3])\n",
      "Quantized weights for model.model.layer3.0.downsample.0: torch.Size([256, 128, 1, 1])\n",
      "Quantized weights for model.model.layer3.1.conv1: torch.Size([256, 256, 3, 3])\n",
      "Quantized weights for model.model.layer3.1.conv2: torch.Size([256, 256, 3, 3])\n",
      "Quantized weights for model.model.layer4.0.conv1: torch.Size([512, 256, 3, 3])\n",
      "Quantized weights for model.model.layer4.0.conv2: torch.Size([512, 512, 3, 3])\n",
      "Quantized weights for model.model.layer4.0.downsample.0: torch.Size([512, 256, 1, 1])\n",
      "Quantized weights for model.model.layer4.1.conv1: torch.Size([512, 512, 3, 3])\n",
      "Quantized weights for model.model.layer4.1.conv2: torch.Size([512, 512, 3, 3])\n",
      "Quantized weights for model.model.fc: torch.Size([10, 512])\n",
      "Original FP32 weights for model.model.conv1: torch.Size([64, 3, 3, 3])\n",
      "Original FP32 weights for model.model.layer1.0.conv1: torch.Size([64, 64, 3, 3])\n",
      "Original FP32 weights for model.model.layer1.0.conv2: torch.Size([64, 64, 3, 3])\n",
      "Original FP32 weights for model.model.layer1.1.conv1: torch.Size([64, 64, 3, 3])\n",
      "Original FP32 weights for model.model.layer1.1.conv2: torch.Size([64, 64, 3, 3])\n",
      "Original FP32 weights for model.model.layer2.0.conv1: torch.Size([128, 64, 3, 3])\n",
      "Original FP32 weights for model.model.layer2.0.conv2: torch.Size([128, 128, 3, 3])\n",
      "Original FP32 weights for model.model.layer2.0.downsample.0: torch.Size([128, 64, 1, 1])\n",
      "Original FP32 weights for model.model.layer2.1.conv1: torch.Size([128, 128, 3, 3])\n",
      "Original FP32 weights for model.model.layer2.1.conv2: torch.Size([128, 128, 3, 3])\n",
      "Original FP32 weights for model.model.layer3.0.conv1: torch.Size([256, 128, 3, 3])\n",
      "Original FP32 weights for model.model.layer3.0.conv2: torch.Size([256, 256, 3, 3])\n",
      "Original FP32 weights for model.model.layer3.0.downsample.0: torch.Size([256, 128, 1, 1])\n",
      "Original FP32 weights for model.model.layer3.1.conv1: torch.Size([256, 256, 3, 3])\n",
      "Original FP32 weights for model.model.layer3.1.conv2: torch.Size([256, 256, 3, 3])\n",
      "Original FP32 weights for model.model.layer4.0.conv1: torch.Size([512, 256, 3, 3])\n",
      "Original FP32 weights for model.model.layer4.0.conv2: torch.Size([512, 512, 3, 3])\n",
      "Original FP32 weights for model.model.layer4.0.downsample.0: torch.Size([512, 256, 1, 1])\n",
      "Original FP32 weights for model.model.layer4.1.conv1: torch.Size([512, 512, 3, 3])\n",
      "Original FP32 weights for model.model.layer4.1.conv2: torch.Size([512, 512, 3, 3])\n",
      "Original FP32 weights for model.model.fc: torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "def extract_qat_weights(model):\n",
    "    quantized_weights = {}\n",
    "    original_weights = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            # Quantized weights (fake quantized)\n",
    "            if hasattr(module.weight_fake_quant, 'fake_quant_enabled') and module.weight_fake_quant.fake_quant_enabled:\n",
    "                quantized_weights[name] = module.weight_fake_quant(module.weight).detach().clone()\n",
    "            \n",
    "            # Original FP32 weights\n",
    "            original_weights[name] = module.weight.detach().clone()\n",
    "\n",
    "    return quantized_weights, original_weights\n",
    "\n",
    "# Extract weights\n",
    "quantized_weights, original_weights = extract_qat_weights(prepared_student)\n",
    "\n",
    "# Display the weights\n",
    "for name, weights in quantized_weights.items():\n",
    "    print(f\"Quantized weights for {name}: {weights.shape}\")\n",
    "\n",
    "for name, weights in original_weights.items():\n",
    "    print(f\"Original FP32 weights for {name}: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): ConvBnReLU2d(\n",
       "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014,\n",
       "                  0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014, 0.0015, 0.0015,\n",
       "                  0.0015, 0.0014, 0.0015, 0.0014, 0.0014, 0.0015, 0.0013, 0.0014, 0.0015,\n",
       "                  0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "                  0.0015, 0.0015, 0.0013, 0.0015, 0.0015, 0.0015, 0.0014, 0.0015, 0.0015,\n",
       "                  0.0014, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014, 0.0015, 0.0015, 0.0014,\n",
       "                  0.0014, 0.0015, 0.0015, 0.0013, 0.0013, 0.0015, 0.0015, 0.0015, 0.0013,\n",
       "                  0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1870, -0.1862, -0.1836, -0.1791, -0.1668, -0.1909, -0.1840, -0.1873,\n",
       "                    -0.1456, -0.1887, -0.1909, -0.1902, -0.1605, -0.1630, -0.1859, -0.1807,\n",
       "                    -0.1899, -0.1881, -0.1917, -0.1731, -0.1858, -0.1565, -0.1789, -0.1876,\n",
       "                    -0.1625, -0.1846, -0.1763, -0.1882, -0.1875, -0.1745, -0.1922, -0.1921,\n",
       "                    -0.1671, -0.1773, -0.1830, -0.1874, -0.1839, -0.1441, -0.1672, -0.1683,\n",
       "                    -0.1907, -0.1874, -0.1764, -0.1855, -0.1703, -0.1794, -0.1924, -0.1783,\n",
       "                    -0.1829, -0.1867, -0.1559, -0.1863, -0.1845, -0.1681, -0.1726, -0.1819,\n",
       "                    -0.1874, -0.1658, -0.1606, -0.1896, -0.1792, -0.1917, -0.1617, -0.1443]), max_val=tensor([0.1853, 0.1885, 0.1847, 0.1900, 0.1884, 0.1865, 0.1862, 0.1843, 0.1720,\n",
       "                    0.1101, 0.1864, 0.1849, 0.1848, 0.1843, 0.1866, 0.1669, 0.1900, 0.1714,\n",
       "                    0.1917, 0.1806, 0.1725, 0.1836, 0.1757, 0.1800, 0.1673, 0.1780, 0.1911,\n",
       "                    0.1872, 0.1762, 0.1864, 0.1875, 0.1822, 0.1908, 0.1914, 0.1872, 0.1787,\n",
       "                    0.1865, 0.1897, 0.1711, 0.1886, 0.1893, 0.1748, 0.1680, 0.1861, 0.1848,\n",
       "                    0.1490, 0.1905, 0.1908, 0.1872, 0.1712, 0.1777, 0.1764, 0.1853, 0.1834,\n",
       "                    0.1835, 0.1881, 0.1892, 0.1665, 0.1583, 0.1848, 0.1921, 0.1868, 0.1697,\n",
       "                    0.1821])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0062, 0.0020, 0.0044, 0.0029, 0.0027, 0.0026, 0.0029, 0.0014, 0.0036,\n",
       "                      0.0033, 0.0014, 0.0025, 0.0035, 0.0019, 0.0023, 0.0032, 0.0031, 0.0019,\n",
       "                      0.0028, 0.0027, 0.0041, 0.0026, 0.0039, 0.0026, 0.0020, 0.0032, 0.0014,\n",
       "                      0.0029, 0.0048, 0.0023, 0.0024, 0.0032, 0.0027, 0.0039, 0.0049, 0.0029,\n",
       "                      0.0029, 0.0031, 0.0033, 0.0043, 0.0053, 0.0039, 0.0019, 0.0025, 0.0014,\n",
       "                      0.0029, 0.0038, 0.0022, 0.0027, 0.0024, 0.0023, 0.0021, 0.0014, 0.0037,\n",
       "                      0.0050, 0.0029, 0.0019, 0.0018, 0.0034, 0.0016, 0.0022, 0.0022, 0.0014,\n",
       "                      0.0035]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.7993, -0.2508, -0.5222, -0.3661, -0.3500, -0.3276, -0.2028, -0.1836,\n",
       "                        -0.4557, -0.4273, -0.1668, -0.3211, -0.4447, -0.2441, -0.2984, -0.4107,\n",
       "                        -0.4008, -0.2382, -0.3608, -0.3423, -0.5282, -0.3272, -0.4952, -0.3370,\n",
       "                        -0.2539, -0.4123, -0.1787, -0.3675, -0.5029, -0.1005, -0.1238, -0.4157,\n",
       "                        -0.3417, -0.5040, -0.6231, -0.3681, -0.3656, -0.3926, -0.4217, -0.5520,\n",
       "                        -0.6800, -0.4970, -0.2418, -0.1017, -0.1828, -0.3774, -0.4847, -0.2792,\n",
       "                        -0.3422, -0.3050, -0.2979, -0.2712, -0.1781, -0.4762, -0.6433, -0.0912,\n",
       "                        -0.2418, -0.2258, -0.4336, -0.1650, -0.2386, -0.2845, -0.0823, -0.4435]), max_val=tensor([0.1414, 0.1672, 0.5557, 0.1569, 0.1136, 0.1633, 0.3736, 0.1825, 0.1194,\n",
       "                        0.2265, 0.1813, 0.1120, 0.1856, 0.2201, 0.1859, 0.3054, 0.1574, 0.1300,\n",
       "                        0.1762, 0.1680, 0.1773, 0.0853, 0.1538, 0.1913, 0.2266, 0.1084, 0.1729,\n",
       "                        0.1231, 0.6034, 0.2959, 0.3012, 0.1336, 0.1937, 0.1298, 0.1250, 0.1271,\n",
       "                        0.1530, 0.3751, 0.0992, 0.1616, 0.6196, 0.1660, 0.2371, 0.3169, 0.0738,\n",
       "                        0.1921, 0.1593, 0.0926, 0.1100, 0.1545, 0.1630, 0.1095, 0.1525, 0.1339,\n",
       "                        0.1300, 0.3678, 0.1256, 0.1192, 0.1647, 0.2045, 0.2850, 0.1057, 0.1761,\n",
       "                        0.1755])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0026, 0.0011, 0.0007, 0.0018, 0.0011, 0.0018, 0.0019, 0.0014, 0.0021,\n",
       "                      0.0020, 0.0012, 0.0020, 0.0019, 0.0017, 0.0019, 0.0018, 0.0018, 0.0020,\n",
       "                      0.0016, 0.0017, 0.0022, 0.0021, 0.0016, 0.0018, 0.0013, 0.0014, 0.0017,\n",
       "                      0.0015, 0.0028, 0.0018, 0.0017, 0.0027, 0.0017, 0.0035, 0.0013, 0.0012,\n",
       "                      0.0011, 0.0038, 0.0014, 0.0015, 0.0015, 0.0012, 0.0031, 0.0019, 0.0017,\n",
       "                      0.0015, 0.0018, 0.0022, 0.0017, 0.0018, 0.0017, 0.0021, 0.0021, 0.0019,\n",
       "                      0.0018, 0.0016, 0.0017, 0.0019, 0.0019, 0.0020, 0.0023, 0.0015, 0.0017,\n",
       "                      0.0026]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.3302, -0.1212, -0.0856, -0.2294, -0.0859, -0.2282, -0.2437, -0.1430,\n",
       "                        -0.2743, -0.1694, -0.1583, -0.2608, -0.2443, -0.0856, -0.2193, -0.2355,\n",
       "                        -0.2359, -0.1557, -0.0701, -0.2216, -0.2756, -0.1695, -0.2011, -0.2347,\n",
       "                        -0.1659, -0.1758, -0.1964, -0.1932, -0.3562, -0.2296, -0.1865, -0.3441,\n",
       "                        -0.1938, -0.4437, -0.1257, -0.1495, -0.1229, -0.4879, -0.1586, -0.1943,\n",
       "                        -0.1963, -0.1495, -0.3995, -0.2427, -0.2187, -0.1953, -0.2365, -0.2867,\n",
       "                        -0.1832, -0.2360, -0.2114, -0.2628, -0.1929, -0.2382, -0.1641, -0.1995,\n",
       "                        -0.2213, -0.2190, -0.1349, -0.2578, -0.2976, -0.1852, -0.2135, -0.3298]), max_val=tensor([0.1202, 0.1437, 0.0919, 0.2183, 0.1371, 0.2306, 0.1988, 0.1808, 0.2356,\n",
       "                        0.2551, 0.1149, 0.2238, 0.1976, 0.2177, 0.2399, 0.0908, 0.1695, 0.2540,\n",
       "                        0.2012, 0.1525, 0.2153, 0.2726, 0.1990, 0.0966, 0.0807, 0.1689, 0.2191,\n",
       "                        0.1690, 0.2856, 0.1737, 0.2114, 0.2301, 0.2186, 0.1355, 0.1640, 0.1216,\n",
       "                        0.1458, 0.0981, 0.1735, 0.1028, 0.1422, 0.1110, 0.1194, 0.2335, 0.1734,\n",
       "                        0.1486, 0.2202, 0.1337, 0.2203, 0.2276, 0.1534, 0.1865, 0.2653, 0.1137,\n",
       "                        0.2270, 0.1151, 0.2040, 0.2351, 0.2421, 0.1472, 0.1824, 0.1869, 0.1936,\n",
       "                        0.2247])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0018, 0.0034, 0.0019, 0.0013, 0.0023, 0.0024, 0.0016, 0.0051, 0.0027,\n",
       "                      0.0028, 0.0015, 0.0016, 0.0021, 0.0037, 0.0022, 0.0021, 0.0043, 0.0015,\n",
       "                      0.0015, 0.0020, 0.0012, 0.0015, 0.0013, 0.0020, 0.0017, 0.0028, 0.0013,\n",
       "                      0.0014, 0.0013, 0.0029, 0.0010, 0.0030, 0.0024, 0.0017, 0.0013, 0.0015,\n",
       "                      0.0025, 0.0016, 0.0022, 0.0016, 0.0015, 0.0018, 0.0013, 0.0022, 0.0017,\n",
       "                      0.0019, 0.0018, 0.0038, 0.0017, 0.0032, 0.0038, 0.0028, 0.0020, 0.0015,\n",
       "                      0.0025, 0.0050, 0.0018, 0.0020, 0.0015, 0.0027, 0.0018, 0.0016, 0.0021,\n",
       "                      0.0021]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.2256, -0.3134, -0.2377, -0.1686, -0.2971, -0.3104, -0.1985, -0.5700,\n",
       "                        -0.3446, -0.3593, -0.1909, -0.2106, -0.2212, -0.4703, -0.1940, -0.1413,\n",
       "                        -0.5486, -0.1984, -0.1912, -0.2605, -0.0952, -0.1949, -0.1672, -0.2513,\n",
       "                        -0.2179, -0.3537, -0.1465, -0.1788, -0.1530, -0.2241, -0.1137, -0.3839,\n",
       "                        -0.3060, -0.1685, -0.1706, -0.1960, -0.3209, -0.1777, -0.2868, -0.2057,\n",
       "                        -0.1294, -0.2316, -0.0994, -0.2871, -0.2217, -0.2404, -0.1627, -0.4902,\n",
       "                        -0.2059, -0.4034, -0.3257, -0.3647, -0.2556, -0.1954, -0.3120, -0.5889,\n",
       "                        -0.2276, -0.2572, -0.1446, -0.3492, -0.2347, -0.2093, -0.1994, -0.2724]), max_val=tensor([0.1749, 0.4316, 0.1631, 0.1171, 0.2834, 0.1438, 0.1275, 0.6491, 0.3176,\n",
       "                        0.2566, 0.1832, 0.1907, 0.2688, 0.2952, 0.2851, 0.2717, 0.4451, 0.1289,\n",
       "                        0.1171, 0.1471, 0.1574, 0.1743, 0.1280, 0.2353, 0.2211, 0.1926, 0.1616,\n",
       "                        0.1598, 0.1632, 0.3709, 0.1268, 0.1941, 0.1725, 0.2220, 0.1384, 0.1856,\n",
       "                        0.3180, 0.2079, 0.2650, 0.1329, 0.1891, 0.2211, 0.1638, 0.1886, 0.1596,\n",
       "                        0.1821, 0.2309, 0.4715, 0.2177, 0.3372, 0.4812, 0.1539, 0.1702, 0.1227,\n",
       "                        0.3117, 0.6394, 0.1909, 0.1863, 0.1861, 0.2499, 0.1506, 0.1545, 0.2694,\n",
       "                        0.1676])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0024, 0.0015, 0.0015, 0.0013, 0.0027, 0.0023, 0.0023, 0.0022,\n",
       "                      0.0017, 0.0010, 0.0017, 0.0023, 0.0012, 0.0014, 0.0019, 0.0019, 0.0022,\n",
       "                      0.0019, 0.0015, 0.0019, 0.0018, 0.0018, 0.0020, 0.0010, 0.0008, 0.0012,\n",
       "                      0.0012, 0.0023, 0.0016, 0.0014, 0.0019, 0.0020, 0.0017, 0.0013, 0.0012,\n",
       "                      0.0017, 0.0016, 0.0017, 0.0014, 0.0019, 0.0008, 0.0015, 0.0015, 0.0015,\n",
       "                      0.0021, 0.0013, 0.0012, 0.0018, 0.0030, 0.0016, 0.0014, 0.0029, 0.0009,\n",
       "                      0.0013, 0.0011, 0.0011, 0.0011, 0.0016, 0.0014, 0.0013, 0.0008, 0.0018,\n",
       "                      0.0015]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.2008, -0.3114, -0.1173, -0.1882, -0.1697, -0.3397, -0.2904, -0.2927,\n",
       "                        -0.2811, -0.2152, -0.1320, -0.2168, -0.2913, -0.1530, -0.1808, -0.1647,\n",
       "                        -0.2434, -0.2358, -0.1418, -0.1118, -0.1537, -0.1285, -0.1931, -0.1782,\n",
       "                        -0.1250, -0.0821, -0.1489, -0.1526, -0.2942, -0.2020, -0.1774, -0.1837,\n",
       "                        -0.2531, -0.2229, -0.1190, -0.1002, -0.2225, -0.2046, -0.2214, -0.1353,\n",
       "                        -0.2438, -0.0988, -0.1953, -0.1866, -0.1902, -0.2634, -0.1723, -0.1303,\n",
       "                        -0.2264, -0.3806, -0.2058, -0.1814, -0.3705, -0.1143, -0.1720, -0.1291,\n",
       "                        -0.1414, -0.1414, -0.2066, -0.1523, -0.1667, -0.0848, -0.2360, -0.1955]), max_val=tensor([0.1083, 0.1854, 0.1843, 0.1841, 0.1677, 0.3031, 0.2795, 0.1839, 0.2071,\n",
       "                        0.1573, 0.1252, 0.2116, 0.2360, 0.1003, 0.1839, 0.2459, 0.1309, 0.2839,\n",
       "                        0.2472, 0.1886, 0.2383, 0.2248, 0.2280, 0.2596, 0.0913, 0.1057, 0.1140,\n",
       "                        0.1548, 0.2920, 0.1646, 0.1284, 0.2379, 0.2390, 0.1451, 0.1651, 0.1487,\n",
       "                        0.2087, 0.1345, 0.1712, 0.1754, 0.2397, 0.1042, 0.1450, 0.1857, 0.1289,\n",
       "                        0.1581, 0.1396, 0.1508, 0.1773, 0.2058, 0.1628, 0.1809, 0.3014, 0.1020,\n",
       "                        0.1416, 0.1383, 0.1380, 0.1404, 0.1972, 0.1757, 0.1341, 0.1071, 0.1873,\n",
       "                        0.1601])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0013, 0.0016, 0.0017, 0.0015, 0.0018, 0.0010, 0.0019, 0.0011,\n",
       "                      0.0014, 0.0023, 0.0014, 0.0022, 0.0013, 0.0010, 0.0014, 0.0020, 0.0017,\n",
       "                      0.0019, 0.0025, 0.0016, 0.0015, 0.0019, 0.0023, 0.0017, 0.0023, 0.0019,\n",
       "                      0.0018, 0.0027, 0.0013, 0.0012, 0.0015, 0.0013, 0.0014, 0.0013, 0.0020,\n",
       "                      0.0015, 0.0018, 0.0014, 0.0014, 0.0017, 0.0014, 0.0020, 0.0014, 0.0011,\n",
       "                      0.0011, 0.0022, 0.0021, 0.0012, 0.0026, 0.0015, 0.0014, 0.0017, 0.0013,\n",
       "                      0.0015, 0.0016, 0.0015, 0.0013, 0.0016, 0.0024, 0.0013, 0.0013, 0.0010,\n",
       "                      0.0018, 0.0010, 0.0021, 0.0016, 0.0014, 0.0013, 0.0025, 0.0022, 0.0011,\n",
       "                      0.0017, 0.0017, 0.0019, 0.0021, 0.0021, 0.0017, 0.0015, 0.0012, 0.0019,\n",
       "                      0.0018, 0.0018, 0.0018, 0.0012, 0.0012, 0.0022, 0.0013, 0.0017, 0.0014,\n",
       "                      0.0016, 0.0014, 0.0017, 0.0020, 0.0012, 0.0016, 0.0013, 0.0011, 0.0015,\n",
       "                      0.0017, 0.0018, 0.0021, 0.0018, 0.0015, 0.0014, 0.0013, 0.0021, 0.0011,\n",
       "                      0.0023, 0.0016, 0.0022, 0.0021, 0.0019, 0.0016, 0.0012, 0.0024, 0.0024,\n",
       "                      0.0017, 0.0020, 0.0020, 0.0011, 0.0020, 0.0011, 0.0022, 0.0020, 0.0012,\n",
       "                      0.0016, 0.0022]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1371, -0.1441, -0.1585, -0.1836, -0.1900, -0.1807, -0.1253, -0.1649,\n",
       "                        -0.1400, -0.1705, -0.1742, -0.1837, -0.2652, -0.1705, -0.1338, -0.1471,\n",
       "                        -0.2181, -0.2127, -0.2397, -0.3230, -0.2088, -0.1519, -0.1654, -0.2758,\n",
       "                        -0.2171, -0.2993, -0.1251, -0.2336, -0.2768, -0.1489, -0.1398, -0.1399,\n",
       "                        -0.1618, -0.1756, -0.1695, -0.1950, -0.1935, -0.2279, -0.1754, -0.1855,\n",
       "                        -0.2143, -0.1763, -0.0908, -0.1756, -0.1416, -0.1448, -0.2202, -0.1743,\n",
       "                        -0.1556, -0.1974, -0.1918, -0.1822, -0.1776, -0.1601, -0.1951, -0.2004,\n",
       "                        -0.1915, -0.1449, -0.2065, -0.2065, -0.1494, -0.1135, -0.1239, -0.1496,\n",
       "                        -0.1330, -0.2638, -0.2110, -0.1806, -0.1582, -0.3228, -0.1729, -0.1211,\n",
       "                        -0.1921, -0.1970, -0.1453, -0.2146, -0.1805, -0.2150, -0.1902, -0.1394,\n",
       "                        -0.2373, -0.2318, -0.1582, -0.2358, -0.0887, -0.1160, -0.2761, -0.1591,\n",
       "                        -0.2152, -0.1630, -0.2059, -0.1779, -0.2158, -0.1387, -0.1476, -0.1820,\n",
       "                        -0.1566, -0.1272, -0.1961, -0.1653, -0.1645, -0.2646, -0.1585, -0.1482,\n",
       "                        -0.1801, -0.1680, -0.2051, -0.1294, -0.2221, -0.2027, -0.2801, -0.2712,\n",
       "                        -0.2295, -0.1992, -0.1325, -0.3024, -0.1990, -0.1865, -0.2261, -0.2598,\n",
       "                        -0.1390, -0.2517, -0.1378, -0.2272, -0.1610, -0.1059, -0.1948, -0.2254]), max_val=tensor([0.1788, 0.1604, 0.2078, 0.2146, 0.1232, 0.2315, 0.1138, 0.2421, 0.1323,\n",
       "                        0.1772, 0.2914, 0.1056, 0.2758, 0.1628, 0.1039, 0.1715, 0.2593, 0.1701,\n",
       "                        0.2097, 0.1079, 0.1948, 0.1892, 0.2377, 0.2875, 0.2044, 0.2289, 0.2384,\n",
       "                        0.1980, 0.3405, 0.1621, 0.1468, 0.1910, 0.1707, 0.1366, 0.1696, 0.2531,\n",
       "                        0.1800, 0.1751, 0.1534, 0.1711, 0.1799, 0.1153, 0.2486, 0.1438, 0.1248,\n",
       "                        0.1305, 0.2813, 0.2701, 0.1411, 0.3319, 0.1381, 0.1597, 0.2148, 0.1424,\n",
       "                        0.1789, 0.1520, 0.1530, 0.1590, 0.2028, 0.3048, 0.1626, 0.1661, 0.1190,\n",
       "                        0.2314, 0.1004, 0.1426, 0.1370, 0.1555, 0.1660, 0.2095, 0.2799, 0.1338,\n",
       "                        0.2181, 0.2124, 0.2463, 0.2626, 0.2724, 0.1479, 0.1322, 0.1506, 0.1324,\n",
       "                        0.1947, 0.2294, 0.2221, 0.1538, 0.1477, 0.1942, 0.1667, 0.1263, 0.1753,\n",
       "                        0.2053, 0.1575, 0.1949, 0.2547, 0.1515, 0.2032, 0.1696, 0.1390, 0.1339,\n",
       "                        0.2171, 0.2234, 0.2006, 0.2316, 0.1914, 0.1704, 0.1303, 0.2695, 0.1433,\n",
       "                        0.2893, 0.1532, 0.2761, 0.2338, 0.2387, 0.1234, 0.1461, 0.2913, 0.3004,\n",
       "                        0.2168, 0.2570, 0.2407, 0.1341, 0.2391, 0.1149, 0.2775, 0.2582, 0.1489,\n",
       "                        0.2093, 0.2854])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0018, 0.0012, 0.0020, 0.0008, 0.0017, 0.0016, 0.0014, 0.0013, 0.0014,\n",
       "                      0.0013, 0.0011, 0.0016, 0.0015, 0.0019, 0.0012, 0.0010, 0.0015, 0.0018,\n",
       "                      0.0012, 0.0012, 0.0032, 0.0015, 0.0009, 0.0027, 0.0013, 0.0011, 0.0015,\n",
       "                      0.0015, 0.0012, 0.0014, 0.0015, 0.0017, 0.0017, 0.0015, 0.0013, 0.0012,\n",
       "                      0.0014, 0.0008, 0.0021, 0.0022, 0.0020, 0.0010, 0.0010, 0.0012, 0.0016,\n",
       "                      0.0016, 0.0020, 0.0022, 0.0020, 0.0024, 0.0012, 0.0019, 0.0013, 0.0014,\n",
       "                      0.0015, 0.0018, 0.0010, 0.0015, 0.0015, 0.0018, 0.0010, 0.0018, 0.0020,\n",
       "                      0.0016, 0.0015, 0.0027, 0.0018, 0.0016, 0.0014, 0.0013, 0.0016, 0.0027,\n",
       "                      0.0025, 0.0012, 0.0015, 0.0015, 0.0012, 0.0010, 0.0013, 0.0020, 0.0013,\n",
       "                      0.0016, 0.0016, 0.0009, 0.0014, 0.0012, 0.0019, 0.0024, 0.0015, 0.0024,\n",
       "                      0.0015, 0.0028, 0.0013, 0.0016, 0.0018, 0.0016, 0.0010, 0.0011, 0.0018,\n",
       "                      0.0018, 0.0023, 0.0014, 0.0016, 0.0015, 0.0026, 0.0013, 0.0015, 0.0013,\n",
       "                      0.0020, 0.0010, 0.0014, 0.0019, 0.0034, 0.0024, 0.0019, 0.0013, 0.0017,\n",
       "                      0.0015, 0.0012, 0.0009, 0.0025, 0.0012, 0.0014, 0.0021, 0.0012, 0.0030,\n",
       "                      0.0012, 0.0020]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.2259, -0.1478, -0.1755, -0.0985, -0.2211, -0.1838, -0.1799, -0.1501,\n",
       "                        -0.1254, -0.1680, -0.1364, -0.2083, -0.1152, -0.1082, -0.0966, -0.1283,\n",
       "                        -0.1883, -0.2262, -0.1016, -0.1094, -0.1398, -0.1928, -0.1205, -0.2122,\n",
       "                        -0.1679, -0.1365, -0.1683, -0.1660, -0.0847, -0.1815, -0.1911, -0.1810,\n",
       "                        -0.2131, -0.1947, -0.1637, -0.1360, -0.1072, -0.1058, -0.1522, -0.2769,\n",
       "                        -0.2505, -0.1337, -0.1213, -0.0910, -0.2011, -0.2086, -0.1132, -0.2834,\n",
       "                        -0.2499, -0.2062, -0.1016, -0.1808, -0.1716, -0.1833, -0.1960, -0.2273,\n",
       "                        -0.1001, -0.1193, -0.1488, -0.1638, -0.1276, -0.2316, -0.2589, -0.1435,\n",
       "                        -0.1781, -0.3429, -0.2241, -0.2036, -0.1733, -0.1199, -0.1492, -0.1610,\n",
       "                        -0.1196, -0.1533, -0.1891, -0.1601, -0.1472, -0.1314, -0.1426, -0.1646,\n",
       "                        -0.1663, -0.2009, -0.1791, -0.1213, -0.1099, -0.1560, -0.2467, -0.3079,\n",
       "                        -0.1953, -0.3118, -0.1567, -0.1515, -0.1611, -0.2071, -0.1687, -0.1987,\n",
       "                        -0.1037, -0.1463, -0.2241, -0.2268, -0.1816, -0.1740, -0.1992, -0.1874,\n",
       "                        -0.1706, -0.1522, -0.1340, -0.1204, -0.2538, -0.1317, -0.1402, -0.2425,\n",
       "                        -0.1778, -0.1304, -0.2280, -0.1112, -0.2115, -0.1517, -0.0967, -0.1164,\n",
       "                        -0.3150, -0.1064, -0.1150, -0.2670, -0.1412, -0.2012, -0.1155, -0.1760]), max_val=tensor([0.1815, 0.1425, 0.2487, 0.1070, 0.1977, 0.2044, 0.1621, 0.1703, 0.1824,\n",
       "                        0.1290, 0.1317, 0.2038, 0.1961, 0.2382, 0.1497, 0.1262, 0.1484, 0.2281,\n",
       "                        0.1499, 0.1528, 0.4097, 0.1813, 0.1093, 0.3387, 0.1203, 0.1391, 0.1961,\n",
       "                        0.1891, 0.1462, 0.1478, 0.1631, 0.2166, 0.1598, 0.1490, 0.1314, 0.1488,\n",
       "                        0.1762, 0.0815, 0.2714, 0.1434, 0.1199, 0.0694, 0.1221, 0.1528, 0.1644,\n",
       "                        0.1718, 0.2563, 0.2055, 0.1284, 0.3067, 0.1490, 0.2425, 0.1247, 0.0991,\n",
       "                        0.1824, 0.1112, 0.1225, 0.1881, 0.1844, 0.2252, 0.1184, 0.1965, 0.2515,\n",
       "                        0.2020, 0.1904, 0.2887, 0.1567, 0.1247, 0.1243, 0.1598, 0.2085, 0.3374,\n",
       "                        0.3135, 0.1059, 0.0984, 0.1879, 0.1491, 0.1095, 0.1602, 0.2526, 0.1295,\n",
       "                        0.1367, 0.2032, 0.1199, 0.1793, 0.1158, 0.1324, 0.2197, 0.1111, 0.1963,\n",
       "                        0.1849, 0.3555, 0.0971, 0.1626, 0.2260, 0.1790, 0.1301, 0.1420, 0.1464,\n",
       "                        0.1742, 0.2926, 0.1397, 0.1774, 0.1462, 0.3319, 0.1704, 0.1939, 0.1681,\n",
       "                        0.1923, 0.0966, 0.1803, 0.1041, 0.4272, 0.3007, 0.2371, 0.1638, 0.2106,\n",
       "                        0.1844, 0.1543, 0.1204, 0.1140, 0.1473, 0.1766, 0.1728, 0.1521, 0.3781,\n",
       "                        0.1513, 0.2531])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0039, 0.0009, 0.0013, 0.0025, 0.0013, 0.0012, 0.0014, 0.0031, 0.0013,\n",
       "                        0.0030, 0.0042, 0.0022, 0.0027, 0.0011, 0.0029, 0.0020, 0.0019, 0.0014,\n",
       "                        0.0032, 0.0007, 0.0033, 0.0007, 0.0035, 0.0008, 0.0027, 0.0005, 0.0017,\n",
       "                        0.0039, 0.0024, 0.0036, 0.0017, 0.0007, 0.0037, 0.0026, 0.0029, 0.0027,\n",
       "                        0.0018, 0.0025, 0.0025, 0.0028, 0.0017, 0.0032, 0.0023, 0.0023, 0.0033,\n",
       "                        0.0032, 0.0033, 0.0016, 0.0005, 0.0019, 0.0036, 0.0010, 0.0011, 0.0039,\n",
       "                        0.0015, 0.0032, 0.0021, 0.0041, 0.0010, 0.0021, 0.0007, 0.0022, 0.0013,\n",
       "                        0.0009, 0.0006, 0.0008, 0.0012, 0.0011, 0.0013, 0.0007, 0.0025, 0.0011,\n",
       "                        0.0010, 0.0009, 0.0018, 0.0010, 0.0006, 0.0006, 0.0037, 0.0018, 0.0021,\n",
       "                        0.0013, 0.0010, 0.0023, 0.0015, 0.0061, 0.0018, 0.0018, 0.0014, 0.0012,\n",
       "                        0.0030, 0.0019, 0.0020, 0.0013, 0.0030, 0.0013, 0.0011, 0.0022, 0.0010,\n",
       "                        0.0025, 0.0022, 0.0032, 0.0020, 0.0020, 0.0038, 0.0015, 0.0013, 0.0010,\n",
       "                        0.0007, 0.0015, 0.0010, 0.0014, 0.0033, 0.0007, 0.0034, 0.0009, 0.0021,\n",
       "                        0.0015, 0.0009, 0.0024, 0.0006, 0.0036, 0.0014, 0.0003, 0.0017, 0.0009,\n",
       "                        0.0061, 0.0011]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.5049, -0.1092, -0.1662, -0.1370, -0.1706, -0.1323, -0.1239, -0.4003,\n",
       "                          -0.1302, -0.3853, -0.5411, -0.2874, -0.3417, -0.1450, -0.2888, -0.2552,\n",
       "                          -0.2425, -0.1343, -0.2855, -0.0819, -0.2737, -0.0809, -0.4490, -0.1088,\n",
       "                          -0.2644, -0.0501, -0.2120, -0.4986, -0.2888, -0.3394, -0.1519, -0.0823,\n",
       "                          -0.4713, -0.3328, -0.3686, -0.3424, -0.2303, -0.1098, -0.3179, -0.2730,\n",
       "                          -0.2120, -0.4054, -0.1670, -0.2891, -0.4244, -0.4070, -0.4182, -0.1601,\n",
       "                          -0.0597, -0.2393, -0.1976, -0.1288, -0.1435, -0.4983, -0.1651, -0.4034,\n",
       "                          -0.1369, -0.3776, -0.1187, -0.2701, -0.0856, -0.2813, -0.1613, -0.1104,\n",
       "                          -0.0764, -0.1011, -0.1520, -0.1464, -0.1392, -0.0694, -0.2552, -0.1120,\n",
       "                          -0.1162, -0.1129, -0.2119, -0.1095, -0.0767, -0.0628, -0.4686, -0.2260,\n",
       "                          -0.1931, -0.1607, -0.1306, -0.2978, -0.1900, -0.7780, -0.2345, -0.1900,\n",
       "                          -0.1422, -0.1380, -0.3817, -0.1018, -0.2581, -0.1616, -0.1202, -0.1344,\n",
       "                          -0.1437, -0.2811, -0.1235, -0.3177, -0.2854, -0.4144, -0.1271, -0.2559,\n",
       "                          -0.2951, -0.1886, -0.1679, -0.1265, -0.0903, -0.1882, -0.1230, -0.1843,\n",
       "                          -0.4173, -0.0901, -0.4363, -0.0579, -0.1736, -0.1859, -0.0787, -0.3040,\n",
       "                          -0.0773, -0.4582, -0.1831, -0.0416, -0.2152, -0.1092, -0.1115, -0.1401]), max_val=tensor([0.2576, 0.0534, 0.0677, 0.3169, 0.1676, 0.1479, 0.1722, 0.2306, 0.1631,\n",
       "                          0.2489, 0.2274, 0.1814, 0.3375, 0.1424, 0.3741, 0.2055, 0.1436, 0.1779,\n",
       "                          0.4041, 0.0873, 0.4226, 0.0850, 0.2857, 0.1079, 0.3390, 0.0630, 0.2042,\n",
       "                          0.3856, 0.3079, 0.4592, 0.2166, 0.0876, 0.1603, 0.3129, 0.2336, 0.1168,\n",
       "                          0.1460, 0.3215, 0.1415, 0.3542, 0.1801, 0.1401, 0.2893, 0.1663, 0.2855,\n",
       "                          0.3893, 0.3025, 0.2068, 0.0562, 0.2095, 0.4541, 0.1049, 0.0926, 0.1920,\n",
       "                          0.1862, 0.2340, 0.2665, 0.5243, 0.1245, 0.2490, 0.0739, 0.1856, 0.1480,\n",
       "                          0.0918, 0.0602, 0.0723, 0.1390, 0.0751, 0.1631, 0.0917, 0.3152, 0.1339,\n",
       "                          0.1283, 0.1191, 0.2304, 0.1310, 0.0679, 0.0724, 0.2046, 0.1957, 0.2626,\n",
       "                          0.0794, 0.0742, 0.1542, 0.1648, 0.4228, 0.0886, 0.2293, 0.1728, 0.1480,\n",
       "                          0.2194, 0.2471, 0.1463, 0.1253, 0.3866, 0.1702, 0.1015, 0.2258, 0.1282,\n",
       "                          0.2015, 0.1422, 0.2858, 0.2539, 0.1409, 0.4774, 0.1225, 0.1313, 0.0975,\n",
       "                          0.0647, 0.1560, 0.1011, 0.0909, 0.2036, 0.0839, 0.3166, 0.1149, 0.2713,\n",
       "                          0.1584, 0.1139, 0.1636, 0.0634, 0.2117, 0.1112, 0.0367, 0.0788, 0.0779,\n",
       "                          0.7802, 0.1012])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0020, 0.0012, 0.0013, 0.0013, 0.0016, 0.0016, 0.0009, 0.0021, 0.0017,\n",
       "                      0.0023, 0.0010, 0.0019, 0.0016, 0.0016, 0.0011, 0.0025, 0.0021, 0.0012,\n",
       "                      0.0016, 0.0015, 0.0019, 0.0011, 0.0019, 0.0012, 0.0010, 0.0017, 0.0019,\n",
       "                      0.0022, 0.0020, 0.0016, 0.0012, 0.0018, 0.0008, 0.0015, 0.0012, 0.0018,\n",
       "                      0.0022, 0.0011, 0.0018, 0.0015, 0.0025, 0.0016, 0.0021, 0.0019, 0.0024,\n",
       "                      0.0019, 0.0024, 0.0021, 0.0013, 0.0026, 0.0035, 0.0027, 0.0014, 0.0021,\n",
       "                      0.0030, 0.0013, 0.0014, 0.0022, 0.0015, 0.0016, 0.0019, 0.0020, 0.0021,\n",
       "                      0.0021, 0.0009, 0.0012, 0.0013, 0.0014, 0.0016, 0.0016, 0.0023, 0.0023,\n",
       "                      0.0014, 0.0012, 0.0016, 0.0021, 0.0014, 0.0015, 0.0013, 0.0023, 0.0010,\n",
       "                      0.0014, 0.0019, 0.0014, 0.0012, 0.0017, 0.0013, 0.0012, 0.0033, 0.0022,\n",
       "                      0.0013, 0.0011, 0.0016, 0.0022, 0.0016, 0.0025, 0.0014, 0.0008, 0.0030,\n",
       "                      0.0014, 0.0017, 0.0016, 0.0023, 0.0033, 0.0013, 0.0011, 0.0019, 0.0013,\n",
       "                      0.0013, 0.0018, 0.0018, 0.0013, 0.0016, 0.0013, 0.0016, 0.0010, 0.0014,\n",
       "                      0.0014, 0.0016, 0.0016, 0.0014, 0.0011, 0.0010, 0.0016, 0.0014, 0.0014,\n",
       "                      0.0013, 0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1965, -0.1295, -0.1660, -0.1180, -0.1570, -0.2016, -0.1172, -0.1315,\n",
       "                        -0.1831, -0.2895, -0.1119, -0.2378, -0.1185, -0.2037, -0.1379, -0.2682,\n",
       "                        -0.2703, -0.1522, -0.0781, -0.1787, -0.1569, -0.1149, -0.2264, -0.1099,\n",
       "                        -0.1044, -0.2117, -0.1435, -0.1226, -0.1487, -0.1976, -0.1597, -0.2250,\n",
       "                        -0.0695, -0.0905, -0.1495, -0.1775, -0.2792, -0.1116, -0.1741, -0.1118,\n",
       "                        -0.3239, -0.1974, -0.2380, -0.2484, -0.2481, -0.2380, -0.3023, -0.2628,\n",
       "                        -0.1715, -0.3342, -0.2922, -0.3462, -0.1852, -0.2128, -0.3853, -0.1344,\n",
       "                        -0.1811, -0.2854, -0.1685, -0.2017, -0.1698, -0.2505, -0.2439, -0.2504,\n",
       "                        -0.1135, -0.1067, -0.1181, -0.1772, -0.2013, -0.0899, -0.2422, -0.2899,\n",
       "                        -0.1519, -0.1538, -0.1986, -0.2641, -0.1132, -0.1862, -0.1629, -0.2854,\n",
       "                        -0.1100, -0.1360, -0.2197, -0.1729, -0.1254, -0.1342, -0.1554, -0.1360,\n",
       "                        -0.2982, -0.1248, -0.1666, -0.0913, -0.2074, -0.2205, -0.1716, -0.3157,\n",
       "                        -0.1737, -0.0976, -0.2232, -0.1705, -0.2231, -0.2016, -0.2414, -0.3190,\n",
       "                        -0.1672, -0.1185, -0.1413, -0.1562, -0.1697, -0.2111, -0.2306, -0.0937,\n",
       "                        -0.1775, -0.0891, -0.0928, -0.1308, -0.1640, -0.1772, -0.1232, -0.2045,\n",
       "                        -0.1723, -0.1121, -0.1189, -0.1851, -0.1469, -0.1752, -0.1417, -0.1682]), max_val=tensor([0.2546, 0.1527, 0.1392, 0.1682, 0.2018, 0.1603, 0.1049, 0.2657, 0.2204,\n",
       "                        0.2890, 0.1299, 0.2122, 0.2034, 0.1184, 0.1406, 0.3170, 0.2168, 0.1528,\n",
       "                        0.2086, 0.1851, 0.2368, 0.1382, 0.2421, 0.1538, 0.1236, 0.2172, 0.2413,\n",
       "                        0.2775, 0.2587, 0.1999, 0.1315, 0.1718, 0.1056, 0.1855, 0.1386, 0.2243,\n",
       "                        0.2171, 0.1339, 0.2326, 0.1949, 0.3053, 0.2041, 0.2640, 0.2199, 0.3051,\n",
       "                        0.2476, 0.2704, 0.2582, 0.1578, 0.1730, 0.4387, 0.2206, 0.1493, 0.2676,\n",
       "                        0.2860, 0.1698, 0.1397, 0.2753, 0.1935, 0.1981, 0.2435, 0.2513, 0.2619,\n",
       "                        0.2647, 0.1153, 0.1494, 0.1662, 0.1677, 0.1891, 0.1981, 0.2888, 0.2195,\n",
       "                        0.1814, 0.1562, 0.1778, 0.1999, 0.1724, 0.1852, 0.1609, 0.2921, 0.1239,\n",
       "                        0.1787, 0.2380, 0.1255, 0.1470, 0.2192, 0.1673, 0.1533, 0.4250, 0.2827,\n",
       "                        0.1452, 0.1432, 0.1843, 0.2840, 0.1986, 0.1284, 0.1506, 0.1004, 0.3752,\n",
       "                        0.1813, 0.1690, 0.1767, 0.2874, 0.4133, 0.1613, 0.1346, 0.2458, 0.1619,\n",
       "                        0.1328, 0.2230, 0.2006, 0.1622, 0.2002, 0.1676, 0.1972, 0.1207, 0.1759,\n",
       "                        0.1627, 0.1971, 0.1536, 0.1815, 0.1442, 0.1218, 0.1982, 0.1808, 0.1560,\n",
       "                        0.1601, 0.1660])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0007, 0.0012, 0.0014, 0.0011, 0.0010, 0.0010, 0.0010, 0.0009,\n",
       "                      0.0012, 0.0012, 0.0008, 0.0011, 0.0012, 0.0010, 0.0010, 0.0008, 0.0014,\n",
       "                      0.0008, 0.0009, 0.0011, 0.0014, 0.0008, 0.0020, 0.0016, 0.0013, 0.0011,\n",
       "                      0.0014, 0.0009, 0.0012, 0.0009, 0.0017, 0.0012, 0.0014, 0.0019, 0.0025,\n",
       "                      0.0012, 0.0009, 0.0010, 0.0012, 0.0009, 0.0010, 0.0018, 0.0008, 0.0010,\n",
       "                      0.0008, 0.0010, 0.0008, 0.0013, 0.0014, 0.0010, 0.0020, 0.0009, 0.0011,\n",
       "                      0.0010, 0.0012, 0.0007, 0.0012, 0.0015, 0.0011, 0.0012, 0.0012, 0.0016,\n",
       "                      0.0011, 0.0009, 0.0007, 0.0009, 0.0010, 0.0010, 0.0016, 0.0016, 0.0017,\n",
       "                      0.0013, 0.0012, 0.0010, 0.0012, 0.0017, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "                      0.0009, 0.0006, 0.0011, 0.0013, 0.0008, 0.0012, 0.0012, 0.0019, 0.0008,\n",
       "                      0.0012, 0.0013, 0.0016, 0.0008, 0.0012, 0.0013, 0.0010, 0.0008, 0.0014,\n",
       "                      0.0013, 0.0011, 0.0012, 0.0009, 0.0010, 0.0028, 0.0012, 0.0013, 0.0007,\n",
       "                      0.0012, 0.0014, 0.0009, 0.0008, 0.0014, 0.0018, 0.0017, 0.0012, 0.0013,\n",
       "                      0.0009, 0.0018, 0.0009, 0.0011, 0.0006, 0.0011, 0.0008, 0.0015, 0.0015,\n",
       "                      0.0016, 0.0015]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1116, -0.0808, -0.1133, -0.0929, -0.1018, -0.1296, -0.1028, -0.0810,\n",
       "                        -0.1084, -0.1587, -0.1318, -0.0952, -0.1016, -0.0871, -0.1015, -0.1271,\n",
       "                        -0.0686, -0.0832, -0.0904, -0.1190, -0.1372, -0.1702, -0.0798, -0.2484,\n",
       "                        -0.2094, -0.1457, -0.0743, -0.0692, -0.1013, -0.0968, -0.1187, -0.2157,\n",
       "                        -0.1132, -0.1465, -0.2429, -0.3256, -0.1586, -0.1000, -0.1220, -0.1242,\n",
       "                        -0.0980, -0.1241, -0.1337, -0.1013, -0.1201, -0.1009, -0.0861, -0.1061,\n",
       "                        -0.1652, -0.1811, -0.1298, -0.2531, -0.1201, -0.1316, -0.0978, -0.1539,\n",
       "                        -0.0799, -0.1230, -0.1410, -0.1466, -0.1537, -0.1507, -0.2016, -0.0909,\n",
       "                        -0.1214, -0.0636, -0.0841, -0.1313, -0.1048, -0.1309, -0.1304, -0.2153,\n",
       "                        -0.1624, -0.1594, -0.0764, -0.0850, -0.1540, -0.1008, -0.1253, -0.2561,\n",
       "                        -0.1217, -0.1106, -0.0746, -0.1399, -0.0931, -0.1027, -0.1302, -0.1482,\n",
       "                        -0.2380, -0.0981, -0.1481, -0.1441, -0.1831, -0.0870, -0.0896, -0.1540,\n",
       "                        -0.0869, -0.0982, -0.1753, -0.1652, -0.1383, -0.1152, -0.1149, -0.1193,\n",
       "                        -0.1531, -0.1537, -0.1208, -0.0812, -0.1033, -0.1742, -0.1114, -0.0925,\n",
       "                        -0.1782, -0.1506, -0.2140, -0.0824, -0.1642, -0.0934, -0.1734, -0.1104,\n",
       "                        -0.1404, -0.0788, -0.1462, -0.0918, -0.1883, -0.1466, -0.0965, -0.1578]), max_val=tensor([0.1359, 0.0945, 0.1542, 0.1781, 0.1437, 0.1272, 0.1312, 0.1282, 0.1148,\n",
       "                        0.1411, 0.1566, 0.1062, 0.1440, 0.1473, 0.1261, 0.1120, 0.1042, 0.1834,\n",
       "                        0.0974, 0.1095, 0.1178, 0.1731, 0.0962, 0.2553, 0.1554, 0.1643, 0.1435,\n",
       "                        0.1787, 0.1118, 0.1544, 0.1176, 0.1894, 0.1570, 0.1721, 0.1120, 0.1716,\n",
       "                        0.1129, 0.1090, 0.1167, 0.1547, 0.1088, 0.1198, 0.2306, 0.0757, 0.1279,\n",
       "                        0.0884, 0.1274, 0.1017, 0.0984, 0.1544, 0.1179, 0.2340, 0.1102, 0.1343,\n",
       "                        0.1277, 0.1302, 0.0835, 0.1587, 0.1940, 0.1085, 0.1098, 0.1323, 0.1728,\n",
       "                        0.1408, 0.1108, 0.0858, 0.1173, 0.1013, 0.1279, 0.1987, 0.1990, 0.1645,\n",
       "                        0.1430, 0.1303, 0.1321, 0.1524, 0.2192, 0.1207, 0.0973, 0.2047, 0.1250,\n",
       "                        0.1125, 0.0698, 0.1171, 0.1649, 0.1055, 0.1505, 0.1273, 0.0961, 0.1063,\n",
       "                        0.1521, 0.1710, 0.2038, 0.1029, 0.1462, 0.1595, 0.1275, 0.0921, 0.1037,\n",
       "                        0.1188, 0.1071, 0.1548, 0.0934, 0.1214, 0.3557, 0.1004, 0.1639, 0.0951,\n",
       "                        0.1475, 0.1463, 0.1049, 0.1077, 0.1451, 0.2250, 0.1772, 0.1487, 0.1701,\n",
       "                        0.1206, 0.2261, 0.0988, 0.0808, 0.0743, 0.1306, 0.1016, 0.1504, 0.1889,\n",
       "                        0.2082, 0.1895])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0022, 0.0010, 0.0014, 0.0009, 0.0010, 0.0010, 0.0009, 0.0010,\n",
       "                      0.0009, 0.0020, 0.0021, 0.0012, 0.0009, 0.0016, 0.0012, 0.0012, 0.0010,\n",
       "                      0.0021, 0.0010, 0.0010, 0.0011, 0.0009, 0.0011, 0.0008, 0.0011, 0.0011,\n",
       "                      0.0013, 0.0028, 0.0008, 0.0011, 0.0013, 0.0010, 0.0011, 0.0018, 0.0011,\n",
       "                      0.0011, 0.0012, 0.0010, 0.0012, 0.0011, 0.0009, 0.0011, 0.0012, 0.0012,\n",
       "                      0.0009, 0.0017, 0.0009, 0.0009, 0.0013, 0.0011, 0.0013, 0.0011, 0.0017,\n",
       "                      0.0015, 0.0013, 0.0015, 0.0013, 0.0013, 0.0011, 0.0013, 0.0011, 0.0016,\n",
       "                      0.0016, 0.0013, 0.0007, 0.0013, 0.0022, 0.0016, 0.0015, 0.0012, 0.0022,\n",
       "                      0.0012, 0.0011, 0.0016, 0.0016, 0.0011, 0.0012, 0.0017, 0.0009, 0.0012,\n",
       "                      0.0020, 0.0020, 0.0017, 0.0011, 0.0014, 0.0013, 0.0009, 0.0010, 0.0010,\n",
       "                      0.0010, 0.0010, 0.0014, 0.0017, 0.0020, 0.0014, 0.0011, 0.0009, 0.0012,\n",
       "                      0.0012, 0.0019, 0.0010, 0.0012, 0.0010, 0.0012, 0.0010, 0.0016, 0.0014,\n",
       "                      0.0010, 0.0018, 0.0011, 0.0023, 0.0017, 0.0011, 0.0010, 0.0013, 0.0020,\n",
       "                      0.0008, 0.0012, 0.0008, 0.0009, 0.0014, 0.0012, 0.0014, 0.0019, 0.0013,\n",
       "                      0.0012, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0012, 0.0016, 0.0011,\n",
       "                      0.0012, 0.0010, 0.0016, 0.0007, 0.0012, 0.0015, 0.0018, 0.0011, 0.0014,\n",
       "                      0.0007, 0.0009, 0.0014, 0.0014, 0.0011, 0.0010, 0.0031, 0.0011, 0.0009,\n",
       "                      0.0011, 0.0020, 0.0016, 0.0013, 0.0015, 0.0020, 0.0009, 0.0011, 0.0016,\n",
       "                      0.0019, 0.0015, 0.0011, 0.0019, 0.0009, 0.0009, 0.0009, 0.0014, 0.0021,\n",
       "                      0.0011, 0.0013, 0.0008, 0.0017, 0.0010, 0.0008, 0.0019, 0.0015, 0.0010,\n",
       "                      0.0011, 0.0012, 0.0011, 0.0014, 0.0017, 0.0010, 0.0010, 0.0013, 0.0008,\n",
       "                      0.0017, 0.0012, 0.0009, 0.0010, 0.0015, 0.0012, 0.0010, 0.0014, 0.0010,\n",
       "                      0.0012, 0.0016, 0.0013, 0.0010, 0.0019, 0.0012, 0.0011, 0.0013, 0.0014,\n",
       "                      0.0010, 0.0026, 0.0022, 0.0011, 0.0024, 0.0015, 0.0017, 0.0012, 0.0009,\n",
       "                      0.0010, 0.0015, 0.0017, 0.0009, 0.0011, 0.0009, 0.0008, 0.0012, 0.0019,\n",
       "                      0.0015, 0.0012, 0.0014, 0.0011, 0.0012, 0.0013, 0.0012, 0.0011, 0.0008,\n",
       "                      0.0016, 0.0016, 0.0017, 0.0017, 0.0009, 0.0009, 0.0009, 0.0011, 0.0009,\n",
       "                      0.0018, 0.0010, 0.0015, 0.0012, 0.0022, 0.0014, 0.0019, 0.0009, 0.0009,\n",
       "                      0.0011, 0.0012, 0.0012, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.1744, -0.1917, -0.1208, -0.1749, -0.0923, -0.0972, -0.1105, -0.0928,\n",
       "                        -0.1315, -0.1043, -0.2508, -0.0827, -0.1523, -0.1069, -0.2020, -0.0873,\n",
       "                        -0.0831, -0.1062, -0.1045, -0.1274, -0.0838, -0.0853, -0.1060, -0.0987,\n",
       "                        -0.1084, -0.1125, -0.1375, -0.1290, -0.1282, -0.0837, -0.1125, -0.1205,\n",
       "                        -0.1230, -0.1330, -0.1624, -0.1213, -0.1406, -0.1109, -0.1223, -0.0807,\n",
       "                        -0.1426, -0.0943, -0.1094, -0.1249, -0.1184, -0.1176, -0.1289, -0.0926,\n",
       "                        -0.1164, -0.1507, -0.0891, -0.1336, -0.0801, -0.1789, -0.0905, -0.1603,\n",
       "                        -0.1918, -0.1626, -0.1104, -0.0801, -0.1180, -0.1027, -0.1870, -0.1201,\n",
       "                        -0.1284, -0.0936, -0.0853, -0.1894, -0.2043, -0.0835, -0.1531, -0.1040,\n",
       "                        -0.1259, -0.1128, -0.0930, -0.0999, -0.1470, -0.1544, -0.1467, -0.1151,\n",
       "                        -0.0998, -0.2508, -0.2505, -0.1370, -0.1225, -0.1531, -0.0791, -0.1146,\n",
       "                        -0.1286, -0.0966, -0.1233, -0.1308, -0.1805, -0.1175, -0.0893, -0.1299,\n",
       "                        -0.1372, -0.1140, -0.1495, -0.1505, -0.1225, -0.0925, -0.0932, -0.1034,\n",
       "                        -0.1145, -0.1283, -0.2071, -0.1734, -0.1105, -0.1672, -0.1412, -0.3003,\n",
       "                        -0.2129, -0.1139, -0.0888, -0.1168, -0.1098, -0.0928, -0.1588, -0.1042,\n",
       "                        -0.1097, -0.0765, -0.1100, -0.1357, -0.2369, -0.1675, -0.0918, -0.0978,\n",
       "                        -0.1106, -0.0929, -0.1245, -0.1256, -0.1187, -0.1279, -0.1176, -0.1546,\n",
       "                        -0.1203, -0.1797, -0.0895, -0.1195, -0.0965, -0.2019, -0.1177, -0.1766,\n",
       "                        -0.0950, -0.1158, -0.1567, -0.1765, -0.1362, -0.1341, -0.3920, -0.1381,\n",
       "                        -0.0663, -0.1171, -0.1214, -0.1006, -0.1694, -0.1921, -0.2561, -0.1126,\n",
       "                        -0.1176, -0.1500, -0.2492, -0.1622, -0.1324, -0.1116, -0.0958, -0.1206,\n",
       "                        -0.1079, -0.1183, -0.1412, -0.1345, -0.1494, -0.0795, -0.2175, -0.0878,\n",
       "                        -0.0924, -0.1977, -0.0931, -0.1013, -0.1291, -0.0950, -0.0842, -0.1519,\n",
       "                        -0.2224, -0.0970, -0.0986, -0.0906, -0.0846, -0.1004, -0.1469, -0.0774,\n",
       "                        -0.0898, -0.1000, -0.0793, -0.1267, -0.1751, -0.1263, -0.1062, -0.2059,\n",
       "                        -0.1091, -0.1272, -0.1573, -0.0866, -0.1361, -0.1719, -0.0992, -0.1098,\n",
       "                        -0.1178, -0.1137, -0.1006, -0.3023, -0.1883, -0.1904, -0.1168, -0.1071,\n",
       "                        -0.1278, -0.0809, -0.2115, -0.1097, -0.1162, -0.1153, -0.0769, -0.1329,\n",
       "                        -0.1568, -0.1390, -0.1522, -0.1365, -0.1367, -0.1574, -0.1676, -0.0935,\n",
       "                        -0.1012, -0.0824, -0.1366, -0.2005, -0.2210, -0.1124, -0.1138, -0.1131,\n",
       "                        -0.0886, -0.1062, -0.1059, -0.0923, -0.1137, -0.0899, -0.1386, -0.1100,\n",
       "                        -0.0814, -0.1577, -0.1061, -0.1123, -0.0968, -0.1582, -0.1539, -0.0842]), max_val=tensor([0.1411, 0.2802, 0.1251, 0.1105, 0.1115, 0.1328, 0.1240, 0.1190, 0.1089,\n",
       "                        0.1113, 0.1318, 0.2667, 0.1053, 0.1110, 0.0985, 0.1523, 0.1570, 0.1220,\n",
       "                        0.2658, 0.1226, 0.1231, 0.1337, 0.1173, 0.1382, 0.0898, 0.1456, 0.1444,\n",
       "                        0.1639, 0.3578, 0.1038, 0.1418, 0.1661, 0.1134, 0.1438, 0.2334, 0.1340,\n",
       "                        0.1055, 0.1473, 0.1269, 0.1498, 0.1206, 0.1114, 0.1339, 0.1568, 0.1582,\n",
       "                        0.0923, 0.2135, 0.1100, 0.1162, 0.1674, 0.1353, 0.1687, 0.1447, 0.2178,\n",
       "                        0.1879, 0.1076, 0.1198, 0.1422, 0.1658, 0.1340, 0.1646, 0.1398, 0.2040,\n",
       "                        0.2047, 0.1683, 0.0879, 0.1668, 0.2737, 0.1487, 0.1846, 0.1089, 0.2818,\n",
       "                        0.1564, 0.1372, 0.1995, 0.2012, 0.1450, 0.1535, 0.2198, 0.1174, 0.1510,\n",
       "                        0.2313, 0.1908, 0.2169, 0.1335, 0.1832, 0.1590, 0.0940, 0.1324, 0.1220,\n",
       "                        0.1330, 0.1019, 0.1562, 0.2122, 0.2488, 0.1833, 0.1258, 0.1051, 0.1261,\n",
       "                        0.1213, 0.2358, 0.1292, 0.1521, 0.1279, 0.1527, 0.1033, 0.1529, 0.1193,\n",
       "                        0.1219, 0.2264, 0.1230, 0.1065, 0.2012, 0.1339, 0.1295, 0.1675, 0.2558,\n",
       "                        0.1070, 0.1448, 0.1077, 0.0982, 0.1750, 0.1511, 0.1754, 0.1982, 0.1104,\n",
       "                        0.1475, 0.1356, 0.1117, 0.1272, 0.1215, 0.1113, 0.1573, 0.1978, 0.1337,\n",
       "                        0.1084, 0.1274, 0.2084, 0.0810, 0.1494, 0.1960, 0.2260, 0.1439, 0.1114,\n",
       "                        0.0913, 0.0981, 0.1747, 0.1381, 0.1413, 0.0971, 0.1114, 0.1200, 0.1155,\n",
       "                        0.1363, 0.2551, 0.2068, 0.1509, 0.1356, 0.2208, 0.1179, 0.1435, 0.1993,\n",
       "                        0.1428, 0.1893, 0.1366, 0.2467, 0.1152, 0.0913, 0.1205, 0.1775, 0.2683,\n",
       "                        0.1357, 0.1646, 0.1026, 0.1132, 0.1301, 0.0968, 0.2446, 0.1957, 0.1322,\n",
       "                        0.1417, 0.1581, 0.1414, 0.1833, 0.1485, 0.1304, 0.1271, 0.1640, 0.1048,\n",
       "                        0.2210, 0.1507, 0.1165, 0.1330, 0.1856, 0.1478, 0.0972, 0.1558, 0.1126,\n",
       "                        0.1494, 0.1363, 0.1637, 0.0951, 0.2387, 0.1495, 0.1440, 0.1241, 0.1728,\n",
       "                        0.1319, 0.3249, 0.2808, 0.1423, 0.1490, 0.1481, 0.2140, 0.1503, 0.1167,\n",
       "                        0.1054, 0.1861, 0.2218, 0.1058, 0.1353, 0.0919, 0.0983, 0.1561, 0.2370,\n",
       "                        0.1878, 0.0952, 0.1834, 0.1440, 0.1350, 0.1404, 0.1516, 0.1443, 0.1061,\n",
       "                        0.1981, 0.1436, 0.1812, 0.2149, 0.0942, 0.1023, 0.1198, 0.1406, 0.1092,\n",
       "                        0.2239, 0.1303, 0.1955, 0.1506, 0.2853, 0.1798, 0.2430, 0.1152, 0.1047,\n",
       "                        0.1349, 0.1468, 0.1510, 0.1100])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0010, 0.0009, 0.0007, 0.0015, 0.0011, 0.0016, 0.0013, 0.0008, 0.0012,\n",
       "                      0.0010, 0.0010, 0.0011, 0.0011, 0.0018, 0.0013, 0.0008, 0.0010, 0.0012,\n",
       "                      0.0009, 0.0009, 0.0017, 0.0007, 0.0014, 0.0013, 0.0009, 0.0019, 0.0010,\n",
       "                      0.0016, 0.0017, 0.0017, 0.0010, 0.0009, 0.0008, 0.0012, 0.0008, 0.0009,\n",
       "                      0.0018, 0.0020, 0.0008, 0.0014, 0.0010, 0.0012, 0.0012, 0.0008, 0.0011,\n",
       "                      0.0011, 0.0009, 0.0014, 0.0013, 0.0020, 0.0017, 0.0016, 0.0008, 0.0007,\n",
       "                      0.0010, 0.0008, 0.0017, 0.0009, 0.0010, 0.0010, 0.0015, 0.0009, 0.0010,\n",
       "                      0.0008, 0.0014, 0.0010, 0.0011, 0.0025, 0.0019, 0.0011, 0.0015, 0.0013,\n",
       "                      0.0022, 0.0009, 0.0020, 0.0007, 0.0013, 0.0008, 0.0009, 0.0008, 0.0014,\n",
       "                      0.0010, 0.0010, 0.0009, 0.0011, 0.0011, 0.0011, 0.0014, 0.0015, 0.0009,\n",
       "                      0.0013, 0.0017, 0.0020, 0.0013, 0.0007, 0.0013, 0.0010, 0.0010, 0.0010,\n",
       "                      0.0019, 0.0012, 0.0011, 0.0010, 0.0015, 0.0012, 0.0019, 0.0010, 0.0016,\n",
       "                      0.0010, 0.0011, 0.0014, 0.0010, 0.0011, 0.0010, 0.0010, 0.0012, 0.0015,\n",
       "                      0.0016, 0.0021, 0.0008, 0.0018, 0.0016, 0.0011, 0.0020, 0.0016, 0.0007,\n",
       "                      0.0007, 0.0014, 0.0016, 0.0021, 0.0016, 0.0015, 0.0009, 0.0012, 0.0014,\n",
       "                      0.0012, 0.0012, 0.0017, 0.0012, 0.0014, 0.0009, 0.0014, 0.0011, 0.0012,\n",
       "                      0.0008, 0.0022, 0.0009, 0.0012, 0.0026, 0.0008, 0.0018, 0.0015, 0.0017,\n",
       "                      0.0019, 0.0018, 0.0009, 0.0016, 0.0007, 0.0012, 0.0015, 0.0021, 0.0009,\n",
       "                      0.0014, 0.0006, 0.0016, 0.0014, 0.0012, 0.0019, 0.0011, 0.0010, 0.0011,\n",
       "                      0.0012, 0.0007, 0.0022, 0.0010, 0.0017, 0.0008, 0.0010, 0.0011, 0.0014,\n",
       "                      0.0009, 0.0010, 0.0011, 0.0010, 0.0016, 0.0008, 0.0009, 0.0019, 0.0008,\n",
       "                      0.0012, 0.0008, 0.0014, 0.0010, 0.0011, 0.0013, 0.0010, 0.0007, 0.0014,\n",
       "                      0.0009, 0.0014, 0.0009, 0.0020, 0.0010, 0.0012, 0.0021, 0.0016, 0.0009,\n",
       "                      0.0019, 0.0014, 0.0013, 0.0011, 0.0011, 0.0016, 0.0012, 0.0012, 0.0018,\n",
       "                      0.0019, 0.0008, 0.0012, 0.0023, 0.0007, 0.0011, 0.0012, 0.0016, 0.0015,\n",
       "                      0.0011, 0.0014, 0.0019, 0.0013, 0.0015, 0.0011, 0.0007, 0.0013, 0.0008,\n",
       "                      0.0022, 0.0011, 0.0016, 0.0010, 0.0013, 0.0008, 0.0011, 0.0016, 0.0015,\n",
       "                      0.0011, 0.0010, 0.0018, 0.0016, 0.0010, 0.0017, 0.0010, 0.0011, 0.0017,\n",
       "                      0.0007, 0.0010, 0.0014, 0.0012]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0823, -0.0931, -0.0857, -0.0897, -0.0957, -0.1987, -0.1606, -0.0849,\n",
       "                        -0.0826, -0.0988, -0.1284, -0.0994, -0.0837, -0.1830, -0.1665, -0.0976,\n",
       "                        -0.1060, -0.1114, -0.1129, -0.1020, -0.1421, -0.0921, -0.1407, -0.1165,\n",
       "                        -0.1112, -0.2433, -0.1246, -0.1322, -0.2176, -0.1813, -0.0906, -0.1144,\n",
       "                        -0.0913, -0.1017, -0.0892, -0.0992, -0.1162, -0.2570, -0.0907, -0.1682,\n",
       "                        -0.1262, -0.1116, -0.1073, -0.0870, -0.1283, -0.1072, -0.0883, -0.1467,\n",
       "                        -0.0919, -0.1691, -0.1152, -0.1986, -0.0848, -0.0960, -0.0955, -0.1069,\n",
       "                        -0.0943, -0.1105, -0.0881, -0.1078, -0.1040, -0.1100, -0.0909, -0.1013,\n",
       "                        -0.1785, -0.0958, -0.1364, -0.2691, -0.2487, -0.1100, -0.1865, -0.1639,\n",
       "                        -0.1594, -0.1107, -0.1185, -0.0884, -0.1029, -0.0782, -0.0887, -0.0901,\n",
       "                        -0.0799, -0.0933, -0.0945, -0.0988, -0.1387, -0.1348, -0.0977, -0.1339,\n",
       "                        -0.1155, -0.0992, -0.1647, -0.1321, -0.1065, -0.1649, -0.0941, -0.1629,\n",
       "                        -0.1186, -0.1224, -0.1301, -0.2470, -0.1471, -0.0964, -0.1055, -0.1967,\n",
       "                        -0.1550, -0.2410, -0.1239, -0.1258, -0.1313, -0.1370, -0.1823, -0.1229,\n",
       "                        -0.0723, -0.1289, -0.1047, -0.1270, -0.0866, -0.0832, -0.2513, -0.0798,\n",
       "                        -0.1300, -0.2094, -0.1052, -0.1672, -0.1941, -0.0916, -0.0856, -0.1286,\n",
       "                        -0.1515, -0.2745, -0.1911, -0.1360, -0.0966, -0.1088, -0.0947, -0.1580,\n",
       "                        -0.1476, -0.1665, -0.1521, -0.0860, -0.0826, -0.1031, -0.1141, -0.1433,\n",
       "                        -0.0936, -0.2377, -0.0842, -0.1086, -0.3335, -0.0967, -0.1545, -0.1894,\n",
       "                        -0.2100, -0.1929, -0.2262, -0.1026, -0.1357, -0.0810, -0.1282, -0.1743,\n",
       "                        -0.2672, -0.1099, -0.1250, -0.0646, -0.1422, -0.1255, -0.1051, -0.1022,\n",
       "                        -0.1179, -0.1306, -0.1351, -0.1185, -0.0878, -0.1390, -0.0989, -0.2188,\n",
       "                        -0.0918, -0.1085, -0.1432, -0.1406, -0.0728, -0.1118, -0.1308, -0.1009,\n",
       "                        -0.0972, -0.1064, -0.1062, -0.1938, -0.0626, -0.1038, -0.1004, -0.1186,\n",
       "                        -0.0879, -0.0889, -0.1654, -0.1053, -0.0582, -0.1254, -0.0772, -0.1676,\n",
       "                        -0.0860, -0.1083, -0.0982, -0.1068, -0.1712, -0.1367, -0.1050, -0.2436,\n",
       "                        -0.0927, -0.0962, -0.1459, -0.1354, -0.1994, -0.1557, -0.1204, -0.2263,\n",
       "                        -0.1907, -0.1086, -0.1550, -0.2588, -0.0852, -0.0783, -0.1109, -0.1567,\n",
       "                        -0.1727, -0.1178, -0.1051, -0.2471, -0.1296, -0.0674, -0.1429, -0.0779,\n",
       "                        -0.1697, -0.0992, -0.2774, -0.1260, -0.1179, -0.1297, -0.0841, -0.1062,\n",
       "                        -0.0770, -0.1187, -0.1895, -0.1076, -0.0989, -0.2272, -0.1930, -0.1188,\n",
       "                        -0.1303, -0.1067, -0.1435, -0.1718, -0.0926, -0.0816, -0.1540, -0.0817]), max_val=tensor([0.1279, 0.1107, 0.0910, 0.1867, 0.1346, 0.0994, 0.1189, 0.1013, 0.1537,\n",
       "                        0.1228, 0.0917, 0.1351, 0.1394, 0.2256, 0.1360, 0.0866, 0.1255, 0.1522,\n",
       "                        0.1148, 0.1120, 0.2104, 0.0701, 0.1813, 0.1601, 0.1147, 0.1860, 0.1240,\n",
       "                        0.2073, 0.1428, 0.2187, 0.1254, 0.0944, 0.1037, 0.1506, 0.1049, 0.1138,\n",
       "                        0.2283, 0.1489, 0.1074, 0.1733, 0.1262, 0.1467, 0.1538, 0.1042, 0.1361,\n",
       "                        0.1337, 0.1100, 0.1764, 0.1610, 0.2591, 0.2134, 0.2083, 0.1011, 0.0813,\n",
       "                        0.1291, 0.0847, 0.2158, 0.1123, 0.1248, 0.1293, 0.1851, 0.1179, 0.1284,\n",
       "                        0.0825, 0.1505, 0.1258, 0.1429, 0.3220, 0.2350, 0.1419, 0.1138, 0.1162,\n",
       "                        0.2778, 0.0911, 0.2513, 0.0722, 0.1644, 0.0990, 0.1131, 0.1008, 0.1792,\n",
       "                        0.1319, 0.1221, 0.1124, 0.1258, 0.1230, 0.1421, 0.1744, 0.1942, 0.1187,\n",
       "                        0.1143, 0.2213, 0.2553, 0.1421, 0.0845, 0.0888, 0.1318, 0.0774, 0.1058,\n",
       "                        0.2328, 0.1558, 0.1358, 0.1256, 0.1141, 0.1439, 0.1855, 0.0994, 0.1986,\n",
       "                        0.1303, 0.1240, 0.1164, 0.1102, 0.1351, 0.0931, 0.1256, 0.1488, 0.1893,\n",
       "                        0.2018, 0.2638, 0.0976, 0.2305, 0.1905, 0.1421, 0.2565, 0.2083, 0.0936,\n",
       "                        0.0847, 0.1817, 0.2011, 0.1270, 0.1998, 0.1900, 0.1104, 0.1576, 0.1803,\n",
       "                        0.1143, 0.1381, 0.2112, 0.1364, 0.1788, 0.1124, 0.1783, 0.1337, 0.1498,\n",
       "                        0.1010, 0.2736, 0.1162, 0.1525, 0.2714, 0.1076, 0.2321, 0.1267, 0.2190,\n",
       "                        0.2427, 0.1026, 0.1096, 0.2092, 0.0866, 0.1572, 0.1921, 0.2364, 0.0971,\n",
       "                        0.1775, 0.0806, 0.2021, 0.1793, 0.1531, 0.2460, 0.1455, 0.0992, 0.1351,\n",
       "                        0.1538, 0.0808, 0.2806, 0.1268, 0.1750, 0.1008, 0.1259, 0.1043, 0.1796,\n",
       "                        0.1110, 0.1325, 0.1387, 0.1301, 0.1975, 0.0862, 0.1084, 0.2359, 0.1039,\n",
       "                        0.1485, 0.1055, 0.1777, 0.1321, 0.1425, 0.1246, 0.1301, 0.0865, 0.1833,\n",
       "                        0.1205, 0.1719, 0.1185, 0.2515, 0.1286, 0.1482, 0.2711, 0.2044, 0.1136,\n",
       "                        0.2064, 0.1723, 0.1622, 0.1137, 0.1273, 0.1221, 0.1167, 0.1476, 0.1342,\n",
       "                        0.2390, 0.0948, 0.1530, 0.2922, 0.0809, 0.1457, 0.1544, 0.2040, 0.1874,\n",
       "                        0.1353, 0.1731, 0.2399, 0.1662, 0.1864, 0.1222, 0.0905, 0.1330, 0.1079,\n",
       "                        0.2211, 0.1359, 0.2010, 0.1201, 0.1635, 0.0927, 0.1397, 0.1986, 0.1949,\n",
       "                        0.1361, 0.1219, 0.2329, 0.2053, 0.1316, 0.2199, 0.1241, 0.1311, 0.2201,\n",
       "                        0.0923, 0.1315, 0.1776, 0.1575])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0008, 0.0006, 0.0012, 0.0008, 0.0011, 0.0008, 0.0008, 0.0005,\n",
       "                        0.0007, 0.0010, 0.0010, 0.0009, 0.0020, 0.0008, 0.0008, 0.0012, 0.0009,\n",
       "                        0.0005, 0.0008, 0.0014, 0.0006, 0.0010, 0.0010, 0.0011, 0.0019, 0.0013,\n",
       "                        0.0008, 0.0009, 0.0008, 0.0010, 0.0012, 0.0006, 0.0010, 0.0006, 0.0011,\n",
       "                        0.0010, 0.0019, 0.0017, 0.0011, 0.0013, 0.0009, 0.0014, 0.0006, 0.0013,\n",
       "                        0.0008, 0.0006, 0.0021, 0.0005, 0.0011, 0.0018, 0.0017, 0.0006, 0.0007,\n",
       "                        0.0008, 0.0006, 0.0011, 0.0008, 0.0013, 0.0010, 0.0016, 0.0015, 0.0009,\n",
       "                        0.0010, 0.0014, 0.0012, 0.0007, 0.0006, 0.0019, 0.0006, 0.0011, 0.0017,\n",
       "                        0.0018, 0.0013, 0.0013, 0.0008, 0.0010, 0.0006, 0.0012, 0.0008, 0.0012,\n",
       "                        0.0009, 0.0010, 0.0009, 0.0014, 0.0008, 0.0005, 0.0015, 0.0007, 0.0011,\n",
       "                        0.0014, 0.0010, 0.0008, 0.0006, 0.0008, 0.0008, 0.0010, 0.0010, 0.0010,\n",
       "                        0.0008, 0.0008, 0.0011, 0.0018, 0.0010, 0.0013, 0.0010, 0.0010, 0.0011,\n",
       "                        0.0008, 0.0008, 0.0015, 0.0009, 0.0006, 0.0014, 0.0007, 0.0004, 0.0009,\n",
       "                        0.0015, 0.0007, 0.0007, 0.0010, 0.0005, 0.0008, 0.0012, 0.0014, 0.0007,\n",
       "                        0.0006, 0.0012, 0.0009, 0.0015, 0.0005, 0.0011, 0.0009, 0.0012, 0.0008,\n",
       "                        0.0007, 0.0010, 0.0018, 0.0017, 0.0006, 0.0009, 0.0005, 0.0012, 0.0007,\n",
       "                        0.0007, 0.0011, 0.0011, 0.0015, 0.0012, 0.0006, 0.0015, 0.0010, 0.0015,\n",
       "                        0.0010, 0.0015, 0.0006, 0.0013, 0.0006, 0.0005, 0.0017, 0.0017, 0.0009,\n",
       "                        0.0019, 0.0004, 0.0012, 0.0006, 0.0008, 0.0009, 0.0009, 0.0007, 0.0007,\n",
       "                        0.0007, 0.0015, 0.0010, 0.0011, 0.0009, 0.0009, 0.0007, 0.0010, 0.0007,\n",
       "                        0.0005, 0.0006, 0.0020, 0.0006, 0.0008, 0.0008, 0.0011, 0.0012, 0.0005,\n",
       "                        0.0010, 0.0009, 0.0008, 0.0005, 0.0013, 0.0012, 0.0009, 0.0010, 0.0015,\n",
       "                        0.0006, 0.0008, 0.0007, 0.0012, 0.0008, 0.0011, 0.0008, 0.0008, 0.0004,\n",
       "                        0.0011, 0.0010, 0.0016, 0.0009, 0.0009, 0.0015, 0.0015, 0.0010, 0.0007,\n",
       "                        0.0007, 0.0014, 0.0008, 0.0008, 0.0004, 0.0012, 0.0011, 0.0009, 0.0007,\n",
       "                        0.0008, 0.0010, 0.0015, 0.0010, 0.0007, 0.0008, 0.0005, 0.0010, 0.0010,\n",
       "                        0.0006, 0.0018, 0.0009, 0.0005, 0.0013, 0.0013, 0.0008, 0.0012, 0.0010,\n",
       "                        0.0020, 0.0008, 0.0005, 0.0012, 0.0009, 0.0010, 0.0014, 0.0011, 0.0012,\n",
       "                        0.0007, 0.0008, 0.0013, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.1160, -0.1058, -0.0650, -0.1150, -0.1000, -0.1369, -0.1015, -0.1000,\n",
       "                          -0.0620, -0.0711, -0.1261, -0.1049, -0.0893, -0.2517, -0.1038, -0.0634,\n",
       "                          -0.1510, -0.1189, -0.0660, -0.0993, -0.1769, -0.0681, -0.1310, -0.1272,\n",
       "                          -0.1434, -0.2451, -0.1437, -0.0962, -0.1107, -0.1078, -0.1192, -0.1477,\n",
       "                          -0.0725, -0.1287, -0.0788, -0.0884, -0.0706, -0.2445, -0.1097, -0.1430,\n",
       "                          -0.1664, -0.1190, -0.1787, -0.0795, -0.1281, -0.1041, -0.0719, -0.1268,\n",
       "                          -0.0690, -0.1320, -0.1399, -0.1247, -0.0719, -0.0870, -0.1072, -0.0823,\n",
       "                          -0.1299, -0.1054, -0.1706, -0.1294, -0.2105, -0.1902, -0.1093, -0.1278,\n",
       "                          -0.1117, -0.1131, -0.0844, -0.0747, -0.2396, -0.0766, -0.1379, -0.1891,\n",
       "                          -0.2234, -0.1699, -0.0859, -0.0668, -0.1170, -0.0640, -0.1542, -0.0869,\n",
       "                          -0.1438, -0.1154, -0.1218, -0.1101, -0.1797, -0.0909, -0.0562, -0.1973,\n",
       "                          -0.0622, -0.0638, -0.1760, -0.1099, -0.0787, -0.0718, -0.1056, -0.1009,\n",
       "                          -0.1222, -0.1241, -0.1228, -0.0589, -0.0834, -0.1400, -0.2276, -0.1281,\n",
       "                          -0.1649, -0.0863, -0.1233, -0.0704, -0.0893, -0.1018, -0.1909, -0.1201,\n",
       "                          -0.0670, -0.1808, -0.0782, -0.0551, -0.1117, -0.1194, -0.0897, -0.0840,\n",
       "                          -0.1064, -0.0691, -0.1079, -0.1039, -0.1772, -0.0867, -0.0543, -0.1422,\n",
       "                          -0.0899, -0.1302, -0.0598, -0.1150, -0.1181, -0.0863, -0.0764, -0.0901,\n",
       "                          -0.1292, -0.2282, -0.1313, -0.0773, -0.1164, -0.0621, -0.1519, -0.0901,\n",
       "                          -0.0832, -0.1358, -0.1435, -0.1869, -0.0633, -0.0772, -0.1420, -0.1098,\n",
       "                          -0.1974, -0.1301, -0.1884, -0.0769, -0.1615, -0.0614, -0.0607, -0.2190,\n",
       "                          -0.2210, -0.1209, -0.2462, -0.0528, -0.0793, -0.0716, -0.1055, -0.1207,\n",
       "                          -0.1112, -0.0899, -0.0916, -0.0880, -0.0612, -0.0698, -0.1014, -0.1023,\n",
       "                          -0.1122, -0.0839, -0.1327, -0.0917, -0.0692, -0.0716, -0.2510, -0.0736,\n",
       "                          -0.1083, -0.0996, -0.1390, -0.1541, -0.0674, -0.0829, -0.1149, -0.1039,\n",
       "                          -0.0557, -0.0957, -0.1582, -0.1130, -0.1241, -0.1882, -0.0725, -0.1085,\n",
       "                          -0.0852, -0.1506, -0.0946, -0.1393, -0.1056, -0.0704, -0.0465, -0.0998,\n",
       "                          -0.1043, -0.2023, -0.1020, -0.0817, -0.1871, -0.1080, -0.1226, -0.0930,\n",
       "                          -0.0692, -0.1741, -0.1005, -0.1083, -0.0548, -0.1597, -0.1392, -0.1154,\n",
       "                          -0.0477, -0.1013, -0.0977, -0.1459, -0.1269, -0.0943, -0.1001, -0.0653,\n",
       "                          -0.1219, -0.0746, -0.0596, -0.2327, -0.1114, -0.0659, -0.1003, -0.1611,\n",
       "                          -0.1048, -0.1463, -0.1307, -0.0819, -0.1024, -0.0581, -0.1516, -0.1056,\n",
       "                          -0.0846, -0.1264, -0.1359, -0.1270, -0.0534, -0.0720, -0.1719, -0.1096]), max_val=tensor([0.0664, 0.1057, 0.0795, 0.1571, 0.0747, 0.1033, 0.0728, 0.0698, 0.0461,\n",
       "                          0.0840, 0.0921, 0.1314, 0.1120, 0.1193, 0.1047, 0.0994, 0.0794, 0.1039,\n",
       "                          0.0504, 0.0565, 0.1346, 0.0721, 0.1015, 0.0915, 0.1243, 0.1360, 0.1616,\n",
       "                          0.0755, 0.0740, 0.0718, 0.1227, 0.1154, 0.0755, 0.0896, 0.0778, 0.1421,\n",
       "                          0.1237, 0.0957, 0.2099, 0.1105, 0.1192, 0.0954, 0.0721, 0.0802, 0.1713,\n",
       "                          0.0965, 0.0671, 0.2677, 0.0637, 0.1388, 0.2310, 0.2165, 0.0578, 0.0669,\n",
       "                          0.1031, 0.0763, 0.1433, 0.1058, 0.0861, 0.1242, 0.1117, 0.0692, 0.0861,\n",
       "                          0.0825, 0.1838, 0.1468, 0.0843, 0.0396, 0.1075, 0.0718, 0.0800, 0.2199,\n",
       "                          0.2301, 0.0762, 0.1624, 0.0965, 0.1322, 0.0744, 0.0801, 0.0961, 0.1477,\n",
       "                          0.0822, 0.0861, 0.1145, 0.1587, 0.0999, 0.0660, 0.1780, 0.0835, 0.1446,\n",
       "                          0.1068, 0.1332, 0.1027, 0.0768, 0.0694, 0.0742, 0.0885, 0.0834, 0.0985,\n",
       "                          0.0993, 0.1052, 0.1279, 0.1120, 0.1196, 0.0674, 0.1239, 0.0563, 0.1412,\n",
       "                          0.1031, 0.0741, 0.1054, 0.1095, 0.0766, 0.0806, 0.0903, 0.0457, 0.0971,\n",
       "                          0.1864, 0.0881, 0.0748, 0.1229, 0.0614, 0.1036, 0.1587, 0.1578, 0.0735,\n",
       "                          0.0825, 0.1563, 0.1183, 0.1895, 0.0678, 0.1452, 0.0993, 0.1568, 0.0987,\n",
       "                          0.0908, 0.0813, 0.0977, 0.2214, 0.0785, 0.0966, 0.0556, 0.0844, 0.0623,\n",
       "                          0.0514, 0.0599, 0.0651, 0.1175, 0.1496, 0.0743, 0.1875, 0.1297, 0.0820,\n",
       "                          0.0878, 0.1872, 0.0801, 0.1219, 0.0794, 0.0604, 0.0807, 0.1834, 0.0840,\n",
       "                          0.2077, 0.0417, 0.1584, 0.0753, 0.1050, 0.0921, 0.1116, 0.0475, 0.0696,\n",
       "                          0.0665, 0.1852, 0.1239, 0.1421, 0.1173, 0.0808, 0.0592, 0.0977, 0.0665,\n",
       "                          0.0647, 0.0788, 0.1032, 0.0671, 0.1061, 0.0864, 0.1312, 0.1067, 0.0533,\n",
       "                          0.1252, 0.0959, 0.0578, 0.0591, 0.1612, 0.0860, 0.0947, 0.0908, 0.1009,\n",
       "                          0.0602, 0.0789, 0.0834, 0.1313, 0.1015, 0.1346, 0.1030, 0.1042, 0.0505,\n",
       "                          0.1440, 0.1292, 0.1186, 0.1129, 0.1185, 0.1106, 0.1963, 0.0635, 0.0857,\n",
       "                          0.0908, 0.1403, 0.0776, 0.0955, 0.0477, 0.1468, 0.1215, 0.0805, 0.0848,\n",
       "                          0.0829, 0.1286, 0.1958, 0.1181, 0.0801, 0.0755, 0.0689, 0.1071, 0.1242,\n",
       "                          0.0773, 0.1059, 0.1199, 0.0495, 0.1711, 0.1335, 0.0939, 0.1464, 0.1187,\n",
       "                          0.2495, 0.0654, 0.0431, 0.0989, 0.1100, 0.1261, 0.1745, 0.1263, 0.1462,\n",
       "                          0.0852, 0.1055, 0.1105, 0.0945])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0016, 0.0008, 0.0016, 0.0007, 0.0008, 0.0018, 0.0012, 0.0007,\n",
       "                      0.0010, 0.0008, 0.0009, 0.0010, 0.0007, 0.0011, 0.0019, 0.0010, 0.0008,\n",
       "                      0.0015, 0.0009, 0.0009, 0.0009, 0.0008, 0.0006, 0.0010, 0.0008, 0.0007,\n",
       "                      0.0012, 0.0012, 0.0013, 0.0007, 0.0011, 0.0011, 0.0008, 0.0010, 0.0007,\n",
       "                      0.0009, 0.0010, 0.0008, 0.0006, 0.0011, 0.0017, 0.0014, 0.0011, 0.0012,\n",
       "                      0.0006, 0.0009, 0.0011, 0.0009, 0.0008, 0.0009, 0.0014, 0.0009, 0.0008,\n",
       "                      0.0012, 0.0007, 0.0007, 0.0015, 0.0012, 0.0009, 0.0008, 0.0008, 0.0014,\n",
       "                      0.0017, 0.0010, 0.0012, 0.0020, 0.0007, 0.0010, 0.0011, 0.0009, 0.0006,\n",
       "                      0.0008, 0.0012, 0.0007, 0.0010, 0.0012, 0.0012, 0.0017, 0.0011, 0.0013,\n",
       "                      0.0009, 0.0007, 0.0011, 0.0006, 0.0008, 0.0010, 0.0009, 0.0010, 0.0008,\n",
       "                      0.0008, 0.0011, 0.0008, 0.0009, 0.0008, 0.0008, 0.0013, 0.0013, 0.0009,\n",
       "                      0.0009, 0.0011, 0.0006, 0.0007, 0.0012, 0.0011, 0.0021, 0.0006, 0.0008,\n",
       "                      0.0010, 0.0011, 0.0013, 0.0008, 0.0010, 0.0012, 0.0007, 0.0009, 0.0010,\n",
       "                      0.0008, 0.0009, 0.0011, 0.0011, 0.0009, 0.0006, 0.0007, 0.0008, 0.0007,\n",
       "                      0.0010, 0.0008, 0.0012, 0.0009, 0.0008, 0.0013, 0.0010, 0.0013, 0.0007,\n",
       "                      0.0012, 0.0014, 0.0009, 0.0007, 0.0011, 0.0008, 0.0007, 0.0010, 0.0007,\n",
       "                      0.0008, 0.0008, 0.0012, 0.0011, 0.0010, 0.0012, 0.0010, 0.0009, 0.0009,\n",
       "                      0.0011, 0.0008, 0.0008, 0.0009, 0.0015, 0.0006, 0.0014, 0.0009, 0.0014,\n",
       "                      0.0014, 0.0007, 0.0007, 0.0013, 0.0010, 0.0015, 0.0017, 0.0012, 0.0008,\n",
       "                      0.0009, 0.0010, 0.0008, 0.0014, 0.0017, 0.0013, 0.0010, 0.0010, 0.0007,\n",
       "                      0.0011, 0.0010, 0.0013, 0.0007, 0.0018, 0.0016, 0.0019, 0.0008, 0.0009,\n",
       "                      0.0011, 0.0011, 0.0011, 0.0010, 0.0009, 0.0008, 0.0006, 0.0011, 0.0012,\n",
       "                      0.0011, 0.0013, 0.0009, 0.0010, 0.0014, 0.0011, 0.0013, 0.0011, 0.0018,\n",
       "                      0.0012, 0.0009, 0.0011, 0.0006, 0.0023, 0.0017, 0.0008, 0.0011, 0.0012,\n",
       "                      0.0009, 0.0010, 0.0012, 0.0023, 0.0009, 0.0009, 0.0012, 0.0009, 0.0008,\n",
       "                      0.0011, 0.0013, 0.0007, 0.0014, 0.0008, 0.0008, 0.0011, 0.0009, 0.0008,\n",
       "                      0.0013, 0.0011, 0.0007, 0.0008, 0.0009, 0.0013, 0.0013, 0.0008, 0.0008,\n",
       "                      0.0009, 0.0013, 0.0015, 0.0013, 0.0010, 0.0007, 0.0009, 0.0009, 0.0008,\n",
       "                      0.0008, 0.0011, 0.0014, 0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0898, -0.2067, -0.0632, -0.2062, -0.0778, -0.0780, -0.1579, -0.1488,\n",
       "                        -0.0626, -0.1042, -0.0921, -0.0977, -0.0606, -0.0753, -0.1071, -0.2447,\n",
       "                        -0.1280, -0.0711, -0.1913, -0.1156, -0.0730, -0.1191, -0.1066, -0.0832,\n",
       "                        -0.1172, -0.1076, -0.0906, -0.0628, -0.1000, -0.1145, -0.0841, -0.1459,\n",
       "                        -0.1422, -0.0655, -0.1105, -0.0930, -0.0672, -0.0841, -0.0689, -0.0825,\n",
       "                        -0.1057, -0.2214, -0.1756, -0.0894, -0.0551, -0.0764, -0.0943, -0.1460,\n",
       "                        -0.1152, -0.0969, -0.1203, -0.0883, -0.0549, -0.1075, -0.1380, -0.0944,\n",
       "                        -0.0839, -0.1862, -0.0586, -0.0709, -0.0686, -0.0655, -0.1819, -0.2159,\n",
       "                        -0.1099, -0.1501, -0.2067, -0.0736, -0.1044, -0.0879, -0.1182, -0.0686,\n",
       "                        -0.0761, -0.1195, -0.0881, -0.1309, -0.1257, -0.0946, -0.1497, -0.1362,\n",
       "                        -0.0873, -0.1149, -0.0738, -0.1446, -0.0746, -0.0987, -0.1299, -0.1215,\n",
       "                        -0.0862, -0.0809, -0.0789, -0.0654, -0.1065, -0.0788, -0.0565, -0.0837,\n",
       "                        -0.1246, -0.1520, -0.1098, -0.1147, -0.1247, -0.0697, -0.0915, -0.1021,\n",
       "                        -0.1386, -0.2655, -0.0615, -0.0754, -0.0921, -0.1429, -0.1398, -0.1047,\n",
       "                        -0.1260, -0.0549, -0.0666, -0.0864, -0.1324, -0.0638, -0.1124, -0.0824,\n",
       "                        -0.1083, -0.0927, -0.0793, -0.0778, -0.0958, -0.0836, -0.1291, -0.0605,\n",
       "                        -0.0954, -0.0947, -0.1083, -0.0617, -0.0982, -0.1672, -0.0699, -0.1528,\n",
       "                        -0.1198, -0.0880, -0.0824, -0.1194, -0.0798, -0.0904, -0.0896, -0.0717,\n",
       "                        -0.0775, -0.0825, -0.1516, -0.1357, -0.0834, -0.1476, -0.1258, -0.0906,\n",
       "                        -0.0725, -0.0993, -0.1081, -0.0958, -0.0643, -0.1909, -0.0621, -0.0962,\n",
       "                        -0.1096, -0.1031, -0.1735, -0.0904, -0.0748, -0.1546, -0.1197, -0.0662,\n",
       "                        -0.2172, -0.0607, -0.1078, -0.0604, -0.0679, -0.0829, -0.0604, -0.0627,\n",
       "                        -0.1467, -0.1236, -0.1128, -0.0759, -0.0910, -0.0906, -0.1005, -0.0692,\n",
       "                        -0.2168, -0.2099, -0.0945, -0.0554, -0.0898, -0.1183, -0.1073, -0.0986,\n",
       "                        -0.1065, -0.1114, -0.0939, -0.0603, -0.1096, -0.0824, -0.0803, -0.1283,\n",
       "                        -0.0964, -0.1284, -0.1098, -0.0970, -0.1567, -0.1015, -0.2121, -0.1581,\n",
       "                        -0.0828, -0.1205, -0.0610, -0.2960, -0.2194, -0.0893, -0.1053, -0.1478,\n",
       "                        -0.1096, -0.0897, -0.0805, -0.2939, -0.1173, -0.1012, -0.1598, -0.0897,\n",
       "                        -0.0909, -0.1134, -0.1027, -0.0901, -0.1126, -0.0998, -0.0778, -0.0831,\n",
       "                        -0.0691, -0.0796, -0.0599, -0.1205, -0.0713, -0.0630, -0.0992, -0.1124,\n",
       "                        -0.1210, -0.0731, -0.0985, -0.0677, -0.0849, -0.1300, -0.1057, -0.0823,\n",
       "                        -0.0807, -0.0846, -0.0871, -0.1060, -0.0910, -0.1120, -0.1203, -0.0977]), max_val=tensor([0.1137, 0.0992, 0.1065, 0.1684, 0.0948, 0.1023, 0.2301, 0.1215, 0.0845,\n",
       "                        0.1293, 0.1048, 0.1146, 0.1284, 0.0906, 0.1338, 0.2277, 0.1274, 0.0994,\n",
       "                        0.1637, 0.1044, 0.1201, 0.1096, 0.1046, 0.0809, 0.1311, 0.0956, 0.0641,\n",
       "                        0.1486, 0.1576, 0.1675, 0.0729, 0.1283, 0.1154, 0.1044, 0.1328, 0.0899,\n",
       "                        0.1100, 0.1226, 0.0988, 0.0823, 0.1370, 0.2216, 0.1295, 0.1375, 0.1550,\n",
       "                        0.0667, 0.1096, 0.1384, 0.0919, 0.0953, 0.1182, 0.1808, 0.1123, 0.0974,\n",
       "                        0.1571, 0.0838, 0.0719, 0.1802, 0.1505, 0.1151, 0.1028, 0.1040, 0.1289,\n",
       "                        0.1790, 0.1304, 0.1135, 0.2581, 0.0912, 0.1249, 0.1385, 0.1159, 0.0808,\n",
       "                        0.0974, 0.1496, 0.0925, 0.1217, 0.1529, 0.1531, 0.2218, 0.1116, 0.1621,\n",
       "                        0.0897, 0.0919, 0.1415, 0.0825, 0.0804, 0.1107, 0.1054, 0.1251, 0.1057,\n",
       "                        0.1020, 0.1383, 0.0953, 0.1080, 0.1018, 0.1025, 0.1643, 0.1631, 0.0991,\n",
       "                        0.0921, 0.1376, 0.0793, 0.0917, 0.1501, 0.1247, 0.1858, 0.0742, 0.1032,\n",
       "                        0.1272, 0.0869, 0.1640, 0.1051, 0.1240, 0.1471, 0.0902, 0.1090, 0.1131,\n",
       "                        0.1058, 0.1086, 0.1451, 0.1378, 0.1137, 0.0740, 0.0915, 0.1020, 0.0921,\n",
       "                        0.0984, 0.1028, 0.1580, 0.1171, 0.0949, 0.1695, 0.1324, 0.1590, 0.0877,\n",
       "                        0.1328, 0.1739, 0.1168, 0.0900, 0.1417, 0.0999, 0.0735, 0.1310, 0.0948,\n",
       "                        0.1067, 0.1002, 0.1388, 0.1193, 0.1315, 0.1506, 0.1107, 0.1189, 0.1099,\n",
       "                        0.1410, 0.0992, 0.1011, 0.1163, 0.1665, 0.0798, 0.1828, 0.1196, 0.1754,\n",
       "                        0.1325, 0.0708, 0.0888, 0.1707, 0.1212, 0.1869, 0.2160, 0.1552, 0.1073,\n",
       "                        0.1151, 0.1290, 0.0971, 0.1796, 0.2206, 0.1604, 0.1112, 0.1290, 0.0857,\n",
       "                        0.1414, 0.1268, 0.1608, 0.0844, 0.2235, 0.2093, 0.2393, 0.1073, 0.1121,\n",
       "                        0.1436, 0.1385, 0.1343, 0.1328, 0.1091, 0.1025, 0.0727, 0.1340, 0.1467,\n",
       "                        0.1375, 0.1630, 0.1121, 0.1146, 0.1801, 0.1375, 0.1681, 0.1423, 0.2332,\n",
       "                        0.1054, 0.1098, 0.1340, 0.0819, 0.2228, 0.1583, 0.0978, 0.1383, 0.1207,\n",
       "                        0.1067, 0.1270, 0.1531, 0.2133, 0.0965, 0.1085, 0.1177, 0.1170, 0.1036,\n",
       "                        0.1440, 0.1679, 0.0799, 0.1741, 0.1017, 0.1043, 0.1403, 0.1146, 0.1052,\n",
       "                        0.1606, 0.1409, 0.0852, 0.1002, 0.1112, 0.1705, 0.1647, 0.1060, 0.0803,\n",
       "                        0.1101, 0.1684, 0.1929, 0.1625, 0.1293, 0.0946, 0.1089, 0.1120, 0.0915,\n",
       "                        0.1006, 0.1396, 0.1759, 0.1078])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0008, 0.0010, 0.0006, 0.0007, 0.0006, 0.0007, 0.0006, 0.0006, 0.0007,\n",
       "                      0.0005, 0.0008, 0.0004, 0.0008, 0.0012, 0.0012, 0.0008, 0.0009, 0.0007,\n",
       "                      0.0009, 0.0016, 0.0010, 0.0016, 0.0024, 0.0012, 0.0006, 0.0009, 0.0005,\n",
       "                      0.0012, 0.0008, 0.0014, 0.0006, 0.0005, 0.0007, 0.0003, 0.0006, 0.0006,\n",
       "                      0.0014, 0.0007, 0.0005, 0.0008, 0.0010, 0.0006, 0.0004, 0.0005, 0.0008,\n",
       "                      0.0006, 0.0007, 0.0013, 0.0008, 0.0012, 0.0007, 0.0010, 0.0011, 0.0006,\n",
       "                      0.0006, 0.0006, 0.0007, 0.0005, 0.0008, 0.0014, 0.0006, 0.0004, 0.0005,\n",
       "                      0.0009, 0.0010, 0.0006, 0.0014, 0.0007, 0.0009, 0.0010, 0.0007, 0.0006,\n",
       "                      0.0005, 0.0007, 0.0008, 0.0007, 0.0006, 0.0007, 0.0004, 0.0006, 0.0011,\n",
       "                      0.0005, 0.0007, 0.0007, 0.0008, 0.0008, 0.0009, 0.0007, 0.0011, 0.0011,\n",
       "                      0.0007, 0.0010, 0.0010, 0.0006, 0.0005, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0008, 0.0011, 0.0007, 0.0006, 0.0007, 0.0004, 0.0012, 0.0005, 0.0008,\n",
       "                      0.0015, 0.0008, 0.0010, 0.0003, 0.0009, 0.0009, 0.0009, 0.0005, 0.0010,\n",
       "                      0.0010, 0.0008, 0.0007, 0.0009, 0.0009, 0.0012, 0.0008, 0.0009, 0.0005,\n",
       "                      0.0010, 0.0009, 0.0016, 0.0004, 0.0006, 0.0006, 0.0007, 0.0007, 0.0006,\n",
       "                      0.0009, 0.0005, 0.0010, 0.0006, 0.0007, 0.0006, 0.0015, 0.0006, 0.0006,\n",
       "                      0.0009, 0.0012, 0.0009, 0.0006, 0.0010, 0.0012, 0.0019, 0.0004, 0.0011,\n",
       "                      0.0009, 0.0015, 0.0008, 0.0009, 0.0005, 0.0013, 0.0011, 0.0017, 0.0006,\n",
       "                      0.0007, 0.0015, 0.0009, 0.0008, 0.0008, 0.0013, 0.0008, 0.0009, 0.0012,\n",
       "                      0.0004, 0.0007, 0.0011, 0.0006, 0.0007, 0.0007, 0.0007, 0.0009, 0.0007,\n",
       "                      0.0009, 0.0015, 0.0008, 0.0008, 0.0007, 0.0006, 0.0009, 0.0023, 0.0010,\n",
       "                      0.0009, 0.0007, 0.0010, 0.0009, 0.0009, 0.0005, 0.0008, 0.0007, 0.0009,\n",
       "                      0.0008, 0.0017, 0.0012, 0.0008, 0.0006, 0.0011, 0.0009, 0.0021, 0.0009,\n",
       "                      0.0011, 0.0004, 0.0006, 0.0006, 0.0009, 0.0004, 0.0009, 0.0013, 0.0009,\n",
       "                      0.0016, 0.0007, 0.0009, 0.0014, 0.0008, 0.0009, 0.0005, 0.0008, 0.0009,\n",
       "                      0.0009, 0.0011, 0.0013, 0.0006, 0.0026, 0.0005, 0.0006, 0.0012, 0.0007,\n",
       "                      0.0008, 0.0006, 0.0007, 0.0005, 0.0004, 0.0006, 0.0007, 0.0007, 0.0009,\n",
       "                      0.0005, 0.0007, 0.0006, 0.0014, 0.0007, 0.0012, 0.0004, 0.0010, 0.0009,\n",
       "                      0.0011, 0.0007, 0.0007, 0.0006]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0638, -0.1343, -0.0597, -0.0850, -0.0671, -0.0956, -0.0700, -0.0717,\n",
       "                        -0.0584, -0.0662, -0.1055, -0.0522, -0.0851, -0.1491, -0.1481, -0.0997,\n",
       "                        -0.1128, -0.0871, -0.0861, -0.1060, -0.1278, -0.2101, -0.3097, -0.1578,\n",
       "                        -0.0810, -0.1011, -0.0688, -0.1551, -0.0671, -0.1745, -0.0556, -0.0600,\n",
       "                        -0.0714, -0.0428, -0.0660, -0.0720, -0.1235, -0.0552, -0.0539, -0.1086,\n",
       "                        -0.1224, -0.0730, -0.0482, -0.0599, -0.0673, -0.0724, -0.0600, -0.1641,\n",
       "                        -0.0699, -0.1248, -0.0842, -0.1164, -0.1449, -0.0675, -0.0795, -0.0721,\n",
       "                        -0.0681, -0.0644, -0.0985, -0.0761, -0.0716, -0.0397, -0.0654, -0.0802,\n",
       "                        -0.1176, -0.0747, -0.1730, -0.0828, -0.1178, -0.0911, -0.0941, -0.0743,\n",
       "                        -0.0581, -0.0843, -0.0844, -0.0934, -0.0633, -0.0689, -0.0548, -0.0825,\n",
       "                        -0.0729, -0.0591, -0.0802, -0.0868, -0.1025, -0.0570, -0.0938, -0.0889,\n",
       "                        -0.1231, -0.0511, -0.0882, -0.1303, -0.1228, -0.0549, -0.0661, -0.0734,\n",
       "                        -0.0793, -0.0825, -0.0657, -0.1068, -0.1457, -0.0790, -0.0635, -0.0834,\n",
       "                        -0.0525, -0.1276, -0.0697, -0.0848, -0.1865, -0.0984, -0.1333, -0.0392,\n",
       "                        -0.1169, -0.0879, -0.0900, -0.0624, -0.0664, -0.1285, -0.0793, -0.0841,\n",
       "                        -0.1008, -0.1067, -0.1516, -0.1015, -0.1140, -0.0671, -0.1265, -0.1147,\n",
       "                        -0.1561, -0.0503, -0.0709, -0.0713, -0.0865, -0.0763, -0.0823, -0.1087,\n",
       "                        -0.0627, -0.1252, -0.0818, -0.0947, -0.0545, -0.1319, -0.0736, -0.0786,\n",
       "                        -0.1094, -0.1345, -0.0738, -0.0713, -0.0876, -0.0953, -0.2462, -0.0532,\n",
       "                        -0.1351, -0.1109, -0.1869, -0.1062, -0.1120, -0.0470, -0.1506, -0.1383,\n",
       "                        -0.1139, -0.0616, -0.0567, -0.1869, -0.0736, -0.0782, -0.0834, -0.1613,\n",
       "                        -0.0805, -0.1200, -0.0934, -0.0536, -0.0835, -0.1136, -0.0766, -0.0762,\n",
       "                        -0.0890, -0.0917, -0.1207, -0.0887, -0.1200, -0.1875, -0.1013, -0.0857,\n",
       "                        -0.0935, -0.0676, -0.1097, -0.1524, -0.1105, -0.0863, -0.0698, -0.1227,\n",
       "                        -0.1065, -0.1200, -0.0699, -0.0569, -0.0918, -0.1135, -0.0939, -0.2117,\n",
       "                        -0.1524, -0.1037, -0.0746, -0.1382, -0.0950, -0.2673, -0.0925, -0.1345,\n",
       "                        -0.0520, -0.0819, -0.0742, -0.1169, -0.0517, -0.1167, -0.1697, -0.1069,\n",
       "                        -0.2065, -0.0702, -0.1134, -0.0983, -0.0960, -0.1199, -0.0692, -0.0707,\n",
       "                        -0.1003, -0.1103, -0.1369, -0.1583, -0.0685, -0.1162, -0.0691, -0.0635,\n",
       "                        -0.1558, -0.0697, -0.0956, -0.0775, -0.0526, -0.0592, -0.0521, -0.0808,\n",
       "                        -0.0633, -0.0684, -0.1189, -0.0507, -0.0815, -0.0721, -0.1415, -0.0871,\n",
       "                        -0.1542, -0.0416, -0.1057, -0.1056, -0.1447, -0.0758, -0.0849, -0.0760]), max_val=tensor([0.0966, 0.0901, 0.0736, 0.0656, 0.0701, 0.0478, 0.0805, 0.0766, 0.0905,\n",
       "                        0.0584, 0.0824, 0.0533, 0.0993, 0.1266, 0.1128, 0.0856, 0.0923, 0.0700,\n",
       "                        0.1101, 0.2008, 0.1014, 0.0889, 0.2694, 0.0844, 0.0613, 0.1165, 0.0668,\n",
       "                        0.1202, 0.0989, 0.1195, 0.0725, 0.0652, 0.0873, 0.0358, 0.0767, 0.0817,\n",
       "                        0.1790, 0.0924, 0.0576, 0.0797, 0.0825, 0.0664, 0.0470, 0.0633, 0.1078,\n",
       "                        0.0445, 0.0878, 0.1568, 0.0960, 0.1510, 0.0714, 0.1262, 0.0807, 0.0794,\n",
       "                        0.0611, 0.0790, 0.0861, 0.0527, 0.0887, 0.1715, 0.0810, 0.0529, 0.0668,\n",
       "                        0.1080, 0.1207, 0.0660, 0.1247, 0.0895, 0.0490, 0.1223, 0.0675, 0.0532,\n",
       "                        0.0574, 0.0668, 0.0964, 0.0666, 0.0718, 0.0941, 0.0547, 0.0773, 0.1349,\n",
       "                        0.0638, 0.0876, 0.0704, 0.0900, 0.1015, 0.1136, 0.0796, 0.1366, 0.1432,\n",
       "                        0.0827, 0.1050, 0.1267, 0.0721, 0.0685, 0.0703, 0.0771, 0.0817, 0.0767,\n",
       "                        0.1078, 0.1213, 0.0896, 0.0772, 0.0659, 0.0518, 0.1549, 0.0651, 0.1016,\n",
       "                        0.1280, 0.0670, 0.0865, 0.0354, 0.1042, 0.1173, 0.1185, 0.0574, 0.1326,\n",
       "                        0.1317, 0.1032, 0.0819, 0.1173, 0.1131, 0.1209, 0.0778, 0.1061, 0.0549,\n",
       "                        0.0908, 0.1154, 0.2052, 0.0494, 0.0633, 0.0823, 0.0640, 0.0883, 0.0794,\n",
       "                        0.1168, 0.0549, 0.0856, 0.0660, 0.0785, 0.0735, 0.1845, 0.0718, 0.0741,\n",
       "                        0.0953, 0.1565, 0.1099, 0.0732, 0.1281, 0.1557, 0.1704, 0.0465, 0.0642,\n",
       "                        0.1155, 0.1243, 0.0980, 0.0686, 0.0647, 0.1694, 0.1058, 0.2113, 0.0745,\n",
       "                        0.0830, 0.0811, 0.1138, 0.0956, 0.0969, 0.1312, 0.1007, 0.1035, 0.1542,\n",
       "                        0.0527, 0.0874, 0.1351, 0.0798, 0.0868, 0.0679, 0.0843, 0.1074, 0.0896,\n",
       "                        0.0741, 0.0975, 0.1006, 0.1039, 0.0836, 0.0728, 0.0646, 0.2888, 0.1225,\n",
       "                        0.1113, 0.0873, 0.0690, 0.1141, 0.0631, 0.0658, 0.1051, 0.0866, 0.0772,\n",
       "                        0.0999, 0.2063, 0.0907, 0.0796, 0.0522, 0.1354, 0.1182, 0.1351, 0.1092,\n",
       "                        0.1352, 0.0473, 0.0771, 0.0731, 0.1010, 0.0407, 0.1177, 0.1268, 0.1123,\n",
       "                        0.2002, 0.0920, 0.0996, 0.1723, 0.0803, 0.0700, 0.0584, 0.1001, 0.1085,\n",
       "                        0.1159, 0.1070, 0.1701, 0.0821, 0.3295, 0.0666, 0.0747, 0.1316, 0.0929,\n",
       "                        0.1073, 0.0538, 0.0838, 0.0676, 0.0468, 0.0708, 0.0852, 0.0893, 0.1093,\n",
       "                        0.0573, 0.0846, 0.0711, 0.1783, 0.0811, 0.1375, 0.0554, 0.1257, 0.1187,\n",
       "                        0.0801, 0.0898, 0.0736, 0.0780])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0008, 0.0006, 0.0010, 0.0008, 0.0006, 0.0007, 0.0007, 0.0011, 0.0007,\n",
       "                      0.0010, 0.0008, 0.0010, 0.0014, 0.0007, 0.0008, 0.0008, 0.0010, 0.0007,\n",
       "                      0.0007, 0.0007, 0.0006, 0.0010, 0.0007, 0.0009, 0.0006, 0.0007, 0.0007,\n",
       "                      0.0009, 0.0006, 0.0008, 0.0007, 0.0007, 0.0009, 0.0010, 0.0006, 0.0006,\n",
       "                      0.0009, 0.0009, 0.0007, 0.0010, 0.0007, 0.0012, 0.0007, 0.0007, 0.0007,\n",
       "                      0.0008, 0.0006, 0.0006, 0.0009, 0.0008, 0.0007, 0.0005, 0.0012, 0.0009,\n",
       "                      0.0015, 0.0007, 0.0010, 0.0007, 0.0006, 0.0009, 0.0007, 0.0015, 0.0006,\n",
       "                      0.0013, 0.0005, 0.0009, 0.0011, 0.0019, 0.0009, 0.0009, 0.0006, 0.0008,\n",
       "                      0.0006, 0.0007, 0.0010, 0.0008, 0.0007, 0.0006, 0.0009, 0.0008, 0.0008,\n",
       "                      0.0008, 0.0012, 0.0007, 0.0005, 0.0006, 0.0014, 0.0005, 0.0009, 0.0007,\n",
       "                      0.0007, 0.0008, 0.0012, 0.0009, 0.0009, 0.0006, 0.0006, 0.0007, 0.0007,\n",
       "                      0.0006, 0.0006, 0.0008, 0.0007, 0.0009, 0.0006, 0.0009, 0.0006, 0.0010,\n",
       "                      0.0006, 0.0010, 0.0007, 0.0008, 0.0006, 0.0007, 0.0011, 0.0010, 0.0009,\n",
       "                      0.0006, 0.0007, 0.0006, 0.0007, 0.0015, 0.0007, 0.0008, 0.0008, 0.0008,\n",
       "                      0.0008, 0.0011, 0.0009, 0.0009, 0.0007, 0.0007, 0.0008, 0.0006, 0.0009,\n",
       "                      0.0010, 0.0005, 0.0008, 0.0010, 0.0009, 0.0010, 0.0009, 0.0006, 0.0008,\n",
       "                      0.0011, 0.0006, 0.0008, 0.0010, 0.0006, 0.0006, 0.0007, 0.0008, 0.0009,\n",
       "                      0.0007, 0.0009, 0.0012, 0.0011, 0.0006, 0.0013, 0.0008, 0.0009, 0.0008,\n",
       "                      0.0006, 0.0007, 0.0008, 0.0007, 0.0008, 0.0013, 0.0007, 0.0012, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0009, 0.0011, 0.0006, 0.0011, 0.0011, 0.0007, 0.0006,\n",
       "                      0.0009, 0.0008, 0.0007, 0.0007, 0.0007, 0.0008, 0.0007, 0.0007, 0.0009,\n",
       "                      0.0009, 0.0009, 0.0007, 0.0008, 0.0008, 0.0006, 0.0007, 0.0012, 0.0006,\n",
       "                      0.0008, 0.0006, 0.0008, 0.0006, 0.0007, 0.0007, 0.0007, 0.0010, 0.0014,\n",
       "                      0.0011, 0.0008, 0.0012, 0.0008, 0.0008, 0.0007, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0030, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008, 0.0009, 0.0006, 0.0008,\n",
       "                      0.0009, 0.0006, 0.0007, 0.0006, 0.0014, 0.0007, 0.0006, 0.0007, 0.0009,\n",
       "                      0.0007, 0.0006, 0.0008, 0.0008, 0.0009, 0.0012, 0.0007, 0.0010, 0.0015,\n",
       "                      0.0007, 0.0010, 0.0006, 0.0006, 0.0009, 0.0009, 0.0010, 0.0007, 0.0008,\n",
       "                      0.0006, 0.0010, 0.0007, 0.0009, 0.0008, 0.0007, 0.0009, 0.0011, 0.0021,\n",
       "                      0.0007, 0.0010, 0.0012, 0.0006, 0.0008, 0.0006, 0.0009, 0.0008, 0.0009,\n",
       "                      0.0007, 0.0009, 0.0007, 0.0009, 0.0008, 0.0016, 0.0007, 0.0008, 0.0005,\n",
       "                      0.0006, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006, 0.0007, 0.0017, 0.0008,\n",
       "                      0.0009, 0.0008, 0.0005, 0.0008, 0.0007, 0.0011, 0.0009, 0.0006, 0.0007,\n",
       "                      0.0009, 0.0009, 0.0007, 0.0009, 0.0007, 0.0008, 0.0006, 0.0006, 0.0008,\n",
       "                      0.0012, 0.0009, 0.0006, 0.0008, 0.0008, 0.0008, 0.0007, 0.0008, 0.0009,\n",
       "                      0.0007, 0.0008, 0.0008, 0.0008, 0.0013, 0.0006, 0.0007, 0.0007, 0.0008,\n",
       "                      0.0008, 0.0011, 0.0007, 0.0008, 0.0010, 0.0008, 0.0007, 0.0007, 0.0009,\n",
       "                      0.0006, 0.0011, 0.0012, 0.0007, 0.0006, 0.0015, 0.0011, 0.0010, 0.0006,\n",
       "                      0.0008, 0.0007, 0.0005, 0.0008, 0.0006, 0.0013, 0.0013, 0.0010, 0.0007,\n",
       "                      0.0005, 0.0010, 0.0006, 0.0007, 0.0009, 0.0007, 0.0005, 0.0009, 0.0006,\n",
       "                      0.0010, 0.0010, 0.0009, 0.0008, 0.0008, 0.0010, 0.0007, 0.0008, 0.0011,\n",
       "                      0.0007, 0.0007, 0.0013, 0.0006, 0.0008, 0.0014, 0.0006, 0.0008, 0.0007,\n",
       "                      0.0014, 0.0006, 0.0011, 0.0008, 0.0008, 0.0009, 0.0007, 0.0007, 0.0013,\n",
       "                      0.0008, 0.0008, 0.0008, 0.0014, 0.0006, 0.0007, 0.0008, 0.0006, 0.0008,\n",
       "                      0.0010, 0.0007, 0.0006, 0.0013, 0.0009, 0.0006, 0.0009, 0.0015, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0013, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0007,\n",
       "                      0.0010, 0.0010, 0.0010, 0.0007, 0.0008, 0.0009, 0.0007, 0.0010, 0.0009,\n",
       "                      0.0009, 0.0006, 0.0008, 0.0007, 0.0009, 0.0008, 0.0007, 0.0007, 0.0009,\n",
       "                      0.0010, 0.0013, 0.0010, 0.0006, 0.0008, 0.0008, 0.0008, 0.0008, 0.0006,\n",
       "                      0.0007, 0.0006, 0.0006, 0.0008, 0.0007, 0.0006, 0.0007, 0.0009, 0.0012,\n",
       "                      0.0008, 0.0006, 0.0009, 0.0007, 0.0006, 0.0008, 0.0006, 0.0005, 0.0009,\n",
       "                      0.0009, 0.0009, 0.0010, 0.0013, 0.0014, 0.0009, 0.0006, 0.0011, 0.0009,\n",
       "                      0.0009, 0.0007, 0.0009, 0.0011, 0.0013, 0.0013, 0.0006, 0.0006, 0.0008,\n",
       "                      0.0012, 0.0005, 0.0010, 0.0007, 0.0008, 0.0007, 0.0007, 0.0015, 0.0012,\n",
       "                      0.0010, 0.0006, 0.0006, 0.0005, 0.0006, 0.0008, 0.0007, 0.0007, 0.0007,\n",
       "                      0.0011, 0.0014, 0.0012, 0.0007, 0.0007, 0.0011, 0.0011, 0.0007, 0.0008,\n",
       "                      0.0006, 0.0008, 0.0007, 0.0007, 0.0010, 0.0008, 0.0008, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0940, -0.0683, -0.0891, -0.0794, -0.0761, -0.0848, -0.0825, -0.0773,\n",
       "                        -0.0580, -0.0925, -0.0854, -0.0889, -0.1157, -0.0685, -0.1029, -0.0681,\n",
       "                        -0.1001, -0.0600, -0.0706, -0.0843, -0.0715, -0.1253, -0.0643, -0.1104,\n",
       "                        -0.0711, -0.0636, -0.0633, -0.0723, -0.0528, -0.0720, -0.0897, -0.0863,\n",
       "                        -0.0657, -0.0832, -0.0663, -0.0699, -0.0943, -0.0787, -0.0820, -0.0620,\n",
       "                        -0.0808, -0.1506, -0.0795, -0.0839, -0.0626, -0.0855, -0.0808, -0.0733,\n",
       "                        -0.0637, -0.0663, -0.0543, -0.0669, -0.0966, -0.0785, -0.1489, -0.0700,\n",
       "                        -0.0725, -0.0761, -0.0725, -0.0923, -0.0699, -0.1161, -0.0568, -0.0886,\n",
       "                        -0.0648, -0.0661, -0.1236, -0.1560, -0.1027, -0.0818, -0.0652, -0.0646,\n",
       "                        -0.0704, -0.0683, -0.0734, -0.0612, -0.0828, -0.0758, -0.0708, -0.0988,\n",
       "                        -0.0902, -0.0753, -0.0841, -0.0559, -0.0683, -0.0720, -0.1434, -0.0627,\n",
       "                        -0.0859, -0.0757, -0.0820, -0.0701, -0.0953, -0.1091, -0.0980, -0.0721,\n",
       "                        -0.0667, -0.0599, -0.0625, -0.0676, -0.0605, -0.0959, -0.0647, -0.0630,\n",
       "                        -0.0635, -0.1015, -0.0765, -0.1253, -0.0685, -0.0959, -0.0703, -0.1078,\n",
       "                        -0.0598, -0.0527, -0.0720, -0.0631, -0.0720, -0.0802, -0.0841, -0.0664,\n",
       "                        -0.0946, -0.0915, -0.0939, -0.0757, -0.0846, -0.0980, -0.0738, -0.0810,\n",
       "                        -0.0672, -0.0744, -0.0865, -0.0919, -0.0733, -0.0745, -0.0939, -0.0907,\n",
       "                        -0.0565, -0.0793, -0.1235, -0.1078, -0.1231, -0.0946, -0.0694, -0.0936,\n",
       "                        -0.0691, -0.0534, -0.0647, -0.1110, -0.0557, -0.0717, -0.0730, -0.0820,\n",
       "                        -0.0866, -0.0589, -0.0704, -0.0808, -0.0685, -0.0707, -0.1617, -0.1076,\n",
       "                        -0.1101, -0.0991, -0.0787, -0.0568, -0.1057, -0.0684, -0.0768, -0.0697,\n",
       "                        -0.0891, -0.0910, -0.0646, -0.0615, -0.0800, -0.1012, -0.1422, -0.0680,\n",
       "                        -0.0680, -0.0703, -0.0836, -0.0733, -0.1145, -0.1059, -0.0663, -0.0778,\n",
       "                        -0.0670, -0.0645, -0.0646, -0.0830, -0.0794, -0.1146, -0.1096, -0.0846,\n",
       "                        -0.1067, -0.0749, -0.0713, -0.0894, -0.0726, -0.0565, -0.1018, -0.0761,\n",
       "                        -0.0832, -0.0828, -0.0875, -0.0913, -0.0685, -0.0870, -0.0691, -0.0799,\n",
       "                        -0.1051, -0.0919, -0.0726, -0.0966, -0.0826, -0.0580, -0.0789, -0.0733,\n",
       "                        -0.1938, -0.0885, -0.0684, -0.0761, -0.0743, -0.0893, -0.1120, -0.0796,\n",
       "                        -0.0891, -0.1135, -0.0775, -0.0761, -0.0735, -0.0779, -0.0537, -0.0758,\n",
       "                        -0.0819, -0.0624, -0.0940, -0.0676, -0.0832, -0.0627, -0.0654, -0.0614,\n",
       "                        -0.0923, -0.1051, -0.1034, -0.0946, -0.1342, -0.0656, -0.0696, -0.1106,\n",
       "                        -0.0722, -0.1216, -0.0639, -0.0936, -0.0740, -0.0866, -0.0678, -0.1109,\n",
       "                        -0.0667, -0.0641, -0.0893, -0.1359, -0.2636, -0.0815, -0.0736, -0.1061,\n",
       "                        -0.0824, -0.0683, -0.0703, -0.0985, -0.0533, -0.0724, -0.0709, -0.1100,\n",
       "                        -0.0950, -0.0693, -0.0994, -0.0726, -0.0718, -0.0815, -0.0688, -0.0639,\n",
       "                        -0.0614, -0.0892, -0.0744, -0.0854, -0.0577, -0.0809, -0.0861, -0.1000,\n",
       "                        -0.1158, -0.0795, -0.0647, -0.0656, -0.0877, -0.1354, -0.0780, -0.0816,\n",
       "                        -0.0846, -0.0811, -0.0751, -0.0943, -0.0584, -0.0700, -0.0610, -0.0571,\n",
       "                        -0.0538, -0.0675, -0.1505, -0.0854, -0.0810, -0.0968, -0.0947, -0.1028,\n",
       "                        -0.0767, -0.0739, -0.0744, -0.0538, -0.1065, -0.0963, -0.0881, -0.1248,\n",
       "                        -0.0750, -0.0833, -0.0654, -0.0728, -0.0977, -0.0799, -0.0654, -0.0671,\n",
       "                        -0.0659, -0.0761, -0.0673, -0.0725, -0.0682, -0.0797, -0.1221, -0.0685,\n",
       "                        -0.0618, -0.0776, -0.1618, -0.0874, -0.1168, -0.0614, -0.0753, -0.0615,\n",
       "                        -0.0612, -0.0764, -0.0740, -0.1350, -0.0953, -0.0763, -0.0828, -0.0527,\n",
       "                        -0.0785, -0.0507, -0.0577, -0.1013, -0.0850, -0.0549, -0.0657, -0.0765,\n",
       "                        -0.0852, -0.0747, -0.0676, -0.0947, -0.0581, -0.1284, -0.0931, -0.0812,\n",
       "                        -0.1381, -0.0712, -0.0685, -0.0860, -0.0582, -0.0949, -0.0710, -0.0668,\n",
       "                        -0.0932, -0.0683, -0.1178, -0.0806, -0.0892, -0.0761, -0.0657, -0.0770,\n",
       "                        -0.0851, -0.0603, -0.1691, -0.0873, -0.0999, -0.0688, -0.1769, -0.0823,\n",
       "                        -0.0589, -0.0968, -0.0634, -0.0923, -0.0791, -0.0876, -0.0662, -0.1180,\n",
       "                        -0.0609, -0.0579, -0.1130, -0.1947, -0.0728, -0.0957, -0.0692, -0.1232,\n",
       "                        -0.0723, -0.0736, -0.0763, -0.0574, -0.0931, -0.0754, -0.1218, -0.0742,\n",
       "                        -0.0744, -0.0554, -0.0985, -0.0970, -0.0686, -0.1290, -0.0904, -0.1145,\n",
       "                        -0.0693, -0.1032, -0.0754, -0.0817, -0.1015, -0.0871, -0.0593, -0.0643,\n",
       "                        -0.0852, -0.0745, -0.0966, -0.0619, -0.1018, -0.0632, -0.0931, -0.0695,\n",
       "                        -0.0521, -0.0834, -0.0762, -0.0627, -0.0957, -0.0774, -0.0594, -0.0788,\n",
       "                        -0.0603, -0.1139, -0.0672, -0.0733, -0.0840, -0.0839, -0.0753, -0.1081,\n",
       "                        -0.0828, -0.0625, -0.0888, -0.0644, -0.0703, -0.1299, -0.1644, -0.0580,\n",
       "                        -0.0614, -0.0698, -0.0635, -0.0890, -0.0786, -0.0651, -0.0759, -0.1362,\n",
       "                        -0.1699, -0.0765, -0.0771, -0.0649, -0.0875, -0.1373, -0.0703, -0.0943,\n",
       "                        -0.0895, -0.0856, -0.0759, -0.0872, -0.0713, -0.0676, -0.0717, -0.0688,\n",
       "                        -0.0760, -0.0595, -0.0658, -0.0628, -0.0684, -0.0826, -0.0942, -0.0920,\n",
       "                        -0.1086, -0.0694, -0.0682, -0.0775, -0.0767, -0.0774, -0.0603, -0.0747,\n",
       "                        -0.0761, -0.0657, -0.0604, -0.0841, -0.0915, -0.0622, -0.0683, -0.1795]), max_val=tensor([0.1017, 0.0705, 0.1253, 0.0995, 0.0736, 0.0870, 0.0930, 0.1335, 0.0947,\n",
       "                        0.1321, 0.0964, 0.1279, 0.1750, 0.0905, 0.0942, 0.0958, 0.1271, 0.0847,\n",
       "                        0.0930, 0.0789, 0.0729, 0.0907, 0.0937, 0.0961, 0.0794, 0.0827, 0.0899,\n",
       "                        0.1086, 0.0763, 0.1011, 0.0824, 0.0817, 0.1103, 0.1254, 0.0823, 0.0816,\n",
       "                        0.1131, 0.1145, 0.0859, 0.1257, 0.0918, 0.1169, 0.0894, 0.0872, 0.0827,\n",
       "                        0.1031, 0.0787, 0.0788, 0.1138, 0.0992, 0.0921, 0.0673, 0.1485, 0.1128,\n",
       "                        0.1844, 0.0870, 0.1301, 0.0947, 0.0777, 0.1165, 0.0935, 0.1913, 0.0804,\n",
       "                        0.1679, 0.0675, 0.1204, 0.1407, 0.2449, 0.1098, 0.1145, 0.0761, 0.1034,\n",
       "                        0.0812, 0.0889, 0.1333, 0.0996, 0.0885, 0.0811, 0.1149, 0.0842, 0.0969,\n",
       "                        0.1031, 0.1526, 0.0942, 0.0650, 0.0736, 0.1766, 0.0637, 0.1197, 0.0839,\n",
       "                        0.0873, 0.1008, 0.1520, 0.1138, 0.1186, 0.0694, 0.0742, 0.0933, 0.0855,\n",
       "                        0.0806, 0.0768, 0.1023, 0.0842, 0.1200, 0.0770, 0.1172, 0.0807, 0.1268,\n",
       "                        0.0745, 0.1259, 0.0882, 0.0655, 0.0786, 0.0850, 0.1345, 0.1210, 0.1088,\n",
       "                        0.0736, 0.0779, 0.0738, 0.0825, 0.1965, 0.0871, 0.1074, 0.1050, 0.0859,\n",
       "                        0.0984, 0.1344, 0.1198, 0.1104, 0.0712, 0.0763, 0.1039, 0.0768, 0.1138,\n",
       "                        0.1225, 0.0678, 0.1048, 0.1175, 0.1095, 0.1297, 0.1161, 0.0769, 0.0961,\n",
       "                        0.1370, 0.0804, 0.0977, 0.1276, 0.0716, 0.0818, 0.0874, 0.1051, 0.1133,\n",
       "                        0.0933, 0.1093, 0.1468, 0.1439, 0.0694, 0.1131, 0.0761, 0.1038, 0.0878,\n",
       "                        0.0780, 0.0889, 0.0945, 0.0909, 0.1061, 0.1597, 0.0852, 0.1577, 0.0803,\n",
       "                        0.0830, 0.0852, 0.1196, 0.1202, 0.0786, 0.1380, 0.1349, 0.0919, 0.0725,\n",
       "                        0.1066, 0.1075, 0.0845, 0.0941, 0.0918, 0.1079, 0.0885, 0.0920, 0.1141,\n",
       "                        0.0793, 0.0946, 0.0930, 0.0788, 0.1078, 0.0662, 0.0833, 0.1488, 0.0797,\n",
       "                        0.0860, 0.0823, 0.0973, 0.0773, 0.0843, 0.0751, 0.0935, 0.1254, 0.1723,\n",
       "                        0.1366, 0.1003, 0.1547, 0.0987, 0.0778, 0.0905, 0.0756, 0.0758, 0.0808,\n",
       "                        0.3826, 0.0956, 0.0879, 0.0982, 0.0831, 0.1039, 0.1052, 0.0719, 0.0987,\n",
       "                        0.0894, 0.0696, 0.0920, 0.0715, 0.1739, 0.0923, 0.0666, 0.0881, 0.1171,\n",
       "                        0.0918, 0.0748, 0.0988, 0.0965, 0.1166, 0.1477, 0.0893, 0.1245, 0.1956,\n",
       "                        0.0894, 0.1180, 0.0809, 0.0729, 0.0821, 0.1147, 0.1318, 0.0902, 0.1054,\n",
       "                        0.0614, 0.1218, 0.0946, 0.1008, 0.1045, 0.0837, 0.1103, 0.1178, 0.2530,\n",
       "                        0.0918, 0.1248, 0.1525, 0.0802, 0.1071, 0.0744, 0.1082, 0.0985, 0.1131,\n",
       "                        0.0911, 0.0845, 0.0770, 0.1087, 0.1069, 0.1978, 0.0827, 0.0964, 0.0630,\n",
       "                        0.0756, 0.0848, 0.0871, 0.0868, 0.0938, 0.0790, 0.0870, 0.2199, 0.0905,\n",
       "                        0.0889, 0.0984, 0.0656, 0.0974, 0.0902, 0.1253, 0.1140, 0.0810, 0.0928,\n",
       "                        0.1191, 0.1093, 0.0922, 0.1110, 0.0903, 0.0959, 0.0794, 0.0789, 0.0995,\n",
       "                        0.0782, 0.1158, 0.0729, 0.1003, 0.0967, 0.0647, 0.0900, 0.0969, 0.1109,\n",
       "                        0.0894, 0.0985, 0.0717, 0.1016, 0.1648, 0.0821, 0.0776, 0.0840, 0.1001,\n",
       "                        0.0776, 0.1345, 0.0850, 0.1058, 0.1263, 0.0967, 0.0829, 0.0863, 0.1103,\n",
       "                        0.0742, 0.1334, 0.1484, 0.0838, 0.0750, 0.1900, 0.1405, 0.1288, 0.0808,\n",
       "                        0.1024, 0.0936, 0.0689, 0.0967, 0.0743, 0.1653, 0.1622, 0.1222, 0.0886,\n",
       "                        0.0670, 0.1303, 0.0722, 0.0839, 0.1084, 0.0855, 0.0660, 0.1106, 0.0659,\n",
       "                        0.1254, 0.1306, 0.1161, 0.0967, 0.0965, 0.0827, 0.0898, 0.1012, 0.1071,\n",
       "                        0.0880, 0.0930, 0.1596, 0.0713, 0.0962, 0.1799, 0.0802, 0.1010, 0.0925,\n",
       "                        0.1823, 0.0652, 0.1412, 0.1037, 0.0974, 0.1159, 0.0862, 0.0863, 0.1021,\n",
       "                        0.0962, 0.0914, 0.1007, 0.1333, 0.0719, 0.0885, 0.0837, 0.0824, 0.1022,\n",
       "                        0.1211, 0.0846, 0.0765, 0.1681, 0.1115, 0.0727, 0.0814, 0.1336, 0.0723,\n",
       "                        0.0679, 0.0907, 0.1689, 0.0753, 0.0803, 0.0756, 0.0797, 0.0927, 0.0851,\n",
       "                        0.1090, 0.1318, 0.1210, 0.0831, 0.0952, 0.1141, 0.0899, 0.0831, 0.1109,\n",
       "                        0.1174, 0.0804, 0.0829, 0.0945, 0.1102, 0.0894, 0.0701, 0.0827, 0.1153,\n",
       "                        0.1269, 0.1595, 0.1296, 0.0732, 0.0803, 0.1046, 0.1010, 0.0996, 0.0787,\n",
       "                        0.0832, 0.0817, 0.0771, 0.1034, 0.0840, 0.0743, 0.0939, 0.1102, 0.1465,\n",
       "                        0.0968, 0.0762, 0.1156, 0.0824, 0.0765, 0.0960, 0.0762, 0.0681, 0.1146,\n",
       "                        0.1197, 0.1168, 0.0916, 0.1553, 0.1746, 0.1099, 0.0791, 0.1459, 0.1144,\n",
       "                        0.1201, 0.0870, 0.1128, 0.1325, 0.1156, 0.1598, 0.0821, 0.0763, 0.1004,\n",
       "                        0.1510, 0.0651, 0.1243, 0.0887, 0.0973, 0.0847, 0.0743, 0.1873, 0.1462,\n",
       "                        0.1291, 0.0705, 0.0680, 0.0662, 0.0725, 0.1036, 0.0949, 0.0875, 0.0695,\n",
       "                        0.1353, 0.1793, 0.1483, 0.0950, 0.0915, 0.1339, 0.1391, 0.0925, 0.1040,\n",
       "                        0.0750, 0.1072, 0.0856, 0.0772, 0.1285, 0.1018, 0.0994, 0.1119])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0008, 0.0008, 0.0006, 0.0008, 0.0005, 0.0011, 0.0008, 0.0006,\n",
       "                      0.0010, 0.0008, 0.0007, 0.0009, 0.0016, 0.0006, 0.0007, 0.0007, 0.0010,\n",
       "                      0.0008, 0.0009, 0.0009, 0.0015, 0.0007, 0.0009, 0.0007, 0.0010, 0.0007,\n",
       "                      0.0006, 0.0007, 0.0011, 0.0009, 0.0010, 0.0006, 0.0009, 0.0008, 0.0014,\n",
       "                      0.0007, 0.0007, 0.0008, 0.0006, 0.0006, 0.0007, 0.0011, 0.0007, 0.0008,\n",
       "                      0.0011, 0.0006, 0.0007, 0.0007, 0.0007, 0.0006, 0.0012, 0.0011, 0.0009,\n",
       "                      0.0006, 0.0009, 0.0007, 0.0005, 0.0006, 0.0007, 0.0009, 0.0013, 0.0007,\n",
       "                      0.0011, 0.0007, 0.0007, 0.0005, 0.0011, 0.0018, 0.0007, 0.0008, 0.0008,\n",
       "                      0.0006, 0.0012, 0.0011, 0.0010, 0.0012, 0.0007, 0.0009, 0.0008, 0.0011,\n",
       "                      0.0006, 0.0010, 0.0008, 0.0010, 0.0005, 0.0007, 0.0006, 0.0007, 0.0006,\n",
       "                      0.0006, 0.0005, 0.0007, 0.0007, 0.0006, 0.0006, 0.0010, 0.0005, 0.0009,\n",
       "                      0.0009, 0.0007, 0.0007, 0.0010, 0.0006, 0.0007, 0.0007, 0.0007, 0.0008,\n",
       "                      0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0012, 0.0012, 0.0008, 0.0007,\n",
       "                      0.0007, 0.0009, 0.0006, 0.0006, 0.0008, 0.0009, 0.0008, 0.0008, 0.0008,\n",
       "                      0.0010, 0.0007, 0.0005, 0.0007, 0.0010, 0.0007, 0.0005, 0.0008, 0.0013,\n",
       "                      0.0008, 0.0010, 0.0010, 0.0005, 0.0007, 0.0006, 0.0015, 0.0005, 0.0005,\n",
       "                      0.0010, 0.0012, 0.0011, 0.0008, 0.0010, 0.0007, 0.0007, 0.0006, 0.0010,\n",
       "                      0.0013, 0.0014, 0.0012, 0.0006, 0.0006, 0.0005, 0.0006, 0.0009, 0.0011,\n",
       "                      0.0009, 0.0005, 0.0020, 0.0007, 0.0007, 0.0006, 0.0009, 0.0009, 0.0006,\n",
       "                      0.0008, 0.0006, 0.0014, 0.0006, 0.0005, 0.0007, 0.0006, 0.0006, 0.0009,\n",
       "                      0.0012, 0.0007, 0.0006, 0.0006, 0.0007, 0.0007, 0.0007, 0.0009, 0.0006,\n",
       "                      0.0011, 0.0007, 0.0014, 0.0009, 0.0006, 0.0006, 0.0008, 0.0008, 0.0015,\n",
       "                      0.0006, 0.0007, 0.0010, 0.0012, 0.0009, 0.0006, 0.0009, 0.0010, 0.0007,\n",
       "                      0.0008, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0006, 0.0010, 0.0006,\n",
       "                      0.0006, 0.0010, 0.0008, 0.0010, 0.0009, 0.0008, 0.0007, 0.0010, 0.0009,\n",
       "                      0.0007, 0.0006, 0.0008, 0.0007, 0.0009, 0.0006, 0.0014, 0.0008, 0.0009,\n",
       "                      0.0008, 0.0008, 0.0008, 0.0009, 0.0006, 0.0011, 0.0006, 0.0010, 0.0013,\n",
       "                      0.0006, 0.0016, 0.0009, 0.0011, 0.0008, 0.0008, 0.0006, 0.0008, 0.0009,\n",
       "                      0.0008, 0.0011, 0.0005, 0.0005, 0.0010, 0.0005, 0.0005, 0.0009, 0.0008,\n",
       "                      0.0008, 0.0006, 0.0007, 0.0006, 0.0006, 0.0014, 0.0017, 0.0006, 0.0006,\n",
       "                      0.0006, 0.0013, 0.0010, 0.0010, 0.0026, 0.0006, 0.0006, 0.0007, 0.0007,\n",
       "                      0.0006, 0.0007, 0.0006, 0.0009, 0.0007, 0.0017, 0.0006, 0.0008, 0.0011,\n",
       "                      0.0005, 0.0010, 0.0005, 0.0007, 0.0006, 0.0007, 0.0008, 0.0009, 0.0010,\n",
       "                      0.0009, 0.0008, 0.0004, 0.0008, 0.0007, 0.0007, 0.0006, 0.0009, 0.0005,\n",
       "                      0.0008, 0.0006, 0.0006, 0.0007, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007,\n",
       "                      0.0007, 0.0008, 0.0010, 0.0006, 0.0019, 0.0005, 0.0010, 0.0009, 0.0012,\n",
       "                      0.0009, 0.0008, 0.0008, 0.0010, 0.0006, 0.0008, 0.0007, 0.0006, 0.0011,\n",
       "                      0.0009, 0.0008, 0.0005, 0.0010, 0.0009, 0.0008, 0.0005, 0.0008, 0.0027,\n",
       "                      0.0007, 0.0007, 0.0006, 0.0009, 0.0008, 0.0006, 0.0005, 0.0019, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0009, 0.0007, 0.0008, 0.0007, 0.0006, 0.0009, 0.0013,\n",
       "                      0.0005, 0.0015, 0.0005, 0.0009, 0.0005, 0.0007, 0.0011, 0.0009, 0.0007,\n",
       "                      0.0008, 0.0006, 0.0007, 0.0022, 0.0010, 0.0006, 0.0007, 0.0006, 0.0008,\n",
       "                      0.0009, 0.0012, 0.0005, 0.0007, 0.0006, 0.0010, 0.0006, 0.0007, 0.0005,\n",
       "                      0.0022, 0.0006, 0.0008, 0.0009, 0.0007, 0.0012, 0.0008, 0.0010, 0.0007,\n",
       "                      0.0006, 0.0007, 0.0006, 0.0007, 0.0012, 0.0007, 0.0008, 0.0006, 0.0008,\n",
       "                      0.0005, 0.0010, 0.0011, 0.0005, 0.0007, 0.0008, 0.0009, 0.0011, 0.0007,\n",
       "                      0.0007, 0.0008, 0.0005, 0.0009, 0.0010, 0.0016, 0.0008, 0.0006, 0.0006,\n",
       "                      0.0008, 0.0009, 0.0010, 0.0008, 0.0008, 0.0007, 0.0012, 0.0010, 0.0008,\n",
       "                      0.0010, 0.0009, 0.0009, 0.0007, 0.0007, 0.0009, 0.0006, 0.0007, 0.0013,\n",
       "                      0.0010, 0.0006, 0.0007, 0.0007, 0.0011, 0.0007, 0.0009, 0.0027, 0.0014,\n",
       "                      0.0008, 0.0009, 0.0010, 0.0007, 0.0008, 0.0006, 0.0005, 0.0007, 0.0009,\n",
       "                      0.0007, 0.0007, 0.0007, 0.0009, 0.0008, 0.0009, 0.0005, 0.0010, 0.0008,\n",
       "                      0.0011, 0.0011, 0.0006, 0.0007, 0.0005, 0.0009, 0.0007, 0.0010, 0.0010,\n",
       "                      0.0008, 0.0009, 0.0007, 0.0005, 0.0007, 0.0007, 0.0006, 0.0008, 0.0009,\n",
       "                      0.0006, 0.0008, 0.0007, 0.0008, 0.0010, 0.0006, 0.0007, 0.0008, 0.0011,\n",
       "                      0.0007, 0.0009, 0.0008, 0.0006, 0.0011, 0.0007, 0.0005, 0.0007, 0.0005,\n",
       "                      0.0007, 0.0006, 0.0009, 0.0010, 0.0007, 0.0007, 0.0008, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0778, -0.0698, -0.0880, -0.0829, -0.0681, -0.0650, -0.0996, -0.0604,\n",
       "                        -0.0716, -0.0993, -0.0887, -0.0559, -0.1111, -0.2054, -0.0827, -0.0616,\n",
       "                        -0.0592, -0.0612, -0.0738, -0.0743, -0.0664, -0.0781, -0.0725, -0.0587,\n",
       "                        -0.0836, -0.1155, -0.0887, -0.0698, -0.0633, -0.0706, -0.0693, -0.1149,\n",
       "                        -0.0781, -0.1129, -0.0928, -0.1196, -0.0835, -0.0811, -0.0801, -0.0629,\n",
       "                        -0.0614, -0.0583, -0.1396, -0.0861, -0.0795, -0.0851, -0.0720, -0.0877,\n",
       "                        -0.0585, -0.0936, -0.0704, -0.0831, -0.0870, -0.0854, -0.0669, -0.1114,\n",
       "                        -0.0842, -0.0597, -0.0779, -0.0914, -0.0750, -0.1288, -0.0887, -0.1019,\n",
       "                        -0.0796, -0.0732, -0.0627, -0.1065, -0.1406, -0.0883, -0.0584, -0.0814,\n",
       "                        -0.0795, -0.1089, -0.1412, -0.0945, -0.0868, -0.0734, -0.0789, -0.0727,\n",
       "                        -0.0766, -0.0717, -0.0625, -0.0756, -0.0744, -0.0565, -0.0616, -0.0697,\n",
       "                        -0.0593, -0.0721, -0.0648, -0.0664, -0.0734, -0.0847, -0.0675, -0.0608,\n",
       "                        -0.0588, -0.0648, -0.0907, -0.0892, -0.0791, -0.0620, -0.0786, -0.0805,\n",
       "                        -0.0580, -0.0706, -0.0896, -0.0856, -0.1007, -0.1054, -0.0985, -0.0672,\n",
       "                        -0.0711, -0.1081, -0.0881, -0.0785, -0.0748, -0.0913, -0.1105, -0.0668,\n",
       "                        -0.0640, -0.0997, -0.0906, -0.0595, -0.0857, -0.0941, -0.1273, -0.0646,\n",
       "                        -0.0640, -0.0617, -0.1279, -0.0555, -0.0533, -0.0707, -0.1132, -0.0783,\n",
       "                        -0.0870, -0.0685, -0.0628, -0.0935, -0.0592, -0.0917, -0.0570, -0.0678,\n",
       "                        -0.0742, -0.0844, -0.1384, -0.0962, -0.0906, -0.0711, -0.0734, -0.0745,\n",
       "                        -0.1149, -0.0882, -0.0786, -0.0706, -0.0751, -0.0644, -0.0537, -0.0661,\n",
       "                        -0.0887, -0.0812, -0.0905, -0.0636, -0.1381, -0.0931, -0.0722, -0.0572,\n",
       "                        -0.0726, -0.0617, -0.0643, -0.0759, -0.0772, -0.1397, -0.0665, -0.0654,\n",
       "                        -0.0922, -0.0621, -0.0746, -0.0728, -0.1040, -0.0741, -0.0595, -0.0637,\n",
       "                        -0.0634, -0.0840, -0.0695, -0.0847, -0.0643, -0.0800, -0.0873, -0.0733,\n",
       "                        -0.1102, -0.0631, -0.0546, -0.0982, -0.0708, -0.1924, -0.0683, -0.0691,\n",
       "                        -0.1213, -0.0915, -0.0911, -0.0655, -0.1013, -0.0681, -0.0944, -0.1073,\n",
       "                        -0.0910, -0.0564, -0.0739, -0.0692, -0.0756, -0.0619, -0.0890, -0.0631,\n",
       "                        -0.0625, -0.0885, -0.0675, -0.0883, -0.0841, -0.0917, -0.0582, -0.0959,\n",
       "                        -0.0757, -0.0578, -0.0604, -0.0808, -0.0785, -0.0884, -0.0636, -0.1075,\n",
       "                        -0.0710, -0.0826, -0.0658, -0.0658, -0.1024, -0.0787, -0.0782, -0.1469,\n",
       "                        -0.0735, -0.1307, -0.1672, -0.0535, -0.0981, -0.0941, -0.0749, -0.0999,\n",
       "                        -0.0716, -0.0764, -0.0911, -0.0921, -0.0925, -0.0782, -0.0573, -0.0666,\n",
       "                        -0.1338, -0.0656, -0.0606, -0.0645, -0.0998, -0.0601, -0.0710, -0.0675,\n",
       "                        -0.0668, -0.0787, -0.0986, -0.1589, -0.0781, -0.0716, -0.0775, -0.0872,\n",
       "                        -0.1332, -0.0768, -0.1455, -0.0571, -0.0634, -0.0758, -0.0654, -0.0688,\n",
       "                        -0.0683, -0.0719, -0.0895, -0.0855, -0.2163, -0.0736, -0.0734, -0.1445,\n",
       "                        -0.0525, -0.1048, -0.0641, -0.0955, -0.0707, -0.0787, -0.0614, -0.1211,\n",
       "                        -0.1295, -0.0556, -0.0691, -0.0504, -0.0919, -0.0868, -0.0789, -0.0668,\n",
       "                        -0.1191, -0.0645, -0.1034, -0.0617, -0.0759, -0.0761, -0.0566, -0.0676,\n",
       "                        -0.0616, -0.0616, -0.0744, -0.0728, -0.0713, -0.1050, -0.0627, -0.2378,\n",
       "                        -0.0688, -0.0704, -0.0656, -0.0936, -0.0907, -0.0866, -0.0779, -0.1016,\n",
       "                        -0.0654, -0.0815, -0.0718, -0.0720, -0.0764, -0.0983, -0.1048, -0.0640,\n",
       "                        -0.1279, -0.0596, -0.0655, -0.0485, -0.0732, -0.1779, -0.0658, -0.0908,\n",
       "                        -0.0689, -0.1132, -0.0836, -0.0818, -0.0532, -0.2364, -0.0634, -0.0704,\n",
       "                        -0.0873, -0.0735, -0.0766, -0.0819, -0.0901, -0.0654, -0.0882, -0.0925,\n",
       "                        -0.0594, -0.1012, -0.0676, -0.0852, -0.0677, -0.0905, -0.0888, -0.1137,\n",
       "                        -0.0843, -0.0668, -0.0707, -0.0642, -0.2833, -0.0894, -0.0699, -0.0908,\n",
       "                        -0.0790, -0.0750, -0.0851, -0.0927, -0.0585, -0.0695, -0.0680, -0.0772,\n",
       "                        -0.0793, -0.0891, -0.0693, -0.1355, -0.0577, -0.0649, -0.0888, -0.0581,\n",
       "                        -0.1007, -0.0614, -0.1306, -0.0732, -0.0641, -0.0704, -0.0694, -0.0933,\n",
       "                        -0.0638, -0.0504, -0.0679, -0.0813, -0.0971, -0.0581, -0.0838, -0.0894,\n",
       "                        -0.0579, -0.0813, -0.1022, -0.1216, -0.0892, -0.0689, -0.0673, -0.0703,\n",
       "                        -0.0569, -0.1144, -0.1106, -0.1064, -0.0693, -0.0650, -0.0656, -0.0615,\n",
       "                        -0.1115, -0.0868, -0.0748, -0.0817, -0.0774, -0.1510, -0.1216, -0.0898,\n",
       "                        -0.1006, -0.0860, -0.0896, -0.0642, -0.0649, -0.0755, -0.0809, -0.0858,\n",
       "                        -0.1487, -0.0800, -0.0727, -0.0715, -0.0649, -0.0841, -0.0693, -0.0699,\n",
       "                        -0.1470, -0.0733, -0.1014, -0.1036, -0.0917, -0.0900, -0.1027, -0.0623,\n",
       "                        -0.0700, -0.0617, -0.0770, -0.0602, -0.0754, -0.0885, -0.0790, -0.0917,\n",
       "                        -0.0856, -0.0678, -0.0952, -0.0778, -0.0780, -0.0625, -0.0713, -0.0651,\n",
       "                        -0.0647, -0.0985, -0.0904, -0.0903, -0.0653, -0.0756, -0.0811, -0.0695,\n",
       "                        -0.0695, -0.0661, -0.0839, -0.0538, -0.0663, -0.1159, -0.0780, -0.0861,\n",
       "                        -0.0601, -0.1031, -0.0872, -0.0694, -0.0767, -0.1074, -0.0857, -0.0959,\n",
       "                        -0.0753, -0.0849, -0.0722, -0.0878, -0.0874, -0.0617, -0.0619, -0.0558,\n",
       "                        -0.0684, -0.0663, -0.0754, -0.0694, -0.0770, -0.0825, -0.0913, -0.1045]), max_val=tensor([0.1097, 0.1064, 0.1011, 0.0704, 0.0993, 0.0656, 0.1373, 0.1074, 0.0632,\n",
       "                        0.1309, 0.1072, 0.0875, 0.1084, 0.1700, 0.0808, 0.0853, 0.0896, 0.1211,\n",
       "                        0.1007, 0.1094, 0.1103, 0.1908, 0.0828, 0.1181, 0.0851, 0.1226, 0.0788,\n",
       "                        0.0745, 0.0831, 0.1423, 0.1205, 0.1252, 0.0726, 0.1126, 0.1031, 0.1720,\n",
       "                        0.0901, 0.0938, 0.0960, 0.0772, 0.0811, 0.0912, 0.0818, 0.0906, 0.1017,\n",
       "                        0.1387, 0.0773, 0.0723, 0.0829, 0.0853, 0.0801, 0.1527, 0.1388, 0.1083,\n",
       "                        0.0703, 0.1057, 0.0828, 0.0670, 0.0635, 0.0692, 0.1180, 0.1701, 0.0737,\n",
       "                        0.1358, 0.0919, 0.0869, 0.0603, 0.1384, 0.2311, 0.0714, 0.1068, 0.0971,\n",
       "                        0.0786, 0.1472, 0.0902, 0.1275, 0.1465, 0.0846, 0.1152, 0.1047, 0.1335,\n",
       "                        0.0786, 0.1309, 0.1032, 0.1212, 0.0665, 0.0935, 0.0775, 0.0889, 0.0686,\n",
       "                        0.0729, 0.0668, 0.0930, 0.0724, 0.0792, 0.0780, 0.1248, 0.0684, 0.1142,\n",
       "                        0.1131, 0.0846, 0.0893, 0.1284, 0.0744, 0.0886, 0.0913, 0.0914, 0.1036,\n",
       "                        0.1060, 0.0830, 0.1011, 0.0899, 0.0880, 0.1536, 0.1482, 0.0970, 0.0919,\n",
       "                        0.0508, 0.0862, 0.0767, 0.0705, 0.0852, 0.1153, 0.0971, 0.0992, 0.1062,\n",
       "                        0.0958, 0.0910, 0.0613, 0.0846, 0.1092, 0.0894, 0.0669, 0.0980, 0.1640,\n",
       "                        0.1021, 0.1299, 0.1228, 0.0698, 0.0818, 0.0734, 0.1946, 0.0693, 0.0657,\n",
       "                        0.1217, 0.1566, 0.1191, 0.1013, 0.1221, 0.0881, 0.0948, 0.0752, 0.1333,\n",
       "                        0.1597, 0.1755, 0.1479, 0.0796, 0.0709, 0.0651, 0.0781, 0.1111, 0.1367,\n",
       "                        0.1102, 0.0504, 0.2585, 0.0835, 0.0950, 0.0770, 0.1115, 0.1189, 0.0783,\n",
       "                        0.0974, 0.0787, 0.1841, 0.0790, 0.0586, 0.0944, 0.0824, 0.0782, 0.1184,\n",
       "                        0.1471, 0.0852, 0.0743, 0.0743, 0.0917, 0.0683, 0.0945, 0.1116, 0.0741,\n",
       "                        0.1351, 0.0770, 0.1762, 0.0979, 0.0820, 0.0776, 0.0887, 0.0986, 0.1294,\n",
       "                        0.0784, 0.0926, 0.1216, 0.1526, 0.1129, 0.0749, 0.1083, 0.1227, 0.0851,\n",
       "                        0.0972, 0.0782, 0.0808, 0.0915, 0.0844, 0.0856, 0.0734, 0.1224, 0.0798,\n",
       "                        0.0818, 0.1233, 0.1000, 0.1260, 0.1123, 0.1027, 0.0921, 0.1225, 0.1152,\n",
       "                        0.0851, 0.0793, 0.1074, 0.0869, 0.1165, 0.0757, 0.1807, 0.1003, 0.1151,\n",
       "                        0.1077, 0.0958, 0.0990, 0.1117, 0.0751, 0.1223, 0.0816, 0.1141, 0.0884,\n",
       "                        0.0725, 0.2075, 0.1155, 0.1354, 0.1076, 0.1028, 0.0719, 0.0967, 0.1153,\n",
       "                        0.1035, 0.1451, 0.0627, 0.0574, 0.1108, 0.0693, 0.0647, 0.1183, 0.0854,\n",
       "                        0.0974, 0.0741, 0.0911, 0.0734, 0.0691, 0.1830, 0.2129, 0.0496, 0.0724,\n",
       "                        0.0640, 0.1643, 0.1198, 0.1216, 0.3304, 0.0782, 0.0799, 0.0913, 0.0837,\n",
       "                        0.0818, 0.0894, 0.0676, 0.1148, 0.0936, 0.2010, 0.0775, 0.1067, 0.1036,\n",
       "                        0.0634, 0.1257, 0.0682, 0.0871, 0.0709, 0.0936, 0.0987, 0.1150, 0.1158,\n",
       "                        0.1089, 0.1070, 0.0559, 0.1017, 0.0936, 0.0940, 0.0752, 0.1030, 0.0616,\n",
       "                        0.0869, 0.0764, 0.0822, 0.0843, 0.0870, 0.0767, 0.0837, 0.0898, 0.0917,\n",
       "                        0.0853, 0.0956, 0.1241, 0.0810, 0.2310, 0.0690, 0.1246, 0.1081, 0.1562,\n",
       "                        0.1090, 0.1009, 0.1071, 0.1323, 0.0708, 0.1028, 0.0826, 0.0802, 0.1384,\n",
       "                        0.1196, 0.1015, 0.0609, 0.0990, 0.1082, 0.1075, 0.0698, 0.1068, 0.3487,\n",
       "                        0.0880, 0.0748, 0.0777, 0.1031, 0.1020, 0.0784, 0.0601, 0.2476, 0.0776,\n",
       "                        0.0862, 0.0848, 0.1127, 0.0826, 0.0986, 0.0696, 0.0764, 0.1107, 0.1641,\n",
       "                        0.0593, 0.1843, 0.0618, 0.1131, 0.0687, 0.0802, 0.1379, 0.0764, 0.0824,\n",
       "                        0.0958, 0.0797, 0.0931, 0.2412, 0.1218, 0.0739, 0.0951, 0.0704, 0.0994,\n",
       "                        0.1103, 0.1583, 0.0618, 0.0907, 0.0753, 0.1237, 0.0742, 0.0798, 0.0637,\n",
       "                        0.2733, 0.0701, 0.1022, 0.1110, 0.0891, 0.1492, 0.0989, 0.0896, 0.0880,\n",
       "                        0.0735, 0.0866, 0.0733, 0.0801, 0.1519, 0.0863, 0.0965, 0.0795, 0.0852,\n",
       "                        0.0693, 0.1271, 0.1442, 0.0682, 0.0874, 0.0799, 0.0953, 0.1405, 0.0929,\n",
       "                        0.0901, 0.0984, 0.0655, 0.1033, 0.1290, 0.2063, 0.1010, 0.0724, 0.0719,\n",
       "                        0.0967, 0.0803, 0.1249, 0.1000, 0.0958, 0.0894, 0.1272, 0.0992, 0.1012,\n",
       "                        0.1239, 0.1144, 0.1154, 0.0846, 0.0902, 0.1173, 0.0704, 0.0941, 0.1700,\n",
       "                        0.1311, 0.0792, 0.0842, 0.0830, 0.1395, 0.0850, 0.1081, 0.3470, 0.1825,\n",
       "                        0.0872, 0.1171, 0.1213, 0.0951, 0.0817, 0.0752, 0.0617, 0.0880, 0.1094,\n",
       "                        0.0827, 0.0829, 0.0792, 0.1125, 0.0986, 0.1200, 0.0664, 0.1211, 0.0956,\n",
       "                        0.1344, 0.1373, 0.0765, 0.0916, 0.0684, 0.1106, 0.0834, 0.1232, 0.1279,\n",
       "                        0.1020, 0.1148, 0.0941, 0.0612, 0.0887, 0.0892, 0.0719, 0.1036, 0.0855,\n",
       "                        0.0736, 0.1016, 0.0829, 0.0990, 0.1241, 0.0810, 0.0884, 0.1000, 0.1414,\n",
       "                        0.0837, 0.1082, 0.0957, 0.0780, 0.1448, 0.0899, 0.0670, 0.0873, 0.0687,\n",
       "                        0.0865, 0.0766, 0.1140, 0.1208, 0.0843, 0.0913, 0.1075, 0.1108])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0007, 0.0010, 0.0008, 0.0022, 0.0006, 0.0008, 0.0011, 0.0016, 0.0006,\n",
       "                        0.0008, 0.0005, 0.0007, 0.0014, 0.0005, 0.0009, 0.0008, 0.0015, 0.0011,\n",
       "                        0.0012, 0.0012, 0.0008, 0.0015, 0.0011, 0.0012, 0.0009, 0.0006, 0.0013,\n",
       "                        0.0009, 0.0014, 0.0009, 0.0010, 0.0013, 0.0009, 0.0012, 0.0007, 0.0011,\n",
       "                        0.0007, 0.0009, 0.0011, 0.0018, 0.0008, 0.0010, 0.0015, 0.0017, 0.0009,\n",
       "                        0.0007, 0.0011, 0.0016, 0.0008, 0.0010, 0.0009, 0.0016, 0.0009, 0.0007,\n",
       "                        0.0008, 0.0018, 0.0011, 0.0009, 0.0009, 0.0006, 0.0021, 0.0018, 0.0006,\n",
       "                        0.0011, 0.0008, 0.0016, 0.0011, 0.0007, 0.0020, 0.0011, 0.0007, 0.0008,\n",
       "                        0.0010, 0.0005, 0.0017, 0.0007, 0.0018, 0.0010, 0.0009, 0.0011, 0.0013,\n",
       "                        0.0010, 0.0011, 0.0005, 0.0009, 0.0007, 0.0016, 0.0015, 0.0008, 0.0007,\n",
       "                        0.0007, 0.0008, 0.0011, 0.0021, 0.0008, 0.0007, 0.0013, 0.0009, 0.0013,\n",
       "                        0.0009, 0.0009, 0.0009, 0.0007, 0.0010, 0.0014, 0.0015, 0.0005, 0.0010,\n",
       "                        0.0007, 0.0014, 0.0015, 0.0008, 0.0014, 0.0016, 0.0009, 0.0012, 0.0005,\n",
       "                        0.0035, 0.0012, 0.0009, 0.0007, 0.0009, 0.0003, 0.0012, 0.0009, 0.0007,\n",
       "                        0.0007, 0.0006, 0.0009, 0.0006, 0.0008, 0.0008, 0.0009, 0.0010, 0.0019,\n",
       "                        0.0015, 0.0005, 0.0010, 0.0006, 0.0015, 0.0011, 0.0027, 0.0010, 0.0007,\n",
       "                        0.0009, 0.0013, 0.0011, 0.0018, 0.0008, 0.0013, 0.0007, 0.0006, 0.0014,\n",
       "                        0.0008, 0.0015, 0.0007, 0.0010, 0.0007, 0.0009, 0.0010, 0.0010, 0.0019,\n",
       "                        0.0011, 0.0059, 0.0015, 0.0004, 0.0013, 0.0007, 0.0009, 0.0009, 0.0008,\n",
       "                        0.0013, 0.0009, 0.0013, 0.0011, 0.0009, 0.0013, 0.0008, 0.0007, 0.0009,\n",
       "                        0.0026, 0.0014, 0.0008, 0.0008, 0.0015, 0.0015, 0.0008, 0.0011, 0.0008,\n",
       "                        0.0011, 0.0010, 0.0019, 0.0016, 0.0022, 0.0006, 0.0011, 0.0010, 0.0013,\n",
       "                        0.0012, 0.0008, 0.0005, 0.0011, 0.0010, 0.0007, 0.0011, 0.0010, 0.0009,\n",
       "                        0.0024, 0.0018, 0.0013, 0.0009, 0.0009, 0.0009, 0.0011, 0.0008, 0.0009,\n",
       "                        0.0008, 0.0011, 0.0009, 0.0008, 0.0013, 0.0024, 0.0013, 0.0009, 0.0015,\n",
       "                        0.0011, 0.0011, 0.0010, 0.0010, 0.0008, 0.0006, 0.0011, 0.0007, 0.0011,\n",
       "                        0.0007, 0.0017, 0.0007, 0.0014, 0.0010, 0.0006, 0.0007, 0.0010, 0.0013,\n",
       "                        0.0009, 0.0008, 0.0008, 0.0011, 0.0010, 0.0010, 0.0007, 0.0012, 0.0020,\n",
       "                        0.0012, 0.0011, 0.0009, 0.0007, 0.0023, 0.0010, 0.0005, 0.0021, 0.0007,\n",
       "                        0.0008, 0.0004, 0.0011, 0.0011, 0.0006, 0.0012, 0.0017, 0.0023, 0.0010,\n",
       "                        0.0008, 0.0009, 0.0007, 0.0011, 0.0017, 0.0007, 0.0010, 0.0007, 0.0009,\n",
       "                        0.0009, 0.0007, 0.0011, 0.0008, 0.0014, 0.0003, 0.0010, 0.0012, 0.0013,\n",
       "                        0.0007, 0.0013, 0.0007, 0.0011, 0.0017, 0.0008, 0.0010, 0.0016, 0.0011,\n",
       "                        0.0015, 0.0008, 0.0007, 0.0021, 0.0009, 0.0006, 0.0010, 0.0013, 0.0012,\n",
       "                        0.0005, 0.0012, 0.0009, 0.0007, 0.0012, 0.0007, 0.0011, 0.0016, 0.0007,\n",
       "                        0.0010, 0.0007, 0.0015, 0.0011, 0.0005, 0.0008, 0.0006, 0.0015, 0.0011,\n",
       "                        0.0011, 0.0012, 0.0005, 0.0012, 0.0008, 0.0014, 0.0007, 0.0011, 0.0008,\n",
       "                        0.0009, 0.0012, 0.0011, 0.0006, 0.0011, 0.0010, 0.0007, 0.0005, 0.0003,\n",
       "                        0.0013, 0.0008, 0.0007, 0.0006, 0.0009, 0.0011, 0.0007, 0.0007, 0.0010,\n",
       "                        0.0013, 0.0020, 0.0009, 0.0008, 0.0020, 0.0009, 0.0015, 0.0013, 0.0011,\n",
       "                        0.0006, 0.0011, 0.0005, 0.0007, 0.0011, 0.0011, 0.0009, 0.0012, 0.0012,\n",
       "                        0.0005, 0.0010, 0.0007, 0.0009, 0.0008, 0.0007, 0.0007, 0.0010, 0.0009,\n",
       "                        0.0013, 0.0020, 0.0011, 0.0017, 0.0009, 0.0015, 0.0009, 0.0009, 0.0012,\n",
       "                        0.0018, 0.0013, 0.0010, 0.0010, 0.0012, 0.0011, 0.0007, 0.0021, 0.0010,\n",
       "                        0.0016, 0.0010, 0.0007, 0.0011, 0.0010, 0.0011, 0.0008, 0.0009, 0.0010,\n",
       "                        0.0008, 0.0010, 0.0009, 0.0011, 0.0007, 0.0010, 0.0007, 0.0018, 0.0007,\n",
       "                        0.0006, 0.0012, 0.0016, 0.0008, 0.0009, 0.0008, 0.0007, 0.0009, 0.0008,\n",
       "                        0.0010, 0.0005, 0.0011, 0.0014, 0.0008, 0.0004, 0.0013, 0.0013, 0.0006,\n",
       "                        0.0017, 0.0005, 0.0013, 0.0012, 0.0006, 0.0010, 0.0009, 0.0005, 0.0008,\n",
       "                        0.0008, 0.0008, 0.0014, 0.0009, 0.0008, 0.0007, 0.0007, 0.0025, 0.0022,\n",
       "                        0.0011, 0.0016, 0.0007, 0.0012, 0.0010, 0.0005, 0.0007, 0.0009, 0.0013,\n",
       "                        0.0011, 0.0015, 0.0005, 0.0007, 0.0007, 0.0019, 0.0009, 0.0011, 0.0013,\n",
       "                        0.0006, 0.0010, 0.0007, 0.0009, 0.0007, 0.0011, 0.0015, 0.0009, 0.0008,\n",
       "                        0.0009, 0.0011, 0.0008, 0.0014, 0.0006, 0.0014, 0.0006, 0.0009, 0.0005,\n",
       "                        0.0010, 0.0016, 0.0021, 0.0013, 0.0014, 0.0016, 0.0007, 0.0009, 0.0006,\n",
       "                        0.0011, 0.0012, 0.0008, 0.0009, 0.0010, 0.0010, 0.0012, 0.0012, 0.0007,\n",
       "                        0.0010, 0.0015, 0.0013, 0.0016, 0.0010, 0.0014, 0.0011, 0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                  min_val=tensor([-0.0900, -0.1107, -0.0843, -0.1011, -0.0672, -0.1085, -0.1370, -0.1128,\n",
       "                          -0.0773, -0.0673, -0.0635, -0.0896, -0.1337, -0.0644, -0.1208, -0.1033,\n",
       "                          -0.1284, -0.1309, -0.1550, -0.1046, -0.0776, -0.1770, -0.0907, -0.1532,\n",
       "                          -0.0986, -0.0647, -0.1677, -0.0873, -0.1641, -0.0957, -0.0743, -0.1684,\n",
       "                          -0.1199, -0.1539, -0.0758, -0.1089, -0.0653, -0.1091, -0.0933, -0.1349,\n",
       "                          -0.1057, -0.1128, -0.1821, -0.2164, -0.1203, -0.0905, -0.0818, -0.1173,\n",
       "                          -0.0712, -0.1107, -0.0825, -0.2052, -0.0839, -0.0903, -0.1027, -0.1460,\n",
       "                          -0.1435, -0.1106, -0.0872, -0.0811, -0.1376, -0.1969, -0.0614, -0.0731,\n",
       "                          -0.0686, -0.2105, -0.1015, -0.0678, -0.1832, -0.1250, -0.0849, -0.0987,\n",
       "                          -0.0894, -0.0485, -0.2209, -0.0857, -0.1356, -0.1077, -0.1195, -0.1119,\n",
       "                          -0.1585, -0.0903, -0.1267, -0.0688, -0.0845, -0.0946, -0.1993, -0.1977,\n",
       "                          -0.0962, -0.0826, -0.0749, -0.0921, -0.1353, -0.1820, -0.0826, -0.0900,\n",
       "                          -0.0950, -0.1205, -0.1690, -0.0779, -0.1164, -0.1063, -0.0843, -0.1286,\n",
       "                          -0.1191, -0.1195, -0.0632, -0.0882, -0.0873, -0.1093, -0.1892, -0.1043,\n",
       "                          -0.0804, -0.1986, -0.0893, -0.1423, -0.0610, -0.1406, -0.1219, -0.1121,\n",
       "                          -0.0730, -0.0703, -0.0364, -0.0953, -0.0574, -0.0836, -0.0851, -0.0609,\n",
       "                          -0.0719, -0.0737, -0.0844, -0.0984, -0.1139, -0.1302, -0.2481, -0.1765,\n",
       "                          -0.0571, -0.1271, -0.0632, -0.1184, -0.1156, -0.2666, -0.0963, -0.0644,\n",
       "                          -0.0889, -0.1157, -0.1061, -0.1856, -0.0817, -0.1330, -0.0724, -0.0665,\n",
       "                          -0.1731, -0.0907, -0.1536, -0.0783, -0.1043, -0.0715, -0.0862, -0.1039,\n",
       "                          -0.1104, -0.2327, -0.0859, -0.6282, -0.1299, -0.0488, -0.1724, -0.0901,\n",
       "                          -0.1209, -0.0979, -0.0984, -0.0798, -0.1080, -0.1198, -0.0744, -0.0891,\n",
       "                          -0.1722, -0.1042, -0.0894, -0.0548, -0.3334, -0.1034, -0.1032, -0.1088,\n",
       "                          -0.1381, -0.1024, -0.0844, -0.0717, -0.1051, -0.0825, -0.0970, -0.2372,\n",
       "                          -0.0878, -0.2839, -0.0781, -0.1439, -0.0969, -0.1676, -0.1510, -0.0779,\n",
       "                          -0.0689, -0.1364, -0.1260, -0.0617, -0.1333, -0.1233, -0.1090, -0.3059,\n",
       "                          -0.0932, -0.1664, -0.1148, -0.0844, -0.0610, -0.0938, -0.0932, -0.0999,\n",
       "                          -0.0930, -0.1090, -0.0950, -0.0725, -0.1661, -0.3127, -0.0947, -0.0841,\n",
       "                          -0.1308, -0.0728, -0.0820, -0.0933, -0.1335, -0.0883, -0.0792, -0.0737,\n",
       "                          -0.0886, -0.1401, -0.0753, -0.1613, -0.0919, -0.1734, -0.1224, -0.0829,\n",
       "                          -0.0631, -0.0916, -0.1682, -0.1060, -0.0694, -0.1006, -0.0741, -0.1220,\n",
       "                          -0.0806, -0.0933, -0.1184, -0.2529, -0.1559, -0.1401, -0.0882, -0.0831,\n",
       "                          -0.2916, -0.0966, -0.0506, -0.2722, -0.0860, -0.0936, -0.0541, -0.1197,\n",
       "                          -0.0948, -0.0819, -0.1586, -0.2209, -0.0754, -0.0749, -0.0993, -0.0682,\n",
       "                          -0.0877, -0.1299, -0.1719, -0.0898, -0.1006, -0.0758, -0.1118, -0.1069,\n",
       "                          -0.0910, -0.1139, -0.0886, -0.1480, -0.0257, -0.1158, -0.0909, -0.0931,\n",
       "                          -0.0878, -0.1012, -0.0730, -0.1226, -0.2208, -0.0899, -0.1273, -0.1213,\n",
       "                          -0.1463, -0.1461, -0.1069, -0.0720, -0.1622, -0.1154, -0.0787, -0.0833,\n",
       "                          -0.1299, -0.1440, -0.0695, -0.1311, -0.1189, -0.0892, -0.1219, -0.0854,\n",
       "                          -0.1377, -0.1832, -0.0647, -0.0744, -0.0852, -0.1641, -0.1351, -0.0401,\n",
       "                          -0.0810, -0.0604, -0.0952, -0.1405, -0.1012, -0.1550, -0.0504, -0.1504,\n",
       "                          -0.0813, -0.1104, -0.0903, -0.0999, -0.0938, -0.1011, -0.0801, -0.1114,\n",
       "                          -0.0822, -0.1133, -0.1237, -0.0937, -0.0513, -0.0322, -0.1462, -0.0854,\n",
       "                          -0.0924, -0.0625, -0.0956, -0.1467, -0.0626, -0.0891, -0.1029, -0.1694,\n",
       "                          -0.1668, -0.0694, -0.0762, -0.2543, -0.0725, -0.1193, -0.1586, -0.1142,\n",
       "                          -0.0801, -0.1401, -0.0681, -0.0728, -0.0643, -0.1349, -0.0959, -0.1094,\n",
       "                          -0.0733, -0.0460, -0.1001, -0.0874, -0.1099, -0.0993, -0.0829, -0.0885,\n",
       "                          -0.1302, -0.1110, -0.1440, -0.2570, -0.1356, -0.2208, -0.1049, -0.1915,\n",
       "                          -0.1207, -0.1180, -0.0921, -0.2188, -0.1699, -0.0933, -0.1319, -0.1099,\n",
       "                          -0.1142, -0.0950, -0.2718, -0.1164, -0.1144, -0.1097, -0.0683, -0.1122,\n",
       "                          -0.0988, -0.1048, -0.0654, -0.1193, -0.0696, -0.0703, -0.1302, -0.1023,\n",
       "                          -0.1423, -0.0957, -0.0944, -0.0890, -0.1242, -0.0678, -0.0807, -0.1319,\n",
       "                          -0.2050, -0.0762, -0.0717, -0.0775, -0.0713, -0.1046, -0.0912, -0.1197,\n",
       "                          -0.0697, -0.1176, -0.0759, -0.0721, -0.0518, -0.1192, -0.1067, -0.0614,\n",
       "                          -0.2113, -0.0661, -0.1189, -0.1080, -0.0700, -0.1177, -0.0988, -0.0690,\n",
       "                          -0.0982, -0.1013, -0.1019, -0.1749, -0.0697, -0.0994, -0.0835, -0.0899,\n",
       "                          -0.1714, -0.1359, -0.1021, -0.2064, -0.0901, -0.1051, -0.1304, -0.0625,\n",
       "                          -0.0909, -0.0876, -0.1517, -0.1420, -0.1905, -0.0638, -0.0766, -0.0939,\n",
       "                          -0.2093, -0.1190, -0.1457, -0.1234, -0.0706, -0.1267, -0.0868, -0.0856,\n",
       "                          -0.0769, -0.0843, -0.1932, -0.1164, -0.0962, -0.1111, -0.1408, -0.0967,\n",
       "                          -0.1026, -0.0776, -0.0793, -0.0653, -0.0831, -0.0544, -0.1264, -0.2104,\n",
       "                          -0.1371, -0.0776, -0.1813, -0.1146, -0.0848, -0.0802, -0.0662, -0.1112,\n",
       "                          -0.0978, -0.0817, -0.1054, -0.0932, -0.1178, -0.0858, -0.0976, -0.0781,\n",
       "                          -0.0966, -0.1782, -0.1583, -0.1285, -0.0919, -0.1142, -0.0890, -0.0781]), max_val=tensor([0.0936, 0.1326, 0.0997, 0.2799, 0.0815, 0.0809, 0.1404, 0.2051, 0.0663,\n",
       "                          0.0989, 0.0614, 0.0816, 0.1814, 0.0559, 0.1192, 0.0848, 0.1858, 0.1374,\n",
       "                          0.1086, 0.1515, 0.0994, 0.1881, 0.1430, 0.1152, 0.1184, 0.0796, 0.1261,\n",
       "                          0.1152, 0.1782, 0.1096, 0.1296, 0.1625, 0.1107, 0.0945, 0.0908, 0.1446,\n",
       "                          0.0834, 0.1182, 0.1394, 0.2275, 0.1028, 0.1268, 0.1873, 0.1798, 0.1065,\n",
       "                          0.0769, 0.1391, 0.2042, 0.0961, 0.1297, 0.1189, 0.2015, 0.1081, 0.0876,\n",
       "                          0.1071, 0.2271, 0.1218, 0.1120, 0.1153, 0.0809, 0.2625, 0.2228, 0.0765,\n",
       "                          0.1367, 0.1001, 0.1383, 0.1376, 0.0939, 0.2532, 0.1386, 0.0850, 0.1047,\n",
       "                          0.1271, 0.0572, 0.0917, 0.0775, 0.2314, 0.1293, 0.1042, 0.1459, 0.1624,\n",
       "                          0.1265, 0.1405, 0.0528, 0.1189, 0.0749, 0.1324, 0.1904, 0.0705, 0.0857,\n",
       "                          0.0941, 0.0953, 0.1266, 0.2626, 0.1061, 0.0951, 0.1674, 0.0881, 0.1575,\n",
       "                          0.1099, 0.0969, 0.1125, 0.0832, 0.1287, 0.1727, 0.1893, 0.0682, 0.1267,\n",
       "                          0.0814, 0.1738, 0.1049, 0.0888, 0.1747, 0.1687, 0.1124, 0.1513, 0.0654,\n",
       "                          0.4484, 0.1506, 0.0991, 0.0857, 0.1138, 0.0398, 0.1465, 0.1204, 0.0784,\n",
       "                          0.0790, 0.0762, 0.1164, 0.0785, 0.1059, 0.0827, 0.0933, 0.1329, 0.2091,\n",
       "                          0.1857, 0.0690, 0.1037, 0.0711, 0.1907, 0.1392, 0.3485, 0.1243, 0.0933,\n",
       "                          0.1169, 0.1714, 0.1460, 0.2291, 0.0970, 0.1610, 0.0935, 0.0818, 0.1073,\n",
       "                          0.1017, 0.1845, 0.0883, 0.1260, 0.0853, 0.1155, 0.1313, 0.1290, 0.2411,\n",
       "                          0.1358, 0.7472, 0.1935, 0.0441, 0.1566, 0.0902, 0.1142, 0.1160, 0.0847,\n",
       "                          0.1636, 0.1119, 0.1700, 0.1371, 0.1091, 0.1415, 0.0980, 0.0882, 0.1185,\n",
       "                          0.2426, 0.1741, 0.0736, 0.1052, 0.1898, 0.1960, 0.0999, 0.1394, 0.0945,\n",
       "                          0.1343, 0.1303, 0.1993, 0.1991, 0.2388, 0.0673, 0.1050, 0.1215, 0.1160,\n",
       "                          0.1493, 0.0953, 0.0551, 0.1340, 0.1176, 0.0917, 0.1416, 0.1261, 0.1025,\n",
       "                          0.2026, 0.2297, 0.1336, 0.1129, 0.1181, 0.1115, 0.1362, 0.1032, 0.1141,\n",
       "                          0.1073, 0.1394, 0.1169, 0.1078, 0.1242, 0.2181, 0.1607, 0.1107, 0.1853,\n",
       "                          0.1349, 0.1435, 0.1228, 0.1090, 0.1060, 0.0695, 0.1370, 0.0912, 0.0823,\n",
       "                          0.0929, 0.2161, 0.0947, 0.1047, 0.0996, 0.0498, 0.0918, 0.1261, 0.1353,\n",
       "                          0.1190, 0.0997, 0.1004, 0.1421, 0.1252, 0.1234, 0.0688, 0.1561, 0.1441,\n",
       "                          0.1132, 0.1189, 0.1126, 0.0886, 0.1583, 0.1318, 0.0681, 0.2114, 0.0849,\n",
       "                          0.1059, 0.0509, 0.1340, 0.1411, 0.0815, 0.1229, 0.1091, 0.2953, 0.1217,\n",
       "                          0.0826, 0.1085, 0.0880, 0.1410, 0.2129, 0.0723, 0.1268, 0.0874, 0.0702,\n",
       "                          0.1199, 0.0846, 0.1368, 0.1029, 0.1793, 0.0426, 0.1331, 0.1560, 0.1690,\n",
       "                          0.0538, 0.1676, 0.0896, 0.1435, 0.1943, 0.0997, 0.1292, 0.2090, 0.1141,\n",
       "                          0.1902, 0.0998, 0.0844, 0.2655, 0.1058, 0.0811, 0.1285, 0.1605, 0.1496,\n",
       "                          0.0526, 0.1564, 0.0853, 0.0945, 0.1552, 0.0926, 0.1198, 0.1999, 0.0863,\n",
       "                          0.1252, 0.0581, 0.1964, 0.1129, 0.0591, 0.1074, 0.0812, 0.1895, 0.1391,\n",
       "                          0.1375, 0.1333, 0.0643, 0.0900, 0.1068, 0.1744, 0.0818, 0.1357, 0.0978,\n",
       "                          0.1183, 0.1563, 0.1449, 0.0625, 0.1390, 0.1326, 0.0946, 0.0590, 0.0290,\n",
       "                          0.1714, 0.1029, 0.0825, 0.0806, 0.1122, 0.1387, 0.0849, 0.0550, 0.1268,\n",
       "                          0.1346, 0.2530, 0.1173, 0.1010, 0.1904, 0.1095, 0.1851, 0.1632, 0.1421,\n",
       "                          0.0807, 0.1376, 0.0660, 0.0934, 0.1353, 0.1354, 0.1145, 0.1540, 0.1522,\n",
       "                          0.0584, 0.1299, 0.0738, 0.1039, 0.0645, 0.0888, 0.0802, 0.1217, 0.1180,\n",
       "                          0.1644, 0.1473, 0.0752, 0.1321, 0.1204, 0.1153, 0.1073, 0.1081, 0.1497,\n",
       "                          0.2320, 0.1577, 0.1216, 0.1270, 0.1552, 0.1358, 0.0905, 0.1931, 0.1226,\n",
       "                          0.1998, 0.1297, 0.0883, 0.1415, 0.1309, 0.1334, 0.0984, 0.1092, 0.1218,\n",
       "                          0.0991, 0.1140, 0.1106, 0.0920, 0.0692, 0.1250, 0.0950, 0.2297, 0.0924,\n",
       "                          0.0785, 0.1511, 0.1591, 0.0991, 0.1112, 0.0988, 0.0919, 0.1165, 0.1075,\n",
       "                          0.1279, 0.0510, 0.1386, 0.1809, 0.1031, 0.0371, 0.1643, 0.1602, 0.0751,\n",
       "                          0.1657, 0.0650, 0.1708, 0.1502, 0.0752, 0.1306, 0.1154, 0.0689, 0.0576,\n",
       "                          0.1030, 0.0968, 0.1174, 0.1087, 0.0819, 0.0940, 0.0705, 0.3221, 0.2844,\n",
       "                          0.1352, 0.1857, 0.0950, 0.1537, 0.1254, 0.0607, 0.0908, 0.1103, 0.1596,\n",
       "                          0.1434, 0.1663, 0.0583, 0.0878, 0.0931, 0.2357, 0.1140, 0.1014, 0.1668,\n",
       "                          0.0747, 0.1278, 0.0609, 0.1190, 0.0850, 0.1446, 0.1888, 0.1141, 0.1064,\n",
       "                          0.0779, 0.1254, 0.0809, 0.1755, 0.0749, 0.1805, 0.0786, 0.1153, 0.0594,\n",
       "                          0.1126, 0.1712, 0.2718, 0.1601, 0.1723, 0.2026, 0.0866, 0.1175, 0.0819,\n",
       "                          0.1429, 0.1496, 0.0977, 0.1167, 0.1292, 0.1260, 0.1512, 0.1466, 0.0933,\n",
       "                          0.1237, 0.1842, 0.1634, 0.2029, 0.1274, 0.1788, 0.1458, 0.1683])\n",
       "                )\n",
       "              )\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "              )\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0007, 0.0008, 0.0006, 0.0005, 0.0006, 0.0005, 0.0006, 0.0008, 0.0005,\n",
       "                      0.0006, 0.0007, 0.0007, 0.0007, 0.0008, 0.0010, 0.0010, 0.0006, 0.0011,\n",
       "                      0.0006, 0.0007, 0.0007, 0.0010, 0.0007, 0.0006, 0.0007, 0.0007, 0.0011,\n",
       "                      0.0007, 0.0010, 0.0007, 0.0007, 0.0007, 0.0006, 0.0007, 0.0006, 0.0007,\n",
       "                      0.0007, 0.0006, 0.0007, 0.0006, 0.0006, 0.0008, 0.0005, 0.0005, 0.0007,\n",
       "                      0.0006, 0.0006, 0.0007, 0.0007, 0.0005, 0.0007, 0.0005, 0.0006, 0.0009,\n",
       "                      0.0008, 0.0006, 0.0008, 0.0008, 0.0006, 0.0006, 0.0007, 0.0009, 0.0006,\n",
       "                      0.0016, 0.0019, 0.0005, 0.0005, 0.0007, 0.0007, 0.0010, 0.0008, 0.0009,\n",
       "                      0.0008, 0.0007, 0.0007, 0.0007, 0.0008, 0.0006, 0.0008, 0.0006, 0.0008,\n",
       "                      0.0005, 0.0009, 0.0007, 0.0007, 0.0007, 0.0009, 0.0014, 0.0005, 0.0007,\n",
       "                      0.0007, 0.0013, 0.0007, 0.0006, 0.0009, 0.0005, 0.0008, 0.0007, 0.0009,\n",
       "                      0.0008, 0.0006, 0.0006, 0.0011, 0.0007, 0.0006, 0.0006, 0.0009, 0.0007,\n",
       "                      0.0009, 0.0006, 0.0017, 0.0007, 0.0006, 0.0008, 0.0007, 0.0010, 0.0009,\n",
       "                      0.0005, 0.0007, 0.0005, 0.0009, 0.0010, 0.0013, 0.0008, 0.0007, 0.0006,\n",
       "                      0.0006, 0.0008, 0.0006, 0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007,\n",
       "                      0.0011, 0.0008, 0.0006, 0.0007, 0.0006, 0.0006, 0.0007, 0.0011, 0.0007,\n",
       "                      0.0006, 0.0006, 0.0006, 0.0007, 0.0008, 0.0005, 0.0005, 0.0005, 0.0007,\n",
       "                      0.0009, 0.0007, 0.0008, 0.0007, 0.0012, 0.0006, 0.0008, 0.0005, 0.0007,\n",
       "                      0.0009, 0.0007, 0.0009, 0.0006, 0.0006, 0.0008, 0.0007, 0.0007, 0.0005,\n",
       "                      0.0006, 0.0005, 0.0008, 0.0007, 0.0006, 0.0008, 0.0008, 0.0005, 0.0009,\n",
       "                      0.0008, 0.0006, 0.0009, 0.0006, 0.0011, 0.0008, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0012, 0.0007, 0.0008, 0.0006, 0.0008, 0.0005, 0.0006, 0.0021, 0.0006,\n",
       "                      0.0013, 0.0011, 0.0015, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0008,\n",
       "                      0.0006, 0.0009, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0009, 0.0007,\n",
       "                      0.0008, 0.0011, 0.0006, 0.0007, 0.0007, 0.0009, 0.0005, 0.0006, 0.0008,\n",
       "                      0.0009, 0.0008, 0.0006, 0.0009, 0.0007, 0.0006, 0.0006, 0.0006, 0.0005,\n",
       "                      0.0007, 0.0008, 0.0009, 0.0006, 0.0007, 0.0006, 0.0008, 0.0008, 0.0005,\n",
       "                      0.0008, 0.0006, 0.0006, 0.0008, 0.0009, 0.0007, 0.0008, 0.0009, 0.0012,\n",
       "                      0.0007, 0.0007, 0.0005, 0.0006, 0.0006, 0.0009, 0.0006, 0.0007, 0.0006,\n",
       "                      0.0005, 0.0009, 0.0007, 0.0008, 0.0007, 0.0008, 0.0008, 0.0008, 0.0006,\n",
       "                      0.0006, 0.0006, 0.0008, 0.0008, 0.0006, 0.0005, 0.0008, 0.0008, 0.0006,\n",
       "                      0.0006, 0.0015, 0.0007, 0.0010, 0.0010, 0.0007, 0.0007, 0.0008, 0.0007,\n",
       "                      0.0006, 0.0006, 0.0009, 0.0005, 0.0006, 0.0010, 0.0006, 0.0006, 0.0004,\n",
       "                      0.0007, 0.0006, 0.0007, 0.0008, 0.0009, 0.0005, 0.0005, 0.0008, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0006, 0.0007, 0.0014, 0.0006, 0.0006, 0.0005, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0009, 0.0007, 0.0007, 0.0007, 0.0006, 0.0009, 0.0006,\n",
       "                      0.0006, 0.0010, 0.0015, 0.0007, 0.0007, 0.0008, 0.0008, 0.0007, 0.0006,\n",
       "                      0.0005, 0.0006, 0.0011, 0.0008, 0.0008, 0.0010, 0.0006, 0.0006, 0.0007,\n",
       "                      0.0008, 0.0007, 0.0006, 0.0010, 0.0007, 0.0008, 0.0005, 0.0009, 0.0011,\n",
       "                      0.0005, 0.0006, 0.0009, 0.0007, 0.0005, 0.0006, 0.0009, 0.0006, 0.0009,\n",
       "                      0.0006, 0.0008, 0.0006, 0.0006, 0.0007, 0.0006, 0.0006, 0.0007, 0.0010,\n",
       "                      0.0007, 0.0005, 0.0007, 0.0006, 0.0007, 0.0008, 0.0006, 0.0007, 0.0005,\n",
       "                      0.0007, 0.0006, 0.0005, 0.0006, 0.0006, 0.0007, 0.0007, 0.0008, 0.0008,\n",
       "                      0.0006, 0.0009, 0.0011, 0.0007, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005,\n",
       "                      0.0006, 0.0007, 0.0009, 0.0006, 0.0015, 0.0006, 0.0007, 0.0007, 0.0007,\n",
       "                      0.0007, 0.0009, 0.0011, 0.0013, 0.0007, 0.0007, 0.0010, 0.0007, 0.0005,\n",
       "                      0.0006, 0.0007, 0.0006, 0.0007, 0.0007, 0.0009, 0.0008, 0.0006, 0.0007,\n",
       "                      0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0006, 0.0006, 0.0008, 0.0010,\n",
       "                      0.0007, 0.0007, 0.0005, 0.0008, 0.0007, 0.0006, 0.0007, 0.0005, 0.0008,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0006, 0.0007, 0.0008, 0.0008, 0.0009, 0.0006,\n",
       "                      0.0005, 0.0006, 0.0008, 0.0006, 0.0007, 0.0006, 0.0008, 0.0011, 0.0008,\n",
       "                      0.0006, 0.0015, 0.0007, 0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0007, 0.0005, 0.0006, 0.0006, 0.0006, 0.0008, 0.0007, 0.0008, 0.0007,\n",
       "                      0.0008, 0.0013, 0.0006, 0.0009, 0.0007, 0.0007, 0.0008, 0.0007, 0.0009,\n",
       "                      0.0006, 0.0009, 0.0012, 0.0007, 0.0010, 0.0008, 0.0006, 0.0007, 0.0009,\n",
       "                      0.0007, 0.0006, 0.0010, 0.0009, 0.0005, 0.0008, 0.0007, 0.0006, 0.0011,\n",
       "                      0.0007, 0.0006, 0.0007, 0.0010, 0.0006, 0.0006, 0.0007, 0.0006]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0706, -0.0744, -0.0667, -0.0618, -0.0647, -0.0663, -0.0787, -0.0703,\n",
       "                        -0.0680, -0.0720, -0.0579, -0.0603, -0.0715, -0.0632, -0.0830, -0.1167,\n",
       "                        -0.0647, -0.0906, -0.0641, -0.0615, -0.0663, -0.0587, -0.0645, -0.0587,\n",
       "                        -0.0833, -0.0744, -0.0822, -0.0859, -0.0992, -0.0698, -0.0660, -0.0710,\n",
       "                        -0.0756, -0.0931, -0.0621, -0.0767, -0.0662, -0.0631, -0.0853, -0.0767,\n",
       "                        -0.0638, -0.0685, -0.0520, -0.0700, -0.0922, -0.0611, -0.0708, -0.0869,\n",
       "                        -0.0879, -0.0661, -0.0621, -0.0704, -0.0705, -0.0744, -0.0710, -0.0676,\n",
       "                        -0.0803, -0.0656, -0.0660, -0.0683, -0.0952, -0.0667, -0.0623, -0.1000,\n",
       "                        -0.0565, -0.0700, -0.0652, -0.0871, -0.0680, -0.0692, -0.0697, -0.0799,\n",
       "                        -0.0810, -0.0865, -0.0895, -0.0553, -0.0667, -0.0618, -0.0718, -0.0670,\n",
       "                        -0.1008, -0.0675, -0.0656, -0.0806, -0.0859, -0.0617, -0.0947, -0.1730,\n",
       "                        -0.0566, -0.0862, -0.0679, -0.0860, -0.0797, -0.0815, -0.0742, -0.0643,\n",
       "                        -0.0708, -0.0637, -0.0773, -0.0699, -0.0795, -0.0753, -0.0720, -0.0761,\n",
       "                        -0.0793, -0.0729, -0.1182, -0.0817, -0.0693, -0.0656, -0.0730, -0.0661,\n",
       "                        -0.0649, -0.0964, -0.0906, -0.0858, -0.1175, -0.0633, -0.0726, -0.0569,\n",
       "                        -0.1103, -0.1248, -0.0840, -0.0677, -0.0879, -0.0827, -0.0632, -0.0834,\n",
       "                        -0.0670, -0.0629, -0.0707, -0.0580, -0.0554, -0.0599, -0.0748, -0.0793,\n",
       "                        -0.0608, -0.0678, -0.0640, -0.0716, -0.0654, -0.0684, -0.0732, -0.0769,\n",
       "                        -0.0811, -0.0713, -0.0717, -0.0690, -0.1003, -0.0676, -0.0672, -0.0644,\n",
       "                        -0.0632, -0.0686, -0.0700, -0.0679, -0.0760, -0.1479, -0.0618, -0.1042,\n",
       "                        -0.0569, -0.0641, -0.1163, -0.0764, -0.0892, -0.0636, -0.0683, -0.0613,\n",
       "                        -0.0842, -0.0739, -0.0618, -0.0680, -0.0517, -0.0996, -0.0713, -0.0783,\n",
       "                        -0.0653, -0.0680, -0.0581, -0.0620, -0.0654, -0.0637, -0.0708, -0.0677,\n",
       "                        -0.0805, -0.0713, -0.0673, -0.0795, -0.0744, -0.0773, -0.0758, -0.0927,\n",
       "                        -0.0609, -0.0594, -0.0560, -0.0732, -0.0760, -0.0764, -0.1646, -0.1452,\n",
       "                        -0.0850, -0.0728, -0.0681, -0.0740, -0.0598, -0.0841, -0.0673, -0.0795,\n",
       "                        -0.0866, -0.0777, -0.0786, -0.0686, -0.0818, -0.0804, -0.0932, -0.0823,\n",
       "                        -0.0593, -0.1421, -0.0758, -0.0688, -0.0783, -0.0561, -0.0593, -0.0684,\n",
       "                        -0.0682, -0.0795, -0.0878, -0.0558, -0.0718, -0.0691, -0.0694, -0.0829,\n",
       "                        -0.0812, -0.0629, -0.0789, -0.0772, -0.0814, -0.0631, -0.0508, -0.0766,\n",
       "                        -0.0791, -0.0664, -0.0660, -0.0626, -0.0642, -0.0716, -0.0710, -0.0637,\n",
       "                        -0.0585, -0.0715, -0.0683, -0.0626, -0.0889, -0.0624, -0.0568, -0.0671,\n",
       "                        -0.0766, -0.0754, -0.0752, -0.0880, -0.0529, -0.0601, -0.0632, -0.0749,\n",
       "                        -0.0700, -0.0719, -0.0930, -0.0702, -0.1037, -0.0602, -0.0523, -0.0729,\n",
       "                        -0.0814, -0.0845, -0.0757, -0.0640, -0.0654, -0.0728, -0.0701, -0.0650,\n",
       "                        -0.0781, -0.0774, -0.0713, -0.0803, -0.0804, -0.0649, -0.0974, -0.0806,\n",
       "                        -0.0664, -0.0573, -0.0611, -0.0599, -0.0721, -0.0637, -0.0707, -0.0652,\n",
       "                        -0.0499, -0.0748, -0.0787, -0.0910, -0.1018, -0.1161, -0.0694, -0.0583,\n",
       "                        -0.0598, -0.0806, -0.0891, -0.0754, -0.0675, -0.0815, -0.0949, -0.0583,\n",
       "                        -0.0803, -0.0571, -0.0726, -0.0818, -0.0930, -0.0772, -0.0934, -0.0778,\n",
       "                        -0.0909, -0.0646, -0.0610, -0.0717, -0.0709, -0.0761, -0.1943, -0.0573,\n",
       "                        -0.0644, -0.0719, -0.0736, -0.0863, -0.0826, -0.0649, -0.0759, -0.0819,\n",
       "                        -0.0701, -0.1007, -0.0627, -0.0545, -0.0736, -0.0636, -0.0781, -0.0774,\n",
       "                        -0.0669, -0.0805, -0.0702, -0.0722, -0.0626, -0.0704, -0.0759, -0.0675,\n",
       "                        -0.0616, -0.1134, -0.0861, -0.0541, -0.0609, -0.0643, -0.0779, -0.0825,\n",
       "                        -0.0665, -0.0964, -0.0737, -0.0656, -0.0743, -0.0635, -0.0663, -0.0704,\n",
       "                        -0.0787, -0.0715, -0.0567, -0.0720, -0.0735, -0.0940, -0.0997, -0.0812,\n",
       "                        -0.0802, -0.0679, -0.0656, -0.0626, -0.0605, -0.0616, -0.0594, -0.0768,\n",
       "                        -0.0876, -0.0843, -0.0770, -0.0666, -0.0744, -0.1003, -0.0557, -0.0587,\n",
       "                        -0.0663, -0.0580, -0.0773, -0.0615, -0.0735, -0.0585, -0.0802, -0.0821,\n",
       "                        -0.1097, -0.0705, -0.0725, -0.0699, -0.0957, -0.0723, -0.1182, -0.1444,\n",
       "                        -0.0828, -0.0764, -0.0885, -0.0661, -0.0874, -0.0483, -0.0564, -0.0931,\n",
       "                        -0.0662, -0.0638, -0.0879, -0.1193, -0.0602, -0.0749, -0.0673, -0.0715,\n",
       "                        -0.0581, -0.0765, -0.0674, -0.0702, -0.0736, -0.0636, -0.0662, -0.0751,\n",
       "                        -0.0827, -0.0685, -0.0604, -0.0980, -0.0707, -0.0606, -0.0774, -0.0626,\n",
       "                        -0.0726, -0.0657, -0.0642, -0.0727, -0.0767, -0.0663, -0.0815, -0.1006,\n",
       "                        -0.0804, -0.0713, -0.0531, -0.0579, -0.0743, -0.0676, -0.0683, -0.0822,\n",
       "                        -0.0686, -0.0600, -0.0752, -0.0770, -0.1010, -0.0775, -0.0723, -0.0688,\n",
       "                        -0.0726, -0.0552, -0.0740, -0.0658, -0.0558, -0.0603, -0.0702, -0.0747,\n",
       "                        -0.0787, -0.0645, -0.0678, -0.0840, -0.0709, -0.0960, -0.0713, -0.0618,\n",
       "                        -0.0617, -0.0851, -0.0647, -0.0632, -0.0923, -0.1173, -0.0590, -0.0882,\n",
       "                        -0.0806, -0.0869, -0.1305, -0.0807, -0.0712, -0.0756, -0.1146, -0.0715,\n",
       "                        -0.0707, -0.0732, -0.0747, -0.0685, -0.0593, -0.0830, -0.0505, -0.1147,\n",
       "                        -0.0757, -0.0670, -0.0948, -0.0901, -0.0725, -0.0723, -0.0728, -0.0726]), max_val=tensor([0.0868, 0.1024, 0.0749, 0.0640, 0.0801, 0.0680, 0.0818, 0.1072, 0.0651,\n",
       "                        0.0630, 0.0865, 0.0862, 0.0850, 0.0995, 0.1210, 0.1252, 0.0774, 0.1354,\n",
       "                        0.0810, 0.0885, 0.0941, 0.1226, 0.0930, 0.0799, 0.0827, 0.0857, 0.1416,\n",
       "                        0.0785, 0.1308, 0.0933, 0.0877, 0.0833, 0.0679, 0.0942, 0.0800, 0.0848,\n",
       "                        0.0867, 0.0825, 0.0905, 0.0721, 0.0730, 0.0958, 0.0638, 0.0637, 0.0762,\n",
       "                        0.0722, 0.0692, 0.0835, 0.0852, 0.0686, 0.0832, 0.0665, 0.0745, 0.1089,\n",
       "                        0.0968, 0.0744, 0.0985, 0.1029, 0.0720, 0.0806, 0.0910, 0.1121, 0.0775,\n",
       "                        0.1970, 0.2460, 0.0631, 0.0666, 0.0633, 0.0871, 0.1313, 0.0975, 0.1143,\n",
       "                        0.0999, 0.0951, 0.0819, 0.0844, 0.1068, 0.0772, 0.0997, 0.0762, 0.0795,\n",
       "                        0.0685, 0.1149, 0.0862, 0.0860, 0.0946, 0.1096, 0.1756, 0.0629, 0.0693,\n",
       "                        0.0933, 0.1648, 0.0826, 0.0705, 0.1147, 0.0694, 0.1016, 0.0860, 0.1190,\n",
       "                        0.1035, 0.0784, 0.0818, 0.1385, 0.0917, 0.0732, 0.0615, 0.0724, 0.0871,\n",
       "                        0.1131, 0.0737, 0.2137, 0.0885, 0.0764, 0.0726, 0.0935, 0.1324, 0.1205,\n",
       "                        0.0569, 0.0939, 0.0645, 0.0731, 0.0862, 0.1683, 0.1021, 0.0634, 0.0652,\n",
       "                        0.0707, 0.0992, 0.0722, 0.0892, 0.0679, 0.0819, 0.0717, 0.0741, 0.0840,\n",
       "                        0.1372, 0.1045, 0.0762, 0.0899, 0.0631, 0.0799, 0.0866, 0.1458, 0.0936,\n",
       "                        0.0801, 0.0789, 0.0731, 0.0876, 0.1045, 0.0684, 0.0631, 0.0655, 0.0837,\n",
       "                        0.1132, 0.0859, 0.1002, 0.0826, 0.0970, 0.0778, 0.0958, 0.0644, 0.0949,\n",
       "                        0.0857, 0.0855, 0.1102, 0.0792, 0.0734, 0.0988, 0.0903, 0.0919, 0.0621,\n",
       "                        0.0715, 0.0638, 0.0968, 0.0884, 0.0803, 0.0957, 0.1079, 0.0666, 0.1147,\n",
       "                        0.0987, 0.0796, 0.1134, 0.0793, 0.1441, 0.0968, 0.0707, 0.0778, 0.0648,\n",
       "                        0.1486, 0.0882, 0.1049, 0.0740, 0.0969, 0.0695, 0.0772, 0.2658, 0.0735,\n",
       "                        0.0852, 0.1050, 0.1855, 0.0720, 0.0773, 0.0710, 0.0738, 0.0912, 0.1017,\n",
       "                        0.0611, 0.1159, 0.0947, 0.0744, 0.0915, 0.0860, 0.0864, 0.1116, 0.0899,\n",
       "                        0.1001, 0.0729, 0.0787, 0.0892, 0.0850, 0.1084, 0.0648, 0.0727, 0.0955,\n",
       "                        0.1120, 0.0963, 0.0752, 0.1122, 0.0902, 0.0723, 0.0773, 0.0717, 0.0662,\n",
       "                        0.0840, 0.0956, 0.1141, 0.0796, 0.0907, 0.0761, 0.1013, 0.1024, 0.0605,\n",
       "                        0.1049, 0.0762, 0.0704, 0.0983, 0.1084, 0.0930, 0.0992, 0.1135, 0.1532,\n",
       "                        0.0806, 0.0932, 0.0625, 0.0707, 0.0729, 0.1188, 0.0751, 0.0749, 0.0805,\n",
       "                        0.0678, 0.1133, 0.0938, 0.0984, 0.0831, 0.0954, 0.1064, 0.1023, 0.0741,\n",
       "                        0.0790, 0.0674, 0.1074, 0.0955, 0.0670, 0.0677, 0.0978, 0.1016, 0.0768,\n",
       "                        0.0711, 0.1938, 0.0835, 0.1259, 0.1282, 0.0850, 0.0877, 0.0706, 0.0937,\n",
       "                        0.0767, 0.0725, 0.1138, 0.0555, 0.0708, 0.1330, 0.0744, 0.0789, 0.0537,\n",
       "                        0.0874, 0.0768, 0.0879, 0.0768, 0.0766, 0.0639, 0.0525, 0.0999, 0.0766,\n",
       "                        0.0879, 0.0880, 0.0748, 0.0845, 0.1733, 0.0774, 0.0699, 0.0697, 0.0642,\n",
       "                        0.0943, 0.0844, 0.1192, 0.0912, 0.0922, 0.0877, 0.0786, 0.1148, 0.0799,\n",
       "                        0.0790, 0.1326, 0.0652, 0.0854, 0.0890, 0.1050, 0.0983, 0.0933, 0.0642,\n",
       "                        0.0673, 0.0743, 0.1349, 0.1070, 0.0757, 0.1244, 0.0764, 0.0701, 0.0865,\n",
       "                        0.1007, 0.0933, 0.0735, 0.1265, 0.0932, 0.1073, 0.0586, 0.1158, 0.1430,\n",
       "                        0.0686, 0.0709, 0.0671, 0.0774, 0.0625, 0.0772, 0.1201, 0.0676, 0.1094,\n",
       "                        0.0749, 0.0688, 0.0822, 0.0746, 0.0937, 0.0742, 0.0716, 0.0852, 0.1259,\n",
       "                        0.0871, 0.0648, 0.0853, 0.0716, 0.0874, 0.0661, 0.0623, 0.0847, 0.0677,\n",
       "                        0.0940, 0.0747, 0.0674, 0.0741, 0.0795, 0.0902, 0.0794, 0.1003, 0.1042,\n",
       "                        0.0820, 0.1085, 0.1368, 0.0923, 0.0715, 0.0759, 0.0697, 0.0762, 0.0650,\n",
       "                        0.0650, 0.0828, 0.1167, 0.0748, 0.1866, 0.0651, 0.0856, 0.0826, 0.0832,\n",
       "                        0.0919, 0.0980, 0.0594, 0.1590, 0.0888, 0.0753, 0.1310, 0.0889, 0.0625,\n",
       "                        0.0718, 0.0815, 0.0778, 0.0886, 0.0724, 0.0774, 0.1007, 0.0793, 0.0831,\n",
       "                        0.0858, 0.0744, 0.0857, 0.0897, 0.0842, 0.0753, 0.0758, 0.1050, 0.1252,\n",
       "                        0.0880, 0.0841, 0.0500, 0.0660, 0.0949, 0.0822, 0.0837, 0.0608, 0.1032,\n",
       "                        0.0612, 0.0661, 0.0749, 0.0788, 0.0862, 0.1000, 0.0813, 0.1135, 0.0703,\n",
       "                        0.0680, 0.0714, 0.1060, 0.0744, 0.0880, 0.0603, 0.1073, 0.1349, 0.0965,\n",
       "                        0.0660, 0.1873, 0.0893, 0.0859, 0.0733, 0.0639, 0.0702, 0.0648, 0.0817,\n",
       "                        0.0848, 0.0590, 0.0781, 0.0744, 0.0785, 0.1039, 0.0906, 0.1014, 0.0884,\n",
       "                        0.0776, 0.1642, 0.0756, 0.1081, 0.0756, 0.0952, 0.0975, 0.0801, 0.0785,\n",
       "                        0.0782, 0.1091, 0.1504, 0.0776, 0.0815, 0.0956, 0.0682, 0.0946, 0.0804,\n",
       "                        0.0910, 0.0725, 0.1296, 0.1128, 0.0593, 0.0957, 0.0906, 0.0721, 0.1377,\n",
       "                        0.0927, 0.0707, 0.0692, 0.1293, 0.0761, 0.0725, 0.0845, 0.0642])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0005, 0.0006, 0.0005, 0.0006, 0.0006, 0.0006, 0.0008, 0.0011, 0.0005,\n",
       "                      0.0007, 0.0006, 0.0006, 0.0006, 0.0012, 0.0005, 0.0005, 0.0004, 0.0007,\n",
       "                      0.0004, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0007, 0.0006,\n",
       "                      0.0006, 0.0008, 0.0011, 0.0005, 0.0011, 0.0007, 0.0005, 0.0007, 0.0008,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0014, 0.0006, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "                      0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0004, 0.0008, 0.0005, 0.0005,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0005, 0.0007, 0.0006, 0.0004, 0.0010, 0.0006,\n",
       "                      0.0006, 0.0009, 0.0005, 0.0004, 0.0009, 0.0014, 0.0006, 0.0005, 0.0005,\n",
       "                      0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0008, 0.0004, 0.0007, 0.0007,\n",
       "                      0.0005, 0.0008, 0.0006, 0.0008, 0.0005, 0.0005, 0.0004, 0.0005, 0.0008,\n",
       "                      0.0006, 0.0007, 0.0006, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005,\n",
       "                      0.0005, 0.0006, 0.0005, 0.0007, 0.0006, 0.0006, 0.0005, 0.0005, 0.0007,\n",
       "                      0.0004, 0.0004, 0.0006, 0.0006, 0.0004, 0.0005, 0.0005, 0.0010, 0.0005,\n",
       "                      0.0009, 0.0009, 0.0006, 0.0005, 0.0004, 0.0005, 0.0005, 0.0006, 0.0006,\n",
       "                      0.0005, 0.0010, 0.0005, 0.0006, 0.0006, 0.0004, 0.0009, 0.0004, 0.0009,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0006, 0.0008, 0.0004, 0.0006, 0.0005, 0.0004,\n",
       "                      0.0005, 0.0006, 0.0005, 0.0006, 0.0007, 0.0006, 0.0004, 0.0005, 0.0005,\n",
       "                      0.0005, 0.0013, 0.0005, 0.0005, 0.0007, 0.0005, 0.0005, 0.0015, 0.0007,\n",
       "                      0.0010, 0.0021, 0.0006, 0.0006, 0.0010, 0.0005, 0.0008, 0.0007, 0.0005,\n",
       "                      0.0006, 0.0006, 0.0008, 0.0008, 0.0005, 0.0006, 0.0008, 0.0007, 0.0007,\n",
       "                      0.0006, 0.0006, 0.0007, 0.0008, 0.0006, 0.0005, 0.0006, 0.0006, 0.0007,\n",
       "                      0.0005, 0.0005, 0.0005, 0.0004, 0.0006, 0.0009, 0.0004, 0.0008, 0.0006,\n",
       "                      0.0004, 0.0005, 0.0005, 0.0009, 0.0009, 0.0005, 0.0009, 0.0005, 0.0005,\n",
       "                      0.0006, 0.0006, 0.0005, 0.0005, 0.0006, 0.0008, 0.0009, 0.0009, 0.0005,\n",
       "                      0.0006, 0.0004, 0.0005, 0.0006, 0.0006, 0.0007, 0.0005, 0.0007, 0.0004,\n",
       "                      0.0004, 0.0008, 0.0006, 0.0008, 0.0005, 0.0008, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0005, 0.0004, 0.0007, 0.0007, 0.0006, 0.0007,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0005, 0.0004, 0.0005, 0.0005, 0.0006, 0.0008,\n",
       "                      0.0007, 0.0007, 0.0004, 0.0005, 0.0005, 0.0004, 0.0010, 0.0004, 0.0004,\n",
       "                      0.0005, 0.0008, 0.0006, 0.0006, 0.0005, 0.0012, 0.0005, 0.0012, 0.0005,\n",
       "                      0.0012, 0.0004, 0.0006, 0.0007, 0.0008, 0.0005, 0.0004, 0.0008, 0.0004,\n",
       "                      0.0015, 0.0010, 0.0004, 0.0005, 0.0009, 0.0005, 0.0005, 0.0004, 0.0005,\n",
       "                      0.0007, 0.0007, 0.0005, 0.0007, 0.0004, 0.0009, 0.0005, 0.0005, 0.0004,\n",
       "                      0.0007, 0.0007, 0.0005, 0.0005, 0.0009, 0.0007, 0.0007, 0.0006, 0.0006,\n",
       "                      0.0007, 0.0006, 0.0004, 0.0005, 0.0005, 0.0005, 0.0006, 0.0004, 0.0004,\n",
       "                      0.0006, 0.0006, 0.0005, 0.0004, 0.0007, 0.0005, 0.0005, 0.0004, 0.0008,\n",
       "                      0.0004, 0.0005, 0.0005, 0.0011, 0.0005, 0.0006, 0.0009, 0.0010, 0.0005,\n",
       "                      0.0005, 0.0005, 0.0004, 0.0004, 0.0007, 0.0005, 0.0005, 0.0006, 0.0007,\n",
       "                      0.0004, 0.0008, 0.0005, 0.0009, 0.0008, 0.0004, 0.0004, 0.0012, 0.0007,\n",
       "                      0.0005, 0.0005, 0.0007, 0.0005, 0.0006, 0.0004, 0.0005, 0.0011, 0.0005,\n",
       "                      0.0005, 0.0006, 0.0011, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "                      0.0007, 0.0010, 0.0008, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005, 0.0005,\n",
       "                      0.0005, 0.0012, 0.0006, 0.0004, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005,\n",
       "                      0.0013, 0.0005, 0.0005, 0.0007, 0.0006, 0.0006, 0.0010, 0.0005, 0.0005,\n",
       "                      0.0004, 0.0004, 0.0011, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005, 0.0006,\n",
       "                      0.0005, 0.0005, 0.0009, 0.0005, 0.0004, 0.0006, 0.0008, 0.0007, 0.0006,\n",
       "                      0.0007, 0.0008, 0.0005, 0.0005, 0.0004, 0.0005, 0.0006, 0.0006, 0.0005,\n",
       "                      0.0007, 0.0005, 0.0010, 0.0006, 0.0007, 0.0006, 0.0005, 0.0005, 0.0007,\n",
       "                      0.0007, 0.0014, 0.0005, 0.0006, 0.0007, 0.0005, 0.0005, 0.0004, 0.0006,\n",
       "                      0.0007, 0.0007, 0.0004, 0.0005, 0.0006, 0.0006, 0.0005, 0.0007, 0.0005,\n",
       "                      0.0005, 0.0007, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0007,\n",
       "                      0.0006, 0.0004, 0.0011, 0.0005, 0.0008, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "                      0.0010, 0.0008, 0.0006, 0.0005, 0.0013, 0.0007, 0.0005, 0.0007, 0.0006,\n",
       "                      0.0005, 0.0006, 0.0005, 0.0004, 0.0006, 0.0004, 0.0006, 0.0006, 0.0006,\n",
       "                      0.0005, 0.0005, 0.0007, 0.0004, 0.0006, 0.0008, 0.0006, 0.0006, 0.0005,\n",
       "                      0.0007, 0.0005, 0.0005, 0.0005, 0.0007, 0.0006, 0.0004, 0.0006, 0.0006,\n",
       "                      0.0004, 0.0005, 0.0006, 0.0005, 0.0006, 0.0006, 0.0012, 0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "                min_val=tensor([-0.0413, -0.0660, -0.0452, -0.0443, -0.0441, -0.0412, -0.0362, -0.0738,\n",
       "                        -0.0448, -0.0535, -0.0444, -0.0467, -0.0387, -0.0354, -0.0417, -0.0397,\n",
       "                        -0.0547, -0.0624, -0.0378, -0.0374, -0.0402, -0.0645, -0.0408, -0.0454,\n",
       "                        -0.0594, -0.0422, -0.0391, -0.0394, -0.0530, -0.0406, -0.0417, -0.0476,\n",
       "                        -0.0391, -0.0458, -0.0375, -0.0407, -0.0446, -0.0461, -0.0456, -0.0393,\n",
       "                        -0.0467, -0.0360, -0.0454, -0.0400, -0.0438, -0.0389, -0.0527, -0.0789,\n",
       "                        -0.0366, -0.0397, -0.0393, -0.0425, -0.0382, -0.0354, -0.0460, -0.0522,\n",
       "                        -0.0695, -0.0676, -0.0445, -0.0550, -0.0444, -0.0481, -0.0430, -0.0423,\n",
       "                        -0.0439, -0.0501, -0.0392, -0.0630, -0.0485, -0.0767, -0.0398, -0.0398,\n",
       "                        -0.0418, -0.0404, -0.0461, -0.0401, -0.0365, -0.0389, -0.0423, -0.0481,\n",
       "                        -0.0371, -0.0417, -0.0472, -0.0574, -0.0339, -0.0387, -0.0434, -0.0380,\n",
       "                        -0.0425, -0.0387, -0.0375, -0.0464, -0.0412, -0.0485, -0.0384, -0.0439,\n",
       "                        -0.0505, -0.0483, -0.0394, -0.0408, -0.0482, -0.0525, -0.0406, -0.0492,\n",
       "                        -0.0388, -0.0421, -0.0543, -0.0464, -0.0370, -0.0438, -0.0493, -0.0361,\n",
       "                        -0.0435, -0.0391, -0.0447, -0.0391, -0.0439, -0.0486, -0.0405, -0.0436,\n",
       "                        -0.0446, -0.0405, -0.0432, -0.0405, -0.0340, -0.0578, -0.0400, -0.0566,\n",
       "                        -0.0401, -0.0459, -0.0459, -0.0434, -0.0426, -0.0392, -0.0504, -0.0629,\n",
       "                        -0.0494, -0.0449, -0.0383, -0.0393, -0.0379, -0.0563, -0.0383, -0.0376,\n",
       "                        -0.0433, -0.0428, -0.0413, -0.0469, -0.0446, -0.0474, -0.0435, -0.0434,\n",
       "                        -0.0419, -0.0375, -0.0342, -0.0424, -0.0429, -0.0413, -0.0437, -0.0364,\n",
       "                        -0.0376, -0.0836, -0.0506, -0.1733, -0.0499, -0.0408, -0.0574, -0.0406,\n",
       "                        -0.0343, -0.0431, -0.0380, -0.0465, -0.0419, -0.0459, -0.0397, -0.0383,\n",
       "                        -0.0411, -0.0428, -0.0375, -0.0412, -0.0518, -0.0452, -0.0661, -0.0406,\n",
       "                        -0.0494, -0.0462, -0.0504, -0.0583, -0.0446, -0.0411, -0.0375, -0.0391,\n",
       "                        -0.0400, -0.0777, -0.0417, -0.0405, -0.0408, -0.0402, -0.0409, -0.0386,\n",
       "                        -0.0428, -0.0355, -0.0708, -0.0355, -0.0688, -0.0462, -0.0514, -0.0575,\n",
       "                        -0.0510, -0.0448, -0.0341, -0.0353, -0.0406, -0.0374, -0.0460, -0.0482,\n",
       "                        -0.0403, -0.0388, -0.0464, -0.0494, -0.0400, -0.0570, -0.0407, -0.0376,\n",
       "                        -0.0432, -0.0409, -0.0394, -0.0411, -0.0442, -0.0511, -0.0491, -0.0390,\n",
       "                        -0.0512, -0.0469, -0.0459, -0.0420, -0.0365, -0.0449, -0.0358, -0.0471,\n",
       "                        -0.0373, -0.0490, -0.0781, -0.0326, -0.0366, -0.0501, -0.0386, -0.0414,\n",
       "                        -0.0401, -0.0461, -0.0563, -0.0589, -0.0372, -0.0501, -0.0476, -0.0635,\n",
       "                        -0.0439, -0.0416, -0.0492, -0.0491, -0.0475, -0.0464, -0.0507, -0.0418,\n",
       "                        -0.0391, -0.0392, -0.0526, -0.0531, -0.0441, -0.0428, -0.0350, -0.0447,\n",
       "                        -0.0500, -0.0481, -0.0970, -0.0354, -0.0399, -0.0354, -0.0466, -0.0365,\n",
       "                        -0.0774, -0.0507, -0.0377, -0.0410, -0.0415, -0.0455, -0.0397, -0.0426,\n",
       "                        -0.0429, -0.0434, -0.0512, -0.0453, -0.0405, -0.0465, -0.0447, -0.0405,\n",
       "                        -0.0431, -0.0652, -0.0373, -0.0407, -0.0693, -0.0492, -0.0406, -0.0412,\n",
       "                        -0.0479, -0.0359, -0.0416, -0.0519, -0.0362, -0.0433, -0.0450, -0.0451,\n",
       "                        -0.0477, -0.0336, -0.0370, -0.0443, -0.0402, -0.0621, -0.0483, -0.0441,\n",
       "                        -0.0445, -0.0381, -0.0455, -0.0417, -0.0441, -0.0419, -0.0430, -0.0335,\n",
       "                        -0.0449, -0.0481, -0.0550, -0.0533, -0.0419, -0.0543, -0.0498, -0.0377,\n",
       "                        -0.0484, -0.0376, -0.0451, -0.0348, -0.0379, -0.0392, -0.0516, -0.0547,\n",
       "                        -0.0430, -0.0431, -0.0374, -0.0375, -0.0368, -0.0415, -0.0355, -0.0409,\n",
       "                        -0.0346, -0.0421, -0.0408, -0.0503, -0.0358, -0.0481, -0.0390, -0.0416,\n",
       "                        -0.0421, -0.0473, -0.0471, -0.0380, -0.0531, -0.0389, -0.0409, -0.0470,\n",
       "                        -0.0413, -0.0400, -0.0551, -0.0368, -0.0540, -0.0386, -0.0364, -0.0393,\n",
       "                        -0.0352, -0.0373, -0.0413, -0.0419, -0.0370, -0.0457, -0.0533, -0.0362,\n",
       "                        -0.0414, -0.0371, -0.0334, -0.0502, -0.0454, -0.0497, -0.0414, -0.0434,\n",
       "                        -0.0495, -0.0499, -0.0438, -0.0399, -0.0396, -0.0443, -0.0394, -0.0427,\n",
       "                        -0.0661, -0.0554, -0.0529, -0.0428, -0.0504, -0.0395, -0.0466, -0.0494,\n",
       "                        -0.0346, -0.0537, -0.0408, -0.0482, -0.0390, -0.0377, -0.0398, -0.0426,\n",
       "                        -0.0413, -0.0578, -0.0387, -0.0383, -0.0431, -0.0350, -0.0408, -0.0388,\n",
       "                        -0.0496, -0.0463, -0.0475, -0.0427, -0.0427, -0.0503, -0.0583, -0.0488,\n",
       "                        -0.0374, -0.0376, -0.0404, -0.0605, -0.0429, -0.0396, -0.0402, -0.0384,\n",
       "                        -0.0410, -0.0407, -0.0395, -0.0362, -0.0615, -0.0409, -0.0729, -0.0426,\n",
       "                        -0.0439, -0.0386, -0.0384, -0.0434, -0.0424, -0.0389, -0.0423, -0.0398,\n",
       "                        -0.0433, -0.0453, -0.0417, -0.0401, -0.0538, -0.0485, -0.0398, -0.0445,\n",
       "                        -0.0390, -0.0417, -0.0409, -0.0384, -0.0443, -0.0404, -0.0387, -0.0408,\n",
       "                        -0.0441, -0.0741, -0.0475, -0.0404, -0.0453, -0.0404, -0.0556, -0.0540,\n",
       "                        -0.0435, -0.0462, -0.0431, -0.0394, -0.0480, -0.0464, -0.0475, -0.0413,\n",
       "                        -0.0471, -0.0385, -0.0463, -0.0455, -0.0456, -0.0708, -0.0386, -0.0495,\n",
       "                        -0.0425, -0.0542, -0.0460, -0.0459, -0.0429, -0.0443, -0.0379, -0.0407,\n",
       "                        -0.0396, -0.0398, -0.0543, -0.0477, -0.0384, -0.0395, -0.0506, -0.0435]), max_val=tensor([0.0684, 0.0702, 0.0609, 0.0715, 0.0791, 0.0824, 0.0970, 0.1385, 0.0687,\n",
       "                        0.0828, 0.0788, 0.0714, 0.0741, 0.1500, 0.0578, 0.0687, 0.0545, 0.0841,\n",
       "                        0.0486, 0.0542, 0.0601, 0.0659, 0.0618, 0.0587, 0.0712, 0.0872, 0.0712,\n",
       "                        0.0769, 0.0970, 0.1344, 0.0694, 0.1436, 0.0942, 0.0629, 0.0900, 0.0978,\n",
       "                        0.0573, 0.0648, 0.0721, 0.1840, 0.0703, 0.0603, 0.0586, 0.0603, 0.0822,\n",
       "                        0.0813, 0.0749, 0.0682, 0.0721, 0.0592, 0.0569, 0.1066, 0.0638, 0.0641,\n",
       "                        0.0615, 0.0591, 0.0712, 0.0502, 0.0826, 0.0816, 0.0492, 0.1211, 0.0716,\n",
       "                        0.0747, 0.1143, 0.0578, 0.0548, 0.1175, 0.1793, 0.0523, 0.0654, 0.0626,\n",
       "                        0.0774, 0.0819, 0.0806, 0.0802, 0.0854, 0.1019, 0.0566, 0.0894, 0.0922,\n",
       "                        0.0576, 0.1035, 0.0809, 0.0974, 0.0676, 0.0634, 0.0519, 0.0605, 0.1001,\n",
       "                        0.0710, 0.0867, 0.0745, 0.0476, 0.0604, 0.0581, 0.0485, 0.0516, 0.0631,\n",
       "                        0.0646, 0.0768, 0.0600, 0.0877, 0.0707, 0.0786, 0.0610, 0.0630, 0.0847,\n",
       "                        0.0557, 0.0512, 0.0718, 0.0773, 0.0569, 0.0610, 0.0581, 0.1211, 0.0597,\n",
       "                        0.1129, 0.1148, 0.0703, 0.0599, 0.0516, 0.0636, 0.0579, 0.0791, 0.0742,\n",
       "                        0.0671, 0.1299, 0.0582, 0.0715, 0.0733, 0.0524, 0.1080, 0.0549, 0.1171,\n",
       "                        0.0614, 0.0644, 0.0703, 0.0759, 0.1049, 0.0565, 0.0738, 0.0672, 0.0570,\n",
       "                        0.0650, 0.0819, 0.0573, 0.0761, 0.0866, 0.0770, 0.0556, 0.0648, 0.0678,\n",
       "                        0.0593, 0.1637, 0.0674, 0.0658, 0.0952, 0.0663, 0.0604, 0.1871, 0.0676,\n",
       "                        0.1269, 0.2718, 0.0787, 0.0744, 0.1324, 0.0669, 0.1058, 0.0912, 0.0628,\n",
       "                        0.0700, 0.0706, 0.0991, 0.1067, 0.0683, 0.0780, 0.0962, 0.0827, 0.0907,\n",
       "                        0.0780, 0.0708, 0.0856, 0.0955, 0.0763, 0.0637, 0.0743, 0.0728, 0.0910,\n",
       "                        0.0599, 0.0621, 0.0644, 0.0516, 0.0532, 0.1193, 0.0492, 0.0962, 0.0770,\n",
       "                        0.0565, 0.0598, 0.0609, 0.1100, 0.1162, 0.0687, 0.1142, 0.0684, 0.0593,\n",
       "                        0.0761, 0.0800, 0.0647, 0.0591, 0.0729, 0.0971, 0.1118, 0.1159, 0.0576,\n",
       "                        0.0709, 0.0551, 0.0634, 0.0781, 0.0757, 0.0949, 0.0608, 0.0939, 0.0538,\n",
       "                        0.0532, 0.1051, 0.0785, 0.0967, 0.0697, 0.0959, 0.0780, 0.0717, 0.0706,\n",
       "                        0.0662, 0.0655, 0.0768, 0.0645, 0.0502, 0.0853, 0.0888, 0.0709, 0.0907,\n",
       "                        0.0695, 0.0655, 0.0740, 0.0593, 0.0495, 0.0675, 0.0674, 0.0745, 0.0965,\n",
       "                        0.0867, 0.0851, 0.0550, 0.0574, 0.0668, 0.0568, 0.1318, 0.0529, 0.0529,\n",
       "                        0.0621, 0.0992, 0.0743, 0.0701, 0.0641, 0.1475, 0.0667, 0.1528, 0.0642,\n",
       "                        0.1487, 0.0541, 0.0700, 0.0933, 0.0672, 0.0629, 0.0529, 0.1066, 0.0544,\n",
       "                        0.1944, 0.1307, 0.0515, 0.0630, 0.1141, 0.0600, 0.0666, 0.0522, 0.0687,\n",
       "                        0.0917, 0.0881, 0.0667, 0.0880, 0.0566, 0.1201, 0.0623, 0.0655, 0.0531,\n",
       "                        0.0873, 0.0827, 0.0605, 0.0465, 0.1126, 0.0882, 0.0937, 0.0790, 0.0753,\n",
       "                        0.0936, 0.0738, 0.0550, 0.0666, 0.0576, 0.0581, 0.0825, 0.0558, 0.0535,\n",
       "                        0.0771, 0.0724, 0.0573, 0.0571, 0.0850, 0.0686, 0.0623, 0.0556, 0.0975,\n",
       "                        0.0529, 0.0616, 0.0589, 0.1431, 0.0673, 0.0776, 0.1094, 0.1254, 0.0601,\n",
       "                        0.0611, 0.0682, 0.0571, 0.0529, 0.0844, 0.0696, 0.0625, 0.0817, 0.0836,\n",
       "                        0.0565, 0.1029, 0.0622, 0.1140, 0.0985, 0.0570, 0.0520, 0.1470, 0.0845,\n",
       "                        0.0610, 0.0616, 0.0845, 0.0625, 0.0764, 0.0521, 0.0657, 0.1427, 0.0623,\n",
       "                        0.0609, 0.0824, 0.1443, 0.0646, 0.0604, 0.0672, 0.0634, 0.0696, 0.0723,\n",
       "                        0.0864, 0.1301, 0.1011, 0.0730, 0.0700, 0.0581, 0.0781, 0.0593, 0.0595,\n",
       "                        0.0625, 0.1519, 0.0742, 0.0486, 0.0725, 0.0784, 0.0601, 0.0720, 0.0609,\n",
       "                        0.1626, 0.0665, 0.0675, 0.0857, 0.0707, 0.0739, 0.1229, 0.0682, 0.0645,\n",
       "                        0.0528, 0.0553, 0.1439, 0.0675, 0.0608, 0.0692, 0.0810, 0.0591, 0.0724,\n",
       "                        0.0687, 0.0659, 0.1086, 0.0621, 0.0538, 0.0815, 0.0995, 0.0843, 0.0728,\n",
       "                        0.0877, 0.1077, 0.0669, 0.0694, 0.0561, 0.0637, 0.0745, 0.0732, 0.0680,\n",
       "                        0.0861, 0.0695, 0.1279, 0.0768, 0.0920, 0.0770, 0.0660, 0.0678, 0.0950,\n",
       "                        0.0874, 0.1756, 0.0668, 0.0774, 0.0952, 0.0576, 0.0631, 0.0563, 0.0709,\n",
       "                        0.0882, 0.0936, 0.0557, 0.0571, 0.0779, 0.0579, 0.0644, 0.0881, 0.0627,\n",
       "                        0.0651, 0.0862, 0.0620, 0.0682, 0.0756, 0.0805, 0.0639, 0.0773, 0.0863,\n",
       "                        0.0753, 0.0513, 0.1352, 0.0597, 0.1049, 0.0686, 0.0619, 0.0606, 0.0708,\n",
       "                        0.1259, 0.0959, 0.0726, 0.0621, 0.1649, 0.0891, 0.0614, 0.0838, 0.0776,\n",
       "                        0.0627, 0.0803, 0.0596, 0.0552, 0.0825, 0.0544, 0.0824, 0.0771, 0.0769,\n",
       "                        0.0600, 0.0644, 0.0866, 0.0541, 0.0823, 0.0965, 0.0759, 0.0538, 0.0587,\n",
       "                        0.0881, 0.0646, 0.0587, 0.0674, 0.0921, 0.0752, 0.0532, 0.0752, 0.0752,\n",
       "                        0.0568, 0.0637, 0.0713, 0.0595, 0.0703, 0.0795, 0.1562, 0.0995])\n",
       "              )\n",
       "            )\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(\n",
       "        in_features=512, out_features=10, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "                  0.0003]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0442, -0.0442, -0.0439, -0.0441, -0.0441, -0.0442, -0.0433, -0.0442,\n",
       "                    -0.0442, -0.0436]), max_val=tensor([0.0442, 0.0439, 0.0435, 0.0439, 0.0440, 0.0442, 0.0439, 0.0440, 0.0438,\n",
       "                    0.0438])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_52056\\92509887.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(student_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.model.conv1.0.weight', 'model.model.conv1.0.weight_mask', 'model.model.conv1.1.weight', 'model.model.conv1.1.bias', 'model.model.conv1.1.running_mean', 'model.model.conv1.1.running_var', 'model.model.layer1.0.conv1.0.weight', 'model.model.layer1.0.conv1.0.weight_mask', 'model.model.layer1.0.conv1.1.weight', 'model.model.layer1.0.conv1.1.bias', 'model.model.layer1.0.conv1.1.running_mean', 'model.model.layer1.0.conv1.1.running_var', 'model.model.layer1.0.conv2.0.weight', 'model.model.layer1.0.conv2.0.weight_mask', 'model.model.layer1.0.conv2.1.weight', 'model.model.layer1.0.conv2.1.bias', 'model.model.layer1.0.conv2.1.running_mean', 'model.model.layer1.0.conv2.1.running_var', 'model.model.layer1.1.conv1.0.weight', 'model.model.layer1.1.conv1.0.weight_mask', 'model.model.layer1.1.conv1.1.weight', 'model.model.layer1.1.conv1.1.bias', 'model.model.layer1.1.conv1.1.running_mean', 'model.model.layer1.1.conv1.1.running_var', 'model.model.layer1.1.conv2.0.weight', 'model.model.layer1.1.conv2.0.weight_mask', 'model.model.layer1.1.conv2.1.weight', 'model.model.layer1.1.conv2.1.bias', 'model.model.layer1.1.conv2.1.running_mean', 'model.model.layer1.1.conv2.1.running_var', 'model.model.layer2.0.conv1.0.weight', 'model.model.layer2.0.conv1.0.weight_mask', 'model.model.layer2.0.conv1.1.weight', 'model.model.layer2.0.conv1.1.bias', 'model.model.layer2.0.conv1.1.running_mean', 'model.model.layer2.0.conv1.1.running_var', 'model.model.layer2.0.conv2.0.weight', 'model.model.layer2.0.conv2.0.weight_mask', 'model.model.layer2.0.conv2.1.weight', 'model.model.layer2.0.conv2.1.bias', 'model.model.layer2.0.conv2.1.running_mean', 'model.model.layer2.0.conv2.1.running_var', 'model.model.layer2.0.downsample.0.0.weight', 'model.model.layer2.0.downsample.0.0.weight_mask', 'model.model.layer2.0.downsample.0.1.weight', 'model.model.layer2.0.downsample.0.1.bias', 'model.model.layer2.0.downsample.0.1.running_mean', 'model.model.layer2.0.downsample.0.1.running_var', 'model.model.layer2.1.conv1.0.weight', 'model.model.layer2.1.conv1.0.weight_mask', 'model.model.layer2.1.conv1.1.weight', 'model.model.layer2.1.conv1.1.bias', 'model.model.layer2.1.conv1.1.running_mean', 'model.model.layer2.1.conv1.1.running_var', 'model.model.layer2.1.conv2.0.weight', 'model.model.layer2.1.conv2.0.weight_mask', 'model.model.layer2.1.conv2.1.weight', 'model.model.layer2.1.conv2.1.bias', 'model.model.layer2.1.conv2.1.running_mean', 'model.model.layer2.1.conv2.1.running_var', 'model.model.layer3.0.conv1.0.weight', 'model.model.layer3.0.conv1.0.weight_mask', 'model.model.layer3.0.conv1.1.weight', 'model.model.layer3.0.conv1.1.bias', 'model.model.layer3.0.conv1.1.running_mean', 'model.model.layer3.0.conv1.1.running_var', 'model.model.layer3.0.conv2.0.weight', 'model.model.layer3.0.conv2.0.weight_mask', 'model.model.layer3.0.conv2.1.weight', 'model.model.layer3.0.conv2.1.bias', 'model.model.layer3.0.conv2.1.running_mean', 'model.model.layer3.0.conv2.1.running_var', 'model.model.layer3.0.downsample.0.0.weight', 'model.model.layer3.0.downsample.0.0.weight_mask', 'model.model.layer3.0.downsample.0.1.weight', 'model.model.layer3.0.downsample.0.1.bias', 'model.model.layer3.0.downsample.0.1.running_mean', 'model.model.layer3.0.downsample.0.1.running_var', 'model.model.layer3.1.conv1.0.weight', 'model.model.layer3.1.conv1.0.weight_mask', 'model.model.layer3.1.conv1.1.weight', 'model.model.layer3.1.conv1.1.bias', 'model.model.layer3.1.conv1.1.running_mean', 'model.model.layer3.1.conv1.1.running_var', 'model.model.layer3.1.conv2.0.weight', 'model.model.layer3.1.conv2.0.weight_mask', 'model.model.layer3.1.conv2.1.weight', 'model.model.layer3.1.conv2.1.bias', 'model.model.layer3.1.conv2.1.running_mean', 'model.model.layer3.1.conv2.1.running_var', 'model.model.layer4.0.conv1.0.weight', 'model.model.layer4.0.conv1.0.weight_mask', 'model.model.layer4.0.conv1.1.weight', 'model.model.layer4.0.conv1.1.bias', 'model.model.layer4.0.conv1.1.running_mean', 'model.model.layer4.0.conv1.1.running_var', 'model.model.layer4.0.conv2.0.weight', 'model.model.layer4.0.conv2.0.weight_mask', 'model.model.layer4.0.conv2.1.weight', 'model.model.layer4.0.conv2.1.bias', 'model.model.layer4.0.conv2.1.running_mean', 'model.model.layer4.0.conv2.1.running_var', 'model.model.layer4.0.downsample.0.0.weight', 'model.model.layer4.0.downsample.0.0.weight_mask', 'model.model.layer4.0.downsample.0.1.weight', 'model.model.layer4.0.downsample.0.1.bias', 'model.model.layer4.0.downsample.0.1.running_mean', 'model.model.layer4.0.downsample.0.1.running_var', 'model.model.layer4.1.conv1.0.weight', 'model.model.layer4.1.conv1.0.weight_mask', 'model.model.layer4.1.conv1.1.weight', 'model.model.layer4.1.conv1.1.bias', 'model.model.layer4.1.conv1.1.running_mean', 'model.model.layer4.1.conv1.1.running_var', 'model.model.layer4.1.conv2.0.weight', 'model.model.layer4.1.conv2.0.weight_mask', 'model.model.layer4.1.conv2.1.weight', 'model.model.layer4.1.conv2.1.bias', 'model.model.layer4.1.conv2.1.running_mean', 'model.model.layer4.1.conv2.1.running_var', 'model.model.fc.weight_mask'], unexpected_keys=['model.model.conv1.weight', 'model.model.conv1.bn.weight', 'model.model.conv1.bn.bias', 'model.model.conv1.bn.running_mean', 'model.model.conv1.bn.running_var', 'model.model.conv1.bn.num_batches_tracked', 'model.model.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.conv1.weight_fake_quant.observer_enabled', 'model.model.conv1.weight_fake_quant.scale', 'model.model.conv1.weight_fake_quant.zero_point', 'model.model.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.conv1.activation_post_process.fake_quant_enabled', 'model.model.conv1.activation_post_process.observer_enabled', 'model.model.conv1.activation_post_process.scale', 'model.model.conv1.activation_post_process.zero_point', 'model.model.conv1.activation_post_process.activation_post_process.eps', 'model.model.conv1.activation_post_process.activation_post_process.min_val', 'model.model.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer1.0.conv1.weight', 'model.model.layer1.0.conv1.bn.weight', 'model.model.layer1.0.conv1.bn.bias', 'model.model.layer1.0.conv1.bn.running_mean', 'model.model.layer1.0.conv1.bn.running_var', 'model.model.layer1.0.conv1.bn.num_batches_tracked', 'model.model.layer1.0.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer1.0.conv1.weight_fake_quant.observer_enabled', 'model.model.layer1.0.conv1.weight_fake_quant.scale', 'model.model.layer1.0.conv1.weight_fake_quant.zero_point', 'model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer1.0.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer1.0.conv1.activation_post_process.observer_enabled', 'model.model.layer1.0.conv1.activation_post_process.scale', 'model.model.layer1.0.conv1.activation_post_process.zero_point', 'model.model.layer1.0.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer1.0.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer1.0.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer1.0.conv2.weight', 'model.model.layer1.0.conv2.bn.weight', 'model.model.layer1.0.conv2.bn.bias', 'model.model.layer1.0.conv2.bn.running_mean', 'model.model.layer1.0.conv2.bn.running_var', 'model.model.layer1.0.conv2.bn.num_batches_tracked', 'model.model.layer1.0.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer1.0.conv2.weight_fake_quant.observer_enabled', 'model.model.layer1.0.conv2.weight_fake_quant.scale', 'model.model.layer1.0.conv2.weight_fake_quant.zero_point', 'model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer1.0.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer1.0.conv2.activation_post_process.observer_enabled', 'model.model.layer1.0.conv2.activation_post_process.scale', 'model.model.layer1.0.conv2.activation_post_process.zero_point', 'model.model.layer1.0.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer1.0.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer1.0.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer1.0.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer1.0.add_func.activation_post_process.observer_enabled', 'model.model.layer1.0.add_func.activation_post_process.scale', 'model.model.layer1.0.add_func.activation_post_process.zero_point', 'model.model.layer1.0.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer1.0.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer1.0.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer1.1.conv1.weight', 'model.model.layer1.1.conv1.bn.weight', 'model.model.layer1.1.conv1.bn.bias', 'model.model.layer1.1.conv1.bn.running_mean', 'model.model.layer1.1.conv1.bn.running_var', 'model.model.layer1.1.conv1.bn.num_batches_tracked', 'model.model.layer1.1.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer1.1.conv1.weight_fake_quant.observer_enabled', 'model.model.layer1.1.conv1.weight_fake_quant.scale', 'model.model.layer1.1.conv1.weight_fake_quant.zero_point', 'model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer1.1.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer1.1.conv1.activation_post_process.observer_enabled', 'model.model.layer1.1.conv1.activation_post_process.scale', 'model.model.layer1.1.conv1.activation_post_process.zero_point', 'model.model.layer1.1.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer1.1.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer1.1.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer1.1.conv2.weight', 'model.model.layer1.1.conv2.bn.weight', 'model.model.layer1.1.conv2.bn.bias', 'model.model.layer1.1.conv2.bn.running_mean', 'model.model.layer1.1.conv2.bn.running_var', 'model.model.layer1.1.conv2.bn.num_batches_tracked', 'model.model.layer1.1.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer1.1.conv2.weight_fake_quant.observer_enabled', 'model.model.layer1.1.conv2.weight_fake_quant.scale', 'model.model.layer1.1.conv2.weight_fake_quant.zero_point', 'model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer1.1.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer1.1.conv2.activation_post_process.observer_enabled', 'model.model.layer1.1.conv2.activation_post_process.scale', 'model.model.layer1.1.conv2.activation_post_process.zero_point', 'model.model.layer1.1.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer1.1.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer1.1.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer1.1.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer1.1.add_func.activation_post_process.observer_enabled', 'model.model.layer1.1.add_func.activation_post_process.scale', 'model.model.layer1.1.add_func.activation_post_process.zero_point', 'model.model.layer1.1.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer1.1.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer1.1.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer2.0.conv1.weight', 'model.model.layer2.0.conv1.bn.weight', 'model.model.layer2.0.conv1.bn.bias', 'model.model.layer2.0.conv1.bn.running_mean', 'model.model.layer2.0.conv1.bn.running_var', 'model.model.layer2.0.conv1.bn.num_batches_tracked', 'model.model.layer2.0.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer2.0.conv1.weight_fake_quant.observer_enabled', 'model.model.layer2.0.conv1.weight_fake_quant.scale', 'model.model.layer2.0.conv1.weight_fake_quant.zero_point', 'model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer2.0.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer2.0.conv1.activation_post_process.observer_enabled', 'model.model.layer2.0.conv1.activation_post_process.scale', 'model.model.layer2.0.conv1.activation_post_process.zero_point', 'model.model.layer2.0.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer2.0.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer2.0.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer2.0.conv2.weight', 'model.model.layer2.0.conv2.bn.weight', 'model.model.layer2.0.conv2.bn.bias', 'model.model.layer2.0.conv2.bn.running_mean', 'model.model.layer2.0.conv2.bn.running_var', 'model.model.layer2.0.conv2.bn.num_batches_tracked', 'model.model.layer2.0.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer2.0.conv2.weight_fake_quant.observer_enabled', 'model.model.layer2.0.conv2.weight_fake_quant.scale', 'model.model.layer2.0.conv2.weight_fake_quant.zero_point', 'model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer2.0.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer2.0.conv2.activation_post_process.observer_enabled', 'model.model.layer2.0.conv2.activation_post_process.scale', 'model.model.layer2.0.conv2.activation_post_process.zero_point', 'model.model.layer2.0.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer2.0.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer2.0.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer2.0.downsample.0.weight', 'model.model.layer2.0.downsample.0.bn.weight', 'model.model.layer2.0.downsample.0.bn.bias', 'model.model.layer2.0.downsample.0.bn.running_mean', 'model.model.layer2.0.downsample.0.bn.running_var', 'model.model.layer2.0.downsample.0.bn.num_batches_tracked', 'model.model.layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled', 'model.model.layer2.0.downsample.0.weight_fake_quant.observer_enabled', 'model.model.layer2.0.downsample.0.weight_fake_quant.scale', 'model.model.layer2.0.downsample.0.weight_fake_quant.zero_point', 'model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps', 'model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val', 'model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val', 'model.model.layer2.0.downsample.0.activation_post_process.fake_quant_enabled', 'model.model.layer2.0.downsample.0.activation_post_process.observer_enabled', 'model.model.layer2.0.downsample.0.activation_post_process.scale', 'model.model.layer2.0.downsample.0.activation_post_process.zero_point', 'model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.eps', 'model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.min_val', 'model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.max_val', 'model.model.layer2.0.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer2.0.add_func.activation_post_process.observer_enabled', 'model.model.layer2.0.add_func.activation_post_process.scale', 'model.model.layer2.0.add_func.activation_post_process.zero_point', 'model.model.layer2.0.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer2.0.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer2.0.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer2.1.conv1.weight', 'model.model.layer2.1.conv1.bn.weight', 'model.model.layer2.1.conv1.bn.bias', 'model.model.layer2.1.conv1.bn.running_mean', 'model.model.layer2.1.conv1.bn.running_var', 'model.model.layer2.1.conv1.bn.num_batches_tracked', 'model.model.layer2.1.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer2.1.conv1.weight_fake_quant.observer_enabled', 'model.model.layer2.1.conv1.weight_fake_quant.scale', 'model.model.layer2.1.conv1.weight_fake_quant.zero_point', 'model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer2.1.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer2.1.conv1.activation_post_process.observer_enabled', 'model.model.layer2.1.conv1.activation_post_process.scale', 'model.model.layer2.1.conv1.activation_post_process.zero_point', 'model.model.layer2.1.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer2.1.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer2.1.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer2.1.conv2.weight', 'model.model.layer2.1.conv2.bn.weight', 'model.model.layer2.1.conv2.bn.bias', 'model.model.layer2.1.conv2.bn.running_mean', 'model.model.layer2.1.conv2.bn.running_var', 'model.model.layer2.1.conv2.bn.num_batches_tracked', 'model.model.layer2.1.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer2.1.conv2.weight_fake_quant.observer_enabled', 'model.model.layer2.1.conv2.weight_fake_quant.scale', 'model.model.layer2.1.conv2.weight_fake_quant.zero_point', 'model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer2.1.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer2.1.conv2.activation_post_process.observer_enabled', 'model.model.layer2.1.conv2.activation_post_process.scale', 'model.model.layer2.1.conv2.activation_post_process.zero_point', 'model.model.layer2.1.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer2.1.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer2.1.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer2.1.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer2.1.add_func.activation_post_process.observer_enabled', 'model.model.layer2.1.add_func.activation_post_process.scale', 'model.model.layer2.1.add_func.activation_post_process.zero_point', 'model.model.layer2.1.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer2.1.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer2.1.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer3.0.conv1.weight', 'model.model.layer3.0.conv1.bn.weight', 'model.model.layer3.0.conv1.bn.bias', 'model.model.layer3.0.conv1.bn.running_mean', 'model.model.layer3.0.conv1.bn.running_var', 'model.model.layer3.0.conv1.bn.num_batches_tracked', 'model.model.layer3.0.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer3.0.conv1.weight_fake_quant.observer_enabled', 'model.model.layer3.0.conv1.weight_fake_quant.scale', 'model.model.layer3.0.conv1.weight_fake_quant.zero_point', 'model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer3.0.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer3.0.conv1.activation_post_process.observer_enabled', 'model.model.layer3.0.conv1.activation_post_process.scale', 'model.model.layer3.0.conv1.activation_post_process.zero_point', 'model.model.layer3.0.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer3.0.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer3.0.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer3.0.conv2.weight', 'model.model.layer3.0.conv2.bn.weight', 'model.model.layer3.0.conv2.bn.bias', 'model.model.layer3.0.conv2.bn.running_mean', 'model.model.layer3.0.conv2.bn.running_var', 'model.model.layer3.0.conv2.bn.num_batches_tracked', 'model.model.layer3.0.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer3.0.conv2.weight_fake_quant.observer_enabled', 'model.model.layer3.0.conv2.weight_fake_quant.scale', 'model.model.layer3.0.conv2.weight_fake_quant.zero_point', 'model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer3.0.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer3.0.conv2.activation_post_process.observer_enabled', 'model.model.layer3.0.conv2.activation_post_process.scale', 'model.model.layer3.0.conv2.activation_post_process.zero_point', 'model.model.layer3.0.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer3.0.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer3.0.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer3.0.downsample.0.weight', 'model.model.layer3.0.downsample.0.bn.weight', 'model.model.layer3.0.downsample.0.bn.bias', 'model.model.layer3.0.downsample.0.bn.running_mean', 'model.model.layer3.0.downsample.0.bn.running_var', 'model.model.layer3.0.downsample.0.bn.num_batches_tracked', 'model.model.layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled', 'model.model.layer3.0.downsample.0.weight_fake_quant.observer_enabled', 'model.model.layer3.0.downsample.0.weight_fake_quant.scale', 'model.model.layer3.0.downsample.0.weight_fake_quant.zero_point', 'model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps', 'model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val', 'model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val', 'model.model.layer3.0.downsample.0.activation_post_process.fake_quant_enabled', 'model.model.layer3.0.downsample.0.activation_post_process.observer_enabled', 'model.model.layer3.0.downsample.0.activation_post_process.scale', 'model.model.layer3.0.downsample.0.activation_post_process.zero_point', 'model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.eps', 'model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.min_val', 'model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.max_val', 'model.model.layer3.0.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer3.0.add_func.activation_post_process.observer_enabled', 'model.model.layer3.0.add_func.activation_post_process.scale', 'model.model.layer3.0.add_func.activation_post_process.zero_point', 'model.model.layer3.0.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer3.0.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer3.0.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer3.1.conv1.weight', 'model.model.layer3.1.conv1.bn.weight', 'model.model.layer3.1.conv1.bn.bias', 'model.model.layer3.1.conv1.bn.running_mean', 'model.model.layer3.1.conv1.bn.running_var', 'model.model.layer3.1.conv1.bn.num_batches_tracked', 'model.model.layer3.1.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer3.1.conv1.weight_fake_quant.observer_enabled', 'model.model.layer3.1.conv1.weight_fake_quant.scale', 'model.model.layer3.1.conv1.weight_fake_quant.zero_point', 'model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer3.1.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer3.1.conv1.activation_post_process.observer_enabled', 'model.model.layer3.1.conv1.activation_post_process.scale', 'model.model.layer3.1.conv1.activation_post_process.zero_point', 'model.model.layer3.1.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer3.1.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer3.1.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer3.1.conv2.weight', 'model.model.layer3.1.conv2.bn.weight', 'model.model.layer3.1.conv2.bn.bias', 'model.model.layer3.1.conv2.bn.running_mean', 'model.model.layer3.1.conv2.bn.running_var', 'model.model.layer3.1.conv2.bn.num_batches_tracked', 'model.model.layer3.1.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer3.1.conv2.weight_fake_quant.observer_enabled', 'model.model.layer3.1.conv2.weight_fake_quant.scale', 'model.model.layer3.1.conv2.weight_fake_quant.zero_point', 'model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer3.1.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer3.1.conv2.activation_post_process.observer_enabled', 'model.model.layer3.1.conv2.activation_post_process.scale', 'model.model.layer3.1.conv2.activation_post_process.zero_point', 'model.model.layer3.1.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer3.1.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer3.1.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer3.1.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer3.1.add_func.activation_post_process.observer_enabled', 'model.model.layer3.1.add_func.activation_post_process.scale', 'model.model.layer3.1.add_func.activation_post_process.zero_point', 'model.model.layer3.1.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer3.1.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer3.1.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer4.0.conv1.weight', 'model.model.layer4.0.conv1.bn.weight', 'model.model.layer4.0.conv1.bn.bias', 'model.model.layer4.0.conv1.bn.running_mean', 'model.model.layer4.0.conv1.bn.running_var', 'model.model.layer4.0.conv1.bn.num_batches_tracked', 'model.model.layer4.0.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer4.0.conv1.weight_fake_quant.observer_enabled', 'model.model.layer4.0.conv1.weight_fake_quant.scale', 'model.model.layer4.0.conv1.weight_fake_quant.zero_point', 'model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer4.0.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer4.0.conv1.activation_post_process.observer_enabled', 'model.model.layer4.0.conv1.activation_post_process.scale', 'model.model.layer4.0.conv1.activation_post_process.zero_point', 'model.model.layer4.0.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer4.0.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer4.0.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer4.0.conv2.weight', 'model.model.layer4.0.conv2.bn.weight', 'model.model.layer4.0.conv2.bn.bias', 'model.model.layer4.0.conv2.bn.running_mean', 'model.model.layer4.0.conv2.bn.running_var', 'model.model.layer4.0.conv2.bn.num_batches_tracked', 'model.model.layer4.0.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer4.0.conv2.weight_fake_quant.observer_enabled', 'model.model.layer4.0.conv2.weight_fake_quant.scale', 'model.model.layer4.0.conv2.weight_fake_quant.zero_point', 'model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer4.0.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer4.0.conv2.activation_post_process.observer_enabled', 'model.model.layer4.0.conv2.activation_post_process.scale', 'model.model.layer4.0.conv2.activation_post_process.zero_point', 'model.model.layer4.0.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer4.0.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer4.0.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer4.0.downsample.0.weight', 'model.model.layer4.0.downsample.0.bn.weight', 'model.model.layer4.0.downsample.0.bn.bias', 'model.model.layer4.0.downsample.0.bn.running_mean', 'model.model.layer4.0.downsample.0.bn.running_var', 'model.model.layer4.0.downsample.0.bn.num_batches_tracked', 'model.model.layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled', 'model.model.layer4.0.downsample.0.weight_fake_quant.observer_enabled', 'model.model.layer4.0.downsample.0.weight_fake_quant.scale', 'model.model.layer4.0.downsample.0.weight_fake_quant.zero_point', 'model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps', 'model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val', 'model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val', 'model.model.layer4.0.downsample.0.activation_post_process.fake_quant_enabled', 'model.model.layer4.0.downsample.0.activation_post_process.observer_enabled', 'model.model.layer4.0.downsample.0.activation_post_process.scale', 'model.model.layer4.0.downsample.0.activation_post_process.zero_point', 'model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.eps', 'model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.min_val', 'model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.max_val', 'model.model.layer4.0.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer4.0.add_func.activation_post_process.observer_enabled', 'model.model.layer4.0.add_func.activation_post_process.scale', 'model.model.layer4.0.add_func.activation_post_process.zero_point', 'model.model.layer4.0.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer4.0.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer4.0.add_func.activation_post_process.activation_post_process.max_val', 'model.model.layer4.1.conv1.weight', 'model.model.layer4.1.conv1.bn.weight', 'model.model.layer4.1.conv1.bn.bias', 'model.model.layer4.1.conv1.bn.running_mean', 'model.model.layer4.1.conv1.bn.running_var', 'model.model.layer4.1.conv1.bn.num_batches_tracked', 'model.model.layer4.1.conv1.weight_fake_quant.fake_quant_enabled', 'model.model.layer4.1.conv1.weight_fake_quant.observer_enabled', 'model.model.layer4.1.conv1.weight_fake_quant.scale', 'model.model.layer4.1.conv1.weight_fake_quant.zero_point', 'model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.eps', 'model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.min_val', 'model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.max_val', 'model.model.layer4.1.conv1.activation_post_process.fake_quant_enabled', 'model.model.layer4.1.conv1.activation_post_process.observer_enabled', 'model.model.layer4.1.conv1.activation_post_process.scale', 'model.model.layer4.1.conv1.activation_post_process.zero_point', 'model.model.layer4.1.conv1.activation_post_process.activation_post_process.eps', 'model.model.layer4.1.conv1.activation_post_process.activation_post_process.min_val', 'model.model.layer4.1.conv1.activation_post_process.activation_post_process.max_val', 'model.model.layer4.1.conv2.weight', 'model.model.layer4.1.conv2.bn.weight', 'model.model.layer4.1.conv2.bn.bias', 'model.model.layer4.1.conv2.bn.running_mean', 'model.model.layer4.1.conv2.bn.running_var', 'model.model.layer4.1.conv2.bn.num_batches_tracked', 'model.model.layer4.1.conv2.weight_fake_quant.fake_quant_enabled', 'model.model.layer4.1.conv2.weight_fake_quant.observer_enabled', 'model.model.layer4.1.conv2.weight_fake_quant.scale', 'model.model.layer4.1.conv2.weight_fake_quant.zero_point', 'model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.eps', 'model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.min_val', 'model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.max_val', 'model.model.layer4.1.conv2.activation_post_process.fake_quant_enabled', 'model.model.layer4.1.conv2.activation_post_process.observer_enabled', 'model.model.layer4.1.conv2.activation_post_process.scale', 'model.model.layer4.1.conv2.activation_post_process.zero_point', 'model.model.layer4.1.conv2.activation_post_process.activation_post_process.eps', 'model.model.layer4.1.conv2.activation_post_process.activation_post_process.min_val', 'model.model.layer4.1.conv2.activation_post_process.activation_post_process.max_val', 'model.model.layer4.1.add_func.activation_post_process.fake_quant_enabled', 'model.model.layer4.1.add_func.activation_post_process.observer_enabled', 'model.model.layer4.1.add_func.activation_post_process.scale', 'model.model.layer4.1.add_func.activation_post_process.zero_point', 'model.model.layer4.1.add_func.activation_post_process.activation_post_process.eps', 'model.model.layer4.1.add_func.activation_post_process.activation_post_process.min_val', 'model.model.layer4.1.add_func.activation_post_process.activation_post_process.max_val', 'model.model.fc.weight_fake_quant.fake_quant_enabled', 'model.model.fc.weight_fake_quant.observer_enabled', 'model.model.fc.weight_fake_quant.scale', 'model.model.fc.weight_fake_quant.zero_point', 'model.model.fc.weight_fake_quant.activation_post_process.eps', 'model.model.fc.weight_fake_quant.activation_post_process.min_val', 'model.model.fc.weight_fake_quant.activation_post_process.max_val', 'model.model.fc.activation_post_process.fake_quant_enabled', 'model.model.fc.activation_post_process.observer_enabled', 'model.model.fc.activation_post_process.scale', 'model.model.fc.activation_post_process.zero_point', 'model.model.fc.activation_post_process.activation_post_process.eps', 'model.model.fc.activation_post_process.activation_post_process.min_val', 'model.model.fc.activation_post_process.activation_post_process.max_val', 'quant.activation_post_process.fake_quant_enabled', 'quant.activation_post_process.observer_enabled', 'quant.activation_post_process.scale', 'quant.activation_post_process.zero_point', 'quant.activation_post_process.activation_post_process.eps', 'quant.activation_post_process.activation_post_process.min_val', 'quant.activation_post_process.activation_post_process.max_val'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(student_path)\n",
    "student_net.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentNetwork(\n",
       "  (model): TeacherNetwork(\n",
       "    (model): QuantizedResNet18(\n",
       "      (conv1): ConvBnReLU2d(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (maxpool): Identity()\n",
       "      (layer1): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (downsample): Sequential(\n",
       "            (0): ConvBn2d(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): QuantizedBasicBlock(\n",
       "          (conv1): ConvBnReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (bn1): Identity()\n",
       "          (relu): Identity()\n",
       "          (conv2): ConvBn2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (bn2): Identity()\n",
       "          (add_func): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_path_template = \"checkpoints_student_QAT/checkpoint_epoch_150.tar\"\n",
    "new_model = torchvision.models.resnet18()\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
    "new_model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "new_model.maxpool = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "modified_dict = OrderedDict((key.replace('model.model.', ''), value) for key, value in checkpoint['model_state_dict'].items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_52056\\4119730563.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(student_path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\", \"conv1.bn.weight\", \"conv1.bn.bias\", \"conv1.bn.running_mean\", \"conv1.bn.running_var\", \"conv1.bn.num_batches_tracked\", \"conv1.weight_fake_quant.fake_quant_enabled\", \"conv1.weight_fake_quant.observer_enabled\", \"conv1.weight_fake_quant.scale\", \"conv1.weight_fake_quant.zero_point\", \"conv1.weight_fake_quant.activation_post_process.eps\", \"conv1.weight_fake_quant.activation_post_process.min_val\", \"conv1.weight_fake_quant.activation_post_process.max_val\", \"conv1.activation_post_process.fake_quant_enabled\", \"conv1.activation_post_process.observer_enabled\", \"conv1.activation_post_process.scale\", \"conv1.activation_post_process.zero_point\", \"conv1.activation_post_process.activation_post_process.eps\", \"conv1.activation_post_process.activation_post_process.min_val\", \"conv1.activation_post_process.activation_post_process.max_val\", \"layer1.0.add_func.activation_post_process.fake_quant_enabled\", \"layer1.0.add_func.activation_post_process.observer_enabled\", \"layer1.0.add_func.activation_post_process.scale\", \"layer1.0.add_func.activation_post_process.zero_point\", \"layer1.0.add_func.activation_post_process.activation_post_process.eps\", \"layer1.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer1.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer1.0.conv1.bn.weight\", \"layer1.0.conv1.bn.bias\", \"layer1.0.conv1.bn.running_mean\", \"layer1.0.conv1.bn.running_var\", \"layer1.0.conv1.bn.num_batches_tracked\", \"layer1.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer1.0.conv1.weight_fake_quant.observer_enabled\", \"layer1.0.conv1.weight_fake_quant.scale\", \"layer1.0.conv1.weight_fake_quant.zero_point\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer1.0.conv1.activation_post_process.fake_quant_enabled\", \"layer1.0.conv1.activation_post_process.observer_enabled\", \"layer1.0.conv1.activation_post_process.scale\", \"layer1.0.conv1.activation_post_process.zero_point\", \"layer1.0.conv1.activation_post_process.activation_post_process.eps\", \"layer1.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer1.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer1.0.conv2.bn.weight\", \"layer1.0.conv2.bn.bias\", \"layer1.0.conv2.bn.running_mean\", \"layer1.0.conv2.bn.running_var\", \"layer1.0.conv2.bn.num_batches_tracked\", \"layer1.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer1.0.conv2.weight_fake_quant.observer_enabled\", \"layer1.0.conv2.weight_fake_quant.scale\", \"layer1.0.conv2.weight_fake_quant.zero_point\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer1.0.conv2.activation_post_process.fake_quant_enabled\", \"layer1.0.conv2.activation_post_process.observer_enabled\", \"layer1.0.conv2.activation_post_process.scale\", \"layer1.0.conv2.activation_post_process.zero_point\", \"layer1.0.conv2.activation_post_process.activation_post_process.eps\", \"layer1.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer1.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer1.1.add_func.activation_post_process.fake_quant_enabled\", \"layer1.1.add_func.activation_post_process.observer_enabled\", \"layer1.1.add_func.activation_post_process.scale\", \"layer1.1.add_func.activation_post_process.zero_point\", \"layer1.1.add_func.activation_post_process.activation_post_process.eps\", \"layer1.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer1.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer1.1.conv1.bn.weight\", \"layer1.1.conv1.bn.bias\", \"layer1.1.conv1.bn.running_mean\", \"layer1.1.conv1.bn.running_var\", \"layer1.1.conv1.bn.num_batches_tracked\", \"layer1.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer1.1.conv1.weight_fake_quant.observer_enabled\", \"layer1.1.conv1.weight_fake_quant.scale\", \"layer1.1.conv1.weight_fake_quant.zero_point\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer1.1.conv1.activation_post_process.fake_quant_enabled\", \"layer1.1.conv1.activation_post_process.observer_enabled\", \"layer1.1.conv1.activation_post_process.scale\", \"layer1.1.conv1.activation_post_process.zero_point\", \"layer1.1.conv1.activation_post_process.activation_post_process.eps\", \"layer1.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer1.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer1.1.conv2.bn.weight\", \"layer1.1.conv2.bn.bias\", \"layer1.1.conv2.bn.running_mean\", \"layer1.1.conv2.bn.running_var\", \"layer1.1.conv2.bn.num_batches_tracked\", \"layer1.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer1.1.conv2.weight_fake_quant.observer_enabled\", \"layer1.1.conv2.weight_fake_quant.scale\", \"layer1.1.conv2.weight_fake_quant.zero_point\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer1.1.conv2.activation_post_process.fake_quant_enabled\", \"layer1.1.conv2.activation_post_process.observer_enabled\", \"layer1.1.conv2.activation_post_process.scale\", \"layer1.1.conv2.activation_post_process.zero_point\", \"layer1.1.conv2.activation_post_process.activation_post_process.eps\", \"layer1.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer1.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer2.0.add_func.activation_post_process.fake_quant_enabled\", \"layer2.0.add_func.activation_post_process.observer_enabled\", \"layer2.0.add_func.activation_post_process.scale\", \"layer2.0.add_func.activation_post_process.zero_point\", \"layer2.0.add_func.activation_post_process.activation_post_process.eps\", \"layer2.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer2.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer2.0.conv1.bn.weight\", \"layer2.0.conv1.bn.bias\", \"layer2.0.conv1.bn.running_mean\", \"layer2.0.conv1.bn.running_var\", \"layer2.0.conv1.bn.num_batches_tracked\", \"layer2.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer2.0.conv1.weight_fake_quant.observer_enabled\", \"layer2.0.conv1.weight_fake_quant.scale\", \"layer2.0.conv1.weight_fake_quant.zero_point\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.conv1.activation_post_process.fake_quant_enabled\", \"layer2.0.conv1.activation_post_process.observer_enabled\", \"layer2.0.conv1.activation_post_process.scale\", \"layer2.0.conv1.activation_post_process.zero_point\", \"layer2.0.conv1.activation_post_process.activation_post_process.eps\", \"layer2.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer2.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer2.0.conv2.bn.weight\", \"layer2.0.conv2.bn.bias\", \"layer2.0.conv2.bn.running_mean\", \"layer2.0.conv2.bn.running_var\", \"layer2.0.conv2.bn.num_batches_tracked\", \"layer2.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer2.0.conv2.weight_fake_quant.observer_enabled\", \"layer2.0.conv2.weight_fake_quant.scale\", \"layer2.0.conv2.weight_fake_quant.zero_point\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.conv2.activation_post_process.fake_quant_enabled\", \"layer2.0.conv2.activation_post_process.observer_enabled\", \"layer2.0.conv2.activation_post_process.scale\", \"layer2.0.conv2.activation_post_process.zero_point\", \"layer2.0.conv2.activation_post_process.activation_post_process.eps\", \"layer2.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer2.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer2.0.downsample.0.bn.weight\", \"layer2.0.downsample.0.bn.bias\", \"layer2.0.downsample.0.bn.running_mean\", \"layer2.0.downsample.0.bn.running_var\", \"layer2.0.downsample.0.bn.num_batches_tracked\", \"layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer2.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer2.0.downsample.0.weight_fake_quant.scale\", \"layer2.0.downsample.0.weight_fake_quant.zero_point\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer2.0.downsample.0.activation_post_process.observer_enabled\", \"layer2.0.downsample.0.activation_post_process.scale\", \"layer2.0.downsample.0.activation_post_process.zero_point\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer2.1.add_func.activation_post_process.fake_quant_enabled\", \"layer2.1.add_func.activation_post_process.observer_enabled\", \"layer2.1.add_func.activation_post_process.scale\", \"layer2.1.add_func.activation_post_process.zero_point\", \"layer2.1.add_func.activation_post_process.activation_post_process.eps\", \"layer2.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer2.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer2.1.conv1.bn.weight\", \"layer2.1.conv1.bn.bias\", \"layer2.1.conv1.bn.running_mean\", \"layer2.1.conv1.bn.running_var\", \"layer2.1.conv1.bn.num_batches_tracked\", \"layer2.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer2.1.conv1.weight_fake_quant.observer_enabled\", \"layer2.1.conv1.weight_fake_quant.scale\", \"layer2.1.conv1.weight_fake_quant.zero_point\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer2.1.conv1.activation_post_process.fake_quant_enabled\", \"layer2.1.conv1.activation_post_process.observer_enabled\", \"layer2.1.conv1.activation_post_process.scale\", \"layer2.1.conv1.activation_post_process.zero_point\", \"layer2.1.conv1.activation_post_process.activation_post_process.eps\", \"layer2.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer2.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer2.1.conv2.bn.weight\", \"layer2.1.conv2.bn.bias\", \"layer2.1.conv2.bn.running_mean\", \"layer2.1.conv2.bn.running_var\", \"layer2.1.conv2.bn.num_batches_tracked\", \"layer2.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer2.1.conv2.weight_fake_quant.observer_enabled\", \"layer2.1.conv2.weight_fake_quant.scale\", \"layer2.1.conv2.weight_fake_quant.zero_point\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer2.1.conv2.activation_post_process.fake_quant_enabled\", \"layer2.1.conv2.activation_post_process.observer_enabled\", \"layer2.1.conv2.activation_post_process.scale\", \"layer2.1.conv2.activation_post_process.zero_point\", \"layer2.1.conv2.activation_post_process.activation_post_process.eps\", \"layer2.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer2.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer3.0.add_func.activation_post_process.fake_quant_enabled\", \"layer3.0.add_func.activation_post_process.observer_enabled\", \"layer3.0.add_func.activation_post_process.scale\", \"layer3.0.add_func.activation_post_process.zero_point\", \"layer3.0.add_func.activation_post_process.activation_post_process.eps\", \"layer3.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer3.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer3.0.conv1.bn.weight\", \"layer3.0.conv1.bn.bias\", \"layer3.0.conv1.bn.running_mean\", \"layer3.0.conv1.bn.running_var\", \"layer3.0.conv1.bn.num_batches_tracked\", \"layer3.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer3.0.conv1.weight_fake_quant.observer_enabled\", \"layer3.0.conv1.weight_fake_quant.scale\", \"layer3.0.conv1.weight_fake_quant.zero_point\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.conv1.activation_post_process.fake_quant_enabled\", \"layer3.0.conv1.activation_post_process.observer_enabled\", \"layer3.0.conv1.activation_post_process.scale\", \"layer3.0.conv1.activation_post_process.zero_point\", \"layer3.0.conv1.activation_post_process.activation_post_process.eps\", \"layer3.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer3.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer3.0.conv2.bn.weight\", \"layer3.0.conv2.bn.bias\", \"layer3.0.conv2.bn.running_mean\", \"layer3.0.conv2.bn.running_var\", \"layer3.0.conv2.bn.num_batches_tracked\", \"layer3.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer3.0.conv2.weight_fake_quant.observer_enabled\", \"layer3.0.conv2.weight_fake_quant.scale\", \"layer3.0.conv2.weight_fake_quant.zero_point\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.conv2.activation_post_process.fake_quant_enabled\", \"layer3.0.conv2.activation_post_process.observer_enabled\", \"layer3.0.conv2.activation_post_process.scale\", \"layer3.0.conv2.activation_post_process.zero_point\", \"layer3.0.conv2.activation_post_process.activation_post_process.eps\", \"layer3.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer3.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer3.0.downsample.0.bn.weight\", \"layer3.0.downsample.0.bn.bias\", \"layer3.0.downsample.0.bn.running_mean\", \"layer3.0.downsample.0.bn.running_var\", \"layer3.0.downsample.0.bn.num_batches_tracked\", \"layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer3.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer3.0.downsample.0.weight_fake_quant.scale\", \"layer3.0.downsample.0.weight_fake_quant.zero_point\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer3.0.downsample.0.activation_post_process.observer_enabled\", \"layer3.0.downsample.0.activation_post_process.scale\", \"layer3.0.downsample.0.activation_post_process.zero_point\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer3.1.add_func.activation_post_process.fake_quant_enabled\", \"layer3.1.add_func.activation_post_process.observer_enabled\", \"layer3.1.add_func.activation_post_process.scale\", \"layer3.1.add_func.activation_post_process.zero_point\", \"layer3.1.add_func.activation_post_process.activation_post_process.eps\", \"layer3.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer3.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer3.1.conv1.bn.weight\", \"layer3.1.conv1.bn.bias\", \"layer3.1.conv1.bn.running_mean\", \"layer3.1.conv1.bn.running_var\", \"layer3.1.conv1.bn.num_batches_tracked\", \"layer3.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer3.1.conv1.weight_fake_quant.observer_enabled\", \"layer3.1.conv1.weight_fake_quant.scale\", \"layer3.1.conv1.weight_fake_quant.zero_point\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer3.1.conv1.activation_post_process.fake_quant_enabled\", \"layer3.1.conv1.activation_post_process.observer_enabled\", \"layer3.1.conv1.activation_post_process.scale\", \"layer3.1.conv1.activation_post_process.zero_point\", \"layer3.1.conv1.activation_post_process.activation_post_process.eps\", \"layer3.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer3.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer3.1.conv2.bn.weight\", \"layer3.1.conv2.bn.bias\", \"layer3.1.conv2.bn.running_mean\", \"layer3.1.conv2.bn.running_var\", \"layer3.1.conv2.bn.num_batches_tracked\", \"layer3.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer3.1.conv2.weight_fake_quant.observer_enabled\", \"layer3.1.conv2.weight_fake_quant.scale\", \"layer3.1.conv2.weight_fake_quant.zero_point\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer3.1.conv2.activation_post_process.fake_quant_enabled\", \"layer3.1.conv2.activation_post_process.observer_enabled\", \"layer3.1.conv2.activation_post_process.scale\", \"layer3.1.conv2.activation_post_process.zero_point\", \"layer3.1.conv2.activation_post_process.activation_post_process.eps\", \"layer3.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer3.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer4.0.add_func.activation_post_process.fake_quant_enabled\", \"layer4.0.add_func.activation_post_process.observer_enabled\", \"layer4.0.add_func.activation_post_process.scale\", \"layer4.0.add_func.activation_post_process.zero_point\", \"layer4.0.add_func.activation_post_process.activation_post_process.eps\", \"layer4.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer4.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer4.0.conv1.bn.weight\", \"layer4.0.conv1.bn.bias\", \"layer4.0.conv1.bn.running_mean\", \"layer4.0.conv1.bn.running_var\", \"layer4.0.conv1.bn.num_batches_tracked\", \"layer4.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer4.0.conv1.weight_fake_quant.observer_enabled\", \"layer4.0.conv1.weight_fake_quant.scale\", \"layer4.0.conv1.weight_fake_quant.zero_point\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.conv1.activation_post_process.fake_quant_enabled\", \"layer4.0.conv1.activation_post_process.observer_enabled\", \"layer4.0.conv1.activation_post_process.scale\", \"layer4.0.conv1.activation_post_process.zero_point\", \"layer4.0.conv1.activation_post_process.activation_post_process.eps\", \"layer4.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer4.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer4.0.conv2.bn.weight\", \"layer4.0.conv2.bn.bias\", \"layer4.0.conv2.bn.running_mean\", \"layer4.0.conv2.bn.running_var\", \"layer4.0.conv2.bn.num_batches_tracked\", \"layer4.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer4.0.conv2.weight_fake_quant.observer_enabled\", \"layer4.0.conv2.weight_fake_quant.scale\", \"layer4.0.conv2.weight_fake_quant.zero_point\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.conv2.activation_post_process.fake_quant_enabled\", \"layer4.0.conv2.activation_post_process.observer_enabled\", \"layer4.0.conv2.activation_post_process.scale\", \"layer4.0.conv2.activation_post_process.zero_point\", \"layer4.0.conv2.activation_post_process.activation_post_process.eps\", \"layer4.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer4.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer4.0.downsample.0.bn.weight\", \"layer4.0.downsample.0.bn.bias\", \"layer4.0.downsample.0.bn.running_mean\", \"layer4.0.downsample.0.bn.running_var\", \"layer4.0.downsample.0.bn.num_batches_tracked\", \"layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer4.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer4.0.downsample.0.weight_fake_quant.scale\", \"layer4.0.downsample.0.weight_fake_quant.zero_point\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer4.0.downsample.0.activation_post_process.observer_enabled\", \"layer4.0.downsample.0.activation_post_process.scale\", \"layer4.0.downsample.0.activation_post_process.zero_point\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer4.1.add_func.activation_post_process.fake_quant_enabled\", \"layer4.1.add_func.activation_post_process.observer_enabled\", \"layer4.1.add_func.activation_post_process.scale\", \"layer4.1.add_func.activation_post_process.zero_point\", \"layer4.1.add_func.activation_post_process.activation_post_process.eps\", \"layer4.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer4.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer4.1.conv1.bn.weight\", \"layer4.1.conv1.bn.bias\", \"layer4.1.conv1.bn.running_mean\", \"layer4.1.conv1.bn.running_var\", \"layer4.1.conv1.bn.num_batches_tracked\", \"layer4.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer4.1.conv1.weight_fake_quant.observer_enabled\", \"layer4.1.conv1.weight_fake_quant.scale\", \"layer4.1.conv1.weight_fake_quant.zero_point\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer4.1.conv1.activation_post_process.fake_quant_enabled\", \"layer4.1.conv1.activation_post_process.observer_enabled\", \"layer4.1.conv1.activation_post_process.scale\", \"layer4.1.conv1.activation_post_process.zero_point\", \"layer4.1.conv1.activation_post_process.activation_post_process.eps\", \"layer4.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer4.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer4.1.conv2.bn.weight\", \"layer4.1.conv2.bn.bias\", \"layer4.1.conv2.bn.running_mean\", \"layer4.1.conv2.bn.running_var\", \"layer4.1.conv2.bn.num_batches_tracked\", \"layer4.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer4.1.conv2.weight_fake_quant.observer_enabled\", \"layer4.1.conv2.weight_fake_quant.scale\", \"layer4.1.conv2.weight_fake_quant.zero_point\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer4.1.conv2.activation_post_process.fake_quant_enabled\", \"layer4.1.conv2.activation_post_process.observer_enabled\", \"layer4.1.conv2.activation_post_process.scale\", \"layer4.1.conv2.activation_post_process.zero_point\", \"layer4.1.conv2.activation_post_process.activation_post_process.eps\", \"layer4.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer4.1.conv2.activation_post_process.activation_post_process.max_val\", \"fc.weight_fake_quant.fake_quant_enabled\", \"fc.weight_fake_quant.observer_enabled\", \"fc.weight_fake_quant.scale\", \"fc.weight_fake_quant.zero_point\", \"fc.weight_fake_quant.activation_post_process.eps\", \"fc.weight_fake_quant.activation_post_process.min_val\", \"fc.weight_fake_quant.activation_post_process.max_val\", \"fc.activation_post_process.fake_quant_enabled\", \"fc.activation_post_process.observer_enabled\", \"fc.activation_post_process.scale\", \"fc.activation_post_process.zero_point\", \"fc.activation_post_process.activation_post_process.eps\", \"fc.activation_post_process.activation_post_process.min_val\", \"fc.activation_post_process.activation_post_process.max_val\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(student_path)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\", \"conv1.bn.weight\", \"conv1.bn.bias\", \"conv1.bn.running_mean\", \"conv1.bn.running_var\", \"conv1.bn.num_batches_tracked\", \"conv1.weight_fake_quant.fake_quant_enabled\", \"conv1.weight_fake_quant.observer_enabled\", \"conv1.weight_fake_quant.scale\", \"conv1.weight_fake_quant.zero_point\", \"conv1.weight_fake_quant.activation_post_process.eps\", \"conv1.weight_fake_quant.activation_post_process.min_val\", \"conv1.weight_fake_quant.activation_post_process.max_val\", \"conv1.activation_post_process.fake_quant_enabled\", \"conv1.activation_post_process.observer_enabled\", \"conv1.activation_post_process.scale\", \"conv1.activation_post_process.zero_point\", \"conv1.activation_post_process.activation_post_process.eps\", \"conv1.activation_post_process.activation_post_process.min_val\", \"conv1.activation_post_process.activation_post_process.max_val\", \"layer1.0.add_func.activation_post_process.fake_quant_enabled\", \"layer1.0.add_func.activation_post_process.observer_enabled\", \"layer1.0.add_func.activation_post_process.scale\", \"layer1.0.add_func.activation_post_process.zero_point\", \"layer1.0.add_func.activation_post_process.activation_post_process.eps\", \"layer1.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer1.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer1.0.conv1.bn.weight\", \"layer1.0.conv1.bn.bias\", \"layer1.0.conv1.bn.running_mean\", \"layer1.0.conv1.bn.running_var\", \"layer1.0.conv1.bn.num_batches_tracked\", \"layer1.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer1.0.conv1.weight_fake_quant.observer_enabled\", \"layer1.0.conv1.weight_fake_quant.scale\", \"layer1.0.conv1.weight_fake_quant.zero_point\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer1.0.conv1.activation_post_process.fake_quant_enabled\", \"layer1.0.conv1.activation_post_process.observer_enabled\", \"layer1.0.conv1.activation_post_process.scale\", \"layer1.0.conv1.activation_post_process.zero_point\", \"layer1.0.conv1.activation_post_process.activation_post_process.eps\", \"layer1.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer1.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer1.0.conv2.bn.weight\", \"layer1.0.conv2.bn.bias\", \"layer1.0.conv2.bn.running_mean\", \"layer1.0.conv2.bn.running_var\", \"layer1.0.conv2.bn.num_batches_tracked\", \"layer1.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer1.0.conv2.weight_fake_quant.observer_enabled\", \"layer1.0.conv2.weight_fake_quant.scale\", \"layer1.0.conv2.weight_fake_quant.zero_point\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer1.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer1.0.conv2.activation_post_process.fake_quant_enabled\", \"layer1.0.conv2.activation_post_process.observer_enabled\", \"layer1.0.conv2.activation_post_process.scale\", \"layer1.0.conv2.activation_post_process.zero_point\", \"layer1.0.conv2.activation_post_process.activation_post_process.eps\", \"layer1.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer1.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer1.1.add_func.activation_post_process.fake_quant_enabled\", \"layer1.1.add_func.activation_post_process.observer_enabled\", \"layer1.1.add_func.activation_post_process.scale\", \"layer1.1.add_func.activation_post_process.zero_point\", \"layer1.1.add_func.activation_post_process.activation_post_process.eps\", \"layer1.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer1.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer1.1.conv1.bn.weight\", \"layer1.1.conv1.bn.bias\", \"layer1.1.conv1.bn.running_mean\", \"layer1.1.conv1.bn.running_var\", \"layer1.1.conv1.bn.num_batches_tracked\", \"layer1.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer1.1.conv1.weight_fake_quant.observer_enabled\", \"layer1.1.conv1.weight_fake_quant.scale\", \"layer1.1.conv1.weight_fake_quant.zero_point\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer1.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer1.1.conv1.activation_post_process.fake_quant_enabled\", \"layer1.1.conv1.activation_post_process.observer_enabled\", \"layer1.1.conv1.activation_post_process.scale\", \"layer1.1.conv1.activation_post_process.zero_point\", \"layer1.1.conv1.activation_post_process.activation_post_process.eps\", \"layer1.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer1.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer1.1.conv2.bn.weight\", \"layer1.1.conv2.bn.bias\", \"layer1.1.conv2.bn.running_mean\", \"layer1.1.conv2.bn.running_var\", \"layer1.1.conv2.bn.num_batches_tracked\", \"layer1.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer1.1.conv2.weight_fake_quant.observer_enabled\", \"layer1.1.conv2.weight_fake_quant.scale\", \"layer1.1.conv2.weight_fake_quant.zero_point\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer1.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer1.1.conv2.activation_post_process.fake_quant_enabled\", \"layer1.1.conv2.activation_post_process.observer_enabled\", \"layer1.1.conv2.activation_post_process.scale\", \"layer1.1.conv2.activation_post_process.zero_point\", \"layer1.1.conv2.activation_post_process.activation_post_process.eps\", \"layer1.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer1.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer2.0.add_func.activation_post_process.fake_quant_enabled\", \"layer2.0.add_func.activation_post_process.observer_enabled\", \"layer2.0.add_func.activation_post_process.scale\", \"layer2.0.add_func.activation_post_process.zero_point\", \"layer2.0.add_func.activation_post_process.activation_post_process.eps\", \"layer2.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer2.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer2.0.conv1.bn.weight\", \"layer2.0.conv1.bn.bias\", \"layer2.0.conv1.bn.running_mean\", \"layer2.0.conv1.bn.running_var\", \"layer2.0.conv1.bn.num_batches_tracked\", \"layer2.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer2.0.conv1.weight_fake_quant.observer_enabled\", \"layer2.0.conv1.weight_fake_quant.scale\", \"layer2.0.conv1.weight_fake_quant.zero_point\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.conv1.activation_post_process.fake_quant_enabled\", \"layer2.0.conv1.activation_post_process.observer_enabled\", \"layer2.0.conv1.activation_post_process.scale\", \"layer2.0.conv1.activation_post_process.zero_point\", \"layer2.0.conv1.activation_post_process.activation_post_process.eps\", \"layer2.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer2.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer2.0.conv2.bn.weight\", \"layer2.0.conv2.bn.bias\", \"layer2.0.conv2.bn.running_mean\", \"layer2.0.conv2.bn.running_var\", \"layer2.0.conv2.bn.num_batches_tracked\", \"layer2.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer2.0.conv2.weight_fake_quant.observer_enabled\", \"layer2.0.conv2.weight_fake_quant.scale\", \"layer2.0.conv2.weight_fake_quant.zero_point\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.conv2.activation_post_process.fake_quant_enabled\", \"layer2.0.conv2.activation_post_process.observer_enabled\", \"layer2.0.conv2.activation_post_process.scale\", \"layer2.0.conv2.activation_post_process.zero_point\", \"layer2.0.conv2.activation_post_process.activation_post_process.eps\", \"layer2.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer2.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer2.0.downsample.0.bn.weight\", \"layer2.0.downsample.0.bn.bias\", \"layer2.0.downsample.0.bn.running_mean\", \"layer2.0.downsample.0.bn.running_var\", \"layer2.0.downsample.0.bn.num_batches_tracked\", \"layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer2.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer2.0.downsample.0.weight_fake_quant.scale\", \"layer2.0.downsample.0.weight_fake_quant.zero_point\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer2.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer2.0.downsample.0.activation_post_process.observer_enabled\", \"layer2.0.downsample.0.activation_post_process.scale\", \"layer2.0.downsample.0.activation_post_process.zero_point\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer2.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer2.1.add_func.activation_post_process.fake_quant_enabled\", \"layer2.1.add_func.activation_post_process.observer_enabled\", \"layer2.1.add_func.activation_post_process.scale\", \"layer2.1.add_func.activation_post_process.zero_point\", \"layer2.1.add_func.activation_post_process.activation_post_process.eps\", \"layer2.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer2.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer2.1.conv1.bn.weight\", \"layer2.1.conv1.bn.bias\", \"layer2.1.conv1.bn.running_mean\", \"layer2.1.conv1.bn.running_var\", \"layer2.1.conv1.bn.num_batches_tracked\", \"layer2.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer2.1.conv1.weight_fake_quant.observer_enabled\", \"layer2.1.conv1.weight_fake_quant.scale\", \"layer2.1.conv1.weight_fake_quant.zero_point\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer2.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer2.1.conv1.activation_post_process.fake_quant_enabled\", \"layer2.1.conv1.activation_post_process.observer_enabled\", \"layer2.1.conv1.activation_post_process.scale\", \"layer2.1.conv1.activation_post_process.zero_point\", \"layer2.1.conv1.activation_post_process.activation_post_process.eps\", \"layer2.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer2.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer2.1.conv2.bn.weight\", \"layer2.1.conv2.bn.bias\", \"layer2.1.conv2.bn.running_mean\", \"layer2.1.conv2.bn.running_var\", \"layer2.1.conv2.bn.num_batches_tracked\", \"layer2.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer2.1.conv2.weight_fake_quant.observer_enabled\", \"layer2.1.conv2.weight_fake_quant.scale\", \"layer2.1.conv2.weight_fake_quant.zero_point\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer2.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer2.1.conv2.activation_post_process.fake_quant_enabled\", \"layer2.1.conv2.activation_post_process.observer_enabled\", \"layer2.1.conv2.activation_post_process.scale\", \"layer2.1.conv2.activation_post_process.zero_point\", \"layer2.1.conv2.activation_post_process.activation_post_process.eps\", \"layer2.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer2.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer3.0.add_func.activation_post_process.fake_quant_enabled\", \"layer3.0.add_func.activation_post_process.observer_enabled\", \"layer3.0.add_func.activation_post_process.scale\", \"layer3.0.add_func.activation_post_process.zero_point\", \"layer3.0.add_func.activation_post_process.activation_post_process.eps\", \"layer3.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer3.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer3.0.conv1.bn.weight\", \"layer3.0.conv1.bn.bias\", \"layer3.0.conv1.bn.running_mean\", \"layer3.0.conv1.bn.running_var\", \"layer3.0.conv1.bn.num_batches_tracked\", \"layer3.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer3.0.conv1.weight_fake_quant.observer_enabled\", \"layer3.0.conv1.weight_fake_quant.scale\", \"layer3.0.conv1.weight_fake_quant.zero_point\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.conv1.activation_post_process.fake_quant_enabled\", \"layer3.0.conv1.activation_post_process.observer_enabled\", \"layer3.0.conv1.activation_post_process.scale\", \"layer3.0.conv1.activation_post_process.zero_point\", \"layer3.0.conv1.activation_post_process.activation_post_process.eps\", \"layer3.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer3.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer3.0.conv2.bn.weight\", \"layer3.0.conv2.bn.bias\", \"layer3.0.conv2.bn.running_mean\", \"layer3.0.conv2.bn.running_var\", \"layer3.0.conv2.bn.num_batches_tracked\", \"layer3.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer3.0.conv2.weight_fake_quant.observer_enabled\", \"layer3.0.conv2.weight_fake_quant.scale\", \"layer3.0.conv2.weight_fake_quant.zero_point\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.conv2.activation_post_process.fake_quant_enabled\", \"layer3.0.conv2.activation_post_process.observer_enabled\", \"layer3.0.conv2.activation_post_process.scale\", \"layer3.0.conv2.activation_post_process.zero_point\", \"layer3.0.conv2.activation_post_process.activation_post_process.eps\", \"layer3.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer3.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer3.0.downsample.0.bn.weight\", \"layer3.0.downsample.0.bn.bias\", \"layer3.0.downsample.0.bn.running_mean\", \"layer3.0.downsample.0.bn.running_var\", \"layer3.0.downsample.0.bn.num_batches_tracked\", \"layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer3.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer3.0.downsample.0.weight_fake_quant.scale\", \"layer3.0.downsample.0.weight_fake_quant.zero_point\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer3.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer3.0.downsample.0.activation_post_process.observer_enabled\", \"layer3.0.downsample.0.activation_post_process.scale\", \"layer3.0.downsample.0.activation_post_process.zero_point\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer3.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer3.1.add_func.activation_post_process.fake_quant_enabled\", \"layer3.1.add_func.activation_post_process.observer_enabled\", \"layer3.1.add_func.activation_post_process.scale\", \"layer3.1.add_func.activation_post_process.zero_point\", \"layer3.1.add_func.activation_post_process.activation_post_process.eps\", \"layer3.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer3.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer3.1.conv1.bn.weight\", \"layer3.1.conv1.bn.bias\", \"layer3.1.conv1.bn.running_mean\", \"layer3.1.conv1.bn.running_var\", \"layer3.1.conv1.bn.num_batches_tracked\", \"layer3.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer3.1.conv1.weight_fake_quant.observer_enabled\", \"layer3.1.conv1.weight_fake_quant.scale\", \"layer3.1.conv1.weight_fake_quant.zero_point\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer3.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer3.1.conv1.activation_post_process.fake_quant_enabled\", \"layer3.1.conv1.activation_post_process.observer_enabled\", \"layer3.1.conv1.activation_post_process.scale\", \"layer3.1.conv1.activation_post_process.zero_point\", \"layer3.1.conv1.activation_post_process.activation_post_process.eps\", \"layer3.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer3.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer3.1.conv2.bn.weight\", \"layer3.1.conv2.bn.bias\", \"layer3.1.conv2.bn.running_mean\", \"layer3.1.conv2.bn.running_var\", \"layer3.1.conv2.bn.num_batches_tracked\", \"layer3.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer3.1.conv2.weight_fake_quant.observer_enabled\", \"layer3.1.conv2.weight_fake_quant.scale\", \"layer3.1.conv2.weight_fake_quant.zero_point\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer3.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer3.1.conv2.activation_post_process.fake_quant_enabled\", \"layer3.1.conv2.activation_post_process.observer_enabled\", \"layer3.1.conv2.activation_post_process.scale\", \"layer3.1.conv2.activation_post_process.zero_point\", \"layer3.1.conv2.activation_post_process.activation_post_process.eps\", \"layer3.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer3.1.conv2.activation_post_process.activation_post_process.max_val\", \"layer4.0.add_func.activation_post_process.fake_quant_enabled\", \"layer4.0.add_func.activation_post_process.observer_enabled\", \"layer4.0.add_func.activation_post_process.scale\", \"layer4.0.add_func.activation_post_process.zero_point\", \"layer4.0.add_func.activation_post_process.activation_post_process.eps\", \"layer4.0.add_func.activation_post_process.activation_post_process.min_val\", \"layer4.0.add_func.activation_post_process.activation_post_process.max_val\", \"layer4.0.conv1.bn.weight\", \"layer4.0.conv1.bn.bias\", \"layer4.0.conv1.bn.running_mean\", \"layer4.0.conv1.bn.running_var\", \"layer4.0.conv1.bn.num_batches_tracked\", \"layer4.0.conv1.weight_fake_quant.fake_quant_enabled\", \"layer4.0.conv1.weight_fake_quant.observer_enabled\", \"layer4.0.conv1.weight_fake_quant.scale\", \"layer4.0.conv1.weight_fake_quant.zero_point\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.eps\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.conv1.activation_post_process.fake_quant_enabled\", \"layer4.0.conv1.activation_post_process.observer_enabled\", \"layer4.0.conv1.activation_post_process.scale\", \"layer4.0.conv1.activation_post_process.zero_point\", \"layer4.0.conv1.activation_post_process.activation_post_process.eps\", \"layer4.0.conv1.activation_post_process.activation_post_process.min_val\", \"layer4.0.conv1.activation_post_process.activation_post_process.max_val\", \"layer4.0.conv2.bn.weight\", \"layer4.0.conv2.bn.bias\", \"layer4.0.conv2.bn.running_mean\", \"layer4.0.conv2.bn.running_var\", \"layer4.0.conv2.bn.num_batches_tracked\", \"layer4.0.conv2.weight_fake_quant.fake_quant_enabled\", \"layer4.0.conv2.weight_fake_quant.observer_enabled\", \"layer4.0.conv2.weight_fake_quant.scale\", \"layer4.0.conv2.weight_fake_quant.zero_point\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.eps\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.conv2.activation_post_process.fake_quant_enabled\", \"layer4.0.conv2.activation_post_process.observer_enabled\", \"layer4.0.conv2.activation_post_process.scale\", \"layer4.0.conv2.activation_post_process.zero_point\", \"layer4.0.conv2.activation_post_process.activation_post_process.eps\", \"layer4.0.conv2.activation_post_process.activation_post_process.min_val\", \"layer4.0.conv2.activation_post_process.activation_post_process.max_val\", \"layer4.0.downsample.0.bn.weight\", \"layer4.0.downsample.0.bn.bias\", \"layer4.0.downsample.0.bn.running_mean\", \"layer4.0.downsample.0.bn.running_var\", \"layer4.0.downsample.0.bn.num_batches_tracked\", \"layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"layer4.0.downsample.0.weight_fake_quant.observer_enabled\", \"layer4.0.downsample.0.weight_fake_quant.scale\", \"layer4.0.downsample.0.weight_fake_quant.zero_point\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"layer4.0.downsample.0.activation_post_process.fake_quant_enabled\", \"layer4.0.downsample.0.activation_post_process.observer_enabled\", \"layer4.0.downsample.0.activation_post_process.scale\", \"layer4.0.downsample.0.activation_post_process.zero_point\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.eps\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"layer4.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"layer4.1.add_func.activation_post_process.fake_quant_enabled\", \"layer4.1.add_func.activation_post_process.observer_enabled\", \"layer4.1.add_func.activation_post_process.scale\", \"layer4.1.add_func.activation_post_process.zero_point\", \"layer4.1.add_func.activation_post_process.activation_post_process.eps\", \"layer4.1.add_func.activation_post_process.activation_post_process.min_val\", \"layer4.1.add_func.activation_post_process.activation_post_process.max_val\", \"layer4.1.conv1.bn.weight\", \"layer4.1.conv1.bn.bias\", \"layer4.1.conv1.bn.running_mean\", \"layer4.1.conv1.bn.running_var\", \"layer4.1.conv1.bn.num_batches_tracked\", \"layer4.1.conv1.weight_fake_quant.fake_quant_enabled\", \"layer4.1.conv1.weight_fake_quant.observer_enabled\", \"layer4.1.conv1.weight_fake_quant.scale\", \"layer4.1.conv1.weight_fake_quant.zero_point\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.eps\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"layer4.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"layer4.1.conv1.activation_post_process.fake_quant_enabled\", \"layer4.1.conv1.activation_post_process.observer_enabled\", \"layer4.1.conv1.activation_post_process.scale\", \"layer4.1.conv1.activation_post_process.zero_point\", \"layer4.1.conv1.activation_post_process.activation_post_process.eps\", \"layer4.1.conv1.activation_post_process.activation_post_process.min_val\", \"layer4.1.conv1.activation_post_process.activation_post_process.max_val\", \"layer4.1.conv2.bn.weight\", \"layer4.1.conv2.bn.bias\", \"layer4.1.conv2.bn.running_mean\", \"layer4.1.conv2.bn.running_var\", \"layer4.1.conv2.bn.num_batches_tracked\", \"layer4.1.conv2.weight_fake_quant.fake_quant_enabled\", \"layer4.1.conv2.weight_fake_quant.observer_enabled\", \"layer4.1.conv2.weight_fake_quant.scale\", \"layer4.1.conv2.weight_fake_quant.zero_point\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.eps\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"layer4.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"layer4.1.conv2.activation_post_process.fake_quant_enabled\", \"layer4.1.conv2.activation_post_process.observer_enabled\", \"layer4.1.conv2.activation_post_process.scale\", \"layer4.1.conv2.activation_post_process.zero_point\", \"layer4.1.conv2.activation_post_process.activation_post_process.eps\", \"layer4.1.conv2.activation_post_process.activation_post_process.min_val\", \"layer4.1.conv2.activation_post_process.activation_post_process.max_val\", \"fc.weight_fake_quant.fake_quant_enabled\", \"fc.weight_fake_quant.observer_enabled\", \"fc.weight_fake_quant.scale\", \"fc.weight_fake_quant.zero_point\", \"fc.weight_fake_quant.activation_post_process.eps\", \"fc.weight_fake_quant.activation_post_process.min_val\", \"fc.weight_fake_quant.activation_post_process.max_val\", \"fc.activation_post_process.fake_quant_enabled\", \"fc.activation_post_process.observer_enabled\", \"fc.activation_post_process.scale\", \"fc.activation_post_process.zero_point\", \"fc.activation_post_process.activation_post_process.eps\", \"fc.activation_post_process.activation_post_process.min_val\", \"fc.activation_post_process.activation_post_process.max_val\". "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(student_path)\n",
    "\n",
    "new_model.load_state_dict(modified_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_56004\\3047527709.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(student_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': {'train_loss': [tensor(3.9311, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.9368, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.8168, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.7657, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.6296, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.4909, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.4825, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.5679, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.3199, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.5112, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.2795, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.4019, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.4146, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.4172, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.2845, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.2229, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0368, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0228, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0057, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0853, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0745, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0226, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.7468, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.8434, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.8738, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.0012, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.8838, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.8214, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.7299, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6773, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.7698, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6608, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6506, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.5466, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.5057, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6232, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6531, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4028, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4829, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4625, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6246, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2506, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2494, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3144, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.5942, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2135, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2673, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1580, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3119, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3697, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2532, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3038, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9408, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1391, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3226, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1399, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9816, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2740, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2320, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3752, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1814, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1709, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2950, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4417, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9992, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2255, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1847, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6414, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2232, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3172, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4467, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.4287, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1125, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0316, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3751, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2203, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3049, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3163, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1299, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.5842, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8936, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1146, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9788, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3336, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0791, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9684, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8670, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9500, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1008, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0972, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0336, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1038, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9330, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9564, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0758, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2758, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9503, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1209, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9577, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0847, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2758, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2552, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1753, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0455, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7933, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.6144, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1616, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2164, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9362, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1232, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9625, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.3662, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0120, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0318, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0279, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0763, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1052, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9912, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9340, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1488, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0388, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9706, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1290, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0242, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9599, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0773, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2265, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0066, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1021, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0253, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8352, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7130, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8307, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1655, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0054, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8221, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2052, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0113, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6566, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7586, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7538, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8926, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0360, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6969, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8671, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8797, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7632, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8472, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9951, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9879, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0520, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9053, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0414, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9765, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0996, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0096, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9261, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1266, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9000, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1253, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0137, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8460, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8463, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8420, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9644, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8254, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9578, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5236, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6039, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9346, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7563, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7290, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1591, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5554, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7788, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9188, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8370, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6841, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7447, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5615, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9149, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8202, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7710, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6771, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6138, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6307, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8189, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7063, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8361, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9309, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7115, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9642, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8591, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8925, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6630, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7884, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6825, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7399, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4581, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6177, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.2049, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7874, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5596, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7056, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8658, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4943, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8279, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8762, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8380, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8780, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8188, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9506, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7888, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8304, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1196, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0711, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6110, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8034, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7908, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8010, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6932, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9143, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6833, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7150, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7431, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4907, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9415, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4796, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9671, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6755, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7750, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7358, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6594, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5714, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6156, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8449, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7049, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8812, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9495, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8214, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9728, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6220, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9923, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6328, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6302, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5508, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8675, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6560, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7844, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5995, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4936, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7674, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7780, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8640, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7649, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8233, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7402, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8261, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9170, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9050, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6847, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5996, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5876, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.1200, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7179, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5108, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8985, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8581, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8352, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7191, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8452, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6208, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5824, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5459, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8537, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6550, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6830, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6595, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7160, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7027, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6991, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6163, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7103, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5403, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4834, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6713, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6302, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4817, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5656, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8281, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7517, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6055, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4962, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7909, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7115, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7505, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5289, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7758, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5031, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6284, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3970, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4907, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6757, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5366, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5769, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4054, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6400, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7519, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5319, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6644, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8013, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5977, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6157, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5467, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5474, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7767, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7241, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6658, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7078, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5183, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6216, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5633, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7884, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5330, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5194, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6561, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5925, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7271, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6661, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4150, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3722, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4266, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4160, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5813, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6703, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6365, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4722, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4355, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6110, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7217, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3955, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5872, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6577, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2658, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5861, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4562, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6944, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7071, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4858, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4919, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4950, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6685, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4489, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4852, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6603, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4755, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6625, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5878, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8165, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5516, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7597, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5231, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6650, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4491, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8173, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4876, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7796, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5547, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8551, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7245, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5653, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7776, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6928, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5011, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6465, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4361, device='cuda:0', requires_grad=True),\n",
       "   tensor(2.0389, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5476, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7167, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6955, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7514, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5036, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6399, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5037, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5980, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4676, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6761, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6013, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7509, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4477, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5639, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6237, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8179, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6260, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4388, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4961, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3566, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5612, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5481, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2998, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6132, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6308, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7344, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4648, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1934, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6344, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3159, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2663, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6000, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5608, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6954, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5348, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6791, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3427, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3818, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5012, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4226, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7539, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4111, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5664, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5793, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7080, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5310, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3355, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4215, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6931, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6721, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3035, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4957, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5543, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1510, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3314, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4601, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8436, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3370, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4873, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7040, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5402, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6728, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3515, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4819, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4596, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8348, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3852, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3354, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6824, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5913, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1843, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5315, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4948, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8716, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8347, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4429, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3841, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3721, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5117, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5756, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4442, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3464, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5009, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5893, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6736, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6976, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6692, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6794, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4302, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4895, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5528, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5442, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4613, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4190, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5184, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5422, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2886, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6503, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3804, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4818, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5284, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6303, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6523, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3857, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4145, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5767, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5297, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6614, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4646, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5129, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6232, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4942, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5266, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4278, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4650, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5907, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4603, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4644, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5575, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7331, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5439, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2864, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5420, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3766, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5436, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2570, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3782, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3949, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4834, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5927, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4614, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3336, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9853, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5624, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7830, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5197, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6804, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6860, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3774, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5226, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3573, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3652, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2332, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5095, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3729, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3328, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4581, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4027, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6878, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5864, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6595, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5618, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6480, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3925, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3725, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4911, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4604, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7203, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2710, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5932, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2889, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3396, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4226, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3473, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5462, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4379, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3087, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5127, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3154, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4444, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5378, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5786, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3097, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2872, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6831, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5702, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6697, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4747, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4357, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3713, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3368, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5858, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3033, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3305, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3881, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2667, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4675, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5968, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1700, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3659, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6960, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3891, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5802, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3187, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5497, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3213, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4298, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2165, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3274, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5467, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5407, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3472, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3402, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3859, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4918, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3218, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4305, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4663, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5820, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2263, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3941, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7086, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3186, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2508, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4297, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3620, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4867, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3834, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2743, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2894, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6706, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0631, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3756, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2282, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5024, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5053, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3098, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4895, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6326, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4275, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2918, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0405, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7008, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5904, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3182, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3418, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5000, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5657, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2868, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3226, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3521, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7012, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4344, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3467, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.9588, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4193, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5708, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2782, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7757, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4126, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3445, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3112, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4457, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5436, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3628, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5013, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5063, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5064, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7414, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3664, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4807, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5631, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4704, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7231, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2645, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7923, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2452, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4151, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6291, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5617, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4650, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6005, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3195, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3796, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2555, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4692, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4599, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3344, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3669, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3087, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5857, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3530, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4711, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4588, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2179, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4083, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0861, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4775, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2097, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3877, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5206, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3674, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3372, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5622, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3726, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4033, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6517, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2003, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2534, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2316, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3598, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2192, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3857, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3639, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4524, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3881, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5285, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6196, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4469, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4910, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3907, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4723, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5255, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3142, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6390, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5139, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5782, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6968, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7075, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6941, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4251, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1585, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3828, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4849, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4304, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3989, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3998, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3415, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3199, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5043, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3495, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1675, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3274, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4844, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5492, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4849, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7507, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4410, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3967, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4527, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4391, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2098, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8783, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2759, device='cuda:0', requires_grad=True),\n",
       "   tensor(0.9904, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5095, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5514, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3104, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7035, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4252, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4656, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3749, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4294, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4878, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3906, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2869, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1414, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3315, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2531, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2179, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3834, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3855, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1922, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3550, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3998, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6420, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3332, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.7173, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5341, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5923, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4345, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6620, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4551, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4998, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3254, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0213, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2246, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3556, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5993, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6324, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4060, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6818, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4164, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3899, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3624, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3986, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3961, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3627, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2808, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4563, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3137, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4129, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5000, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3769, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2386, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3148, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3220, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3572, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6308, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3641, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3018, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3782, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4655, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4580, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4920, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3623, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0339, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3478, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3922, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4134, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4207, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3607, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2552, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4410, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0315, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3786, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5428, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6801, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3822, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4261, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3616, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5328, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3149, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2107, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3524, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2978, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4531, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3877, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3533, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2980, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3989, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2165, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4974, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2490, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2312, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0971, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1162, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2115, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2777, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1964, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2307, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0591, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3927, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3197, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1789, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4143, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4023, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3855, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4696, device='cuda:0', requires_grad=True),\n",
       "   tensor(0.9703, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1578, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1531, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0615, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2523, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3829, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2123, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3785, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2392, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4526, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4290, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0242, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4332, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2006, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5981, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0969, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2640, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3921, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4660, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1498, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5037, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3016, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3648, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5284, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3012, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2589, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1487, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4787, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3812, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3616, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3885, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1812, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2040, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3202, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6274, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2905, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2463, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4868, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2966, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2298, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5245, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3251, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0347, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3256, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1568, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4558, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3316, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3394, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4064, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3930, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3438, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2377, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2006, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2270, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2982, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3575, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5029, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6622, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1343, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2797, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0916, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1463, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2148, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3371, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2813, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2890, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2023, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4857, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5882, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6763, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4100, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2792, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1408, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1925, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2720, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1877, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3302, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3492, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3091, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2003, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3066, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3797, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0681, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4377, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3409, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2417, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3210, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3313, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4810, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3268, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3904, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2372, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3289, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3546, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3749, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3500, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4992, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4497, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1332, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1878, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1437, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1152, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5145, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1220, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5709, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2509, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5909, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3482, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3256, device='cuda:0', requires_grad=True),\n",
       "   tensor(0.9449, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3246, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1527, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3831, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2468, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0565, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.8150, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3243, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3778, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1392, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2744, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2500, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3576, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4587, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3489, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3344, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2424, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2379, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3830, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5054, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5920, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2282, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1651, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3022, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1426, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4244, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1606, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1909, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1743, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3461, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.6338, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4598, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4091, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3034, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5914, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3996, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2224, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3431, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3383, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2176, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.5576, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1844, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2697, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2711, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.0021, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2995, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3088, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2967, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4464, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1625, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4143, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2139, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4294, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2527, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2938, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.1330, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4342, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4109, device='cuda:0', requires_grad=True),\n",
       "   tensor(0.9198, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.3835, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2798, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4165, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2654, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.2303, device='cuda:0', requires_grad=True),\n",
       "   tensor(1.4185, device='cuda:0', requires_grad=True),\n",
       "   ...],\n",
       "  'train_acc': [0.296875,\n",
       "   0.3359375,\n",
       "   0.3359375,\n",
       "   0.3671875,\n",
       "   0.3203125,\n",
       "   0.4453125,\n",
       "   0.3984375,\n",
       "   0.359375,\n",
       "   0.4453125,\n",
       "   0.421875,\n",
       "   0.4375,\n",
       "   0.4375,\n",
       "   0.3984375,\n",
       "   0.421875,\n",
       "   0.5078125,\n",
       "   0.484375,\n",
       "   0.5390625,\n",
       "   0.515625,\n",
       "   0.5,\n",
       "   0.4921875,\n",
       "   0.4765625,\n",
       "   0.515625,\n",
       "   0.6171875,\n",
       "   0.5625,\n",
       "   0.5078125,\n",
       "   0.5390625,\n",
       "   0.5078125,\n",
       "   0.5390625,\n",
       "   0.578125,\n",
       "   0.53125,\n",
       "   0.515625,\n",
       "   0.515625,\n",
       "   0.578125,\n",
       "   0.5390625,\n",
       "   0.6640625,\n",
       "   0.6015625,\n",
       "   0.5625,\n",
       "   0.59375,\n",
       "   0.609375,\n",
       "   0.6015625,\n",
       "   0.5546875,\n",
       "   0.703125,\n",
       "   0.65625,\n",
       "   0.6171875,\n",
       "   0.515625,\n",
       "   0.6328125,\n",
       "   0.6171875,\n",
       "   0.640625,\n",
       "   0.625,\n",
       "   0.578125,\n",
       "   0.609375,\n",
       "   0.6171875,\n",
       "   0.7265625,\n",
       "   0.6484375,\n",
       "   0.6171875,\n",
       "   0.65625,\n",
       "   0.6875,\n",
       "   0.6640625,\n",
       "   0.6015625,\n",
       "   0.6015625,\n",
       "   0.609375,\n",
       "   0.6484375,\n",
       "   0.609375,\n",
       "   0.625,\n",
       "   0.6875,\n",
       "   0.59375,\n",
       "   0.609375,\n",
       "   0.515625,\n",
       "   0.609375,\n",
       "   0.609375,\n",
       "   0.5859375,\n",
       "   0.6171875,\n",
       "   0.6796875,\n",
       "   0.7109375,\n",
       "   0.5859375,\n",
       "   0.6484375,\n",
       "   0.6015625,\n",
       "   0.5859375,\n",
       "   0.6640625,\n",
       "   0.5078125,\n",
       "   0.7109375,\n",
       "   0.671875,\n",
       "   0.65625,\n",
       "   0.6171875,\n",
       "   0.703125,\n",
       "   0.703125,\n",
       "   0.6796875,\n",
       "   0.6875,\n",
       "   0.65625,\n",
       "   0.6796875,\n",
       "   0.609375,\n",
       "   0.6171875,\n",
       "   0.671875,\n",
       "   0.6953125,\n",
       "   0.65625,\n",
       "   0.5625,\n",
       "   0.6953125,\n",
       "   0.6796875,\n",
       "   0.703125,\n",
       "   0.6484375,\n",
       "   0.6484375,\n",
       "   0.578125,\n",
       "   0.6015625,\n",
       "   0.640625,\n",
       "   0.75,\n",
       "   0.5390625,\n",
       "   0.640625,\n",
       "   0.5859375,\n",
       "   0.6640625,\n",
       "   0.6484375,\n",
       "   0.671875,\n",
       "   0.59375,\n",
       "   0.65625,\n",
       "   0.6796875,\n",
       "   0.6484375,\n",
       "   0.71875,\n",
       "   0.6640625,\n",
       "   0.6328125,\n",
       "   0.671875,\n",
       "   0.6015625,\n",
       "   0.671875,\n",
       "   0.6171875,\n",
       "   0.65625,\n",
       "   0.6953125,\n",
       "   0.6640625,\n",
       "   0.6484375,\n",
       "   0.609375,\n",
       "   0.6328125,\n",
       "   0.6640625,\n",
       "   0.6484375,\n",
       "   0.6640625,\n",
       "   0.7734375,\n",
       "   0.6328125,\n",
       "   0.6328125,\n",
       "   0.6953125,\n",
       "   0.703125,\n",
       "   0.5859375,\n",
       "   0.6171875,\n",
       "   0.7578125,\n",
       "   0.7109375,\n",
       "   0.65625,\n",
       "   0.71875,\n",
       "   0.625,\n",
       "   0.7109375,\n",
       "   0.6484375,\n",
       "   0.71875,\n",
       "   0.65625,\n",
       "   0.671875,\n",
       "   0.671875,\n",
       "   0.640625,\n",
       "   0.6484375,\n",
       "   0.6796875,\n",
       "   0.609375,\n",
       "   0.6796875,\n",
       "   0.6015625,\n",
       "   0.6640625,\n",
       "   0.7265625,\n",
       "   0.640625,\n",
       "   0.6875,\n",
       "   0.65625,\n",
       "   0.6484375,\n",
       "   0.703125,\n",
       "   0.734375,\n",
       "   0.6796875,\n",
       "   0.6484375,\n",
       "   0.7265625,\n",
       "   0.671875,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.625,\n",
       "   0.6953125,\n",
       "   0.703125,\n",
       "   0.625,\n",
       "   0.765625,\n",
       "   0.703125,\n",
       "   0.6484375,\n",
       "   0.65625,\n",
       "   0.703125,\n",
       "   0.734375,\n",
       "   0.78125,\n",
       "   0.640625,\n",
       "   0.7109375,\n",
       "   0.703125,\n",
       "   0.7421875,\n",
       "   0.6875,\n",
       "   0.734375,\n",
       "   0.703125,\n",
       "   0.7265625,\n",
       "   0.7109375,\n",
       "   0.6796875,\n",
       "   0.6953125,\n",
       "   0.703125,\n",
       "   0.7265625,\n",
       "   0.6875,\n",
       "   0.71875,\n",
       "   0.703125,\n",
       "   0.7421875,\n",
       "   0.6953125,\n",
       "   0.7734375,\n",
       "   0.734375,\n",
       "   0.6484375,\n",
       "   0.7578125,\n",
       "   0.7421875,\n",
       "   0.6953125,\n",
       "   0.640625,\n",
       "   0.75,\n",
       "   0.671875,\n",
       "   0.65625,\n",
       "   0.6875,\n",
       "   0.6875,\n",
       "   0.7109375,\n",
       "   0.6484375,\n",
       "   0.7265625,\n",
       "   0.6171875,\n",
       "   0.609375,\n",
       "   0.5859375,\n",
       "   0.71875,\n",
       "   0.7109375,\n",
       "   0.71875,\n",
       "   0.6796875,\n",
       "   0.7109375,\n",
       "   0.625,\n",
       "   0.703125,\n",
       "   0.6953125,\n",
       "   0.7265625,\n",
       "   0.7890625,\n",
       "   0.6640625,\n",
       "   0.734375,\n",
       "   0.65625,\n",
       "   0.7421875,\n",
       "   0.671875,\n",
       "   0.6875,\n",
       "   0.703125,\n",
       "   0.7578125,\n",
       "   0.765625,\n",
       "   0.6875,\n",
       "   0.703125,\n",
       "   0.71875,\n",
       "   0.6171875,\n",
       "   0.7265625,\n",
       "   0.625,\n",
       "   0.7421875,\n",
       "   0.6640625,\n",
       "   0.734375,\n",
       "   0.6953125,\n",
       "   0.734375,\n",
       "   0.6484375,\n",
       "   0.7421875,\n",
       "   0.71875,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.6953125,\n",
       "   0.7109375,\n",
       "   0.6796875,\n",
       "   0.7109375,\n",
       "   0.6640625,\n",
       "   0.75,\n",
       "   0.6796875,\n",
       "   0.671875,\n",
       "   0.6484375,\n",
       "   0.7265625,\n",
       "   0.75,\n",
       "   0.7421875,\n",
       "   0.6484375,\n",
       "   0.6875,\n",
       "   0.734375,\n",
       "   0.6640625,\n",
       "   0.7109375,\n",
       "   0.671875,\n",
       "   0.703125,\n",
       "   0.6484375,\n",
       "   0.7578125,\n",
       "   0.7109375,\n",
       "   0.734375,\n",
       "   0.7109375,\n",
       "   0.734375,\n",
       "   0.734375,\n",
       "   0.6875,\n",
       "   0.703125,\n",
       "   0.7109375,\n",
       "   0.7109375,\n",
       "   0.71875,\n",
       "   0.671875,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.734375,\n",
       "   0.7421875,\n",
       "   0.734375,\n",
       "   0.7421875,\n",
       "   0.65625,\n",
       "   0.71875,\n",
       "   0.75,\n",
       "   0.6953125,\n",
       "   0.6953125,\n",
       "   0.7421875,\n",
       "   0.7421875,\n",
       "   0.703125,\n",
       "   0.765625,\n",
       "   0.7109375,\n",
       "   0.7734375,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.75,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.6875,\n",
       "   0.6484375,\n",
       "   0.75,\n",
       "   0.703125,\n",
       "   0.7109375,\n",
       "   0.6875,\n",
       "   0.7265625,\n",
       "   0.703125,\n",
       "   0.75,\n",
       "   0.71875,\n",
       "   0.6484375,\n",
       "   0.7109375,\n",
       "   0.6875,\n",
       "   0.75,\n",
       "   0.71875,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.7109375,\n",
       "   0.7578125,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.7109375,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.796875,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   0.765625,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.734375,\n",
       "   0.75,\n",
       "   0.75,\n",
       "   0.6171875,\n",
       "   0.796875,\n",
       "   0.7578125,\n",
       "   0.7265625,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.7265625,\n",
       "   0.7890625,\n",
       "   0.75,\n",
       "   0.7421875,\n",
       "   0.7421875,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.703125,\n",
       "   0.6953125,\n",
       "   0.7578125,\n",
       "   0.671875,\n",
       "   0.7421875,\n",
       "   0.7109375,\n",
       "   0.7734375,\n",
       "   0.6875,\n",
       "   0.765625,\n",
       "   0.703125,\n",
       "   0.75,\n",
       "   0.671875,\n",
       "   0.6796875,\n",
       "   0.6953125,\n",
       "   0.6875,\n",
       "   0.7578125,\n",
       "   0.765625,\n",
       "   0.7265625,\n",
       "   0.734375,\n",
       "   0.6015625,\n",
       "   0.765625,\n",
       "   0.71875,\n",
       "   0.703125,\n",
       "   0.6953125,\n",
       "   0.7265625,\n",
       "   0.703125,\n",
       "   0.8125,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.75,\n",
       "   0.734375,\n",
       "   0.7109375,\n",
       "   0.734375,\n",
       "   0.75,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.7109375,\n",
       "   0.7265625,\n",
       "   0.796875,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.7578125,\n",
       "   0.78125,\n",
       "   0.7109375,\n",
       "   0.671875,\n",
       "   0.6953125,\n",
       "   0.75,\n",
       "   0.7734375,\n",
       "   0.7265625,\n",
       "   0.796875,\n",
       "   0.78125,\n",
       "   0.7109375,\n",
       "   0.7578125,\n",
       "   0.6953125,\n",
       "   0.7578125,\n",
       "   0.734375,\n",
       "   0.796875,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   0.734375,\n",
       "   0.7578125,\n",
       "   0.7890625,\n",
       "   0.734375,\n",
       "   0.7109375,\n",
       "   0.703125,\n",
       "   0.7421875,\n",
       "   0.8046875,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.6796875,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.8203125,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.6875,\n",
       "   0.8046875,\n",
       "   0.7265625,\n",
       "   0.671875,\n",
       "   0.7421875,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.75,\n",
       "   0.671875,\n",
       "   0.78125,\n",
       "   0.7890625,\n",
       "   0.703125,\n",
       "   0.6796875,\n",
       "   0.7734375,\n",
       "   0.7265625,\n",
       "   0.75,\n",
       "   0.65625,\n",
       "   0.703125,\n",
       "   0.78125,\n",
       "   0.71875,\n",
       "   0.7421875,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.7421875,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.71875,\n",
       "   0.734375,\n",
       "   0.7109375,\n",
       "   0.71875,\n",
       "   0.703125,\n",
       "   0.75,\n",
       "   0.7578125,\n",
       "   0.734375,\n",
       "   0.796875,\n",
       "   0.75,\n",
       "   0.765625,\n",
       "   0.734375,\n",
       "   0.734375,\n",
       "   0.765625,\n",
       "   0.671875,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.7421875,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.7578125,\n",
       "   0.7265625,\n",
       "   0.7578125,\n",
       "   0.703125,\n",
       "   0.703125,\n",
       "   0.71875,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.75,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.6796875,\n",
       "   0.7421875,\n",
       "   0.84375,\n",
       "   0.7265625,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.8203125,\n",
       "   0.75,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.7265625,\n",
       "   0.640625,\n",
       "   0.71875,\n",
       "   0.71875,\n",
       "   0.75,\n",
       "   0.734375,\n",
       "   0.7109375,\n",
       "   0.75,\n",
       "   0.734375,\n",
       "   0.796875,\n",
       "   0.8046875,\n",
       "   0.78125,\n",
       "   0.703125,\n",
       "   0.765625,\n",
       "   0.7890625,\n",
       "   0.7109375,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.6796875,\n",
       "   0.7265625,\n",
       "   0.78125,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.8125,\n",
       "   0.78125,\n",
       "   0.7421875,\n",
       "   0.703125,\n",
       "   0.796875,\n",
       "   0.71875,\n",
       "   0.7890625,\n",
       "   0.7734375,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.7578125,\n",
       "   0.7890625,\n",
       "   0.6953125,\n",
       "   0.7890625,\n",
       "   0.7578125,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.8046875,\n",
       "   0.765625,\n",
       "   0.671875,\n",
       "   0.7578125,\n",
       "   0.6875,\n",
       "   0.7421875,\n",
       "   0.8125,\n",
       "   0.765625,\n",
       "   0.7890625,\n",
       "   0.6953125,\n",
       "   0.7890625,\n",
       "   0.7890625,\n",
       "   0.7421875,\n",
       "   0.78125,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.8125,\n",
       "   0.7578125,\n",
       "   0.6953125,\n",
       "   0.7734375,\n",
       "   0.71875,\n",
       "   0.8125,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.765625,\n",
       "   0.8046875,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.75,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.75,\n",
       "   0.75,\n",
       "   0.7890625,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.71875,\n",
       "   0.7890625,\n",
       "   0.7578125,\n",
       "   0.703125,\n",
       "   0.75,\n",
       "   0.765625,\n",
       "   0.78125,\n",
       "   0.78125,\n",
       "   0.71875,\n",
       "   0.7578125,\n",
       "   0.8203125,\n",
       "   0.75,\n",
       "   0.71875,\n",
       "   0.859375,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.71875,\n",
       "   0.8125,\n",
       "   0.75,\n",
       "   0.7578125,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.828125,\n",
       "   0.7109375,\n",
       "   0.7109375,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.7890625,\n",
       "   0.796875,\n",
       "   0.7578125,\n",
       "   0.703125,\n",
       "   0.765625,\n",
       "   0.78125,\n",
       "   0.6640625,\n",
       "   0.7421875,\n",
       "   0.703125,\n",
       "   0.796875,\n",
       "   0.6953125,\n",
       "   0.7421875,\n",
       "   0.78125,\n",
       "   0.765625,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.7578125,\n",
       "   0.7265625,\n",
       "   0.71875,\n",
       "   0.71875,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.7265625,\n",
       "   0.7578125,\n",
       "   0.703125,\n",
       "   0.8046875,\n",
       "   0.7109375,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.734375,\n",
       "   0.71875,\n",
       "   0.7109375,\n",
       "   0.7890625,\n",
       "   0.765625,\n",
       "   0.7734375,\n",
       "   0.734375,\n",
       "   0.765625,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.8046875,\n",
       "   0.7265625,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.7890625,\n",
       "   0.75,\n",
       "   0.84375,\n",
       "   0.7734375,\n",
       "   0.8203125,\n",
       "   0.7578125,\n",
       "   0.75,\n",
       "   0.7890625,\n",
       "   0.796875,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.7734375,\n",
       "   0.8046875,\n",
       "   0.828125,\n",
       "   0.7578125,\n",
       "   0.796875,\n",
       "   0.7890625,\n",
       "   0.7890625,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.71875,\n",
       "   0.703125,\n",
       "   0.7421875,\n",
       "   0.7734375,\n",
       "   0.75,\n",
       "   0.7890625,\n",
       "   0.765625,\n",
       "   0.7265625,\n",
       "   0.7265625,\n",
       "   0.734375,\n",
       "   0.65625,\n",
       "   0.703125,\n",
       "   0.7109375,\n",
       "   0.7734375,\n",
       "   0.8046875,\n",
       "   0.78125,\n",
       "   0.75,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.7578125,\n",
       "   0.8203125,\n",
       "   0.78125,\n",
       "   0.6953125,\n",
       "   0.78125,\n",
       "   0.828125,\n",
       "   0.734375,\n",
       "   0.703125,\n",
       "   0.7265625,\n",
       "   0.75,\n",
       "   0.6875,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.7421875,\n",
       "   0.734375,\n",
       "   0.84375,\n",
       "   0.6328125,\n",
       "   0.7890625,\n",
       "   0.828125,\n",
       "   0.7109375,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.734375,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.703125,\n",
       "   0.7734375,\n",
       "   0.8203125,\n",
       "   0.8203125,\n",
       "   0.765625,\n",
       "   0.7734375,\n",
       "   0.8046875,\n",
       "   0.7265625,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.7109375,\n",
       "   0.7109375,\n",
       "   0.765625,\n",
       "   0.7265625,\n",
       "   0.7578125,\n",
       "   0.7421875,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.734375,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   0.8203125,\n",
       "   0.8125,\n",
       "   0.75,\n",
       "   0.7890625,\n",
       "   0.6875,\n",
       "   0.7734375,\n",
       "   0.6953125,\n",
       "   0.7109375,\n",
       "   0.7890625,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.8125,\n",
       "   0.7109375,\n",
       "   0.75,\n",
       "   0.7734375,\n",
       "   0.75,\n",
       "   0.8125,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.8125,\n",
       "   0.7421875,\n",
       "   0.7265625,\n",
       "   0.796875,\n",
       "   0.75,\n",
       "   0.765625,\n",
       "   0.7109375,\n",
       "   0.7,\n",
       "   0.796875,\n",
       "   0.828125,\n",
       "   0.7734375,\n",
       "   0.765625,\n",
       "   0.7109375,\n",
       "   0.75,\n",
       "   0.7734375,\n",
       "   0.796875,\n",
       "   0.7578125,\n",
       "   0.8125,\n",
       "   0.75,\n",
       "   0.703125,\n",
       "   0.6875,\n",
       "   0.7734375,\n",
       "   0.7890625,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   0.7890625,\n",
       "   0.796875,\n",
       "   0.7734375,\n",
       "   0.7734375,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.765625,\n",
       "   0.7578125,\n",
       "   0.765625,\n",
       "   0.8359375,\n",
       "   0.7734375,\n",
       "   0.7578125,\n",
       "   0.7890625,\n",
       "   0.8203125,\n",
       "   0.7890625,\n",
       "   0.78125,\n",
       "   0.765625,\n",
       "   0.796875,\n",
       "   0.8046875,\n",
       "   0.8359375,\n",
       "   0.75,\n",
       "   0.7578125,\n",
       "   0.8359375,\n",
       "   0.734375,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.875,\n",
       "   0.7890625,\n",
       "   0.8203125,\n",
       "   0.8359375,\n",
       "   0.765625,\n",
       "   0.78125,\n",
       "   0.796875,\n",
       "   0.75,\n",
       "   0.8125,\n",
       "   0.7421875,\n",
       "   0.75,\n",
       "   0.8203125,\n",
       "   0.71875,\n",
       "   0.8046875,\n",
       "   0.7109375,\n",
       "   0.8359375,\n",
       "   0.8203125,\n",
       "   0.7890625,\n",
       "   0.7578125,\n",
       "   0.828125,\n",
       "   0.734375,\n",
       "   0.7890625,\n",
       "   0.75,\n",
       "   0.7421875,\n",
       "   0.7578125,\n",
       "   0.8125,\n",
       "   0.8125,\n",
       "   0.71875,\n",
       "   0.7734375,\n",
       "   0.765625,\n",
       "   0.7578125,\n",
       "   0.7890625,\n",
       "   0.8046875,\n",
       "   0.765625,\n",
       "   0.71875,\n",
       "   0.7578125,\n",
       "   0.7890625,\n",
       "   0.734375,\n",
       "   0.765625,\n",
       "   0.7890625,\n",
       "   0.7265625,\n",
       "   0.7578125,\n",
       "   0.8515625,\n",
       "   0.7734375,\n",
       "   0.8203125,\n",
       "   0.7421875,\n",
       "   0.78125,\n",
       "   0.765625,\n",
       "   0.8046875,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.7890625,\n",
       "   0.8203125,\n",
       "   0.84375,\n",
       "   0.78125,\n",
       "   0.734375,\n",
       "   0.703125,\n",
       "   0.7109375,\n",
       "   0.8359375,\n",
       "   0.8203125,\n",
       "   0.828125,\n",
       "   0.8359375,\n",
       "   0.7890625,\n",
       "   0.71875,\n",
       "   0.765625,\n",
       "   0.734375,\n",
       "   0.8046875,\n",
       "   0.765625,\n",
       "   0.71875,\n",
       "   0.71875,\n",
       "   0.734375,\n",
       "   0.7890625,\n",
       "   0.8359375,\n",
       "   0.7421875,\n",
       "   0.765625,\n",
       "   0.7890625,\n",
       "   0.7421875,\n",
       "   0.78125,\n",
       "   0.765625,\n",
       "   0.8125,\n",
       "   0.78125,\n",
       "   0.7265625,\n",
       "   0.8359375,\n",
       "   0.78125,\n",
       "   0.78125,\n",
       "   0.78125,\n",
       "   0.78125,\n",
       "   0.78125,\n",
       "   0.7421875,\n",
       "   0.7890625,\n",
       "   0.7734375,\n",
       "   0.8046875,\n",
       "   0.7578125,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.734375,\n",
       "   0.7265625,\n",
       "   0.7421875,\n",
       "   0.8203125,\n",
       "   0.78125,\n",
       "   0.8125,\n",
       "   0.796875,\n",
       "   0.703125,\n",
       "   0.828125,\n",
       "   0.71875,\n",
       "   0.8046875,\n",
       "   0.7578125,\n",
       "   0.765625,\n",
       "   0.7734375,\n",
       "   0.890625,\n",
       "   0.8125,\n",
       "   0.8046875,\n",
       "   0.765625,\n",
       "   0.828125,\n",
       "   0.8125,\n",
       "   0.6953125,\n",
       "   0.796875,\n",
       "   0.7421875,\n",
       "   0.7734375,\n",
       "   0.8125,\n",
       "   0.78125,\n",
       "   0.7734375,\n",
       "   0.78125,\n",
       "   0.7265625,\n",
       "   0.78125,\n",
       "   0.8046875,\n",
       "   0.8125,\n",
       "   0.75,\n",
       "   0.7421875,\n",
       "   0.7265625,\n",
       "   0.8203125,\n",
       "   0.796875,\n",
       "   0.8046875,\n",
       "   0.8046875,\n",
       "   0.7578125,\n",
       "   0.828125,\n",
       "   0.796875,\n",
       "   0.8046875,\n",
       "   0.78125,\n",
       "   0.71875,\n",
       "   0.734375,\n",
       "   0.734375,\n",
       "   0.7578125,\n",
       "   0.671875,\n",
       "   0.734375,\n",
       "   0.7578125,\n",
       "   0.7578125,\n",
       "   0.7578125,\n",
       "   0.7734375,\n",
       "   0.7265625,\n",
       "   0.859375,\n",
       "   0.78125,\n",
       "   0.7578125,\n",
       "   0.859375,\n",
       "   0.7890625,\n",
       "   0.7734375,\n",
       "   0.7890625,\n",
       "   0.7578125,\n",
       "   0.796875,\n",
       "   0.7421875,\n",
       "   0.7890625,\n",
       "   0.765625,\n",
       "   0.796875,\n",
       "   0.78125,\n",
       "   0.8203125,\n",
       "   0.765625,\n",
       "   0.7421875,\n",
       "   0.875,\n",
       "   0.75,\n",
       "   0.78125,\n",
       "   0.7421875,\n",
       "   0.796875,\n",
       "   0.765625,\n",
       "   0.75,\n",
       "   ...],\n",
       "  'val_acc': []},\n",
       " 'model_state_dict': OrderedDict([('model.model.conv1.weight',\n",
       "               tensor([[[[ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.2764,  0.0702,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0588, -0.0241,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.3090,  0.2275,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.5886, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.1747,  0.2368,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0108,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.2367,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [-0.0000, -0.3913,  0.0000,  ...,  0.2352,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "                         ...,\n",
       "                         [ 0.0000,  0.0000,  0.0000,  ...,  0.1974,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]]],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "                         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.bn1.weight',\n",
       "               tensor([ 3.6042e-01,  1.2144e-01, -4.3121e-09,  1.6334e-01,  2.9034e-10,\n",
       "                        2.1405e-01,  3.6987e-01,  1.1101e-08,  2.8522e-01,  1.2788e-07,\n",
       "                        2.5911e-01,  2.4714e-01,  1.1583e-01,  9.1676e-07,  1.1605e-01,\n",
       "                        2.6701e-01,  7.2994e-02,  9.5088e-02,  1.2639e-01,  1.0667e-01,\n",
       "                        3.1754e-01,  8.5837e-02,  1.1535e-01,  2.4172e-01,  3.2674e-01,\n",
       "                        1.0747e-01,  9.8086e-02,  1.1586e-01,  2.4438e-01,  3.2172e-01,\n",
       "                        2.8960e-01,  2.5787e-01,  2.3530e-01,  2.0609e-01,  1.3119e-01,\n",
       "                        1.6734e-01,  6.3136e-09,  9.3512e-02,  1.2440e-09,  2.4373e-01,\n",
       "                        2.2224e-01,  2.5703e-01,  2.5672e-01,  8.9811e-02,  2.7764e-01,\n",
       "                        8.4238e-02,  6.1240e-02,  2.6215e-01,  1.8569e-09,  2.0916e-01,\n",
       "                        1.9601e-01,  3.0445e-01,  2.8186e-01,  2.3626e-01,  1.2840e-01,\n",
       "                        2.1812e-01,  1.0983e-01,  9.1482e-02,  1.4968e-01,  3.4941e-01,\n",
       "                        3.3475e-01,  9.4653e-02,  9.6862e-02,  2.2746e-01], device='cuda:0')),\n",
       "              ('model.model.bn1.bias',\n",
       "               tensor([ 6.6619e-02, -1.1878e-02, -8.8983e-08, -1.3127e-01, -1.3986e-09,\n",
       "                       -8.1574e-03,  2.4630e-01, -3.6309e-08,  2.1145e-02, -6.7562e-07,\n",
       "                        1.6669e-01,  1.2615e-01, -7.0081e-03, -2.9328e-06, -9.5915e-03,\n",
       "                        1.2146e-01,  1.1263e-01, -9.4998e-02, -1.1208e-01,  1.5186e-01,\n",
       "                        2.9829e-01,  2.1851e-01,  7.1012e-02,  8.9107e-02,  3.1274e-02,\n",
       "                        2.2251e-01,  1.5604e-01,  2.0507e-01,  3.5834e-01, -2.6899e-02,\n",
       "                        1.3802e-01,  7.1168e-02,  3.9636e-01,  3.2046e-01, -6.6296e-02,\n",
       "                       -7.2681e-02, -2.0684e-08,  1.7629e-01, -4.1484e-09,  8.3737e-02,\n",
       "                        1.4447e-01,  3.1450e-02,  1.4907e-01,  2.1243e-01,  2.1901e-02,\n",
       "                        2.1738e-01,  2.2661e-01,  1.3601e-01, -7.3347e-09,  5.4395e-02,\n",
       "                        1.7721e-02,  2.2682e-01,  2.5975e-01,  5.5039e-02, -8.2677e-02,\n",
       "                        5.0873e-02,  1.4770e-01,  1.5989e-01, -1.2550e-01,  3.2403e-02,\n",
       "                        3.3167e-01,  3.8798e-01, -8.3634e-02,  1.0123e-01], device='cuda:0')),\n",
       "              ('model.model.bn1.running_mean',\n",
       "               tensor([ 1.6306e-01,  5.6052e-45, -5.6052e-45,  5.6052e-45, -5.6052e-45,\n",
       "                        4.0662e-03,  2.4324e-02,  5.6052e-45,  2.0205e-01, -5.6052e-45,\n",
       "                       -1.2225e-01, -1.3142e-01,  5.6052e-45,  5.6052e-45, -5.6052e-45,\n",
       "                        1.3707e-02, -5.6052e-45, -5.6052e-45, -5.6052e-45,  5.6052e-45,\n",
       "                        2.2457e-02, -5.6052e-45,  5.6052e-45, -1.6560e-02, -1.9239e-02,\n",
       "                       -5.6052e-45, -5.6052e-45, -5.6052e-45, -1.6380e-01,  6.7093e-02,\n",
       "                       -4.5886e-02,  5.8973e-02, -1.6392e-01, -2.9948e-01, -5.6052e-45,\n",
       "                        5.6052e-45,  5.6052e-45, -2.6325e-02,  5.6052e-45, -1.0006e-01,\n",
       "                        5.2794e-02,  7.0604e-02, -3.7714e-02, -5.6052e-45, -5.1948e-02,\n",
       "                        5.6052e-45, -5.6052e-45, -1.3370e-01, -5.6052e-45,  1.0139e-01,\n",
       "                       -7.0460e-02,  2.3388e-01, -9.3883e-03,  2.3112e-02,  5.6052e-45,\n",
       "                        4.8476e-02, -5.6052e-45, -5.6052e-45,  5.6052e-45,  2.3596e-02,\n",
       "                        2.3779e-01, -5.6052e-45,  5.6052e-45,  4.7140e-02], device='cuda:0')),\n",
       "              ('model.model.bn1.running_var',\n",
       "               tensor([2.1543e+00, 5.6052e-45, 5.6052e-45, 5.6052e-45, 5.6052e-45, 3.4850e-01,\n",
       "                       1.2574e+00, 5.6052e-45, 2.0354e+00, 5.6052e-45, 2.7947e-01, 3.1661e-01,\n",
       "                       5.6052e-45, 5.6052e-45, 5.6052e-45, 7.2912e-01, 5.6052e-45, 5.6052e-45,\n",
       "                       5.6052e-45, 5.6052e-45, 8.4086e-03, 5.6052e-45, 5.6052e-45, 4.4747e-01,\n",
       "                       1.1605e+00, 5.6052e-45, 5.6052e-45, 5.6052e-45, 7.1740e-01, 1.0393e-01,\n",
       "                       3.0489e-01, 5.4608e-01, 5.0536e-01, 1.5165e+00, 5.6052e-45, 5.6052e-45,\n",
       "                       5.6052e-45, 1.4552e-02, 5.6052e-45, 7.7467e-01, 4.8296e-01, 1.1172e+00,\n",
       "                       4.1249e-01, 5.6052e-45, 5.8535e-01, 5.6052e-45, 5.6052e-45, 9.1615e-01,\n",
       "                       5.6052e-45, 6.6192e-01, 5.9009e-01, 1.2814e+00, 9.1928e-01, 5.8672e-01,\n",
       "                       5.6052e-45, 5.5761e-01, 5.6052e-45, 5.6052e-45, 5.6052e-45, 2.0153e+00,\n",
       "                       9.1123e-01, 5.6052e-45, 5.6052e-45, 5.1679e-01], device='cuda:0')),\n",
       "              ('model.model.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer1.0.conv1.weight',\n",
       "               tensor([[[[ 0.0393,  0.0991,  0.0000],\n",
       "                         [-0.0340, -0.4480, -0.0204],\n",
       "                         [ 0.0000,  0.0068, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0536,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0468, -0.0366],\n",
       "                         [ 0.0000,  0.0342,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0406,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0880, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0804]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.0.conv1.weight_mask',\n",
       "               tensor([[[[1., 1., 0.],\n",
       "                         [1., 1., 1.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn1.weight',\n",
       "               tensor([0.2428, 0.1925, 0.1615, 0.4059, 0.3676, 0.1492, 0.1914, 0.1398, 0.3689,\n",
       "                       0.1814, 0.1747, 0.2673, 0.2228, 0.1141, 0.2099, 0.2376, 0.2250, 0.1574,\n",
       "                       0.2527, 0.1936, 0.1960, 0.3132, 0.3301, 0.3415, 0.2833, 0.3238, 0.2307,\n",
       "                       0.2573, 0.2052, 0.2543, 0.0970, 0.2154, 0.2481, 0.3095, 0.3635, 0.2454,\n",
       "                       0.2125, 0.3022, 0.1648, 0.3518, 0.1737, 0.1953, 0.2056, 0.1622, 0.0508,\n",
       "                       0.2203, 0.3121, 0.1781, 0.3398, 0.2293, 0.0311, 0.2245, 0.1426, 0.3493,\n",
       "                       0.2030, 0.5340, 0.2838, 0.2515, 0.1732, 0.2966, 0.1494, 0.2452, 0.1770,\n",
       "                       0.2517], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn1.bias',\n",
       "               tensor([ 0.2801,  0.3163,  0.1762, -0.1470, -0.1696,  0.2646,  0.1948,  0.0889,\n",
       "                       -0.1555,  0.1471,  0.0552, -0.0050,  0.1196,  0.2078,  0.0217,  0.2572,\n",
       "                        0.1682,  0.1459,  0.0831,  0.1487,  0.1731, -0.1029, -0.0576,  0.0187,\n",
       "                       -0.1417,  0.1563,  0.0522,  0.0726,  0.1642, -0.0831, -0.0852,  0.0323,\n",
       "                       -0.0612, -0.0253,  0.0203, -0.1036,  0.1112,  0.0244,  0.2335, -0.0733,\n",
       "                        0.1094,  0.2779,  0.2111,  0.2188, -0.0869,  0.1640,  0.0078,  0.1677,\n",
       "                       -0.0471,  0.4122, -0.0496,  0.0850,  0.0904, -0.0356,  0.1099, -0.3545,\n",
       "                        0.1045, -0.0433,  0.3713, -0.1249,  0.3103,  0.0517,  0.0381,  0.1364],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn1.running_mean',\n",
       "               tensor([-1.8427e-01,  2.7993e-02,  1.8042e-01, -3.3531e-01, -2.6516e-01,\n",
       "                       -3.4272e-01, -2.6587e-02, -1.1761e-01, -2.7418e-01, -7.6938e-02,\n",
       "                        8.6612e-02, -2.1401e-01, -1.1110e-01, -7.0214e-02, -4.0356e-02,\n",
       "                       -2.8294e-01, -2.1189e-01, -2.9040e-01, -1.7064e-01, -1.7764e-01,\n",
       "                       -1.5935e-01, -1.1422e-01, -1.9400e-01,  2.2620e-02,  2.9952e-02,\n",
       "                       -2.7007e-01,  1.6936e-01, -3.9342e-01,  3.8300e-03,  1.1934e-01,\n",
       "                       -4.0785e-03, -3.7677e-01, -2.0256e-01, -4.2378e-01, -2.8785e-01,\n",
       "                       -2.3216e-01, -1.4887e-01,  3.5638e-03, -4.4039e-01, -1.5784e-01,\n",
       "                       -1.1156e-01, -2.5931e-01, -7.8396e-02,  4.9020e-01, -1.3776e-01,\n",
       "                       -8.4028e-05, -2.2123e-01, -4.7588e-01, -2.4741e-01, -5.1739e-01,\n",
       "                       -6.3942e-02, -2.0584e-01, -1.5937e-01, -2.5337e-01, -3.4260e-01,\n",
       "                        3.6774e-01, -9.7612e-02, -9.1795e-02, -3.0432e-01, -1.8179e-01,\n",
       "                       -1.0398e-02, -1.5156e-01,  2.7178e-01, -3.2897e-02], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn1.running_var',\n",
       "               tensor([6.6669e-02, 1.5527e-02, 3.5599e-02, 7.5184e-02, 2.7809e-02, 4.5543e-02,\n",
       "                       3.0920e-02, 3.9944e-03, 2.9930e-02, 1.7915e-02, 8.3723e-03, 1.0562e-02,\n",
       "                       6.4136e-02, 5.6311e-03, 2.5289e-03, 6.8121e-02, 2.2061e-02, 1.9159e-02,\n",
       "                       2.7154e-02, 3.5021e-02, 8.7213e-03, 9.2923e-03, 2.2742e-02, 1.5117e-02,\n",
       "                       4.7131e-03, 3.1096e-02, 4.2958e-02, 2.6930e-02, 3.9380e-02, 5.7030e-03,\n",
       "                       4.0869e-19, 1.2920e-02, 1.1241e-02, 3.6776e-02, 3.5395e-02, 1.5628e-02,\n",
       "                       1.1841e-02, 2.2292e-02, 2.7614e-02, 3.4428e-02, 2.8638e-02, 3.1516e-02,\n",
       "                       2.9829e-02, 2.0120e-02, 3.7389e-03, 6.6588e-02, 1.8987e-02, 1.8770e-02,\n",
       "                       1.4409e-02, 2.2397e-02, 3.6795e-05, 1.0303e-02, 9.9941e-03, 3.0419e-02,\n",
       "                       4.8465e-02, 3.0665e-02, 1.8011e-02, 8.0353e-03, 4.5074e-02, 5.6699e-03,\n",
       "                       2.1996e-02, 1.1669e-02, 1.8591e-02, 3.4383e-02], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer1.0.conv2.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.1960, -0.0000],\n",
       "                         [-0.0000, -0.0190, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.1252,  0.0609, -0.0179],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.1318,  0.0000, -0.0345]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0392,  0.1257]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.1837,  0.0000,  0.2367],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0057, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0955],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0267,  0.0612, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0570]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0491, -0.0000],\n",
       "                         [ 0.0000,  0.0465,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0844, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.1912, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.0.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [0., 0., 0.],\n",
       "                         [1., 0., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [1., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn2.weight',\n",
       "               tensor([0.1613, 0.1687, 0.2819, 0.3812, 0.3071, 0.1953, 0.1986, 0.2865, 0.2424,\n",
       "                       0.2358, 0.2684, 0.2075, 0.4032, 0.3340, 0.3738, 0.2088, 0.1900, 0.2357,\n",
       "                       0.2743, 0.1867, 0.3166, 0.2858, 0.3338, 0.1070, 0.0904, 0.2744, 0.3268,\n",
       "                       0.4058, 0.2059, 0.2040, 0.1411, 0.2165, 0.2129, 0.2925, 0.1945, 0.2401,\n",
       "                       0.2551, 0.1993, 0.2022, 0.2154, 0.2003, 0.0930, 0.0766, 0.2966, 0.2698,\n",
       "                       0.3410, 0.2942, 0.2271, 0.1973, 0.4195, 0.2391, 0.2426, 0.1234, 0.1643,\n",
       "                       0.2033, 0.1973, 0.2930, 0.4230, 0.1864, 0.1435, 0.2654, 0.3316, 0.3915,\n",
       "                       0.2749], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn2.bias',\n",
       "               tensor([ 0.1126,  0.1230,  0.1277,  0.0169,  0.4014, -0.0479, -0.0285,  0.1456,\n",
       "                        0.3098, -0.0308,  0.0049, -0.0184, -0.0675, -0.1863,  0.0606, -0.0782,\n",
       "                        0.0500,  0.0792,  0.1939,  0.0540, -0.4414,  0.1020,  0.0177, -0.0267,\n",
       "                       -0.0295,  0.2276,  0.1364,  0.0648, -0.0479, -0.0221,  0.0492, -0.0855,\n",
       "                       -0.0580, -0.1034,  0.0299,  0.1405,  0.0136,  0.0474,  0.2707,  0.1051,\n",
       "                        0.0556,  0.1315,  0.0622,  0.2270,  0.1496,  0.0753,  0.2358,  0.0277,\n",
       "                        0.0644,  0.0541,  0.2101,  0.1673, -0.0094,  0.1223, -0.0407,  0.1573,\n",
       "                        0.1104,  0.0014,  0.1593,  0.0207, -0.0026,  0.1909, -0.0804,  0.1281],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn2.running_mean',\n",
       "               tensor([ 5.1597e-02,  2.3124e-02,  7.6268e-02, -9.5601e-02,  2.2591e-01,\n",
       "                       -6.0509e-03,  3.2107e-02,  1.0074e-02, -1.1112e-01,  8.5944e-02,\n",
       "                       -9.6921e-02, -3.4925e-02, -3.4228e-02,  2.8338e-02,  3.3520e-02,\n",
       "                       -1.6092e-01,  6.9387e-02,  7.7412e-02,  5.4572e-02, -7.9788e-03,\n",
       "                       -1.4224e-02,  5.7491e-02,  1.0380e-01, -7.5415e-02, -4.3697e-02,\n",
       "                       -2.5654e-01, -4.8747e-02,  9.2463e-02, -2.8778e-01,  3.4467e-03,\n",
       "                       -1.7568e-01,  6.4976e-03, -1.3614e-01, -1.8372e-01,  1.4270e-02,\n",
       "                        1.7780e-02,  1.1495e-01, -1.4400e-02,  9.4488e-02, -1.5192e-01,\n",
       "                       -1.1654e-01, -8.0482e-02, -3.2774e-02,  1.3133e-02, -4.7141e-03,\n",
       "                        5.1186e-02, -2.0495e-02,  1.5708e-02,  1.3240e-01,  7.1028e-02,\n",
       "                       -7.6183e-02,  5.5921e-02, -8.0994e-02, -5.6231e-02, -3.1285e-04,\n",
       "                       -6.6144e-02, -6.8089e-02, -9.7260e-02,  1.3488e-01, -7.4045e-02,\n",
       "                       -4.1794e-02,  5.3764e-02, -2.8893e-02, -2.6632e-04], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn2.running_var',\n",
       "               tensor([0.0050, 0.0019, 0.0059, 0.0223, 0.0271, 0.0068, 0.0046, 0.0077, 0.0116,\n",
       "                       0.0049, 0.0026, 0.0117, 0.0124, 0.0021, 0.0277, 0.0057, 0.0022, 0.0075,\n",
       "                       0.0022, 0.0050, 0.0111, 0.0238, 0.0104, 0.0030, 0.0004, 0.0090, 0.0210,\n",
       "                       0.0206, 0.0140, 0.0090, 0.0038, 0.0079, 0.0068, 0.0125, 0.0020, 0.0016,\n",
       "                       0.0050, 0.0031, 0.0049, 0.0053, 0.0069, 0.0027, 0.0017, 0.0124, 0.0056,\n",
       "                       0.0150, 0.0192, 0.0085, 0.0048, 0.0148, 0.0126, 0.0192, 0.0047, 0.0039,\n",
       "                       0.0021, 0.0053, 0.0125, 0.0240, 0.0040, 0.0035, 0.0103, 0.0233, 0.0115,\n",
       "                       0.0097], device='cuda:0')),\n",
       "              ('model.model.layer1.0.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer1.1.conv1.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0771,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0031,  0.0006,  0.0000],\n",
       "                         [-0.0000,  0.1421,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0746]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0580,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0174,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0461,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0418],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0169,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0311,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.1353],\n",
       "                         [ 0.0000, -0.0000, -0.0060]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.1.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 1., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 1.]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn1.weight',\n",
       "               tensor([0.2652, 0.3032, 0.1755, 0.3231, 0.2288, 0.2225, 0.1529, 0.1914, 0.1969,\n",
       "                       0.2724, 0.0006, 0.1084, 0.3886, 0.2296, 0.1483, 0.2901, 0.1800, 0.3215,\n",
       "                       0.2130, 0.2778, 0.2567, 0.1468, 0.2531, 0.2672, 0.2423, 0.2419, 0.2050,\n",
       "                       0.2438, 0.2364, 0.1787, 0.2241, 0.2822, 0.2381, 0.2828, 0.2207, 0.2960,\n",
       "                       0.1157, 0.2189, 0.3363, 0.2145, 0.4298, 0.2315, 0.1806, 0.2296, 0.1800,\n",
       "                       0.2255, 0.2657, 0.1376, 0.3874, 0.1966, 0.1848, 0.2490, 0.2051, 0.2658,\n",
       "                       0.2626, 0.1753, 0.2896, 0.2594, 0.2249, 0.3423, 0.2555, 0.2409, 0.2558,\n",
       "                       0.1815], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn1.bias',\n",
       "               tensor([-0.1192, -0.3537,  0.1141, -0.1532, -0.0482, -0.1030,  0.1131,  0.0258,\n",
       "                        0.0110, -0.0285, -0.0418,  0.1752, -0.1941, -0.0186,  0.1740, -0.1004,\n",
       "                        0.1204, -0.0976,  0.0428,  0.0123, -0.2272,  0.1946, -0.0421, -0.0944,\n",
       "                       -0.1615,  0.0898,  0.0390,  0.0133, -0.1731,  0.1240, -0.0650, -0.0394,\n",
       "                        0.0815, -0.1547, -0.0647, -0.0865,  0.1051, -0.0888, -0.0907,  0.0384,\n",
       "                       -0.2548,  0.0005, -0.1233,  0.1331,  0.0707,  0.0557, -0.2650,  0.1204,\n",
       "                       -0.2822,  0.1023,  0.0590,  0.0743,  0.1380, -0.0326, -0.0362,  0.1173,\n",
       "                       -0.0567, -0.0219, -0.0472, -0.0915, -0.0341, -0.0202, -0.2826,  0.0794],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn1.running_mean',\n",
       "               tensor([ 0.1084,  0.4651, -0.2131, -0.0045,  0.0246, -0.2329,  0.0768, -0.0121,\n",
       "                       -0.1854, -0.1175, -0.1170, -0.0418, -0.4555,  0.0403,  0.0172,  0.1126,\n",
       "                       -0.2130, -0.0704, -0.2136, -0.1162,  0.2247,  0.0178,  0.0495, -0.1029,\n",
       "                       -0.1828, -0.2906, -0.2195, -0.1513,  0.1776,  0.2406,  0.0882, -0.2132,\n",
       "                       -0.2951, -0.0912,  0.1422,  0.0878,  0.0187,  0.1685, -0.1820, -0.1270,\n",
       "                       -0.1660,  0.1830,  0.2051, -0.0575, -0.1390,  0.1359,  0.0729, -0.0779,\n",
       "                        0.3492, -0.0428, -0.0191, -0.5203, -0.1134, -0.0035, -0.2376, -0.0727,\n",
       "                       -0.1986, -0.1362,  0.1222, -0.4320, -0.0805,  0.0378,  0.3984, -0.3358],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn1.running_var',\n",
       "               tensor([0.0374, 0.0416, 0.0154, 0.0158, 0.0384, 0.0177, 0.0086, 0.0578, 0.0200,\n",
       "                       0.0381, 0.0070, 0.0210, 0.0455, 0.0480, 0.0280, 0.0359, 0.0370, 0.0496,\n",
       "                       0.0259, 0.0127, 0.0258, 0.0160, 0.0073, 0.0532, 0.0198, 0.1007, 0.0169,\n",
       "                       0.0269, 0.0190, 0.0393, 0.0110, 0.0384, 0.0424, 0.0162, 0.0198, 0.0702,\n",
       "                       0.0281, 0.0234, 0.0476, 0.0262, 0.0680, 0.0773, 0.0099, 0.0490, 0.0202,\n",
       "                       0.0268, 0.0186, 0.0275, 0.0668, 0.0493, 0.0303, 0.0818, 0.0432, 0.0061,\n",
       "                       0.0381, 0.0698, 0.0630, 0.0327, 0.0115, 0.0786, 0.0185, 0.0387, 0.0285,\n",
       "                       0.0321], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer1.1.conv2.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0157],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0783],\n",
       "                         [ 0.0000,  0.0640,  0.1091],\n",
       "                         [ 0.0000,  0.0604,  0.0693]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0225,  0.0217]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0733,  0.0000],\n",
       "                         [ 0.0665,  0.0474,  0.0682],\n",
       "                         [ 0.0000,  0.0587,  0.0437]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0458,  0.0000,  0.0290],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0450, -0.0628,  0.0000],\n",
       "                         [-0.0320,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.1293],\n",
       "                         [ 0.0000,  0.1526,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.1.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 1.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [1., 1., 1.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 0., 1.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 1., 0.],\n",
       "                         [1., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 1., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn2.weight',\n",
       "               tensor([ 3.6097e-01,  3.6155e-01,  3.5335e-01,  3.3065e-01,  1.7514e-01,\n",
       "                        3.1150e-01,  2.4399e-01,  3.7718e-01,  5.4650e-01,  3.8532e-01,\n",
       "                       -3.8630e-03,  2.5035e-01,  3.7572e-01,  5.0109e-01,  2.8855e-01,\n",
       "                        1.8779e-01,  2.8556e-01,  3.6551e-01,  1.3088e-01,  2.8946e-01,\n",
       "                        4.0183e-01,  2.7573e-01,  2.1487e-01,  1.7527e-01,  2.3927e-01,\n",
       "                        1.1234e-23,  2.1002e-01,  9.3491e-02,  2.7286e-01,  3.2603e-01,\n",
       "                        1.7758e-01,  2.1444e-01,  1.8706e-01,  3.3473e-01,  1.9929e-01,\n",
       "                        1.0358e-01,  3.3132e-01,  3.6273e-01,  4.0870e-01,  3.1399e-01,\n",
       "                        4.8257e-01,  6.2683e-02,  3.0957e-01,  2.3424e-01,  2.5745e-01,\n",
       "                        1.3391e-01,  2.9207e-01,  2.1853e-01,  3.4083e-01,  3.9586e-01,\n",
       "                        3.0711e-01,  1.9500e-01,  2.0403e-01,  1.0182e-01,  3.4118e-01,\n",
       "                        2.2222e-01,  3.0011e-01,  9.5209e-02,  2.6944e-01,  2.3159e-01,\n",
       "                        1.0491e-01,  1.6883e-02,  3.1224e-01,  2.8219e-01], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn2.bias',\n",
       "               tensor([-2.0609e-02,  1.2300e-01,  2.4006e-01, -8.9582e-02,  2.1132e-01,\n",
       "                       -8.8342e-02, -2.3299e-01, -1.5456e-04,  1.0589e-01, -6.4092e-02,\n",
       "                       -2.1589e-02,  6.0996e-02, -1.2444e-01,  7.8405e-02, -1.1876e-01,\n",
       "                       -1.8270e-02,  1.1495e-01,  1.5312e-02,  5.4494e-02,  1.2575e-01,\n",
       "                       -1.2257e-01,  2.7355e-01,  2.0925e-01, -1.9506e-03, -5.0375e-02,\n",
       "                        4.4450e-03, -5.3439e-02,  1.1547e-01, -1.3534e-01,  5.2258e-02,\n",
       "                       -9.2506e-02,  7.4996e-02, -6.2639e-02, -2.0903e-01,  1.9367e-01,\n",
       "                        7.0869e-02,  1.2895e-01,  8.3248e-02,  3.6173e-02, -4.7234e-02,\n",
       "                       -1.5510e-01,  1.4424e-02, -5.7651e-02,  1.4479e-01,  2.7855e-05,\n",
       "                        7.8215e-02,  5.6531e-02, -3.0515e-02,  5.1938e-02, -3.9058e-02,\n",
       "                        7.7527e-02,  1.6674e-01, -5.1784e-02, -7.6510e-03, -7.3672e-02,\n",
       "                       -4.5631e-02, -1.0075e-01,  7.5743e-03,  2.3963e-01, -7.4260e-02,\n",
       "                        2.9862e-02,  1.2076e-02,  2.9878e-02,  2.1377e-02], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn2.running_mean',\n",
       "               tensor([-6.6347e-02, -5.4893e-02,  2.6638e-01,  3.6202e-03,  1.4137e-01,\n",
       "                       -6.7120e-04,  2.8516e-02, -1.4166e-02, -2.3446e-01, -7.2741e-02,\n",
       "                        1.8642e-03,  8.3605e-02,  1.1848e-01, -8.2637e-02, -1.3806e-02,\n",
       "                        2.9439e-02,  6.8153e-02, -1.2039e-03,  6.1516e-02,  4.9404e-02,\n",
       "                       -1.0193e-01,  1.3370e-01,  2.0399e-02,  5.7915e-03, -1.6333e-02,\n",
       "                       -1.0280e-24,  1.3434e-03,  1.8289e-02, -1.0621e-01,  1.6368e-02,\n",
       "                       -6.8966e-02, -1.1256e-02, -3.3897e-02, -8.2047e-03,  5.4139e-02,\n",
       "                        1.8318e-03,  3.5622e-03, -6.9550e-02, -1.1989e-01,  9.7807e-03,\n",
       "                       -7.7644e-02, -7.1920e-03, -8.0788e-02,  3.9919e-03, -8.4363e-03,\n",
       "                       -1.5966e-02, -5.2862e-02,  1.8100e-02, -8.9823e-02, -8.8268e-02,\n",
       "                        1.2393e-02,  5.8432e-02, -5.9668e-02, -2.0163e-02,  1.8156e-02,\n",
       "                        3.2132e-03,  1.0774e-01, -7.1710e-03,  1.9043e-04,  1.7832e-02,\n",
       "                        1.3670e-02, -5.6052e-45,  3.9657e-03,  2.9026e-02], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn2.running_var',\n",
       "               tensor([3.9710e-03, 5.3368e-03, 1.6237e-02, 9.8545e-03, 8.0508e-03, 7.8648e-03,\n",
       "                       4.4155e-03, 1.0344e-02, 3.1184e-02, 7.1793e-03, 1.0074e-04, 3.1974e-03,\n",
       "                       6.4828e-03, 8.2373e-03, 1.1357e-02, 2.4236e-03, 3.2218e-03, 8.4399e-03,\n",
       "                       1.9879e-03, 4.7506e-03, 1.0797e-02, 1.0816e-02, 2.9282e-03, 2.5421e-03,\n",
       "                       1.4056e-03, 5.6052e-45, 4.4305e-03, 1.4294e-03, 9.2944e-03, 4.9695e-03,\n",
       "                       1.4794e-03, 2.4715e-03, 4.9885e-03, 7.9655e-03, 2.4195e-03, 5.7400e-04,\n",
       "                       5.5061e-03, 5.3379e-03, 7.8865e-03, 4.8687e-03, 2.3207e-02, 4.7305e-04,\n",
       "                       3.8994e-03, 5.2293e-03, 5.8963e-03, 2.6713e-03, 6.2878e-03, 1.8845e-03,\n",
       "                       8.0665e-03, 6.2928e-03, 6.8886e-03, 6.2161e-03, 5.3465e-03, 9.6062e-04,\n",
       "                       3.2344e-03, 1.9108e-03, 8.4506e-03, 7.5865e-04, 4.2201e-03, 1.9957e-03,\n",
       "                       7.9755e-04, 5.6052e-45, 7.6845e-03, 4.8601e-03], device='cuda:0')),\n",
       "              ('model.model.layer1.1.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer2.0.conv1.weight',\n",
       "               tensor([[[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.1326,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0809],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0363, -0.0033, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0581,  0.0391,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0678,  0.0303]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0310, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [1., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn1.weight',\n",
       "               tensor([0.2618, 0.1941, 0.2310, 0.2456, 0.2388, 0.2673, 0.1747, 0.2219, 0.1748,\n",
       "                       0.2465, 0.2952, 0.2291, 0.2258, 0.1970, 0.2065, 0.2043, 0.2599, 0.2174,\n",
       "                       0.2964, 0.2280, 0.2098, 0.2947, 0.2449, 0.2409, 0.2671, 0.2534, 0.2552,\n",
       "                       0.2381, 0.2431, 0.2405, 0.2038, 0.2362, 0.1842, 0.2096, 0.2350, 0.3200,\n",
       "                       0.2449, 0.2453, 0.2595, 0.2502, 0.1865, 0.1933, 0.2247, 0.2652, 0.2685,\n",
       "                       0.2205, 0.1729, 0.2404, 0.2223, 0.2651, 0.2551, 0.2284, 0.2301, 0.2470,\n",
       "                       0.2025, 0.2345, 0.2349, 0.2388, 0.2107, 0.2546, 0.1997, 0.2058, 0.1853,\n",
       "                       0.2281, 0.2496, 0.1672, 0.3037, 0.2935, 0.2437, 0.2492, 0.2300, 0.1729,\n",
       "                       0.2442, 0.2350, 0.2833, 0.2611, 0.2468, 0.2016, 0.2022, 0.2372, 0.2257,\n",
       "                       0.2805, 0.2206, 0.2243, 0.1844, 0.2607, 0.2679, 0.2202, 0.1668, 0.2637,\n",
       "                       0.2393, 0.2292, 0.2539, 0.2394, 0.2701, 0.2623, 0.1965, 0.2167, 0.2543,\n",
       "                       0.2782, 0.2969, 0.1979, 0.2659, 0.2534, 0.1716, 0.2036, 0.2298, 0.2382,\n",
       "                       0.2626, 0.2302, 0.2437, 0.2816, 0.2641, 0.1955, 0.1890, 0.2322, 0.2739,\n",
       "                       0.2039, 0.2198, 0.2203, 0.2041, 0.2407, 0.1523, 0.2515, 0.2735, 0.1850,\n",
       "                       0.1959, 0.2321], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn1.bias',\n",
       "               tensor([-0.0651,  0.0425,  0.0546,  0.0592, -0.0417,  0.0085,  0.0347, -0.1404,\n",
       "                        0.1002, -0.1110, -0.0642,  0.0029, -0.0503,  0.1094,  0.0371, -0.0493,\n",
       "                       -0.0776,  0.1507, -0.1002,  0.2122, -0.0734, -0.0706, -0.0277, -0.0155,\n",
       "                       -0.0464, -0.1164, -0.0220, -0.0854, -0.0412,  0.0704, -0.0077, -0.0347,\n",
       "                        0.1081, -0.1236, -0.0950, -0.0807, -0.0278, -0.1102, -0.0768, -0.0337,\n",
       "                        0.0588, -0.0660, -0.1131, -0.0785,  0.0421, -0.0692,  0.1658, -0.0558,\n",
       "                       -0.1028,  0.0206, -0.0839, -0.1104, -0.0615, -0.0251, -0.0428, -0.1086,\n",
       "                       -0.0696, -0.0494,  0.0094, -0.1862, -0.0737, -0.0604,  0.0523,  0.0854,\n",
       "                       -0.0786,  0.2587, -0.0452, -0.1316, -0.0225, -0.0189,  0.0487,  0.1420,\n",
       "                       -0.0378, -0.0321, -0.1440, -0.0675,  0.0306,  0.0839,  0.0008,  0.0069,\n",
       "                       -0.0335, -0.0842, -0.0320,  0.0204,  0.0030, -0.0195, -0.0582, -0.0767,\n",
       "                        0.1966, -0.0214, -0.0892,  0.0502,  0.0198, -0.0324, -0.0833, -0.0603,\n",
       "                       -0.0830,  0.1450, -0.1064, -0.0433, -0.0419, -0.0341, -0.0352, -0.0494,\n",
       "                        0.1044,  0.0214, -0.0860, -0.1117, -0.0315, -0.0739, -0.0365, -0.1986,\n",
       "                       -0.0406,  0.1482,  0.0587,  0.0020, -0.1171, -0.0832, -0.0670, -0.0306,\n",
       "                        0.0645, -0.0596,  0.0271, -0.0922, -0.0552,  0.0259, -0.0058, -0.0813],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn1.running_mean',\n",
       "               tensor([ 1.4368e-01,  3.3519e-01,  2.4989e-01,  3.5460e-01,  1.7294e-02,\n",
       "                       -3.7062e-01, -1.4786e-01,  1.2632e-01, -4.4589e-01,  1.6860e-01,\n",
       "                        9.3651e-02,  8.3781e-02,  1.7642e-01, -2.5635e-01, -1.8576e-02,\n",
       "                        2.2937e-01,  3.4081e-01, -4.2700e-01,  7.4665e-02, -3.5677e-01,\n",
       "                        1.9367e-01,  1.4906e-01, -1.1491e-01, -1.0099e-02,  3.6984e-01,\n",
       "                        2.3982e-01,  4.6500e-02, -4.1326e-02, -5.5916e-02, -2.4183e-01,\n",
       "                        1.7018e-01,  3.5903e-02, -2.9557e-01, -2.2878e-02,  1.6331e-01,\n",
       "                        1.8030e-01,  4.2589e-02, -2.2976e-01, -1.9939e-01,  8.6979e-02,\n",
       "                        5.9942e-02,  2.1956e-01,  3.3636e-01,  1.2387e-01, -2.6732e-01,\n",
       "                        5.4024e-02, -2.5708e-01,  2.0330e-01,  4.7210e-02,  5.8206e-04,\n",
       "                       -1.7701e-02,  8.5745e-02,  9.9589e-02,  1.0048e-01,  6.6265e-03,\n",
       "                       -1.2283e-01,  5.0590e-02,  5.9833e-02,  1.6335e-01,  1.8340e-01,\n",
       "                        1.2761e-01,  1.5655e-01,  1.3677e-01,  2.9628e-01, -2.0850e-02,\n",
       "                       -1.1731e-01, -4.3905e-01,  4.6335e-01,  2.6826e-03,  1.7600e-01,\n",
       "                        2.0559e-01,  5.0651e-02,  4.4082e-04,  1.0215e-01,  2.0575e-01,\n",
       "                       -2.4343e-01,  1.2832e-02,  1.9328e-01,  3.3942e-02, -3.0064e-02,\n",
       "                       -2.1313e-01,  8.2933e-03, -1.1489e-01, -5.9018e-02,  9.0186e-02,\n",
       "                        2.3689e-01, -6.8731e-02,  4.9606e-01, -5.5579e-02,  2.9517e-01,\n",
       "                        2.4771e-01, -1.3907e-01,  7.7688e-02,  1.5059e-01,  1.2669e-01,\n",
       "                        2.5022e-02,  1.4594e-01, -1.7978e-01,  1.3563e-01, -5.1958e-02,\n",
       "                        2.7187e-01,  6.9743e-02, -6.8879e-02, -1.7591e-01,  7.4450e-02,\n",
       "                        3.4188e-01, -6.0630e-02, -1.5492e-02, -4.3521e-01,  3.0751e-01,\n",
       "                        4.8018e-02,  7.2153e-02, -3.4308e-02, -6.3523e-01, -6.6904e-02,\n",
       "                       -5.3311e-02,  8.5679e-02,  2.8822e-01,  2.8601e-01,  1.4790e-01,\n",
       "                        6.5635e-02,  1.1919e-02,  1.7695e-01,  2.3322e-01, -2.2355e-02,\n",
       "                        6.2954e-02,  2.3008e-01,  9.8833e-02], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn1.running_var',\n",
       "               tensor([0.0635, 0.0348, 0.0592, 0.1693, 0.0413, 0.0866, 0.0100, 0.0411, 0.0371,\n",
       "                       0.0652, 0.0816, 0.0262, 0.0401, 0.0454, 0.0343, 0.0300, 0.0321, 0.0621,\n",
       "                       0.0539, 0.0501, 0.0209, 0.0790, 0.0571, 0.0721, 0.1083, 0.0609, 0.0234,\n",
       "                       0.0316, 0.1013, 0.0685, 0.0381, 0.0368, 0.0224, 0.0158, 0.0340, 0.1334,\n",
       "                       0.0647, 0.0328, 0.0704, 0.0485, 0.0136, 0.0179, 0.0270, 0.0659, 0.0536,\n",
       "                       0.0196, 0.0320, 0.0378, 0.0264, 0.1317, 0.0583, 0.0234, 0.0216, 0.0926,\n",
       "                       0.0372, 0.0377, 0.0507, 0.0677, 0.0239, 0.0333, 0.0131, 0.0150, 0.0120,\n",
       "                       0.1611, 0.0311, 0.0359, 0.1945, 0.1096, 0.0335, 0.0329, 0.0512, 0.0123,\n",
       "                       0.0589, 0.0388, 0.0945, 0.0573, 0.0557, 0.0588, 0.0294, 0.0436, 0.0285,\n",
       "                       0.0927, 0.0431, 0.0472, 0.0274, 0.0407, 0.0586, 0.0590, 0.0249, 0.0717,\n",
       "                       0.0215, 0.0270, 0.0668, 0.0261, 0.0443, 0.0508, 0.0357, 0.0526, 0.0353,\n",
       "                       0.0500, 0.0904, 0.0160, 0.0656, 0.0638, 0.0144, 0.0381, 0.0412, 0.0360,\n",
       "                       0.0706, 0.0333, 0.0524, 0.0715, 0.0768, 0.0809, 0.0189, 0.0964, 0.0465,\n",
       "                       0.0271, 0.0388, 0.0512, 0.0480, 0.0592, 0.0268, 0.0522, 0.0558, 0.0123,\n",
       "                       0.0385, 0.0485], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer2.0.conv2.weight',\n",
       "               tensor([[[[ 0.0724,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.1324,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0704,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0226],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0403,  0.0212, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0429],\n",
       "                         [ 0.0000,  0.0000, -0.0592]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.1007],\n",
       "                         [-0.0389, -0.0264, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0875,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0875],\n",
       "                         [ 0.0000, -0.0647,  0.0646]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0512,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0365, -0.0000],\n",
       "                         [-0.0425, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0557, -0.0662],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0479]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0845,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.conv2.weight_mask',\n",
       "               tensor([[[[1., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [1., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [1., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn2.weight',\n",
       "               tensor([0.1814, 0.2441, 0.2554, 0.2444, 0.2985, 0.2061, 0.2595, 0.1786, 0.2640,\n",
       "                       0.2088, 0.2429, 0.2568, 0.1373, 0.3548, 0.1518, 0.1466, 0.2107, 0.2200,\n",
       "                       0.1867, 0.2541, 0.2487, 0.2580, 0.1608, 0.2761, 0.1617, 0.1902, 0.1991,\n",
       "                       0.2345, 0.1553, 0.1309, 0.2629, 0.2498, 0.1153, 0.1868, 0.1888, 0.2669,\n",
       "                       0.1783, 0.0977, 0.1676, 0.2929, 0.2795, 0.2005, 0.2468, 0.1702, 0.1962,\n",
       "                       0.1247, 0.1426, 0.2152, 0.2497, 0.2091, 0.1346, 0.3115, 0.2046, 0.1420,\n",
       "                       0.2575, 0.1223, 0.1338, 0.1328, 0.2699, 0.2530, 0.1217, 0.3260, 0.3205,\n",
       "                       0.2610, 0.2481, 0.2645, 0.3035, 0.1905, 0.2334, 0.2400, 0.1727, 0.2892,\n",
       "                       0.2851, 0.2784, 0.2926, 0.2015, 0.2188, 0.2244, 0.1146, 0.3073, 0.2117,\n",
       "                       0.1963, 0.2702, 0.1676, 0.2225, 0.1382, 0.2126, 0.3004, 0.1829, 0.2777,\n",
       "                       0.1746, 0.2895, 0.2262, 0.2637, 0.1373, 0.3116, 0.2587, 0.1057, 0.2761,\n",
       "                       0.2436, 0.2272, 0.1563, 0.2383, 0.1793, 0.0906, 0.2453, 0.1369, 0.3090,\n",
       "                       0.2418, 0.1383, 0.2324, 0.3581, 0.1811, 0.2927, 0.1130, 0.1549, 0.2235,\n",
       "                       0.2619, 0.2786, 0.1447, 0.3087, 0.1593, 0.2601, 0.3085, 0.2301, 0.2667,\n",
       "                       0.1279, 0.2540], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn2.bias',\n",
       "               tensor([ 3.8436e-02,  2.7365e-02,  5.6222e-02,  1.3534e-02, -6.3424e-02,\n",
       "                       -2.0417e-02, -4.5117e-02,  4.3942e-02, -2.2069e-02,  1.2505e-01,\n",
       "                        3.9674e-02, -8.1499e-02, -2.0976e-02, -7.6189e-02,  3.0561e-02,\n",
       "                        1.3855e-02,  2.3924e-02,  3.1633e-02,  7.6257e-03, -3.5905e-02,\n",
       "                       -4.9273e-02, -5.3553e-02, -2.0599e-02,  2.9781e-02,  8.0935e-02,\n",
       "                        4.5247e-02, -4.4272e-02, -8.0298e-03,  3.8564e-02,  1.2462e-01,\n",
       "                        2.4327e-03, -9.5103e-03,  6.3587e-02,  5.2118e-02,  3.1797e-02,\n",
       "                       -4.5390e-02,  1.0037e-01,  3.9114e-02,  2.6292e-02,  5.9238e-02,\n",
       "                        9.3012e-02,  4.2026e-02, -1.7829e-02,  3.3945e-02,  8.1991e-03,\n",
       "                        1.0904e-01,  3.4343e-02, -2.7375e-04,  4.9552e-02,  3.0342e-02,\n",
       "                        5.6780e-02, -5.4247e-02,  3.4473e-03,  1.2464e-01,  2.6517e-02,\n",
       "                        3.1442e-02,  3.9483e-02,  5.1348e-02, -6.4773e-02,  2.0961e-03,\n",
       "                        2.9603e-02,  1.1886e-02, -2.6105e-03,  1.0572e-02,  7.6980e-02,\n",
       "                        5.8990e-02,  8.1179e-02,  3.4457e-02, -2.3538e-02, -3.0806e-02,\n",
       "                       -5.9550e-03,  8.5778e-03, -3.1503e-02, -2.4917e-02,  9.8686e-02,\n",
       "                       -1.4186e-02,  5.0488e-03,  6.9599e-03,  6.1337e-02,  3.0075e-03,\n",
       "                        3.4316e-02,  8.3314e-02,  5.9660e-02,  1.3783e-01, -2.6172e-02,\n",
       "                        8.4715e-02,  8.5633e-02, -2.6574e-02,  2.7353e-02,  1.0263e-01,\n",
       "                        8.4740e-02, -3.9518e-02, -1.8327e-02, -1.0891e-02,  8.0216e-02,\n",
       "                       -2.6265e-02,  2.1245e-02,  1.3893e-01, -2.1022e-02,  1.0382e-01,\n",
       "                        3.2142e-03,  8.2102e-02,  2.8000e-02, -6.3002e-02,  3.0838e-02,\n",
       "                        7.2953e-02,  6.2710e-02,  6.1459e-02,  1.2575e-04,  8.3550e-02,\n",
       "                       -1.9997e-02,  5.2504e-02,  4.6527e-02, -2.5974e-02,  8.9632e-02,\n",
       "                        2.5051e-02, -4.8414e-03, -2.3994e-04, -7.0525e-02,  1.2087e-03,\n",
       "                        1.5696e-01,  7.5407e-02, -2.0017e-02,  7.2662e-02,  5.2765e-02,\n",
       "                        5.5078e-02,  1.1231e-01, -6.4157e-02], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn2.running_mean',\n",
       "               tensor([ 1.4235e-02,  3.7254e-02,  3.5547e-02,  3.3136e-02, -2.7493e-02,\n",
       "                       -1.3441e-02, -1.0134e-02, -1.9604e-02,  3.7683e-02,  4.9889e-02,\n",
       "                        2.6785e-03,  1.2991e-01,  4.8671e-02,  1.2547e-01, -3.0613e-02,\n",
       "                       -8.8285e-03, -6.8499e-02,  7.2120e-02,  9.7429e-02,  4.2365e-02,\n",
       "                        1.7643e-02,  4.1935e-02, -1.2253e-02,  5.6661e-02,  4.0066e-02,\n",
       "                        2.5016e-02,  4.0841e-02,  6.1360e-02, -7.3004e-04,  2.8877e-02,\n",
       "                       -1.6370e-02,  3.3560e-03, -2.0513e-03,  4.4063e-02,  2.4864e-02,\n",
       "                        1.4048e-01,  4.8284e-02,  1.7164e-02,  4.4080e-03, -1.3540e-01,\n",
       "                       -5.6843e-02, -4.3310e-02,  6.7625e-02,  1.8170e-02,  6.1526e-02,\n",
       "                       -6.8488e-03,  3.2936e-02, -4.2747e-02,  5.9101e-02,  6.5871e-02,\n",
       "                       -5.7632e-02,  6.5111e-02,  5.4807e-02, -4.9825e-02,  2.9387e-02,\n",
       "                        2.5880e-02,  5.9011e-02,  5.2732e-02,  7.4534e-02,  6.1475e-02,\n",
       "                        1.8022e-03,  6.7213e-02,  3.6524e-02,  1.0133e-01, -3.6199e-02,\n",
       "                        2.9090e-02, -1.2538e-01,  8.6519e-03, -5.5014e-02,  4.8875e-02,\n",
       "                        1.5338e-02,  3.9848e-02,  1.4309e-02, -4.1018e-03, -8.9136e-02,\n",
       "                        4.1527e-02,  1.3619e-02,  1.9285e-02,  1.2987e-03,  8.6304e-02,\n",
       "                        2.3568e-02, -4.9433e-02,  1.2267e-01,  9.5749e-03,  1.9121e-02,\n",
       "                        2.0569e-02,  6.5725e-05,  2.9276e-02,  8.5715e-03, -9.7002e-02,\n",
       "                        8.0469e-02,  1.1070e-01,  2.7010e-02,  5.7749e-02,  1.2602e-01,\n",
       "                        8.6621e-02,  6.5339e-02,  2.4671e-02,  1.6147e-02, -1.2477e-01,\n",
       "                        4.7555e-02,  4.6514e-02,  4.4924e-02,  6.4441e-02,  4.3657e-02,\n",
       "                       -1.2874e-01,  2.4485e-02,  1.1769e-01,  8.6158e-02, -3.6043e-02,\n",
       "                        6.4555e-02, -1.2378e-01,  4.5965e-02,  4.7246e-02,  3.5271e-02,\n",
       "                        2.4658e-02, -2.1266e-03,  1.9619e-02,  7.0653e-02,  4.3113e-03,\n",
       "                       -2.0572e-01,  4.3081e-02, -2.4005e-02, -6.7184e-02,  1.8410e-02,\n",
       "                        9.7428e-02, -9.7032e-03,  1.9433e-02], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn2.running_var',\n",
       "               tensor([0.0057, 0.0044, 0.0149, 0.0051, 0.0058, 0.0044, 0.0100, 0.0040, 0.0059,\n",
       "                       0.0066, 0.0087, 0.0059, 0.0021, 0.0117, 0.0030, 0.0024, 0.0047, 0.0018,\n",
       "                       0.0067, 0.0095, 0.0068, 0.0077, 0.0082, 0.0161, 0.0047, 0.0025, 0.0018,\n",
       "                       0.0053, 0.0028, 0.0021, 0.0067, 0.0082, 0.0025, 0.0102, 0.0071, 0.0086,\n",
       "                       0.0032, 0.0007, 0.0026, 0.0149, 0.0112, 0.0051, 0.0065, 0.0018, 0.0052,\n",
       "                       0.0032, 0.0029, 0.0043, 0.0105, 0.0073, 0.0036, 0.0168, 0.0042, 0.0058,\n",
       "                       0.0092, 0.0039, 0.0023, 0.0028, 0.0040, 0.0128, 0.0006, 0.0207, 0.0220,\n",
       "                       0.0081, 0.0095, 0.0078, 0.0164, 0.0021, 0.0037, 0.0047, 0.0032, 0.0178,\n",
       "                       0.0107, 0.0109, 0.0117, 0.0022, 0.0028, 0.0029, 0.0014, 0.0098, 0.0046,\n",
       "                       0.0030, 0.0107, 0.0090, 0.0042, 0.0056, 0.0044, 0.0081, 0.0050, 0.0080,\n",
       "                       0.0077, 0.0211, 0.0022, 0.0045, 0.0054, 0.0071, 0.0086, 0.0014, 0.0059,\n",
       "                       0.0192, 0.0087, 0.0112, 0.0068, 0.0024, 0.0021, 0.0129, 0.0011, 0.0096,\n",
       "                       0.0095, 0.0014, 0.0036, 0.0142, 0.0088, 0.0109, 0.0034, 0.0009, 0.0059,\n",
       "                       0.0044, 0.0066, 0.0017, 0.0287, 0.0101, 0.0103, 0.0117, 0.0169, 0.0123,\n",
       "                       0.0020, 0.0055], device='cuda:0')),\n",
       "              ('model.model.layer2.0.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.0.weight',\n",
       "               tensor([[[[ 0.0000]],\n",
       "               \n",
       "                        [[-0.1818]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0629]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[-0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0389]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[ 0.0865]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.2732]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[ 0.0000]],\n",
       "               \n",
       "                        [[-0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.0.weight_mask',\n",
       "               tensor([[[[0.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.1.weight',\n",
       "               tensor([0.2277, 0.0271, 0.1292, 0.0832, 0.0727, 0.0226, 0.1348, 0.1675, 0.0601,\n",
       "                       0.1825, 0.1047, 0.0265, 0.2757, 0.0514, 0.2671, 0.1331, 0.1480, 0.0894,\n",
       "                       0.2833, 0.0345, 0.2404, 0.0119, 0.2541, 0.0292, 0.1862, 0.0255, 0.1412,\n",
       "                       0.0972, 0.2499, 0.3093, 0.1043, 0.0249, 0.2367, 0.2449, 0.2472, 0.0979,\n",
       "                       0.0234, 0.1505, 0.1058, 0.1293, 0.1150, 0.2121, 0.0983, 0.1913, 0.1169,\n",
       "                       0.1937, 0.1524, 0.1332, 0.0114, 0.1182, 0.2635, 0.0288, 0.0284, 0.2040,\n",
       "                       0.0424, 0.2855, 0.1879, 0.1916, 0.0947, 0.1596, 0.0268, 0.0522, 0.0493,\n",
       "                       0.0277, 0.0086, 0.0204, 0.0267, 0.0282, 0.0443, 0.0195, 0.1912, 0.0615,\n",
       "                       0.0889, 0.0808, 0.0642, 0.1189, 0.0238, 0.0168, 0.2343, 0.0878, 0.1855,\n",
       "                       0.0140, 0.0777, 0.1765, 0.1695, 0.2523, 0.0155, 0.1042, 0.1225, 0.0301,\n",
       "                       0.1683, 0.0687, 0.1123, 0.0405, 0.2108, 0.0175, 0.0513, 0.2014, 0.0493,\n",
       "                       0.1337, 0.1416, 0.1783, 0.1074, 0.1074, 0.2471, 0.1618, 0.0265, 0.0199,\n",
       "                       0.0297, 0.1986, 0.0367, 0.0290, 0.2016, 0.0242, 0.2315, 0.0393, 0.1089,\n",
       "                       0.0311, 0.1010, 0.2890, 0.0069, 0.2994, 0.1112, 0.0070, 0.1551, 0.0306,\n",
       "                       0.2483, 0.0633], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.1.bias',\n",
       "               tensor([ 3.8436e-02,  2.7365e-02,  5.6222e-02,  1.3534e-02, -6.3424e-02,\n",
       "                       -2.0417e-02, -4.5117e-02,  4.3942e-02, -2.2069e-02,  1.2505e-01,\n",
       "                        3.9674e-02, -8.1499e-02, -2.0976e-02, -7.6189e-02,  3.0561e-02,\n",
       "                        1.3855e-02,  2.3924e-02,  3.1633e-02,  7.6257e-03, -3.5905e-02,\n",
       "                       -4.9273e-02, -5.3553e-02, -2.0599e-02,  2.9781e-02,  8.0935e-02,\n",
       "                        4.5247e-02, -4.4272e-02, -8.0298e-03,  3.8564e-02,  1.2462e-01,\n",
       "                        2.4327e-03, -9.5103e-03,  6.3587e-02,  5.2118e-02,  3.1797e-02,\n",
       "                       -4.5390e-02,  1.0037e-01,  3.9114e-02,  2.6292e-02,  5.9238e-02,\n",
       "                        9.3012e-02,  4.2026e-02, -1.7829e-02,  3.3945e-02,  8.1991e-03,\n",
       "                        1.0904e-01,  3.4343e-02, -2.7375e-04,  4.9552e-02,  3.0342e-02,\n",
       "                        5.6780e-02, -5.4247e-02,  3.4473e-03,  1.2464e-01,  2.6517e-02,\n",
       "                        3.1442e-02,  3.9483e-02,  5.1348e-02, -6.4773e-02,  2.0961e-03,\n",
       "                        2.9603e-02,  1.1886e-02, -2.6105e-03,  1.0572e-02,  7.6980e-02,\n",
       "                        5.8990e-02,  8.1179e-02,  3.4457e-02, -2.3538e-02, -3.0806e-02,\n",
       "                       -5.9550e-03,  8.5778e-03, -3.1503e-02, -2.4917e-02,  9.8686e-02,\n",
       "                       -1.4186e-02,  5.0488e-03,  6.9599e-03,  6.1337e-02,  3.0075e-03,\n",
       "                        3.4316e-02,  8.3314e-02,  5.9660e-02,  1.3783e-01, -2.6172e-02,\n",
       "                        8.4715e-02,  8.5633e-02, -2.6574e-02,  2.7353e-02,  1.0263e-01,\n",
       "                        8.4740e-02, -3.9518e-02, -1.8327e-02, -1.0891e-02,  8.0216e-02,\n",
       "                       -2.6265e-02,  2.1245e-02,  1.3893e-01, -2.1022e-02,  1.0382e-01,\n",
       "                        3.2142e-03,  8.2102e-02,  2.8000e-02, -6.3002e-02,  3.0838e-02,\n",
       "                        7.2953e-02,  6.2710e-02,  6.1459e-02,  1.2575e-04,  8.3550e-02,\n",
       "                       -1.9997e-02,  5.2504e-02,  4.6527e-02, -2.5974e-02,  8.9632e-02,\n",
       "                        2.5051e-02, -4.8414e-03, -2.3994e-04, -7.0525e-02,  1.2087e-03,\n",
       "                        1.5696e-01,  7.5407e-02, -2.0017e-02,  7.2662e-02,  5.2765e-02,\n",
       "                        5.5078e-02,  1.1231e-01, -6.4157e-02], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.1.running_mean',\n",
       "               tensor([-9.5674e-02,  5.6052e-45, -2.4758e-02,  1.2538e-01, -4.3863e-02,\n",
       "                        7.7164e-03,  4.2924e-02,  3.7023e-02,  1.0919e-01,  8.9736e-02,\n",
       "                       -5.8986e-02,  2.4284e-02, -1.1828e-02, -5.6052e-45,  8.1981e-02,\n",
       "                        1.2401e-01,  2.4444e-02,  2.6447e-02,  2.8694e-01,  5.6052e-45,\n",
       "                        1.0240e-01, -5.6052e-45, -2.1997e-01,  5.6052e-45, -1.5625e-01,\n",
       "                        5.6052e-45,  3.7400e-02,  2.4654e-03,  8.4220e-02, -1.4348e-01,\n",
       "                        1.1716e-01, -5.6052e-45, -1.7761e-01, -1.9869e-01, -8.9560e-02,\n",
       "                       -2.5451e-02,  5.6052e-45,  8.2263e-02,  8.4541e-04, -2.3918e-02,\n",
       "                        5.8903e-02, -1.9859e-01,  1.0701e-01, -2.2835e-01, -1.2643e-01,\n",
       "                        5.1208e-03,  8.3600e-02, -8.0537e-02, -5.6052e-45,  1.7085e-02,\n",
       "                        2.0331e-01, -5.6052e-45, -5.6052e-45, -6.6905e-02, -3.8557e-02,\n",
       "                       -1.6602e-01,  8.2527e-02,  1.8590e-01, -4.2053e-02,  5.8618e-02,\n",
       "                       -5.6052e-45, -1.3520e-01,  1.4699e-02,  5.6052e-45, -5.6052e-45,\n",
       "                       -5.6052e-45, -2.8416e-02, -5.6052e-45,  1.4017e-02, -5.6052e-45,\n",
       "                        1.5073e-02,  1.7171e-02, -2.6485e-02, -3.0744e-02, -1.7020e-02,\n",
       "                        3.5571e-02, -5.6052e-45,  5.6052e-45,  2.9003e-02,  3.7946e-02,\n",
       "                        3.8826e-02, -5.6052e-45, -4.7826e-02, -2.7123e-01,  3.6317e-02,\n",
       "                       -3.2103e-02,  5.6052e-45, -3.9970e-02,  3.8588e-02,  5.6052e-45,\n",
       "                       -1.8488e-01,  2.6301e-02, -1.3296e-02, -5.6052e-45,  1.7492e-01,\n",
       "                        5.6052e-45,  1.0523e-02, -1.2810e-01, -5.6052e-45,  5.0058e-02,\n",
       "                       -3.4985e-03, -1.0374e-01,  6.4193e-02, -2.1175e-02, -4.9933e-02,\n",
       "                       -1.5186e-01,  5.6052e-45,  5.6052e-45,  5.6052e-45,  2.0371e-01,\n",
       "                       -5.6052e-45, -5.6052e-45, -1.7374e-01, -5.6052e-45, -2.2541e-02,\n",
       "                       -5.6052e-45,  6.4372e-02, -5.6052e-45, -3.0403e-02, -3.2809e-02,\n",
       "                        5.6052e-45, -2.1117e-01, -4.3897e-02, -5.6052e-45, -1.1299e-01,\n",
       "                       -5.6052e-45,  4.4616e-01,  9.3070e-04], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.1.running_var',\n",
       "               tensor([2.4792e-02, 5.6052e-45, 5.7937e-04, 5.2234e-03, 2.3255e-03, 9.1999e-05,\n",
       "                       2.3179e-03, 9.2690e-03, 3.1344e-03, 1.6256e-02, 7.1192e-03, 1.1780e-03,\n",
       "                       9.0987e-03, 5.6052e-45, 1.9525e-02, 6.5964e-03, 5.7336e-03, 2.0759e-04,\n",
       "                       3.7132e-02, 5.6052e-45, 2.6994e-02, 5.6052e-45, 3.1771e-02, 5.6052e-45,\n",
       "                       2.3565e-02, 5.6052e-45, 1.2413e-03, 4.5045e-03, 2.6870e-02, 5.4651e-02,\n",
       "                       6.3021e-03, 5.6052e-45, 2.0675e-02, 4.6035e-02, 4.4198e-02, 4.2599e-03,\n",
       "                       5.6052e-45, 2.1309e-03, 1.7265e-03, 1.1148e-02, 1.5428e-03, 1.4770e-02,\n",
       "                       5.0499e-03, 1.2036e-02, 6.2167e-03, 7.0368e-03, 6.7976e-03, 2.4908e-03,\n",
       "                       5.6052e-45, 6.2178e-03, 4.3089e-02, 5.6052e-45, 5.6052e-45, 1.6724e-02,\n",
       "                       9.0385e-04, 3.6078e-02, 7.2260e-03, 1.8449e-02, 1.1497e-03, 1.5143e-02,\n",
       "                       5.6052e-45, 8.0465e-03, 3.1561e-04, 5.6052e-45, 5.6052e-45, 5.6052e-45,\n",
       "                       1.8606e-04, 5.6052e-45, 1.9818e-04, 5.6052e-45, 5.8376e-03, 3.3513e-04,\n",
       "                       2.5918e-04, 8.9337e-04, 1.9193e-03, 2.9152e-04, 5.6052e-45, 5.6052e-45,\n",
       "                       1.9536e-02, 3.0158e-03, 1.1677e-02, 5.6052e-45, 3.5792e-03, 2.2803e-02,\n",
       "                       5.8650e-04, 2.3320e-02, 5.6052e-45, 5.1842e-03, 1.7825e-03, 5.6052e-45,\n",
       "                       2.3917e-02, 7.8623e-04, 2.2781e-04, 5.6052e-45, 1.7207e-02, 5.6052e-45,\n",
       "                       2.0456e-04, 1.4382e-02, 5.6052e-45, 8.0663e-03, 6.2680e-03, 4.5240e-02,\n",
       "                       2.3255e-03, 1.9789e-04, 1.3077e-02, 9.7107e-03, 5.6052e-45, 5.6052e-45,\n",
       "                       5.6052e-45, 1.4354e-02, 5.6052e-45, 5.6052e-45, 2.2224e-02, 5.6052e-45,\n",
       "                       2.4062e-02, 5.6052e-45, 4.0775e-03, 5.6052e-45, 8.7367e-04, 7.5994e-03,\n",
       "                       5.6052e-45, 3.3568e-02, 7.0299e-03, 5.6052e-45, 1.9658e-02, 5.6052e-45,\n",
       "                       2.8002e-02, 2.3678e-03], device='cuda:0')),\n",
       "              ('model.model.layer2.0.downsample.1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer2.1.conv1.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0249,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0945,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0790, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0667],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0213,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0132, -0.0562]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0759, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0125]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0626],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0919]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0453, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0328, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0149, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0384,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0086]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0383,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0247],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0432,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.1123]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.1.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [1., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn1.weight',\n",
       "               tensor([0.2436, 0.1843, 0.2047, 0.2276, 0.2106, 0.2242, 0.0692, 0.2377, 0.2238,\n",
       "                       0.2388, 0.1806, 0.2089, 0.2459, 0.2255, 0.1880, 0.2362, 0.2125, 0.2870,\n",
       "                       0.1533, 0.2016, 0.1711, 0.1529, 0.1995, 0.2017, 0.1454, 0.1898, 0.2238,\n",
       "                       0.2100, 0.1938, 0.2346, 0.2457, 0.2204, 0.1596, 0.0569, 0.1966, 0.2692,\n",
       "                       0.1596, 0.2047, 0.2532, 0.2127, 0.2595, 0.2063, 0.2876, 0.1937, 0.1950,\n",
       "                       0.2403, 0.2463, 0.2176, 0.2145, 0.1856, 0.2566, 0.1807, 0.2314, 0.2155,\n",
       "                       0.1960, 0.2476, 0.1501, 0.2370, 0.2040, 0.2361, 0.1553, 0.2560, 0.1756,\n",
       "                       0.1936, 0.2084, 0.1750, 0.2168, 0.2293, 0.2030, 0.2211, 0.1918, 0.1247,\n",
       "                       0.2321, 0.2178, 0.2674, 0.2141, 0.2386, 0.2035, 0.2655, 0.2401, 0.2100,\n",
       "                       0.2428, 0.2408, 0.1197, 0.1100, 0.1827, 0.2408, 0.1773, 0.2172, 0.2379,\n",
       "                       0.2635, 0.1891, 0.2054, 0.2197, 0.2129, 0.2047, 0.2553, 0.1382, 0.1601,\n",
       "                       0.1870, 0.2022, 0.2353, 0.2305, 0.2240, 0.2202, 0.1845, 0.1820, 0.2357,\n",
       "                       0.2317, 0.2108, 0.2020, 0.2104, 0.2026, 0.1838, 0.1168, 0.1842, 0.2597,\n",
       "                       0.2233, 0.2387, 0.2234, 0.2394, 0.2799, 0.2281, 0.2283, 0.1823, 0.2306,\n",
       "                       0.2779, 0.1830], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn1.bias',\n",
       "               tensor([-0.1697, -0.0664, -0.1452, -0.1551, -0.0618, -0.0766, -0.0502, -0.1867,\n",
       "                       -0.1326, -0.1359, -0.0656, -0.0800, -0.1863, -0.0653, -0.0693, -0.1509,\n",
       "                       -0.1179, -0.2432, -0.0214, -0.0644,  0.0453,  0.0148, -0.0654, -0.1035,\n",
       "                       -0.0250, -0.1133, -0.1536, -0.1401, -0.0650, -0.2074, -0.0978, -0.1070,\n",
       "                       -0.0802, -0.0303, -0.0748, -0.2457,  0.0122, -0.0961, -0.1682, -0.1235,\n",
       "                       -0.1736, -0.0696, -0.2039, -0.0688, -0.0203, -0.1728, -0.1194, -0.1319,\n",
       "                       -0.0381, -0.0702, -0.1807, -0.0441, -0.1597, -0.1981, -0.1853, -0.0996,\n",
       "                       -0.0575, -0.0853, -0.1115, -0.1729, -0.0727, -0.2117, -0.0157, -0.0645,\n",
       "                       -0.1488, -0.1625, -0.0557, -0.0528, -0.0924, -0.1393, -0.1120, -0.0457,\n",
       "                       -0.1535, -0.0730, -0.0980, -0.0695, -0.1719, -0.1827, -0.1887, -0.1985,\n",
       "                       -0.0959, -0.1890, -0.1343, -0.0593,  0.0410, -0.0759, -0.1443,  0.0151,\n",
       "                       -0.0949, -0.1069, -0.2376, -0.1147, -0.1396, -0.0763, -0.1885, -0.1630,\n",
       "                       -0.1622,  0.0004, -0.0467, -0.0695, -0.0890, -0.0519, -0.1587, -0.1117,\n",
       "                       -0.1727, -0.1083, -0.1017, -0.0889, -0.1370, -0.0900, -0.0612, -0.1929,\n",
       "                       -0.0347, -0.1099,  0.0056, -0.0503, -0.2804, -0.1039, -0.2576, -0.1444,\n",
       "                       -0.1825, -0.2349, -0.1734, -0.1667, -0.0787, -0.0683, -0.2865, -0.1751],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn1.running_mean',\n",
       "               tensor([ 3.4659e-02, -3.4403e-02,  7.2970e-02,  1.3460e-01, -4.4033e-02,\n",
       "                       -4.8523e-02, -5.6052e-45,  3.7234e-02,  5.9688e-02, -2.2603e-02,\n",
       "                        1.0821e-01,  8.3887e-03,  1.1642e-02, -1.8586e-02,  3.6940e-02,\n",
       "                       -5.7582e-03, -7.2405e-02,  8.3487e-02,  2.0662e-02, -4.1883e-02,\n",
       "                        6.5011e-02, -2.4903e-02, -1.5106e-02,  5.0116e-02,  4.6221e-02,\n",
       "                       -5.9541e-02,  4.4043e-02,  1.3470e-01,  7.3058e-03,  5.8722e-02,\n",
       "                       -7.1522e-02, -6.8903e-02,  7.6062e-02,  9.3701e-03,  5.3660e-02,\n",
       "                        9.8312e-02,  9.8510e-03,  6.2087e-02, -4.0788e-02,  1.0275e-01,\n",
       "                        1.8543e-02, -4.8335e-02,  2.1113e-02,  6.4169e-02, -7.3187e-03,\n",
       "                       -3.0362e-03,  3.7104e-02,  4.6797e-03, -9.3294e-02,  5.4542e-02,\n",
       "                       -1.1311e-01,  2.3584e-02, -5.1378e-03, -2.2856e-02, -5.4131e-03,\n",
       "                       -2.3460e-02,  4.6603e-02,  4.1574e-02,  9.2501e-02, -1.9843e-02,\n",
       "                        2.0990e-02,  1.3871e-01,  2.5476e-02,  4.6922e-02, -9.8693e-03,\n",
       "                        9.7000e-02,  6.6313e-02,  1.8970e-02,  6.4704e-02,  1.2013e-01,\n",
       "                       -1.4585e-02,  1.5252e-02, -2.7869e-02,  7.4356e-02,  5.2681e-02,\n",
       "                       -1.0895e-01,  6.8673e-02,  1.0123e-02, -4.3706e-02, -2.4396e-02,\n",
       "                        3.4746e-02,  1.6458e-01,  6.5780e-02,  6.6563e-03,  5.3921e-02,\n",
       "                        2.7058e-03,  6.1048e-02, -5.4241e-02, -3.8883e-02,  1.8264e-01,\n",
       "                        4.5051e-02, -6.3267e-02,  8.2112e-02, -8.0347e-02,  1.3980e-01,\n",
       "                       -2.4909e-02,  9.6670e-02, -1.9063e-02,  4.7142e-02,  6.7176e-02,\n",
       "                        6.5110e-02, -3.1016e-02,  6.8624e-02, -6.8222e-02,  7.1683e-02,\n",
       "                        5.9819e-02,  1.2029e-01,  1.0112e-02,  3.8785e-02, -4.9676e-02,\n",
       "                        1.7247e-02,  1.4285e-02, -2.3802e-02,  6.6816e-02,  1.7991e-02,\n",
       "                       -3.2250e-03,  4.8222e-02,  3.1556e-02,  5.3933e-02,  1.0573e-01,\n",
       "                        1.0685e-02,  4.9993e-02, -6.3208e-02,  3.5522e-02,  2.7088e-02,\n",
       "                        4.1267e-02, -5.3691e-03,  5.8032e-02], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn1.running_var',\n",
       "               tensor([1.9540e-02, 5.1380e-03, 7.2691e-03, 9.6743e-03, 8.1919e-03, 1.2707e-02,\n",
       "                       5.6052e-45, 2.5837e-02, 1.0665e-02, 1.5929e-02, 5.1347e-03, 1.4632e-02,\n",
       "                       1.4530e-02, 1.8441e-02, 6.9641e-03, 1.7812e-02, 1.4436e-02, 2.8788e-02,\n",
       "                       1.3596e-03, 2.1928e-02, 6.7178e-03, 5.7408e-03, 1.2081e-02, 8.8865e-03,\n",
       "                       1.7058e-03, 1.4943e-02, 1.5956e-02, 1.1728e-02, 1.2109e-02, 1.1400e-02,\n",
       "                       1.3601e-02, 1.8294e-02, 2.7007e-03, 3.4455e-04, 4.3272e-03, 2.9645e-02,\n",
       "                       6.3649e-03, 1.1093e-02, 2.1634e-02, 1.0036e-02, 2.3909e-02, 1.2040e-02,\n",
       "                       3.8672e-02, 1.8511e-02, 8.8207e-03, 3.3276e-02, 2.7849e-02, 2.1573e-02,\n",
       "                       7.6012e-03, 1.7247e-02, 2.6796e-02, 6.0493e-03, 1.5646e-02, 1.0403e-02,\n",
       "                       4.6089e-03, 1.3620e-02, 3.0442e-03, 2.1560e-02, 1.9061e-02, 2.6398e-02,\n",
       "                       1.0027e-02, 1.2661e-02, 9.7493e-03, 9.3661e-03, 9.8097e-03, 5.8029e-03,\n",
       "                       1.2285e-02, 1.6387e-02, 1.9716e-02, 1.8583e-02, 7.2428e-03, 6.6444e-04,\n",
       "                       1.7405e-02, 1.4591e-02, 3.0779e-02, 2.5812e-02, 1.6867e-02, 1.3096e-02,\n",
       "                       2.6917e-02, 2.2864e-02, 1.7433e-02, 2.0736e-02, 1.0228e-02, 2.3485e-03,\n",
       "                       1.4545e-03, 4.4720e-03, 1.8896e-02, 1.1405e-02, 1.4691e-02, 1.6702e-02,\n",
       "                       2.6799e-02, 6.5255e-03, 1.2227e-02, 1.5087e-02, 1.0969e-02, 1.6610e-02,\n",
       "                       1.2667e-02, 5.0441e-03, 9.7352e-03, 1.0499e-02, 1.3998e-02, 1.9187e-02,\n",
       "                       1.2627e-02, 1.0528e-02, 1.3150e-02, 7.7552e-03, 5.6977e-03, 7.3978e-03,\n",
       "                       1.1238e-02, 1.7324e-02, 1.5899e-02, 7.6077e-03, 1.0550e-02, 5.1888e-03,\n",
       "                       2.8134e-04, 7.3368e-03, 2.3421e-02, 1.5640e-02, 1.5729e-02, 1.6601e-02,\n",
       "                       1.5668e-02, 2.9930e-02, 1.7276e-02, 7.1933e-03, 5.4457e-03, 1.9318e-02,\n",
       "                       3.6072e-02, 7.1788e-03], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer2.1.conv2.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0016,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0939],\n",
       "                         [-0.0152,  0.0691,  0.0727],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0056,  0.0000, -0.0373]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0628, -0.0662]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0295,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0766],\n",
       "                         [ 0.0000, -0.0383, -0.0412],\n",
       "                         [ 0.0000, -0.0696, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0176, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0660,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0161,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0600,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0500,  0.0000, -0.0884],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0114,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0066, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0432, -0.0000,  0.0000],\n",
       "                         [ 0.0245,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.1.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [1., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 1.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [1., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [1., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn2.weight',\n",
       "               tensor([ 0.0436,  0.1082,  0.3089,  0.2928,  0.1989,  0.3147,  0.2948,  0.1655,\n",
       "                        0.2158,  0.0617,  0.2092,  0.2427,  0.1277,  0.2453,  0.1171,  0.1411,\n",
       "                        0.1503,  0.1692,  0.1045,  0.2366,  0.1308,  0.3215,  0.2773,  0.1838,\n",
       "                        0.1828,  0.2197,  0.1944,  0.1089,  0.0870,  0.0451,  0.1693,  0.2468,\n",
       "                        0.0619,  0.1077,  0.1267,  0.3585,  0.2146,  0.1084,  0.1711,  0.1094,\n",
       "                        0.0624,  0.0765,  0.2286, -0.0062,  0.3011,  0.1010,  0.1678,  0.1048,\n",
       "                        0.2100,  0.1683,  0.0222,  0.3214,  0.2555,  0.2048,  0.0786,  0.1744,\n",
       "                        0.1151,  0.0438,  0.1625,  0.1333,  0.2041,  0.2603,  0.2444,  0.0766,\n",
       "                        0.2275,  0.1384,  0.1243,  0.1812,  0.2712,  0.3067,  0.1101,  0.2551,\n",
       "                        0.1966,  0.3282,  0.1058,  0.1377,  0.2645,  0.1506,  0.1290,  0.1250,\n",
       "                        0.1250,  0.1447,  0.0895,  0.1485,  0.1775,  0.0569,  0.2051,  0.1675,\n",
       "                        0.3169,  0.0906,  0.2489,  0.2696,  0.2056,  0.1152,  0.0724,  0.1641,\n",
       "                        0.2523,  0.0163,  0.3870,  0.1199,  0.2625,  0.1391,  0.1189,  0.2878,\n",
       "                        0.1082,  0.1641,  0.1706,  0.0793,  0.3306,  0.1823,  0.1856,  0.1355,\n",
       "                        0.1531,  0.2843,  0.0562,  0.2813,  0.1304,  0.1518,  0.2258,  0.0978,\n",
       "                        0.0194,  0.0778,  0.2276,  0.0263,  0.3093,  0.1581,  0.0342,  0.2742],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn2.bias',\n",
       "               tensor([-0.0698,  0.0174, -0.3481, -0.2483, -0.0844, -0.2071, -0.2277, -0.0040,\n",
       "                       -0.0562, -0.0124, -0.1507, -0.1503, -0.0416, -0.1573,  0.0461, -0.0557,\n",
       "                       -0.0915, -0.0265, -0.0010, -0.0203, -0.0466, -0.1476, -0.2361, -0.0141,\n",
       "                       -0.1238, -0.0027, -0.1112,  0.0013, -0.0090, -0.1156, -0.0797, -0.0808,\n",
       "                        0.0409, -0.0754, -0.1148, -0.2584,  0.0683,  0.0083,  0.1068,  0.0509,\n",
       "                        0.0284,  0.0734, -0.0054, -0.0124, -0.3660, -0.0360, -0.0437,  0.0458,\n",
       "                       -0.0209, -0.1114,  0.0760, -0.1816, -0.1460, -0.1608, -0.0799, -0.1541,\n",
       "                       -0.0219,  0.0350,  0.0930, -0.0851,  0.0235, -0.2380, -0.1402,  0.0826,\n",
       "                       -0.1587, -0.1572,  0.0504,  0.0179, -0.1485,  0.0307, -0.0553, -0.2136,\n",
       "                       -0.0463, -0.2663,  0.0388, -0.0178, -0.0976, -0.0008, -0.0479, -0.0763,\n",
       "                       -0.1085,  0.0718,  0.0150, -0.1000,  0.0593, -0.0347, -0.0113, -0.0721,\n",
       "                       -0.2058,  0.0016, -0.2699, -0.2015, -0.1617,  0.0764, -0.1208, -0.0773,\n",
       "                       -0.0553,  0.0101, -0.1238, -0.0013, -0.2107, -0.1269,  0.0190, -0.0466,\n",
       "                       -0.0637, -0.0612,  0.0568, -0.0240, -0.1836,  0.0466, -0.0747, -0.1633,\n",
       "                       -0.2047, -0.2101, -0.0294, -0.0393, -0.0411, -0.1547, -0.0731, -0.0330,\n",
       "                        0.0086,  0.0561,  0.0209,  0.0446, -0.2469, -0.0195,  0.0103, -0.1497],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn2.running_mean',\n",
       "               tensor([-0.0076, -0.0046, -0.0307, -0.0257,  0.0195, -0.0100, -0.0406,  0.0106,\n",
       "                        0.0009, -0.0105,  0.0110,  0.0075,  0.0247, -0.0153,  0.0053,  0.0221,\n",
       "                        0.0086,  0.0165,  0.0354, -0.0206, -0.0107,  0.0162, -0.0433,  0.0029,\n",
       "                       -0.0124,  0.0160,  0.0163,  0.0006, -0.0127, -0.0094, -0.0009, -0.0032,\n",
       "                        0.0045,  0.0106,  0.0186, -0.0757, -0.0524,  0.0127, -0.0249, -0.0003,\n",
       "                        0.0065, -0.0076,  0.0362,  0.0009, -0.0154,  0.0216,  0.0123,  0.0106,\n",
       "                       -0.0100,  0.0134, -0.0025, -0.0110, -0.0154,  0.0069, -0.0129, -0.0069,\n",
       "                       -0.0133,  0.0009, -0.0045, -0.0250, -0.0083, -0.0180, -0.0482, -0.0064,\n",
       "                       -0.0100,  0.0035,  0.0160, -0.0224,  0.0149,  0.0166,  0.0159, -0.0015,\n",
       "                        0.0033, -0.0194,  0.0219,  0.0115,  0.0344,  0.0121,  0.0075, -0.0005,\n",
       "                       -0.0085, -0.0098, -0.0003, -0.0067,  0.0049,  0.0076, -0.0371, -0.0072,\n",
       "                       -0.0483,  0.0099,  0.0034, -0.0524, -0.0365,  0.0007,  0.0007,  0.0030,\n",
       "                       -0.0119,  0.0053, -0.0942, -0.0082,  0.0072,  0.0129,  0.0095,  0.0156,\n",
       "                        0.0163,  0.0045, -0.0097,  0.0008,  0.0125,  0.0447,  0.0109,  0.0265,\n",
       "                       -0.0074,  0.0022, -0.0188, -0.0232,  0.0070,  0.0014, -0.0159, -0.0102,\n",
       "                       -0.0067, -0.0031, -0.0204, -0.0075, -0.0801,  0.0194, -0.0075, -0.0185],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn2.running_var',\n",
       "               tensor([4.5472e-04, 3.4528e-04, 8.0102e-03, 4.3899e-03, 9.8127e-04, 3.2869e-03,\n",
       "                       4.2173e-03, 1.3481e-03, 1.5200e-03, 2.1981e-04, 2.6291e-03, 1.5931e-03,\n",
       "                       8.0457e-04, 3.5410e-03, 7.5588e-04, 7.5308e-04, 3.7434e-04, 3.7886e-04,\n",
       "                       1.2458e-03, 2.1570e-03, 2.0565e-03, 4.1066e-03, 4.0384e-03, 3.5010e-03,\n",
       "                       1.6225e-03, 2.6330e-03, 1.0724e-03, 1.1960e-03, 3.4873e-04, 5.9931e-04,\n",
       "                       1.6113e-03, 2.9918e-03, 3.8210e-04, 1.5479e-03, 1.7006e-03, 1.1049e-02,\n",
       "                       2.3483e-03, 3.0716e-04, 2.0676e-03, 4.6219e-04, 1.6854e-04, 7.3972e-04,\n",
       "                       4.1644e-03, 2.4626e-05, 4.0842e-03, 8.3759e-04, 1.2269e-03, 4.1998e-04,\n",
       "                       1.5641e-03, 1.3068e-03, 1.8253e-04, 4.5368e-03, 2.9384e-03, 2.5986e-03,\n",
       "                       3.5856e-04, 2.2081e-03, 6.2579e-04, 1.3872e-04, 1.0352e-03, 1.0057e-03,\n",
       "                       7.1101e-04, 5.8135e-03, 5.0371e-03, 3.9350e-04, 1.2237e-03, 7.4068e-04,\n",
       "                       7.8183e-04, 9.5047e-04, 2.1168e-03, 3.2959e-03, 3.4085e-04, 3.9612e-03,\n",
       "                       1.6932e-03, 9.1885e-03, 7.2494e-04, 2.0453e-04, 1.9465e-03, 4.8076e-04,\n",
       "                       9.7670e-04, 7.1018e-04, 7.7603e-04, 7.7611e-04, 6.4777e-04, 1.8315e-03,\n",
       "                       1.8025e-03, 3.2479e-04, 1.5060e-03, 8.1941e-04, 4.0435e-03, 6.9168e-04,\n",
       "                       4.2362e-03, 8.3195e-03, 1.9492e-03, 3.9792e-04, 4.0188e-04, 6.0309e-04,\n",
       "                       2.0443e-03, 1.3230e-04, 5.1170e-03, 1.9321e-03, 4.7225e-03, 3.4236e-03,\n",
       "                       6.9636e-04, 2.0635e-03, 8.2500e-04, 9.3216e-04, 8.0520e-04, 3.1319e-04,\n",
       "                       5.3405e-03, 1.7174e-03, 7.1224e-04, 8.7145e-04, 1.2071e-03, 3.0404e-03,\n",
       "                       5.4321e-04, 1.9311e-03, 1.0486e-03, 5.8256e-04, 2.0627e-03, 3.4673e-04,\n",
       "                       2.6433e-04, 6.5476e-04, 2.3553e-03, 2.3874e-04, 1.0527e-02, 1.9357e-03,\n",
       "                       4.6857e-04, 2.8192e-03], device='cuda:0')),\n",
       "              ('model.model.layer2.1.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer3.0.conv1.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0058, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0598, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0807,  0.1129],\n",
       "                         [ 0.0000,  0.0572,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0196, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0687]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0595,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0489,  0.0413]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0262,  0.0714,  0.0095],\n",
       "                         [ 0.0604,  0.0951,  0.0900],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0077],\n",
       "                         [-0.0401, -0.0289,  0.0010]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn1.weight',\n",
       "               tensor([0.1863, 0.2078, 0.1767, 0.1912, 0.1481, 0.1819, 0.1537, 0.1419, 0.1100,\n",
       "                       0.1414, 0.1589, 0.1487, 0.1789, 0.1890, 0.1614, 0.1875, 0.1606, 0.1597,\n",
       "                       0.1820, 0.1717, 0.1476, 0.2227, 0.1250, 0.2372, 0.1460, 0.2300, 0.1356,\n",
       "                       0.1625, 0.1768, 0.1695, 0.1651, 0.1898, 0.1598, 0.1530, 0.2420, 0.1777,\n",
       "                       0.1546, 0.1458, 0.1853, 0.1988, 0.1779, 0.1541, 0.1615, 0.1322, 0.2053,\n",
       "                       0.1827, 0.1389, 0.1438, 0.1642, 0.1779, 0.1567, 0.1705, 0.1774, 0.1841,\n",
       "                       0.1765, 0.1736, 0.1441, 0.2391, 0.1658, 0.2322, 0.1427, 0.1167, 0.2036,\n",
       "                       0.1439, 0.2244, 0.1472, 0.1439, 0.1948, 0.1674, 0.1796, 0.1318, 0.2054,\n",
       "                       0.1765, 0.1361, 0.1849, 0.2145, 0.1963, 0.1600, 0.1808, 0.1489, 0.2088,\n",
       "                       0.2225, 0.2013, 0.1182, 0.1415, 0.1556, 0.1461, 0.1584, 0.1627, 0.1771,\n",
       "                       0.2026, 0.1581, 0.1866, 0.1763, 0.1695, 0.1435, 0.1720, 0.1739, 0.1380,\n",
       "                       0.1755, 0.1594, 0.1822, 0.1679, 0.1690, 0.2104, 0.1447, 0.2126, 0.1645,\n",
       "                       0.1669, 0.1809, 0.1702, 0.1794, 0.2055, 0.1611, 0.1823, 0.2408, 0.2372,\n",
       "                       0.1628, 0.1698, 0.1701, 0.1655, 0.1933, 0.1654, 0.1585, 0.1714, 0.1835,\n",
       "                       0.1519, 0.1913, 0.1579, 0.1887, 0.1578, 0.1624, 0.1209, 0.1917, 0.1912,\n",
       "                       0.1622, 0.1490, 0.1914, 0.1812, 0.2300, 0.2019, 0.1977, 0.1797, 0.2026,\n",
       "                       0.1999, 0.1985, 0.1740, 0.1971, 0.1347, 0.1824, 0.2013, 0.1835, 0.1471,\n",
       "                       0.1246, 0.1952, 0.2102, 0.1884, 0.1591, 0.2158, 0.2092, 0.1780, 0.2056,\n",
       "                       0.1712, 0.1628, 0.1707, 0.1985, 0.1733, 0.1601, 0.2027, 0.1934, 0.2283,\n",
       "                       0.2220, 0.1734, 0.1987, 0.1815, 0.1527, 0.1563, 0.2001, 0.2236, 0.1977,\n",
       "                       0.1902, 0.1957, 0.1809, 0.1504, 0.1999, 0.1798, 0.1296, 0.1912, 0.1825,\n",
       "                       0.1962, 0.1871, 0.1912, 0.1864, 0.1415, 0.1786, 0.1829, 0.1849, 0.1958,\n",
       "                       0.1914, 0.2157, 0.1848, 0.2027, 0.2040, 0.1865, 0.1688, 0.2422, 0.1904,\n",
       "                       0.1541, 0.1978, 0.1874, 0.1549, 0.2049, 0.1678, 0.1740, 0.1660, 0.1750,\n",
       "                       0.1757, 0.1999, 0.1712, 0.2399, 0.2151, 0.1965, 0.1966, 0.1606, 0.1660,\n",
       "                       0.1618, 0.1749, 0.1949, 0.2016, 0.1711, 0.2181, 0.1491, 0.1584, 0.1822,\n",
       "                       0.1410, 0.2008, 0.1843, 0.1715, 0.1479, 0.1600, 0.2078, 0.1275, 0.1522,\n",
       "                       0.1604, 0.1646, 0.1644, 0.1952, 0.2097, 0.1869, 0.1881, 0.1327, 0.1819,\n",
       "                       0.1827, 0.1678, 0.1960, 0.2025], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn1.bias',\n",
       "               tensor([ 0.0290, -0.0247, -0.0427,  0.0508,  0.0319,  0.0133, -0.0157,  0.0417,\n",
       "                       -0.0303,  0.0134,  0.1123,  0.0057,  0.0238,  0.0283,  0.0894,  0.0174,\n",
       "                        0.0118,  0.0552, -0.0585, -0.0583,  0.0604, -0.0474,  0.0440, -0.0522,\n",
       "                        0.0228, -0.1164,  0.0874, -0.0397,  0.0300,  0.0386,  0.0203,  0.0321,\n",
       "                        0.0066,  0.1001, -0.0232, -0.0203,  0.0371,  0.0648,  0.0391, -0.0335,\n",
       "                        0.0082,  0.0374, -0.0006,  0.0719, -0.0722, -0.0571,  0.0754,  0.0854,\n",
       "                        0.0693,  0.0200, -0.0537,  0.0333, -0.0182,  0.0107, -0.0206,  0.0589,\n",
       "                        0.1033, -0.0622, -0.0644, -0.0068,  0.0519, -0.0516,  0.0697,  0.0306,\n",
       "                       -0.0439, -0.0219,  0.0852, -0.0005,  0.1157, -0.0500,  0.0685, -0.0190,\n",
       "                       -0.0617,  0.0602,  0.0075,  0.1108,  0.0028,  0.0088,  0.0512,  0.0552,\n",
       "                       -0.0033, -0.0180, -0.0401,  0.0120,  0.0733,  0.0216, -0.0464, -0.0022,\n",
       "                        0.0197, -0.0764, -0.0730,  0.0252,  0.0255,  0.0241,  0.0912,  0.0698,\n",
       "                       -0.0077,  0.0679,  0.0725, -0.0043, -0.0633, -0.0061, -0.0353,  0.0428,\n",
       "                       -0.0531, -0.0646,  0.0432,  0.0377, -0.0142,  0.0333,  0.0482, -0.0001,\n",
       "                       -0.0097,  0.0125, -0.0176, -0.0720, -0.0857, -0.0280, -0.0068, -0.0058,\n",
       "                       -0.0034, -0.0523,  0.0720, -0.0076, -0.0262,  0.0163,  0.0139,  0.0044,\n",
       "                       -0.0154,  0.0136,  0.0503, -0.0057,  0.0585,  0.0384, -0.0578, -0.0313,\n",
       "                        0.0190,  0.0720,  0.0263, -0.0669, -0.0052, -0.0587,  0.0932,  0.0400,\n",
       "                       -0.0365, -0.0255,  0.0236, -0.0413,  0.0168, -0.0476,  0.0743, -0.0360,\n",
       "                       -0.0248,  0.0324,  0.0116, -0.0714, -0.0538,  0.0418,  0.0769, -0.0041,\n",
       "                       -0.0036,  0.0706,  0.1011,  0.1052, -0.0578, -0.0401, -0.0402,  0.0317,\n",
       "                       -0.0594, -0.0325, -0.0215, -0.0307,  0.0803, -0.0514,  0.0383,  0.0165,\n",
       "                       -0.0105, -0.0034, -0.0748, -0.0086,  0.0379,  0.0208, -0.0342,  0.1098,\n",
       "                        0.0027,  0.0174,  0.1281, -0.0600, -0.0166, -0.0027,  0.0201, -0.0837,\n",
       "                        0.0011,  0.0277, -0.0167,  0.0359, -0.0480, -0.0048,  0.0163,  0.0333,\n",
       "                        0.0107,  0.0241, -0.1170, -0.0228,  0.0159,  0.0864,  0.0443,  0.1043,\n",
       "                       -0.0102, -0.0269, -0.0174,  0.0107,  0.0760, -0.0205,  0.0158, -0.0376,\n",
       "                        0.0806, -0.0180,  0.0632, -0.1012, -0.0578, -0.0250, -0.0009, -0.0451,\n",
       "                       -0.0070, -0.0469, -0.0404, -0.0693, -0.0272, -0.0069, -0.0269,  0.0048,\n",
       "                        0.0404, -0.0176,  0.0527, -0.0270,  0.0626,  0.0286,  0.0314,  0.0549,\n",
       "                       -0.0245,  0.0330, -0.0279,  0.0336, -0.0443, -0.0403, -0.0455, -0.0781,\n",
       "                       -0.0653,  0.0273,  0.0943, -0.0306,  0.0461,  0.0531, -0.0061,  0.0553],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn1.running_mean',\n",
       "               tensor([ 0.0111,  0.0911,  0.0616, -0.0540,  0.0042,  0.0996,  0.0079,  0.0696,\n",
       "                        0.0466,  0.0263, -0.0558,  0.0625, -0.0673, -0.2392, -0.0049, -0.0668,\n",
       "                        0.1744, -0.0613,  0.1280,  0.0839,  0.0156, -0.0035, -0.0407,  0.0643,\n",
       "                        0.0248,  0.0285,  0.0809,  0.0835,  0.1843,  0.0579,  0.1035,  0.0551,\n",
       "                        0.0182, -0.0952,  0.0572,  0.1049, -0.0737,  0.0281, -0.0221,  0.0851,\n",
       "                        0.0056,  0.0520, -0.0202,  0.0839,  0.1066,  0.0587, -0.0052,  0.0072,\n",
       "                       -0.0303, -0.0317,  0.1233,  0.0179,  0.0933,  0.0251,  0.0892,  0.0922,\n",
       "                       -0.0833, -0.0900,  0.0586,  0.0838,  0.0085,  0.0067, -0.0477,  0.0893,\n",
       "                        0.1670, -0.0062,  0.0225,  0.0905, -0.2374,  0.0966,  0.0378, -0.0822,\n",
       "                        0.0929,  0.1536,  0.1199,  0.2426,  0.1034,  0.0374,  0.0151,  0.0329,\n",
       "                        0.2249,  0.0043, -0.1656,  0.0370, -0.0581,  0.1001,  0.0438,  0.0761,\n",
       "                       -0.0626,  0.0985,  0.0303, -0.1221,  0.0116,  0.0659,  0.0966,  0.0356,\n",
       "                       -0.0192,  0.0749, -0.1016,  0.0528,  0.1211,  0.0811,  0.0569,  0.0113,\n",
       "                        0.1881,  0.0499, -0.1021, -0.1982, -0.0023,  0.0936, -0.0757,  0.1045,\n",
       "                       -0.0458,  0.0082,  0.1053,  0.2093,  0.0637,  0.0042,  0.0376, -0.0554,\n",
       "                        0.0675,  0.0184, -0.0278,  0.0037, -0.0391, -0.0250,  0.0823,  0.1121,\n",
       "                       -0.0290,  0.0873, -0.0047,  0.0290,  0.0648,  0.1405,  0.0559, -0.0821,\n",
       "                        0.0315,  0.1163,  0.0396,  0.1058,  0.0565,  0.0828,  0.1017, -0.0804,\n",
       "                        0.0430, -0.0600, -0.0064,  0.0490,  0.0110,  0.1012, -0.2693,  0.0763,\n",
       "                        0.0068,  0.0305,  0.1537,  0.0378,  0.1365,  0.0501,  0.0738, -0.0043,\n",
       "                        0.0396,  0.0117,  0.0450,  0.0931, -0.1118,  0.0709, -0.0178,  0.0761,\n",
       "                        0.1674,  0.0584,  0.0866,  0.0127,  0.0398,  0.0886, -0.0960,  0.0702,\n",
       "                        0.0192,  0.1066,  0.0952,  0.0172,  0.1902,  0.0588,  0.0452,  0.0905,\n",
       "                        0.0501,  0.1685,  0.1511,  0.0959,  0.0469,  0.0968,  0.1246,  0.1209,\n",
       "                        0.0663,  0.0559,  0.0939, -0.0322,  0.0007, -0.2799,  0.1074, -0.1366,\n",
       "                        0.1013, -0.0616,  0.0643, -0.0668,  0.0603, -0.1013, -0.0396,  0.0556,\n",
       "                        0.0432,  0.1737,  0.0393, -0.0722, -0.0265,  0.0321, -0.0211,  0.0290,\n",
       "                        0.0363,  0.1509,  0.0909,  0.0926, -0.0472, -0.0027,  0.0383,  0.0867,\n",
       "                        0.0395,  0.1322,  0.1768,  0.0486,  0.1002, -0.0461, -0.1567,  0.0086,\n",
       "                       -0.0447,  0.1026,  0.0436,  0.0014,  0.0567,  0.0509, -0.0019, -0.0402,\n",
       "                        0.1343,  0.0294,  0.0553,  0.0443,  0.0622, -0.0008, -0.0665,  0.0700,\n",
       "                        0.0573,  0.0069,  0.0825,  0.1593, -0.0474,  0.0571,  0.0472,  0.2375],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn1.running_var',\n",
       "               tensor([0.0198, 0.0218, 0.0111, 0.0094, 0.0068, 0.0079, 0.0048, 0.0084, 0.0014,\n",
       "                       0.0017, 0.0241, 0.0052, 0.0113, 0.0132, 0.0113, 0.0192, 0.0109, 0.0151,\n",
       "                       0.0163, 0.0092, 0.0071, 0.0168, 0.0030, 0.0186, 0.0074, 0.0165, 0.0071,\n",
       "                       0.0057, 0.0223, 0.0084, 0.0115, 0.0092, 0.0089, 0.0170, 0.0289, 0.0093,\n",
       "                       0.0105, 0.0052, 0.0239, 0.0118, 0.0087, 0.0064, 0.0154, 0.0044, 0.0108,\n",
       "                       0.0123, 0.0136, 0.0181, 0.0210, 0.0214, 0.0066, 0.0120, 0.0129, 0.0105,\n",
       "                       0.0109, 0.0191, 0.0108, 0.0256, 0.0050, 0.0166, 0.0034, 0.0019, 0.0213,\n",
       "                       0.0149, 0.0179, 0.0075, 0.0175, 0.0233, 0.0253, 0.0135, 0.0046, 0.0307,\n",
       "                       0.0109, 0.0110, 0.0166, 0.0295, 0.0142, 0.0033, 0.0127, 0.0033, 0.0225,\n",
       "                       0.0266, 0.0169, 0.0055, 0.0075, 0.0080, 0.0040, 0.0081, 0.0161, 0.0120,\n",
       "                       0.0129, 0.0110, 0.0170, 0.0146, 0.0248, 0.0016, 0.0089, 0.0156, 0.0089,\n",
       "                       0.0162, 0.0159, 0.0139, 0.0119, 0.0114, 0.0181, 0.0055, 0.0238, 0.0122,\n",
       "                       0.0053, 0.0160, 0.0148, 0.0064, 0.0204, 0.0180, 0.0111, 0.0235, 0.0135,\n",
       "                       0.0020, 0.0084, 0.0149, 0.0137, 0.0112, 0.0192, 0.0102, 0.0215, 0.0223,\n",
       "                       0.0094, 0.0177, 0.0057, 0.0166, 0.0068, 0.0095, 0.0037, 0.0177, 0.0074,\n",
       "                       0.0178, 0.0122, 0.0200, 0.0179, 0.0156, 0.0185, 0.0064, 0.0069, 0.0192,\n",
       "                       0.0143, 0.0085, 0.0144, 0.0066, 0.0021, 0.0139, 0.0380, 0.0082, 0.0023,\n",
       "                       0.0045, 0.0414, 0.0149, 0.0179, 0.0135, 0.0356, 0.0101, 0.0180, 0.0395,\n",
       "                       0.0249, 0.0227, 0.0081, 0.0163, 0.0086, 0.0132, 0.0291, 0.0126, 0.0296,\n",
       "                       0.0146, 0.0206, 0.0141, 0.0264, 0.0096, 0.0051, 0.0196, 0.0120, 0.0224,\n",
       "                       0.0274, 0.0184, 0.0081, 0.0185, 0.0117, 0.0267, 0.0172, 0.0108, 0.0129,\n",
       "                       0.0172, 0.0140, 0.0114, 0.0112, 0.0049, 0.0092, 0.0171, 0.0055, 0.0254,\n",
       "                       0.0128, 0.0171, 0.0180, 0.0124, 0.0105, 0.0121, 0.0071, 0.0301, 0.0103,\n",
       "                       0.0146, 0.0318, 0.0163, 0.0051, 0.0156, 0.0173, 0.0143, 0.0066, 0.0146,\n",
       "                       0.0148, 0.0104, 0.0208, 0.0189, 0.0206, 0.0091, 0.0067, 0.0033, 0.0079,\n",
       "                       0.0075, 0.0123, 0.0099, 0.0188, 0.0044, 0.0167, 0.0061, 0.0068, 0.0188,\n",
       "                       0.0061, 0.0107, 0.0171, 0.0140, 0.0051, 0.0119, 0.0167, 0.0042, 0.0071,\n",
       "                       0.0102, 0.0044, 0.0072, 0.0087, 0.0201, 0.0098, 0.0156, 0.0069, 0.0086,\n",
       "                       0.0142, 0.0166, 0.0140, 0.0129], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer3.0.conv2.weight',\n",
       "               tensor([[[[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0277,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0027,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0564, -0.0363],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0211]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0369],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0050, -0.0243],\n",
       "                         [ 0.0000, -0.0000, -0.0256]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0420, -0.0627],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0210]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0871],\n",
       "                         [ 0.0000,  0.0341,  0.0310]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0177,  0.0491],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn2.weight',\n",
       "               tensor([0.1799, 0.1846, 0.1356, 0.2119, 0.2418, 0.1977, 0.1786, 0.1346, 0.1622,\n",
       "                       0.1412, 0.1792, 0.1810, 0.1323, 0.2433, 0.2026, 0.2040, 0.1735, 0.1878,\n",
       "                       0.2048, 0.1135, 0.1499, 0.1348, 0.0530, 0.2167, 0.2032, 0.1560, 0.1787,\n",
       "                       0.1592, 0.2019, 0.2079, 0.2084, 0.1598, 0.2004, 0.1370, 0.2312, 0.1635,\n",
       "                       0.2467, 0.1904, 0.1911, 0.2591, 0.1392, 0.1957, 0.1449, 0.1927, 0.1792,\n",
       "                       0.1615, 0.1510, 0.2017, 0.1208, 0.2034, 0.1874, 0.1405, 0.1677, 0.2109,\n",
       "                       0.2287, 0.1393, 0.1678, 0.1802, 0.2333, 0.2294, 0.1401, 0.1546, 0.1773,\n",
       "                       0.1665, 0.1973, 0.1902, 0.1689, 0.1943, 0.2122, 0.1831, 0.2015, 0.1772,\n",
       "                       0.2168, 0.1822, 0.1660, 0.2000, 0.1585, 0.1349, 0.1909, 0.1766, 0.1688,\n",
       "                       0.1936, 0.1985, 0.1632, 0.2034, 0.1786, 0.2051, 0.2337, 0.2029, 0.2298,\n",
       "                       0.1944, 0.1739, 0.2617, 0.1588, 0.1672, 0.2135, 0.1866, 0.2267, 0.2149,\n",
       "                       0.2079, 0.1251, 0.2029, 0.1583, 0.1957, 0.1704, 0.1866, 0.1998, 0.2283,\n",
       "                       0.1078, 0.1770, 0.2571, 0.1895, 0.2509, 0.1454, 0.2272, 0.1968, 0.1966,\n",
       "                       0.2022, 0.2124, 0.1624, 0.1734, 0.2185, 0.1813, 0.1700, 0.1773, 0.1714,\n",
       "                       0.1606, 0.1884, 0.1708, 0.1783, 0.1340, 0.2423, 0.1737, 0.1573, 0.1836,\n",
       "                       0.1833, 0.2188, 0.1832, 0.2116, 0.2506, 0.1385, 0.1146, 0.1607, 0.1766,\n",
       "                       0.1421, 0.2224, 0.2437, 0.2051, 0.2091, 0.1388, 0.2353, 0.1843, 0.2295,\n",
       "                       0.2446, 0.1842, 0.1689, 0.1862, 0.1437, 0.1009, 0.2172, 0.2254, 0.2122,\n",
       "                       0.1211, 0.1745, 0.1167, 0.1930, 0.1839, 0.2120, 0.2221, 0.1302, 0.1805,\n",
       "                       0.2072, 0.0770, 0.2014, 0.1189, 0.2151, 0.2017, 0.1977, 0.1209, 0.1546,\n",
       "                       0.1644, 0.1220, 0.1159, 0.1820, 0.1669, 0.1840, 0.2743, 0.1793, 0.0686,\n",
       "                       0.1486, 0.2107, 0.1640, 0.2086, 0.2272, 0.1964, 0.1259, 0.1504, 0.1991,\n",
       "                       0.1538, 0.1987, 0.1736, 0.1390, 0.2064, 0.1965, 0.1689, 0.1522, 0.1283,\n",
       "                       0.2002, 0.1855, 0.1578, 0.1739, 0.2009, 0.1864, 0.2273, 0.1752, 0.1334,\n",
       "                       0.1511, 0.1920, 0.2461, 0.1747, 0.2132, 0.1966, 0.1866, 0.2129, 0.1932,\n",
       "                       0.1938, 0.1519, 0.2343, 0.1768, 0.1867, 0.2171, 0.2474, 0.1689, 0.2087,\n",
       "                       0.1950, 0.1511, 0.2193, 0.2138, 0.1598, 0.2109, 0.1594, 0.1923, 0.1808,\n",
       "                       0.2234, 0.1701, 0.2423, 0.1742, 0.2519, 0.2313, 0.2094, 0.2264, 0.1503,\n",
       "                       0.1684, 0.1802, 0.1869, 0.1930], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn2.bias',\n",
       "               tensor([ 4.5934e-02,  4.2592e-02,  3.7389e-02, -3.0128e-02, -1.5799e-02,\n",
       "                        3.6009e-02, -3.5003e-03,  2.7407e-02,  3.4955e-02,  6.9078e-02,\n",
       "                        4.6359e-02,  3.9584e-02, -2.4880e-02, -4.7596e-02,  2.8942e-03,\n",
       "                       -2.9679e-02,  4.5475e-02, -3.6403e-02,  1.5325e-02,  5.3453e-02,\n",
       "                        1.7910e-02,  3.9816e-03, -4.1071e-03,  1.9326e-02, -8.4076e-03,\n",
       "                        3.0478e-02, -1.1929e-02, -2.5503e-02,  5.7653e-02, -4.5482e-02,\n",
       "                        7.3332e-03,  2.9072e-02,  8.6387e-03,  7.5257e-02, -3.6496e-02,\n",
       "                        1.8719e-02, -3.9596e-02,  5.6141e-02,  2.0190e-02, -4.9112e-02,\n",
       "                        5.9422e-02,  4.5236e-02,  3.1770e-02,  2.5831e-02, -2.8611e-02,\n",
       "                        5.8427e-02,  5.4215e-02,  3.0102e-02,  1.6919e-02, -2.9939e-02,\n",
       "                        4.9095e-03, -3.6357e-03,  1.0270e-02,  6.5963e-03,  1.9432e-02,\n",
       "                        5.9178e-02, -7.5261e-03,  4.3464e-02, -7.6121e-03,  4.9525e-02,\n",
       "                        7.1908e-02,  5.7186e-02,  4.9975e-02,  5.1933e-02, -3.3861e-02,\n",
       "                        3.7449e-02, -4.8780e-03,  2.0009e-02,  5.7508e-02, -5.6697e-03,\n",
       "                        1.1214e-04,  2.9034e-02, -7.0343e-03,  3.4245e-02,  2.5590e-02,\n",
       "                       -4.2398e-03,  3.0726e-02,  4.1433e-02,  4.3384e-02,  2.0243e-02,\n",
       "                       -3.6085e-03,  4.1667e-03, -2.9710e-03,  1.5612e-02,  9.4901e-03,\n",
       "                        4.3825e-02,  3.2607e-03, -1.6984e-03,  4.8611e-04, -1.8106e-02,\n",
       "                        2.2571e-02,  1.6761e-02, -1.1810e-02,  8.1232e-02,  4.6534e-02,\n",
       "                        6.4395e-02,  5.3988e-03,  2.2500e-02,  5.2275e-02, -6.4835e-03,\n",
       "                       -8.3711e-03, -1.9452e-02,  4.4038e-02,  3.6448e-02,  9.5229e-02,\n",
       "                        2.4506e-02,  4.5093e-02, -3.0183e-02, -1.2485e-02,  8.9328e-02,\n",
       "                        1.8299e-02,  1.5141e-02,  9.9706e-05, -1.1462e-02, -1.1752e-02,\n",
       "                        3.9209e-02,  1.4478e-02, -7.6250e-02, -1.2334e-02,  2.2930e-02,\n",
       "                       -1.2907e-02, -5.4448e-03, -1.2789e-02,  2.6778e-02,  1.0019e-02,\n",
       "                        3.3146e-02, -3.2281e-02,  2.4901e-02, -1.9738e-02,  5.9678e-02,\n",
       "                        6.4259e-02, -9.7856e-03,  1.9417e-02, -7.0203e-03,  5.8591e-02,\n",
       "                       -1.1441e-03,  4.2846e-02, -2.4348e-02,  2.8537e-02,  1.7693e-02,\n",
       "                        5.1049e-02, -6.2564e-03,  3.6411e-02,  4.3575e-02,  3.8938e-02,\n",
       "                       -2.3040e-02,  1.6476e-02,  1.3321e-02,  1.4910e-02,  1.6114e-02,\n",
       "                       -3.6016e-02,  5.4506e-02,  4.4394e-02, -1.0198e-02,  2.3563e-02,\n",
       "                        1.3104e-02,  2.5398e-02, -5.3893e-03,  8.3383e-02,  2.1712e-02,\n",
       "                       -1.6267e-02,  5.0155e-02,  1.0718e-02,  9.2051e-03,  3.9994e-02,\n",
       "                       -1.3743e-02, -1.7993e-02,  2.6396e-02, -4.0328e-04,  3.4923e-02,\n",
       "                       -5.0341e-02, -1.9610e-02,  2.7322e-02, -9.0771e-03, -6.0092e-03,\n",
       "                       -4.8738e-02,  2.1479e-02,  1.4825e-02,  1.1077e-02, -6.3332e-03,\n",
       "                       -3.4962e-03,  2.8905e-02,  3.8488e-02, -1.1570e-02,  2.2958e-02,\n",
       "                        1.2013e-02, -2.1164e-02, -3.4218e-02,  3.4751e-03, -4.9166e-04,\n",
       "                        7.0520e-02,  4.8781e-02,  6.6888e-03, -3.1023e-02,  9.4066e-03,\n",
       "                        4.2159e-02, -3.8792e-03,  1.5623e-02, -7.8828e-03, -4.4297e-02,\n",
       "                        1.3209e-02,  4.6188e-02,  3.0365e-02,  3.7479e-02, -1.9028e-03,\n",
       "                        2.8646e-02,  5.5923e-02,  6.4274e-04,  2.9003e-02,  4.4486e-02,\n",
       "                        5.9941e-02,  8.3131e-03,  6.6551e-02, -9.2136e-03,  2.4333e-03,\n",
       "                        2.8077e-02,  3.9146e-02,  6.0697e-02, -5.1796e-02,  1.0263e-02,\n",
       "                       -1.5618e-02,  7.0842e-03, -9.7429e-03, -3.7487e-02,  6.2338e-03,\n",
       "                       -4.8829e-02, -3.1070e-02, -3.3254e-03, -1.0372e-04, -1.0732e-03,\n",
       "                        7.2711e-02, -3.2373e-02,  1.6531e-02,  3.9261e-03,  1.2355e-02,\n",
       "                        4.1381e-02,  1.5677e-02,  1.1403e-02,  3.0126e-02, -5.0426e-04,\n",
       "                        8.7361e-02, -6.4398e-03,  6.7904e-03,  3.7605e-03,  7.3348e-02,\n",
       "                       -4.3904e-04, -5.2938e-03, -1.2666e-02,  1.8772e-02,  4.5045e-03,\n",
       "                       -5.2625e-03,  3.8644e-02, -2.1377e-02,  3.9967e-02,  3.1332e-02,\n",
       "                        2.5532e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn2.running_mean',\n",
       "               tensor([ 1.2632e-02, -6.3339e-02,  3.2406e-02,  2.2563e-02, -1.8388e-02,\n",
       "                       -3.5757e-02, -3.0172e-02,  5.9363e-03,  2.0001e-02,  1.7427e-02,\n",
       "                       -2.6751e-02, -5.7706e-02,  1.3184e-02,  5.8405e-03, -1.5351e-02,\n",
       "                        4.8086e-02, -4.9856e-03,  3.2793e-02,  1.4952e-02,  2.8365e-03,\n",
       "                        4.3792e-02,  1.4629e-03, -2.3255e-03,  2.6170e-04, -7.4611e-03,\n",
       "                       -1.7105e-02,  2.7146e-02,  1.7769e-02, -3.3690e-02,  3.0770e-02,\n",
       "                        1.3178e-02, -1.3618e-02, -2.9775e-02,  4.2479e-02,  5.4747e-02,\n",
       "                       -1.9527e-03,  1.8631e-02, -7.4870e-02,  2.7523e-02, -3.5533e-02,\n",
       "                        4.5933e-03,  1.1095e-02,  6.2354e-03, -1.1323e-01, -2.7284e-02,\n",
       "                        1.2346e-02,  1.1088e-02, -9.0191e-03,  7.0352e-03,  4.1016e-03,\n",
       "                       -9.8044e-03,  4.6757e-03,  1.5817e-02,  3.1514e-05,  2.6492e-02,\n",
       "                       -3.1067e-02,  5.2809e-03, -9.5238e-02,  1.9872e-04, -2.6979e-02,\n",
       "                       -2.9036e-02, -6.7769e-02, -4.2901e-02,  1.5981e-02, -2.0716e-02,\n",
       "                       -1.0664e-02,  2.2850e-02,  1.6295e-02, -3.4557e-02, -3.1547e-02,\n",
       "                       -1.5184e-02, -2.1099e-02, -4.4838e-03,  1.8405e-02,  2.7807e-02,\n",
       "                       -5.7540e-02,  8.9420e-03, -5.5720e-03,  3.1758e-02,  1.6848e-02,\n",
       "                        1.0492e-03,  3.5297e-02,  3.1353e-02, -2.0209e-02,  5.5360e-02,\n",
       "                       -4.4216e-02, -2.8512e-02, -1.4682e-02,  8.7452e-02,  6.9129e-02,\n",
       "                       -2.9801e-02,  1.4461e-03, -5.6901e-02, -3.4360e-02, -1.3539e-02,\n",
       "                       -4.9896e-02,  1.2778e-02, -1.5133e-02, -4.3263e-02, -3.5411e-03,\n",
       "                        1.1256e-02,  5.2600e-02, -2.8139e-02, -5.1485e-02, -6.1870e-02,\n",
       "                        1.7306e-02, -6.4725e-02,  2.3933e-02,  1.6682e-02, -3.4118e-02,\n",
       "                       -1.7356e-02, -1.2393e-04, -6.1219e-02,  1.2251e-02,  1.6788e-02,\n",
       "                       -1.4276e-02,  1.4968e-03,  4.1610e-02,  8.1358e-04,  6.7841e-03,\n",
       "                        1.8884e-02, -1.2051e-02, -3.1106e-02, -2.0823e-02,  4.8175e-03,\n",
       "                        1.1139e-02,  2.0148e-02,  7.5178e-03,  4.1027e-02,  1.7078e-02,\n",
       "                       -9.4602e-03,  5.3267e-02, -1.6011e-02,  3.5450e-02,  2.6949e-02,\n",
       "                       -1.8950e-02, -1.4837e-02,  3.3760e-02, -4.1971e-02, -6.1313e-02,\n",
       "                       -8.6007e-03,  1.3610e-02, -1.7146e-02,  3.2677e-03, -2.0536e-02,\n",
       "                        3.0978e-04, -7.1677e-02,  1.6446e-02, -9.9895e-03, -4.0313e-03,\n",
       "                       -1.5864e-02, -4.9455e-03,  3.9717e-02,  2.7564e-02,  1.3635e-02,\n",
       "                        2.4749e-02,  7.8743e-02,  2.7760e-03, -3.0582e-02, -7.0871e-02,\n",
       "                       -1.0235e-01, -4.8467e-03,  2.1162e-02,  1.1603e-02,  1.4056e-02,\n",
       "                        9.6739e-03,  5.1698e-03,  7.7455e-03,  1.0428e-02, -1.3203e-02,\n",
       "                        6.3894e-02,  3.2457e-03, -2.3556e-03,  5.4097e-02, -9.4559e-04,\n",
       "                        4.1996e-03, -1.8711e-02,  4.4336e-02, -1.9606e-03,  1.8776e-02,\n",
       "                        3.9811e-02,  1.8504e-02, -7.0630e-03, -1.3150e-03,  1.5175e-02,\n",
       "                       -1.8912e-03, -1.0736e-01,  2.7430e-02,  1.5575e-03,  7.8287e-03,\n",
       "                       -6.5861e-02, -5.8219e-03, -3.0650e-02,  9.2876e-03, -2.1786e-03,\n",
       "                        1.6148e-02,  2.0887e-02, -6.2303e-03, -7.1005e-03,  6.1311e-02,\n",
       "                        5.7081e-03, -2.4241e-02, -8.5164e-02, -1.2077e-02,  1.4667e-02,\n",
       "                        4.5082e-02,  2.1084e-02,  3.6096e-03, -1.5000e-02, -1.4938e-02,\n",
       "                        1.8960e-03, -6.1294e-03, -8.4361e-02,  1.2592e-02, -4.3743e-03,\n",
       "                        1.1016e-02, -7.3502e-03, -1.5900e-02,  3.1048e-02, -8.7698e-03,\n",
       "                        9.1422e-03,  3.3454e-02, -3.2571e-02,  9.3437e-03,  3.2560e-02,\n",
       "                       -1.0677e-02,  2.8081e-02, -1.9871e-02, -2.2161e-02,  2.6368e-02,\n",
       "                        8.3946e-03, -1.3520e-02, -7.0964e-03,  1.7034e-02,  1.4656e-02,\n",
       "                        3.1345e-02,  2.8821e-03,  2.0859e-02, -1.6929e-02,  3.0514e-02,\n",
       "                       -2.2035e-02,  4.0219e-03, -1.6221e-02,  4.8229e-03, -2.5540e-02,\n",
       "                       -9.0734e-03,  2.6541e-02, -1.8325e-02,  6.4395e-02, -8.2372e-03,\n",
       "                        7.8235e-03, -3.3977e-03,  4.9155e-02, -1.2745e-02,  2.2341e-02,\n",
       "                       -2.8393e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn2.running_var',\n",
       "               tensor([2.2828e-03, 2.6191e-03, 1.9054e-03, 2.8738e-03, 4.7117e-03, 1.0867e-02,\n",
       "                       4.3037e-03, 1.4020e-03, 2.0500e-03, 2.8813e-03, 7.5240e-03, 8.9826e-03,\n",
       "                       8.9172e-04, 6.8560e-03, 4.6330e-03, 2.7119e-03, 2.0390e-03, 1.6654e-03,\n",
       "                       3.7238e-03, 9.3903e-04, 2.1177e-03, 1.5566e-03, 4.0335e-05, 2.3713e-03,\n",
       "                       7.4583e-03, 3.1675e-03, 5.4533e-03, 8.4900e-04, 7.3486e-03, 2.0175e-03,\n",
       "                       5.0674e-03, 1.5079e-03, 2.0267e-03, 3.5608e-03, 3.8100e-03, 2.6695e-03,\n",
       "                       6.6159e-03, 7.9154e-03, 2.5210e-03, 4.2941e-03, 2.6178e-03, 3.4762e-03,\n",
       "                       5.0925e-03, 3.9562e-03, 3.1478e-03, 3.6525e-03, 2.2508e-03, 6.3900e-03,\n",
       "                       8.4995e-04, 1.4282e-03, 6.5630e-03, 4.1387e-04, 6.1428e-04, 2.2402e-03,\n",
       "                       7.2244e-03, 2.4773e-03, 1.4566e-03, 3.1448e-03, 4.0995e-03, 8.4025e-03,\n",
       "                       4.8278e-03, 5.3668e-03, 4.1332e-03, 3.2184e-03, 2.4301e-03, 5.3588e-03,\n",
       "                       1.3562e-03, 2.4719e-03, 8.1336e-03, 5.1459e-03, 3.1180e-03, 3.0231e-03,\n",
       "                       3.7780e-03, 2.3921e-03, 3.5324e-03, 3.0198e-03, 1.2430e-03, 1.8667e-03,\n",
       "                       3.3676e-03, 2.2697e-03, 1.8584e-03, 4.2223e-03, 3.8905e-03, 2.0046e-03,\n",
       "                       4.9761e-03, 5.5134e-03, 1.9143e-03, 3.9177e-03, 3.0083e-03, 3.9550e-03,\n",
       "                       7.2676e-03, 2.7478e-03, 7.6245e-03, 5.3814e-03, 1.9508e-03, 1.1941e-02,\n",
       "                       2.5041e-03, 3.8786e-03, 3.1555e-03, 4.1464e-03, 7.8732e-04, 2.6325e-03,\n",
       "                       2.7794e-03, 9.2946e-03, 6.6422e-03, 3.7826e-03, 3.7127e-03, 3.0563e-03,\n",
       "                       2.1615e-04, 9.9325e-03, 1.2083e-02, 1.8363e-03, 3.3357e-03, 3.1943e-04,\n",
       "                       2.8930e-03, 3.3127e-03, 1.5323e-03, 1.3346e-03, 2.3302e-03, 1.7407e-03,\n",
       "                       1.2357e-03, 4.5441e-03, 2.8364e-03, 5.5464e-03, 1.1291e-03, 2.3928e-03,\n",
       "                       9.8466e-04, 1.4678e-03, 2.4146e-03, 1.1454e-02, 3.1069e-03, 4.3251e-03,\n",
       "                       3.5627e-03, 2.7677e-03, 9.7480e-03, 1.5955e-03, 8.6956e-03, 1.0140e-03,\n",
       "                       6.4093e-03, 9.0464e-03, 2.8411e-03, 4.6126e-04, 1.8406e-03, 4.8048e-03,\n",
       "                       6.5371e-04, 2.4867e-03, 4.1283e-03, 6.5307e-03, 3.9426e-03, 7.9362e-04,\n",
       "                       3.4818e-03, 6.5077e-03, 8.8016e-03, 7.0279e-03, 2.5398e-03, 7.6816e-04,\n",
       "                       4.6258e-03, 7.7310e-04, 1.1296e-03, 1.1710e-02, 5.5768e-03, 8.4274e-03,\n",
       "                       3.1006e-03, 2.2611e-03, 7.7720e-04, 3.0264e-03, 3.3619e-03, 3.2045e-03,\n",
       "                       3.4902e-03, 9.0801e-04, 2.5992e-03, 4.3881e-03, 7.8854e-04, 4.4697e-03,\n",
       "                       1.9870e-03, 2.0448e-03, 3.0556e-03, 3.2677e-03, 2.3330e-04, 1.2701e-03,\n",
       "                       1.1345e-03, 4.7791e-04, 8.0208e-04, 8.2183e-04, 1.8112e-03, 3.0533e-03,\n",
       "                       1.1006e-02, 1.8678e-03, 6.6950e-05, 9.3758e-04, 6.5775e-03, 4.3499e-03,\n",
       "                       3.6384e-03, 3.9156e-03, 5.0482e-03, 2.1621e-03, 3.8070e-04, 4.6206e-03,\n",
       "                       1.1684e-03, 2.1899e-03, 1.5997e-03, 1.6749e-03, 6.0378e-03, 2.2041e-03,\n",
       "                       1.5607e-03, 1.5929e-03, 1.0089e-03, 2.3822e-03, 6.6308e-03, 2.1881e-03,\n",
       "                       4.3945e-03, 2.7633e-03, 1.0435e-02, 2.3125e-03, 1.1050e-03, 8.0030e-04,\n",
       "                       2.5403e-03, 3.6956e-03, 5.2694e-03, 2.4779e-03, 2.0605e-03, 3.5230e-03,\n",
       "                       3.3757e-03, 2.5479e-03, 2.7417e-03, 3.0475e-03, 6.7344e-04, 3.4438e-03,\n",
       "                       1.8672e-03, 2.1191e-03, 7.5382e-03, 3.3233e-03, 6.1863e-03, 2.6141e-03,\n",
       "                       1.0133e-03, 2.7676e-03, 6.6035e-03, 2.8737e-03, 3.4503e-03, 2.9109e-03,\n",
       "                       5.5088e-03, 2.3782e-03, 2.2749e-03, 4.1375e-03, 7.0806e-03, 4.0392e-03,\n",
       "                       3.5703e-03, 3.5993e-03, 3.1747e-03, 4.2903e-03, 8.9376e-03, 2.5158e-03,\n",
       "                       1.2982e-03, 2.5944e-03, 3.7007e-03, 3.0388e-03], device='cuda:0')),\n",
       "              ('model.model.layer3.0.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.0.weight',\n",
       "               tensor([[[[-0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        [[-0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[-0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[-0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[-0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.0.weight_mask',\n",
       "               tensor([[[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.1.weight',\n",
       "               tensor([ 1.4694e-02,  7.3268e-02,  2.4016e-03,  8.0044e-02,  2.5945e-03,\n",
       "                        5.1593e-02,  1.0340e-01,  1.0847e-01,  1.1964e-02,  1.7575e-02,\n",
       "                        8.6445e-02,  5.7162e-02,  6.4289e-02,  9.0078e-02,  8.3941e-02,\n",
       "                        1.4642e-01,  5.2736e-02,  7.6257e-02,  9.9230e-03,  4.2691e-02,\n",
       "                        1.4654e-01,  1.0058e-01,  3.6052e-02,  5.9760e-02,  1.2245e-01,\n",
       "                        1.0925e-01,  1.0550e-01,  3.2892e-02,  8.2834e-02,  4.3385e-02,\n",
       "                        7.1135e-02,  7.5875e-02,  5.5300e-02,  7.7549e-02,  3.3216e-02,\n",
       "                        1.2172e-01,  1.6559e-02,  1.2206e-01,  6.5530e-02,  1.0720e-01,\n",
       "                        3.7028e-02,  4.4238e-02,  1.0206e-01,  7.8790e-02,  1.3796e-01,\n",
       "                        3.2832e-02,  8.5848e-02,  7.9463e-02,  1.0474e-01,  4.5216e-02,\n",
       "                        7.4502e-02,  9.6167e-02,  5.7740e-02,  8.0960e-03,  6.7766e-02,\n",
       "                        7.8653e-02,  8.4149e-02,  5.0853e-02,  3.0677e-02,  4.7442e-02,\n",
       "                        1.0052e-01,  9.3734e-02,  1.0581e-01,  7.4281e-02,  7.2934e-02,\n",
       "                        6.5786e-02,  4.6374e-02, -7.8103e-03,  7.2836e-02,  2.9967e-02,\n",
       "                        6.8661e-02,  2.5472e-02,  8.5678e-02,  1.9755e-02,  1.2474e-01,\n",
       "                        6.7354e-02,  9.6768e-02,  7.0735e-02,  8.6441e-02,  7.1614e-02,\n",
       "                        3.6614e-02,  7.8099e-02,  8.0621e-02,  7.6575e-02,  4.9450e-02,\n",
       "                        3.8518e-02, -1.0023e-03,  7.1780e-02,  8.6368e-02,  4.8297e-13,\n",
       "                        5.3058e-02,  1.0590e-01,  7.4643e-02,  2.3448e-02,  5.8808e-02,\n",
       "                        7.0947e-02,  8.0209e-02,  2.3883e-02,  6.8210e-02,  8.4280e-02,\n",
       "                        5.3884e-02,  5.8303e-02,  1.0148e-01,  5.5420e-02,  1.8926e-02,\n",
       "                        1.2903e-01,  1.8101e-02,  3.2725e-02,  2.7764e-02,  8.9189e-02,\n",
       "                        5.7872e-02,  6.4442e-02,  2.2952e-02,  4.8632e-02,  6.7915e-02,\n",
       "                        4.5755e-03,  7.2044e-02,  5.7690e-03,  7.0139e-02,  4.6070e-02,\n",
       "                        6.0307e-02,  6.3253e-02,  1.4016e-01,  1.1253e-01,  6.5130e-02,\n",
       "                        4.9987e-02,  1.8070e-01,  3.6465e-02,  1.0787e-01,  2.6288e-02,\n",
       "                        6.6346e-03,  9.4665e-02,  1.0122e-01,  1.2252e-01,  9.4982e-04,\n",
       "                        7.5390e-02,  2.0811e-02,  7.2411e-02,  7.0569e-02,  3.4201e-02,\n",
       "                        1.1382e-01,  2.1129e-02,  3.8982e-02,  6.1587e-02,  8.1597e-02,\n",
       "                        2.7475e-02,  4.2700e-02,  1.3189e-01,  6.1730e-02,  5.4403e-02,\n",
       "                        9.5820e-02,  4.9619e-02,  6.4713e-02,  9.1267e-02,  6.8239e-02,\n",
       "                        2.2766e-02,  7.7104e-02,  4.5267e-02,  5.7030e-02,  6.7502e-02,\n",
       "                        8.1049e-02,  5.3932e-04,  1.2039e-01,  3.0641e-02,  1.1949e-01,\n",
       "                        9.4067e-02,  6.2483e-02,  3.1976e-02,  9.3374e-02,  1.0665e-01,\n",
       "                        6.0238e-02,  9.8673e-02,  1.1447e-01,  5.0852e-02,  9.6913e-02,\n",
       "                        1.0033e-01,  8.0622e-02,  1.8077e-02,  4.4527e-02,  5.7040e-02,\n",
       "                        1.7474e-02,  2.4017e-02,  1.0169e-01,  3.0708e-03,  7.5860e-02,\n",
       "                        5.2526e-02,  6.0708e-02,  4.8574e-02,  2.1120e-02,  8.7136e-02,\n",
       "                        8.4932e-02, -2.4596e-04,  6.0813e-02,  8.8490e-02,  7.2633e-02,\n",
       "                        2.8330e-02,  1.6617e-02,  8.1336e-02,  9.8103e-02,  6.3571e-02,\n",
       "                        2.6266e-02,  8.9091e-02,  8.2963e-02,  9.0342e-02,  6.8877e-02,\n",
       "                        8.7936e-02,  4.8618e-02,  3.1765e-02,  7.3272e-02,  1.3433e-01,\n",
       "                        2.5326e-02, -1.6477e-02,  8.1309e-02,  7.4408e-02,  2.5862e-02,\n",
       "                        1.6586e-02,  7.2139e-02,  4.6072e-36,  6.1418e-02,  2.0604e-02,\n",
       "                        3.8409e-03,  7.0267e-02,  1.0731e-01,  5.5731e-03,  1.7744e-02,\n",
       "                        8.8811e-02,  1.1156e-01,  6.2936e-02,  8.9217e-02,  1.0535e-01,\n",
       "                        6.8802e-03,  6.8824e-02,  4.2823e-02,  1.0313e-01,  1.3817e-02,\n",
       "                        1.8790e-02,  7.3977e-02,  4.0193e-02, -1.0429e-03,  3.5369e-02,\n",
       "                        1.0689e-02,  9.2724e-02,  1.9687e-02,  9.6712e-02,  4.6553e-03,\n",
       "                       -4.6216e-03,  8.3238e-02,  1.6906e-02,  4.8202e-02,  6.3969e-02,\n",
       "                        6.2838e-02,  1.0661e-01,  6.4492e-02,  4.2522e-02,  5.3819e-02,\n",
       "                        9.6555e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.1.bias',\n",
       "               tensor([ 4.5934e-02,  4.2592e-02,  3.7389e-02, -3.0128e-02, -1.5799e-02,\n",
       "                        3.6009e-02, -3.5003e-03,  2.7407e-02,  3.4955e-02,  6.9078e-02,\n",
       "                        4.6359e-02,  3.9584e-02, -2.4880e-02, -4.7596e-02,  2.8942e-03,\n",
       "                       -2.9679e-02,  4.5475e-02, -3.6403e-02,  1.5325e-02,  5.3453e-02,\n",
       "                        1.7910e-02,  3.9816e-03, -4.1071e-03,  1.9326e-02, -8.4076e-03,\n",
       "                        3.0478e-02, -1.1929e-02, -2.5503e-02,  5.7653e-02, -4.5482e-02,\n",
       "                        7.3332e-03,  2.9072e-02,  8.6387e-03,  7.5257e-02, -3.6496e-02,\n",
       "                        1.8719e-02, -3.9596e-02,  5.6141e-02,  2.0190e-02, -4.9112e-02,\n",
       "                        5.9422e-02,  4.5236e-02,  3.1770e-02,  2.5831e-02, -2.8611e-02,\n",
       "                        5.8427e-02,  5.4215e-02,  3.0102e-02,  1.6919e-02, -2.9939e-02,\n",
       "                        4.9095e-03, -3.6357e-03,  1.0270e-02,  6.5963e-03,  1.9432e-02,\n",
       "                        5.9178e-02, -7.5261e-03,  4.3464e-02, -7.6121e-03,  4.9525e-02,\n",
       "                        7.1908e-02,  5.7186e-02,  4.9975e-02,  5.1933e-02, -3.3861e-02,\n",
       "                        3.7449e-02, -4.8780e-03,  2.0009e-02,  5.7508e-02, -5.6697e-03,\n",
       "                        1.1214e-04,  2.9034e-02, -7.0343e-03,  3.4245e-02,  2.5590e-02,\n",
       "                       -4.2398e-03,  3.0726e-02,  4.1433e-02,  4.3384e-02,  2.0243e-02,\n",
       "                       -3.6085e-03,  4.1667e-03, -2.9710e-03,  1.5612e-02,  9.4901e-03,\n",
       "                        4.3825e-02,  3.2607e-03, -1.6984e-03,  4.8611e-04, -1.8106e-02,\n",
       "                        2.2571e-02,  1.6761e-02, -1.1810e-02,  8.1232e-02,  4.6534e-02,\n",
       "                        6.4395e-02,  5.3988e-03,  2.2500e-02,  5.2275e-02, -6.4835e-03,\n",
       "                       -8.3711e-03, -1.9452e-02,  4.4038e-02,  3.6448e-02,  9.5229e-02,\n",
       "                        2.4506e-02,  4.5093e-02, -3.0183e-02, -1.2485e-02,  8.9328e-02,\n",
       "                        1.8299e-02,  1.5141e-02,  9.9706e-05, -1.1462e-02, -1.1752e-02,\n",
       "                        3.9209e-02,  1.4478e-02, -7.6250e-02, -1.2334e-02,  2.2930e-02,\n",
       "                       -1.2907e-02, -5.4448e-03, -1.2789e-02,  2.6778e-02,  1.0019e-02,\n",
       "                        3.3146e-02, -3.2281e-02,  2.4901e-02, -1.9738e-02,  5.9678e-02,\n",
       "                        6.4259e-02, -9.7856e-03,  1.9417e-02, -7.0203e-03,  5.8591e-02,\n",
       "                       -1.1441e-03,  4.2846e-02, -2.4348e-02,  2.8537e-02,  1.7693e-02,\n",
       "                        5.1049e-02, -6.2564e-03,  3.6411e-02,  4.3575e-02,  3.8938e-02,\n",
       "                       -2.3040e-02,  1.6476e-02,  1.3321e-02,  1.4910e-02,  1.6114e-02,\n",
       "                       -3.6016e-02,  5.4506e-02,  4.4394e-02, -1.0198e-02,  2.3563e-02,\n",
       "                        1.3104e-02,  2.5398e-02, -5.3893e-03,  8.3383e-02,  2.1712e-02,\n",
       "                       -1.6267e-02,  5.0155e-02,  1.0718e-02,  9.2051e-03,  3.9994e-02,\n",
       "                       -1.3743e-02, -1.7993e-02,  2.6396e-02, -4.0328e-04,  3.4923e-02,\n",
       "                       -5.0341e-02, -1.9610e-02,  2.7322e-02, -9.0771e-03, -6.0092e-03,\n",
       "                       -4.8738e-02,  2.1479e-02,  1.4825e-02,  1.1077e-02, -6.3332e-03,\n",
       "                       -3.4962e-03,  2.8905e-02,  3.8488e-02, -1.1570e-02,  2.2958e-02,\n",
       "                        1.2013e-02, -2.1164e-02, -3.4218e-02,  3.4751e-03, -4.9166e-04,\n",
       "                        7.0520e-02,  4.8781e-02,  6.6888e-03, -3.1023e-02,  9.4066e-03,\n",
       "                        4.2159e-02, -3.8792e-03,  1.5623e-02, -7.8828e-03, -4.4297e-02,\n",
       "                        1.3209e-02,  4.6188e-02,  3.0365e-02,  3.7479e-02, -1.9028e-03,\n",
       "                        2.8646e-02,  5.5923e-02,  6.4274e-04,  2.9003e-02,  4.4486e-02,\n",
       "                        5.9941e-02,  8.3131e-03,  6.6551e-02, -9.2136e-03,  2.4333e-03,\n",
       "                        2.8077e-02,  3.9146e-02,  6.0697e-02, -5.1796e-02,  1.0263e-02,\n",
       "                       -1.5618e-02,  7.0842e-03, -9.7429e-03, -3.7487e-02,  6.2338e-03,\n",
       "                       -4.8829e-02, -3.1070e-02, -3.3254e-03, -1.0372e-04, -1.0732e-03,\n",
       "                        7.2711e-02, -3.2373e-02,  1.6531e-02,  3.9261e-03,  1.2355e-02,\n",
       "                        4.1381e-02,  1.5677e-02,  1.1403e-02,  3.0126e-02, -5.0426e-04,\n",
       "                        8.7361e-02, -6.4398e-03,  6.7904e-03,  3.7605e-03,  7.3348e-02,\n",
       "                       -4.3904e-04, -5.2938e-03, -1.2666e-02,  1.8772e-02,  4.5045e-03,\n",
       "                       -5.2625e-03,  3.8644e-02, -2.1377e-02,  3.9967e-02,  3.1332e-02,\n",
       "                        2.5532e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.1.running_mean',\n",
       "               tensor([ 1.6887e-02, -7.7529e-02, -5.6052e-45,  2.6470e-02, -5.6052e-45,\n",
       "                       -4.5053e-02,  4.9809e-02, -5.1588e-02,  5.6052e-45,  1.8922e-03,\n",
       "                        1.4162e-02, -1.8034e-02,  7.7684e-03, -8.2985e-03,  2.0267e-02,\n",
       "                       -4.3505e-02, -8.7538e-03,  1.1579e-02,  5.6052e-45, -1.4610e-02,\n",
       "                       -4.1254e-02,  1.3107e-02, -6.1844e-03, -1.0191e-02,  6.7052e-02,\n",
       "                        1.2109e-02, -2.8357e-02,  9.8796e-03,  7.2260e-03,  8.5996e-03,\n",
       "                        2.1617e-02, -2.5228e-03, -7.9344e-03,  3.0225e-02,  2.7059e-03,\n",
       "                        1.1980e-02,  1.0929e-02, -1.9143e-02, -1.0684e-02,  5.6955e-03,\n",
       "                        2.0383e-02,  1.1562e-02,  1.2702e-02,  1.2272e-02, -5.8395e-02,\n",
       "                       -6.6506e-03,  8.5341e-03,  4.4477e-02,  3.1174e-02,  2.7145e-02,\n",
       "                        1.7699e-03,  2.9790e-02,  5.1863e-03, -5.6052e-45, -2.8883e-02,\n",
       "                       -1.8710e-02, -7.0673e-03, -2.3513e-02, -5.3694e-03,  1.2667e-02,\n",
       "                       -6.2300e-02, -1.0640e-01,  4.7069e-03, -1.9690e-02,  1.6822e-02,\n",
       "                       -2.8674e-02,  1.0055e-02, -5.6052e-45, -1.3509e-02, -2.8841e-02,\n",
       "                       -7.8336e-03,  2.5126e-02,  7.2179e-03, -1.8681e-02,  5.1809e-02,\n",
       "                        7.1502e-03, -3.1118e-02,  1.7224e-02, -2.1063e-02, -2.6740e-02,\n",
       "                        3.4833e-04, -7.2541e-02, -9.4378e-03, -3.6160e-02, -6.1487e-03,\n",
       "                        2.6097e-03,  9.0385e-03,  1.6718e-02,  3.2043e-02,  1.4375e-13,\n",
       "                       -1.2269e-02, -1.8959e-02,  2.1281e-02,  1.1785e-02,  2.8953e-03,\n",
       "                       -1.9668e-02,  3.9200e-02,  1.3738e-02, -6.5660e-03,  1.1521e-02,\n",
       "                        1.3668e-02, -2.2790e-02, -3.0768e-02, -4.3325e-02,  6.5728e-03,\n",
       "                        2.7382e-02,  1.3646e-02, -2.4540e-03,  2.6984e-03, -5.5151e-04,\n",
       "                        7.8625e-03,  1.6079e-02, -4.0187e-03, -8.4252e-03, -1.0172e-02,\n",
       "                       -5.6052e-45, -3.2564e-02, -5.6052e-45, -2.2986e-02,  3.0101e-02,\n",
       "                        8.6260e-03, -1.2707e-02, -6.1278e-03, -1.9320e-02,  8.7603e-03,\n",
       "                       -9.9634e-04,  3.2912e-02,  2.8433e-03, -1.2473e-02, -2.1173e-02,\n",
       "                       -5.6052e-45, -1.0582e-02, -6.0014e-02, -2.8111e-02, -5.6052e-45,\n",
       "                        2.6146e-03, -1.1829e-02,  1.0673e-02,  2.0546e-02,  2.6399e-02,\n",
       "                        1.4698e-02, -5.6052e-45, -2.1027e-02, -3.4395e-02,  1.0256e-03,\n",
       "                        8.0186e-04,  1.0032e-02,  7.5822e-02,  3.9022e-03,  4.5185e-03,\n",
       "                       -9.6187e-04, -2.7982e-02,  7.1747e-03, -7.8793e-02,  1.3546e-02,\n",
       "                       -5.6052e-45, -1.0224e-02, -6.9448e-03, -1.1352e-02, -4.5070e-02,\n",
       "                        2.9343e-03, -8.3796e-03,  1.6558e-02, -1.2232e-02, -2.1999e-03,\n",
       "                        3.6252e-02,  1.5835e-02,  7.6473e-03, -1.2744e-03,  5.6328e-03,\n",
       "                       -7.7057e-03,  5.9306e-03,  6.3780e-02,  1.0020e-02,  7.6145e-02,\n",
       "                        4.2674e-02,  1.3276e-02,  5.6052e-45,  4.9403e-03, -4.3882e-03,\n",
       "                       -5.6052e-45, -5.6052e-45, -9.5027e-03, -5.6052e-45,  2.4422e-02,\n",
       "                        6.4608e-03, -5.2966e-02, -1.0388e-03, -5.6052e-45, -1.1455e-02,\n",
       "                        3.7790e-03,  7.5961e-03,  6.3918e-03,  5.6412e-02, -5.9898e-02,\n",
       "                       -2.1548e-03, -5.6052e-45,  6.2559e-03,  2.2108e-02,  1.8155e-03,\n",
       "                        5.6052e-45, -9.0899e-03, -3.1477e-02, -1.3028e-02,  1.2293e-02,\n",
       "                        3.6862e-02, -1.1680e-02, -2.0297e-02,  6.1928e-02, -7.6885e-02,\n",
       "                        2.2439e-02,  7.8238e-03, -4.6743e-02,  3.3547e-02,  1.5060e-02,\n",
       "                       -5.6052e-45,  3.3751e-02, -1.1829e-36,  2.6863e-02,  5.6052e-45,\n",
       "                       -4.8621e-04, -3.4438e-02,  1.9072e-02, -2.0082e-03,  5.6052e-45,\n",
       "                        3.4110e-02,  2.8162e-02, -1.6140e-02, -2.9867e-02, -8.1859e-03,\n",
       "                        5.6052e-45,  1.0343e-02, -1.9100e-03,  2.2538e-02, -4.1274e-03,\n",
       "                       -1.3830e-02,  1.7898e-02,  1.3897e-02,  5.9460e-03, -5.0340e-03,\n",
       "                       -5.6052e-45,  6.0993e-02, -3.9530e-03, -4.1384e-02, -5.9200e-03,\n",
       "                        5.7592e-03,  1.7783e-02,  7.7950e-03,  2.6314e-02,  2.0562e-03,\n",
       "                       -2.4624e-02, -1.2305e-02,  5.0305e-02,  7.0058e-03,  2.5940e-02,\n",
       "                        1.4256e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.1.running_var',\n",
       "               tensor([2.4804e-04, 1.5188e-03, 5.6052e-45, 1.3815e-03, 5.6052e-45, 1.2037e-03,\n",
       "                       3.5098e-03, 1.2187e-03, 5.6052e-45, 1.2410e-05, 1.2447e-03, 1.8608e-03,\n",
       "                       1.5923e-04, 2.8472e-03, 3.0136e-03, 2.0416e-03, 5.5419e-04, 7.7761e-04,\n",
       "                       5.6052e-45, 5.0069e-04, 3.5080e-03, 1.4162e-03, 2.2245e-05, 8.9305e-04,\n",
       "                       6.7072e-03, 2.8606e-03, 4.8765e-03, 1.0206e-04, 1.6867e-03, 3.7938e-04,\n",
       "                       1.7574e-03, 6.7539e-04, 2.3508e-04, 2.2457e-03, 2.9863e-04, 2.9856e-03,\n",
       "                       2.2009e-04, 3.4122e-03, 5.8727e-04, 2.8829e-03, 1.5970e-03, 5.8619e-04,\n",
       "                       3.1348e-03, 2.8799e-03, 3.3547e-03, 3.0251e-05, 8.4803e-04, 4.3183e-03,\n",
       "                       1.0391e-03, 6.9742e-04, 4.3189e-03, 3.9269e-04, 6.1139e-05, 5.6052e-45,\n",
       "                       1.1817e-03, 2.8193e-03, 9.5412e-04, 6.1536e-04, 8.8576e-05, 7.4435e-04,\n",
       "                       4.5011e-03, 4.1945e-03, 2.5155e-03, 5.4489e-04, 8.3387e-04, 2.9015e-03,\n",
       "                       2.4242e-04, 5.6052e-45, 4.9517e-04, 6.8335e-04, 5.7698e-05, 6.3290e-04,\n",
       "                       2.3150e-03, 4.2186e-04, 5.0377e-03, 6.8883e-05, 1.7893e-03, 2.4925e-03,\n",
       "                       1.4302e-03, 3.1809e-04, 9.7002e-05, 1.2349e-03, 1.7476e-03, 1.7009e-03,\n",
       "                       1.5526e-03, 6.1747e-04, 1.0863e-04, 6.4201e-04, 1.1076e-03, 1.5986e-26,\n",
       "                       1.9808e-03, 3.1098e-03, 2.6774e-03, 2.5790e-04, 8.8379e-04, 1.3464e-03,\n",
       "                       1.5133e-03, 4.5366e-04, 9.0327e-04, 4.4979e-04, 3.1087e-04, 2.9917e-04,\n",
       "                       2.5960e-03, 1.8886e-03, 1.3736e-04, 5.0237e-03, 4.1434e-04, 6.0071e-04,\n",
       "                       2.5472e-05, 2.5223e-03, 1.3699e-03, 6.1191e-04, 1.9701e-04, 4.6465e-05,\n",
       "                       4.3404e-04, 5.6052e-45, 1.1256e-03, 5.6052e-45, 4.5726e-04, 6.9155e-04,\n",
       "                       2.8675e-04, 2.2804e-03, 3.7143e-03, 3.6103e-03, 2.7305e-04, 1.4639e-04,\n",
       "                       1.2444e-03, 2.8074e-05, 2.3749e-03, 6.9130e-04, 5.6052e-45, 2.2118e-03,\n",
       "                       3.2307e-03, 3.9667e-03, 5.6052e-45, 5.5306e-04, 6.3634e-04, 6.5314e-04,\n",
       "                       1.7289e-03, 3.4977e-04, 2.2559e-03, 5.6052e-45, 2.7884e-04, 4.6190e-04,\n",
       "                       3.9445e-04, 1.3384e-04, 4.7375e-04, 4.6763e-03, 4.6746e-05, 5.8587e-05,\n",
       "                       2.1454e-03, 8.6572e-04, 5.7069e-04, 1.9135e-03, 6.2753e-04, 5.6052e-45,\n",
       "                       5.2943e-04, 5.1116e-05, 9.6404e-04, 1.0902e-03, 3.1208e-03, 1.1309e-04,\n",
       "                       4.7855e-03, 5.2333e-04, 4.7197e-04, 2.1853e-03, 1.8681e-03, 5.5724e-04,\n",
       "                       1.7628e-03, 4.4745e-04, 7.7494e-04, 1.7109e-03, 3.9831e-03, 1.3569e-03,\n",
       "                       2.4871e-03, 1.4950e-03, 9.5959e-04, 5.6052e-45, 4.4013e-05, 4.3737e-05,\n",
       "                       5.6052e-45, 5.6052e-45, 1.3706e-03, 5.6052e-45, 1.4509e-03, 5.5043e-05,\n",
       "                       1.2813e-03, 1.7142e-04, 5.6052e-45, 5.7091e-04, 2.7926e-03, 9.3645e-05,\n",
       "                       1.5292e-03, 9.1355e-04, 1.5802e-03, 2.3609e-04, 5.6052e-45, 1.3936e-03,\n",
       "                       1.2116e-03, 4.3536e-04, 5.6052e-45, 1.4477e-03, 2.1426e-03, 8.7451e-04,\n",
       "                       2.8814e-04, 7.8163e-04, 2.6835e-04, 2.3583e-04, 2.2928e-03, 4.1794e-03,\n",
       "                       3.9954e-04, 2.9423e-04, 3.2424e-03, 1.2050e-03, 4.6274e-04, 5.6052e-45,\n",
       "                       1.8309e-03, 5.6052e-45, 1.4939e-03, 5.6052e-45, 7.2938e-07, 1.8269e-03,\n",
       "                       2.8731e-03, 2.6915e-06, 5.6052e-45, 1.9415e-03, 4.8232e-04, 9.0264e-04,\n",
       "                       8.9785e-04, 6.4729e-04, 5.6052e-45, 7.8914e-04, 4.3375e-04, 2.1377e-03,\n",
       "                       1.0345e-05, 1.7643e-04, 3.3105e-03, 2.0832e-04, 1.6933e-04, 4.1485e-05,\n",
       "                       5.6052e-45, 1.6731e-03, 3.5896e-04, 2.6458e-03, 1.3574e-04, 9.1395e-05,\n",
       "                       1.5590e-03, 7.0196e-04, 9.7575e-04, 9.7442e-04, 3.0919e-03, 1.1363e-03,\n",
       "                       5.3678e-04, 1.0948e-03, 8.8114e-04, 2.2394e-03], device='cuda:0')),\n",
       "              ('model.model.layer3.0.downsample.1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer3.1.conv1.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0101,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0170, -0.0122,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0244, -0.0175, -0.0000],\n",
       "                         [ 0.0000, -0.0063, -0.0104],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0155,  0.0093],\n",
       "                         [ 0.0000,  0.0058,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0199,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0357,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.1.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn1.weight',\n",
       "               tensor([0.0121, 0.1502, 0.0637, 0.0964, 0.0742, 0.0653, 0.0970, 0.0716, 0.0536,\n",
       "                       0.0560, 0.0432, 0.1255, 0.1047, 0.1316, 0.1036, 0.0642, 0.0335, 0.0874,\n",
       "                       0.0555, 0.1187, 0.0528, 0.1118, 0.0103, 0.0778, 0.1344, 0.0823, 0.0665,\n",
       "                       0.0575, 0.0743, 0.0980, 0.0529, 0.0643, 0.0679, 0.0947, 0.1025, 0.0525,\n",
       "                       0.0998, 0.0676, 0.0920, 0.0952, 0.1164, 0.1287, 0.1117, 0.0700, 0.1314,\n",
       "                       0.0415, 0.0758, 0.1150, 0.1379, 0.0781, 0.0128, 0.0972, 0.0763, 0.0375,\n",
       "                       0.0537, 0.0402, 0.0671, 0.0086, 0.1154, 0.1086, 0.1232, 0.1168, 0.1075,\n",
       "                       0.0705, 0.0892, 0.1172, 0.1000, 0.0427, 0.0968, 0.0732, 0.0891, 0.1293,\n",
       "                       0.1089, 0.0788, 0.0396, 0.0365, 0.0979, 0.1353, 0.1518, 0.1316, 0.1278,\n",
       "                       0.0467, 0.0878, 0.0764, 0.0604, 0.0486, 0.0672, 0.1105, 0.1059, 0.0874,\n",
       "                       0.1365, 0.1251, 0.0603, 0.0799, 0.1207, 0.0656, 0.1227, 0.1472, 0.0852,\n",
       "                       0.0578, 0.1199, 0.0627, 0.0812, 0.1045, 0.0398, 0.0729, 0.1044, 0.1128,\n",
       "                       0.0943, 0.0932, 0.1329, 0.0651, 0.0596, 0.0886, 0.0731, 0.0853, 0.0379,\n",
       "                       0.1211, 0.1157, 0.0984, 0.0863, 0.1131, 0.1037, 0.0968, 0.1007, 0.0966,\n",
       "                       0.1325, 0.1052, 0.1079, 0.0936, 0.0591, 0.1349, 0.1006, 0.1109, 0.0858,\n",
       "                       0.1032, 0.0552, 0.1042, 0.0820, 0.1463, 0.0885, 0.0727, 0.0952, 0.1009,\n",
       "                       0.1018, 0.1436, 0.0860, 0.0215, 0.1035, 0.0883, 0.0580, 0.0667, 0.1442,\n",
       "                       0.0047, 0.1130, 0.0588, 0.1505, 0.0043, 0.0968, 0.0483, 0.1065, 0.1119,\n",
       "                       0.0542, 0.0787, 0.0907, 0.1071, 0.1019, 0.0465, 0.1116, 0.0086, 0.0552,\n",
       "                       0.1221, 0.1292, 0.1179, 0.0778, 0.0475, 0.1626, 0.1373, 0.0583, 0.1076,\n",
       "                       0.0912, 0.0618, 0.0958, 0.0991, 0.0081, 0.0880, 0.1185, 0.0783, 0.0725,\n",
       "                       0.0778, 0.0730, 0.0789, 0.1031, 0.0776, 0.1035, 0.1238, 0.1073, 0.0564,\n",
       "                       0.1590, 0.0779, 0.0896, 0.0936, 0.0572, 0.0277, 0.0883, 0.0642, 0.0948,\n",
       "                       0.0926, 0.1513, 0.0720, 0.0428, 0.0668, 0.0503, 0.0513, 0.1010, 0.1235,\n",
       "                       0.0932, 0.0737, 0.1266, 0.0971, 0.0598, 0.1174, 0.0917, 0.0536, 0.0785,\n",
       "                       0.0932, 0.1354, 0.0582, 0.1186, 0.1026, 0.1006, 0.0764, 0.0916, 0.0985,\n",
       "                       0.1552, 0.1023, 0.1116, 0.0839, 0.0957, 0.0816, 0.0760, 0.0445, 0.1209,\n",
       "                       0.0900, 0.1156, 0.0976, 0.0846, 0.0558, 0.0554, 0.1388, 0.0865, 0.0768,\n",
       "                       0.0978, 0.0107, 0.0010, 0.1451], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn1.bias',\n",
       "               tensor([-0.0398, -0.1079, -0.0549, -0.0522, -0.0286, -0.0385, -0.0737, -0.0565,\n",
       "                       -0.0253, -0.0421, -0.0264, -0.0538, -0.0985, -0.0812, -0.0681, -0.0190,\n",
       "                       -0.0370, -0.0498, -0.0159, -0.0705, -0.0118, -0.0244, -0.0345, -0.0687,\n",
       "                       -0.0684, -0.0497, -0.0664, -0.0516, -0.0388, -0.0342, -0.0080, -0.0197,\n",
       "                       -0.0551, -0.0552, -0.0297, -0.0402, -0.0863, -0.0127, -0.0394, -0.0264,\n",
       "                       -0.0894, -0.0643, -0.0576, -0.0245, -0.0709, -0.0329, -0.0523, -0.0612,\n",
       "                       -0.0662, -0.0418, -0.0427, -0.0717, -0.0415, -0.0370, -0.0081, -0.0202,\n",
       "                       -0.0217, -0.0458, -0.0664, -0.0664, -0.1003, -0.1193, -0.0710, -0.0625,\n",
       "                       -0.0356, -0.0265, -0.0267, -0.0137, -0.1161, -0.0759, -0.0615, -0.1002,\n",
       "                       -0.0575, -0.0387,  0.0041, -0.0409, -0.0041, -0.1144, -0.0740, -0.1101,\n",
       "                       -0.0943, -0.0482, -0.0273, -0.0295, -0.0345, -0.0232, -0.0280, -0.0870,\n",
       "                       -0.0633, -0.0722, -0.0733, -0.1106, -0.0515, -0.0536, -0.1131, -0.0321,\n",
       "                       -0.0879, -0.1097, -0.0625, -0.0180, -0.1065, -0.0477, -0.1088, -0.0717,\n",
       "                       -0.0366, -0.0303, -0.0492, -0.0652, -0.0429, -0.0214, -0.0947, -0.0341,\n",
       "                       -0.0438, -0.0465, -0.0375, -0.0361, -0.0162, -0.0678, -0.1310, -0.0239,\n",
       "                       -0.0562, -0.0784, -0.0836, -0.0545, -0.0358, -0.0525, -0.0913, -0.1027,\n",
       "                       -0.0832, -0.0616, -0.0288, -0.0514, -0.0368, -0.0282, -0.0856, -0.0533,\n",
       "                       -0.0479, -0.0591, -0.0622, -0.1442, -0.0417, -0.0454, -0.0764, -0.0593,\n",
       "                       -0.0922, -0.1026, -0.0587, -0.0661, -0.0903,  0.0045, -0.0692, -0.0116,\n",
       "                       -0.0469, -0.0231, -0.0573, -0.0549, -0.1189, -0.0219, -0.0519, -0.0159,\n",
       "                       -0.0547, -0.0893, -0.0144, -0.0816, -0.0656, -0.0883, -0.0630, -0.0482,\n",
       "                       -0.0631, -0.0548, -0.0228, -0.1059, -0.0852, -0.0252, -0.0117, -0.0671,\n",
       "                       -0.0938, -0.1271, -0.0710, -0.0510, -0.0281, -0.0311, -0.0776, -0.0804,\n",
       "                        0.0307, -0.0374, -0.1075, -0.0661, -0.0463, -0.0374, -0.0345, -0.0454,\n",
       "                       -0.0270, -0.0229, -0.0549, -0.0903, -0.0497, -0.0343, -0.1503, -0.0573,\n",
       "                       -0.0618, -0.0785,  0.0050, -0.0341, -0.0483, -0.0662, -0.0536, -0.0722,\n",
       "                       -0.0798, -0.0242, -0.0422, -0.0272, -0.0295, -0.0324, -0.1057, -0.0478,\n",
       "                       -0.0753, -0.0045, -0.1025, -0.0239,  0.0009, -0.0499, -0.0328, -0.0322,\n",
       "                       -0.0184, -0.0365, -0.1646, -0.0312, -0.0827, -0.0704, -0.0388, -0.0335,\n",
       "                       -0.0288, -0.0594, -0.0944, -0.0518, -0.0756, -0.0570, -0.0810, -0.0250,\n",
       "                       -0.0402, -0.0308, -0.0945, -0.0507, -0.1019, -0.0640, -0.0300, -0.0230,\n",
       "                       -0.0597, -0.0930, -0.0715, -0.0273, -0.0434, -0.0418, -0.0067, -0.1149],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn1.running_mean',\n",
       "               tensor([ 7.5483e-03, -4.4760e-02,  2.7366e-02, -4.3077e-02,  6.1931e-04,\n",
       "                        1.1304e-02,  3.0427e-02,  2.4276e-02,  1.2121e-02,  8.2977e-03,\n",
       "                        5.5502e-03,  2.4096e-02,  2.4326e-02,  4.8662e-02,  6.7466e-02,\n",
       "                       -1.0927e-02,  9.5796e-04,  1.3602e-02,  4.5778e-03,  1.5714e-03,\n",
       "                       -4.1925e-03, -3.2572e-02,  2.8867e-03,  7.1573e-02, -1.7268e-02,\n",
       "                       -1.3952e-02,  3.4622e-02,  1.3672e-02, -3.3259e-02, -3.8837e-02,\n",
       "                        5.1808e-03, -2.5163e-03,  7.0712e-03,  2.4085e-02, -7.6411e-03,\n",
       "                        8.4234e-03,  5.0338e-02,  3.6056e-02,  5.9480e-02,  2.4775e-03,\n",
       "                        2.4340e-02,  1.0586e-02, -4.1859e-03,  2.2986e-02,  1.8948e-01,\n",
       "                        5.6014e-03,  2.0303e-02, -7.3830e-03,  5.0251e-02,  2.2747e-03,\n",
       "                       -2.4759e-03, -5.7644e-03,  2.6016e-02,  3.4376e-04,  1.4316e-02,\n",
       "                       -2.6383e-03,  2.3231e-02, -8.8502e-03, -3.5340e-02,  2.7397e-04,\n",
       "                        6.8094e-02,  4.2726e-02,  3.2323e-02,  7.4221e-03,  3.6469e-02,\n",
       "                       -8.0943e-03,  3.3261e-02,  4.7594e-03,  2.5364e-02,  2.8173e-02,\n",
       "                        9.8160e-03,  2.0304e-02, -4.7686e-03,  1.1674e-02,  1.3222e-02,\n",
       "                        3.7690e-03, -1.2973e-02, -3.3844e-02,  7.3758e-02,  3.9212e-02,\n",
       "                       -2.4202e-02,  8.9789e-03,  9.0726e-03, -5.9959e-03,  2.0964e-03,\n",
       "                       -7.3712e-03,  2.0270e-03,  3.2158e-02, -1.2605e-02,  4.0150e-02,\n",
       "                        1.3139e-02,  1.0079e-01,  1.2414e-02, -7.4388e-03,  9.2330e-03,\n",
       "                        2.2189e-03,  2.3715e-02,  3.3026e-02,  1.4673e-02,  6.5776e-03,\n",
       "                        5.9502e-02,  2.6690e-02,  4.5921e-02,  3.5503e-02,  4.7405e-03,\n",
       "                        5.9881e-03,  7.1848e-02,  3.4232e-02,  3.5751e-02,  1.2585e-02,\n",
       "                       -1.1016e-03,  8.3395e-03, -1.6410e-03,  4.1460e-02,  2.6510e-02,\n",
       "                        1.6319e-02,  4.8198e-03,  2.6685e-02, -1.3703e-02, -2.0906e-02,\n",
       "                        1.6486e-02,  3.2367e-02,  1.1843e-02, -8.8806e-03,  1.9332e-02,\n",
       "                        7.3690e-02,  3.1357e-02,  3.7031e-02,  4.1536e-03,  1.3051e-03,\n",
       "                        4.7283e-04,  9.8860e-02, -4.1580e-03,  3.9593e-02,  3.0840e-02,\n",
       "                       -1.0639e-02,  1.2187e-02, -2.0979e-03,  1.6544e-02, -3.1183e-02,\n",
       "                       -6.6898e-03,  3.7463e-02,  1.3066e-02, -1.1304e-02,  5.8134e-02,\n",
       "                        2.1784e-02,  1.4512e-02,  9.7228e-03,  6.4882e-04,  4.1740e-02,\n",
       "                       -1.4425e-02,  2.6184e-03,  1.4448e-01, -1.2272e-03, -1.9360e-02,\n",
       "                        2.9748e-02,  6.5637e-02,  1.8346e-03,  4.5790e-02,  6.8145e-03,\n",
       "                        5.5686e-02,  8.8895e-03,  2.7442e-03,  8.3492e-03,  2.1476e-02,\n",
       "                        2.0271e-02, -3.6761e-03,  5.6028e-03,  2.0166e-02,  1.2160e-02,\n",
       "                        8.1433e-03,  6.7471e-02,  1.0313e-01,  2.9423e-03,  1.7438e-02,\n",
       "                        7.2342e-03, -6.8786e-02, -4.6714e-03,  4.2449e-03,  4.1237e-02,\n",
       "                        1.4822e-02, -8.6003e-03,  1.7862e-02,  2.6509e-02, -1.3866e-02,\n",
       "                       -1.6412e-02, -2.2476e-02,  2.4764e-02,  1.8743e-02,  2.2933e-02,\n",
       "                       -1.6671e-02, -1.1790e-02, -1.1214e-02, -2.4817e-03,  3.9350e-02,\n",
       "                        4.3917e-02,  9.1528e-03,  1.6286e-02,  4.2153e-02,  8.9667e-02,\n",
       "                        4.1279e-02,  3.0410e-02,  9.9325e-03,  6.9773e-03,  9.3207e-03,\n",
       "                        1.8907e-02,  3.9366e-03,  3.1620e-03,  2.2160e-01,  8.5791e-03,\n",
       "                        8.8362e-03,  8.7288e-03, -1.5807e-03,  1.9143e-02,  9.3145e-03,\n",
       "                        1.4735e-02,  1.0473e-02,  1.7428e-02,  3.7695e-02, -9.0414e-03,\n",
       "                       -4.3894e-03, -4.0087e-02,  3.5767e-03,  1.1429e-02, -7.6124e-03,\n",
       "                       -1.7425e-02,  3.9547e-02,  5.9922e-03, -1.8862e-04, -4.6031e-02,\n",
       "                        1.5861e-01,  1.5424e-02,  4.7612e-02,  3.7472e-03,  8.3131e-02,\n",
       "                        2.5015e-02,  5.2969e-02,  1.0225e-02,  4.1740e-02,  1.7989e-02,\n",
       "                        1.8063e-02,  1.0169e-02,  3.3069e-02,  1.5295e-02,  1.2827e-02,\n",
       "                        1.3586e-02,  2.9764e-02,  1.1099e-03,  1.3617e-03,  7.8474e-02,\n",
       "                        1.6113e-02, -6.1317e-03,  3.5716e-02,  2.5083e-04, -3.0051e-03,\n",
       "                        1.2103e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn1.running_var',\n",
       "               tensor([6.6534e-05, 5.0322e-03, 4.6947e-04, 1.5083e-03, 1.5414e-04, 3.0821e-04,\n",
       "                       1.1419e-03, 4.7606e-04, 2.2369e-04, 4.2697e-04, 5.3660e-05, 1.4580e-03,\n",
       "                       1.9400e-03, 1.7080e-03, 1.7150e-03, 1.6493e-04, 4.6324e-05, 6.3448e-04,\n",
       "                       1.7149e-04, 1.6156e-03, 1.0482e-04, 1.8880e-03, 3.8051e-05, 1.8358e-03,\n",
       "                       2.9876e-03, 1.3103e-03, 3.6817e-04, 1.5825e-04, 1.0026e-03, 1.4092e-03,\n",
       "                       8.6705e-05, 6.3959e-04, 7.6448e-04, 1.7775e-03, 8.1396e-04, 1.8307e-04,\n",
       "                       1.8683e-03, 9.2772e-04, 2.0154e-03, 6.9612e-04, 1.7952e-03, 1.9990e-03,\n",
       "                       4.1586e-03, 1.0390e-03, 5.7341e-03, 5.2905e-05, 7.4082e-04, 3.2491e-03,\n",
       "                       3.3788e-03, 6.8307e-04, 1.0167e-04, 1.3517e-03, 2.3967e-04, 9.1538e-05,\n",
       "                       2.1450e-04, 4.6821e-05, 4.8228e-04, 6.3217e-04, 2.5732e-03, 1.3799e-03,\n",
       "                       2.7499e-03, 2.7993e-03, 1.5435e-03, 6.3333e-04, 1.6186e-03, 1.1946e-03,\n",
       "                       1.0241e-03, 5.8884e-05, 1.2439e-03, 4.5603e-04, 1.9905e-03, 1.7110e-03,\n",
       "                       7.6378e-04, 4.3566e-04, 1.8793e-04, 1.0380e-04, 2.6472e-03, 3.5818e-03,\n",
       "                       5.2591e-03, 3.9636e-03, 2.5587e-03, 1.3774e-04, 1.4294e-03, 4.0035e-04,\n",
       "                       6.1898e-04, 1.9106e-04, 2.8730e-04, 1.0889e-03, 1.9786e-03, 1.0415e-03,\n",
       "                       3.7862e-03, 2.4302e-03, 1.8724e-04, 9.9265e-04, 2.5805e-03, 4.7019e-04,\n",
       "                       1.6558e-03, 4.0086e-03, 7.9976e-04, 2.3863e-04, 2.3012e-03, 4.4773e-04,\n",
       "                       1.7395e-03, 1.4102e-03, 8.1531e-05, 4.1439e-04, 1.3190e-03, 2.2065e-03,\n",
       "                       1.3028e-03, 1.2868e-03, 2.3830e-03, 1.8831e-04, 3.2359e-04, 1.6988e-03,\n",
       "                       1.4900e-03, 5.2089e-04, 6.8690e-05, 1.6354e-03, 2.1484e-03, 1.0942e-03,\n",
       "                       2.7988e-03, 2.4352e-03, 1.2542e-03, 7.3375e-04, 1.0740e-03, 1.4577e-03,\n",
       "                       2.1491e-03, 2.1382e-03, 1.7980e-03, 6.9802e-04, 2.0411e-04, 2.5011e-03,\n",
       "                       7.2028e-04, 3.2702e-03, 1.3871e-03, 1.5991e-03, 4.0775e-04, 1.7517e-03,\n",
       "                       7.0405e-04, 4.6728e-03, 1.2762e-03, 4.4870e-04, 6.7034e-04, 2.1416e-03,\n",
       "                       2.7357e-03, 2.0363e-03, 1.2283e-03, 8.0054e-05, 1.4171e-03, 1.2853e-03,\n",
       "                       4.3460e-04, 7.7410e-04, 4.9776e-03, 9.0771e-05, 1.8202e-03, 4.9297e-04,\n",
       "                       3.8666e-03, 4.0001e-05, 1.6885e-03, 1.4846e-04, 1.1995e-03, 1.5355e-03,\n",
       "                       1.7554e-04, 8.6347e-04, 8.3119e-04, 1.9607e-03, 1.0885e-03, 3.0258e-05,\n",
       "                       1.9927e-03, 1.6537e-04, 6.1053e-04, 2.4346e-03, 3.4113e-03, 3.2080e-03,\n",
       "                       6.2137e-04, 1.1562e-04, 7.1851e-03, 3.5264e-03, 6.9161e-04, 1.0982e-03,\n",
       "                       8.6146e-04, 5.9824e-04, 1.9650e-03, 1.3058e-03, 1.8148e-04, 7.6939e-04,\n",
       "                       3.0357e-03, 5.3308e-04, 3.7400e-04, 4.3468e-04, 5.9680e-04, 8.1723e-04,\n",
       "                       1.2086e-03, 4.6013e-04, 1.6607e-03, 1.4707e-03, 1.5831e-03, 2.4256e-04,\n",
       "                       4.5323e-03, 2.6476e-03, 9.1084e-04, 6.9307e-04, 3.3470e-04, 1.1861e-04,\n",
       "                       9.1743e-04, 2.5193e-04, 2.8348e-03, 2.1504e-03, 8.0683e-03, 2.3040e-04,\n",
       "                       1.0026e-04, 6.1611e-04, 3.5800e-04, 1.7687e-04, 2.3684e-03, 1.4894e-03,\n",
       "                       1.3103e-03, 3.0409e-04, 2.7251e-03, 1.4805e-03, 7.1753e-04, 2.9143e-03,\n",
       "                       9.5810e-04, 2.6167e-04, 7.3069e-04, 6.6185e-04, 5.8601e-03, 3.9133e-04,\n",
       "                       2.2999e-03, 2.2090e-03, 6.2793e-03, 7.7238e-04, 1.2926e-03, 4.1889e-03,\n",
       "                       5.3731e-03, 1.3771e-03, 3.3407e-03, 9.9044e-04, 2.0522e-03, 8.3308e-04,\n",
       "                       2.4957e-04, 9.6173e-05, 1.6125e-03, 7.3951e-04, 2.6345e-03, 2.1950e-03,\n",
       "                       9.6824e-04, 2.5086e-04, 1.6974e-04, 5.2475e-03, 7.4565e-04, 1.6422e-03,\n",
       "                       1.1329e-03, 2.7554e-04, 4.6586e-05, 2.5364e-03], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer3.1.conv2.weight',\n",
       "               tensor([[[[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0064],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0077, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0073, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0060, -0.0062,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0016, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0094,  0.0060],\n",
       "                         [ 0.0000,  0.0395,  0.0258]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0157,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0034, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0035,  0.0009],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0194, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0054,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0529,  0.0000],\n",
       "                         [ 0.0000, -0.0477,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.1.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 1., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [1., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 1., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn2.weight',\n",
       "               tensor([ 9.1058e-02,  5.8880e-02,  6.1783e-02,  8.5044e-02,  9.4532e-02,\n",
       "                       -3.9348e-04,  9.3072e-02,  8.3069e-02,  1.1869e-01,  2.7260e-02,\n",
       "                       -5.7185e-03,  5.2112e-03,  1.1015e-01,  1.2865e-01,  1.0937e-01,\n",
       "                        4.6789e-02,  1.1127e-01,  6.7661e-02,  1.2890e-01,  6.4021e-02,\n",
       "                        5.1391e-02,  1.3341e-01,  4.9649e-02,  1.2373e-01,  1.0189e-01,\n",
       "                        1.3109e-01,  7.4752e-02,  1.5032e-01,  2.6239e-02,  1.4204e-01,\n",
       "                        9.3220e-02,  2.8961e-02,  5.3920e-02,  3.9293e-03,  1.2335e-01,\n",
       "                        9.0765e-02,  1.3109e-01,  1.0266e-02,  5.5285e-02,  7.9853e-02,\n",
       "                        7.2064e-02,  7.0512e-02,  2.4683e-02,  1.1715e-01,  1.0094e-01,\n",
       "                       -5.2473e-04,  3.2662e-02,  9.6695e-02,  9.7360e-02,  6.8948e-02,\n",
       "                        6.3686e-02,  5.6840e-02,  1.0835e-01,  4.3635e-02,  7.0523e-02,\n",
       "                        5.5948e-02,  3.2931e-02,  2.7939e-02,  5.3689e-02,  6.3968e-02,\n",
       "                       -4.5885e-04,  9.4304e-03,  4.9978e-02,  9.6120e-02,  1.2420e-01,\n",
       "                        8.3339e-02,  1.2208e-01,  5.2706e-02,  4.0396e-03,  1.3125e-01,\n",
       "                        8.1492e-02,  1.5430e-02,  1.7181e-02,  4.7266e-02,  8.9382e-02,\n",
       "                        4.2999e-02,  4.9275e-02,  7.5131e-02,  2.4305e-02,  5.4379e-02,\n",
       "                        8.4052e-02,  5.9441e-02,  5.5234e-02,  6.5576e-02,  9.8747e-02,\n",
       "                        3.6737e-02,  7.3658e-02,  6.6157e-02,  1.3314e-01,  3.9236e-02,\n",
       "                        2.3627e-02,  1.0417e-01,  1.0014e-01,  4.0846e-02,  4.2162e-02,\n",
       "                        2.2608e-02,  7.3198e-02,  1.6271e-02,  9.2130e-02,  7.0577e-02,\n",
       "                        6.1785e-02,  6.4702e-02,  6.3776e-02, -3.2240e-03,  1.4348e-03,\n",
       "                        1.0098e-01,  2.4443e-02,  9.6022e-02,  1.6258e-01,  2.0679e-02,\n",
       "                        5.9333e-02,  1.6219e-02,  1.3403e-01,  4.6682e-02,  1.3223e-01,\n",
       "                        1.4784e-02,  9.9983e-02,  1.0069e-01,  8.7621e-02,  1.0963e-01,\n",
       "                        9.8278e-02,  4.0134e-02,  1.2183e-01,  7.1244e-02,  1.3901e-01,\n",
       "                        2.0244e-02,  9.3308e-02,  9.5733e-02,  1.2999e-01, -9.3609e-04,\n",
       "                        4.0187e-02,  8.0177e-02,  2.5886e-02,  5.6836e-02, -2.2925e-03,\n",
       "                        1.1121e-01, -5.4183e-04,  7.3862e-02,  5.8991e-02,  4.6888e-03,\n",
       "                       -2.2163e-03,  1.6543e-01,  5.7209e-02, -6.5459e-04,  1.3568e-01,\n",
       "                        8.8369e-02,  4.1617e-02,  3.7994e-02,  9.0000e-02,  1.4074e-01,\n",
       "                        8.4474e-02,  2.5657e-02,  1.2292e-02,  8.7579e-02,  7.6289e-02,\n",
       "                        9.7338e-02,  5.8119e-02,  5.4175e-02,  6.8312e-02,  3.6297e-03,\n",
       "                        9.6663e-02,  3.6440e-02,  3.2205e-02,  7.8952e-02,  1.0978e-01,\n",
       "                        3.9270e-02,  6.4992e-02,  1.7624e-01,  7.3419e-02,  9.9792e-02,\n",
       "                        1.6552e-01,  3.6752e-02,  9.0631e-02,  1.1667e-01,  3.4328e-02,\n",
       "                        4.1689e-02,  8.5955e-02,  5.5735e-02,  3.7680e-02,  9.7232e-02,\n",
       "                        1.2393e-01,  1.0716e-01,  5.9479e-02,  8.3812e-02,  9.2055e-02,\n",
       "                        8.2997e-02,  6.3419e-02,  1.4420e-01,  1.5992e-01,  1.2498e-01,\n",
       "                        7.1991e-02,  1.8181e-02,  4.6428e-02,  1.1482e-01,  3.5415e-02,\n",
       "                        3.5770e-02,  1.3741e-01,  3.2718e-02,  1.2508e-01,  9.7403e-02,\n",
       "                        1.2243e-01,  9.1541e-02,  5.3734e-02,  7.7118e-02,  1.0258e-01,\n",
       "                        1.4455e-01,  9.6926e-02,  1.0827e-01,  3.0562e-02,  7.7534e-02,\n",
       "                        1.2235e-02,  7.7046e-02, -1.8482e-02,  2.7538e-02,  9.0657e-02,\n",
       "                        6.8417e-02,  7.4108e-02,  5.9838e-02,  9.9246e-02,  7.3271e-02,\n",
       "                        5.5197e-02,  6.5310e-02,  8.6899e-02,  1.1401e-01,  1.0770e-01,\n",
       "                        4.0438e-02,  6.4011e-02,  1.0644e-01,  6.9222e-02,  1.3116e-01,\n",
       "                        1.0039e-01,  8.2369e-02,  1.1746e-01,  6.5277e-02,  8.0863e-02,\n",
       "                        1.5553e-05,  2.9726e-02,  8.2762e-02,  2.1399e-03,  6.7175e-02,\n",
       "                        3.7184e-02,  5.4765e-02,  1.4653e-01,  5.4700e-02,  5.1875e-03,\n",
       "                        8.9324e-02,  9.6347e-02,  6.9926e-02,  1.6919e-01,  7.2785e-02,\n",
       "                        7.3528e-02,  4.0962e-02,  8.3504e-02,  5.5090e-02,  3.4474e-02,\n",
       "                        9.9501e-02], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn2.bias',\n",
       "               tensor([-0.0179, -0.0003,  0.0013, -0.0699, -0.0722,  0.0026, -0.0775, -0.0261,\n",
       "                       -0.0409,  0.0100,  0.0097,  0.0020,  0.0521,  0.0221, -0.0984,  0.0433,\n",
       "                       -0.0580, -0.0549, -0.0061, -0.0018, -0.0357, -0.0557, -0.0282, -0.0573,\n",
       "                       -0.0942, -0.0925,  0.0117, -0.0502,  0.0322, -0.0763, -0.0711, -0.0055,\n",
       "                       -0.0131,  0.0002, -0.0838, -0.0526, -0.1167,  0.0284, -0.0098, -0.0885,\n",
       "                       -0.0415, -0.0436,  0.0179, -0.0448, -0.0841,  0.0028,  0.0198, -0.0877,\n",
       "                       -0.0808, -0.0304, -0.0201, -0.0517,  0.0572,  0.0260, -0.0074, -0.0177,\n",
       "                       -0.0013,  0.0070, -0.0700, -0.0209,  0.0102,  0.0025, -0.0063, -0.0063,\n",
       "                       -0.0336, -0.0565, -0.0730, -0.0536,  0.0050, -0.0652, -0.0514,  0.0095,\n",
       "                        0.0064, -0.0059, -0.0506,  0.0025,  0.0036, -0.0261, -0.0037, -0.0327,\n",
       "                        0.0046, -0.0661, -0.0073,  0.0109, -0.0698,  0.0071, -0.0430, -0.0690,\n",
       "                       -0.0124,  0.0267,  0.0228, -0.0702, -0.0747, -0.0002,  0.0052,  0.0093,\n",
       "                       -0.0761,  0.0033, -0.0228, -0.0818, -0.0467, -0.0700, -0.0428,  0.0071,\n",
       "                        0.0012, -0.0973, -0.0100, -0.0498, -0.0404, -0.0059, -0.0039,  0.0009,\n",
       "                       -0.0781, -0.0361, -0.0482,  0.0097, -0.0211, -0.0157, -0.0628, -0.0310,\n",
       "                       -0.0192, -0.0113, -0.1063, -0.0435, -0.0511,  0.0205, -0.0159, -0.0295,\n",
       "                       -0.0736,  0.0028, -0.0111, -0.0955,  0.0196, -0.0836,  0.0082, -0.0387,\n",
       "                        0.0059, -0.0738, -0.0315, -0.0033,  0.0015, -0.0533, -0.0244,  0.0029,\n",
       "                       -0.0354, -0.0843, -0.0003,  0.0063, -0.0398, -0.0045, -0.0916, -0.0002,\n",
       "                        0.0090, -0.1241, -0.0703, -0.0205, -0.0242,  0.0016,  0.0214,  0.0104,\n",
       "                       -0.0798, -0.0533, -0.0019,  0.0092, -0.0366, -0.0820, -0.0081, -0.0795,\n",
       "                       -0.0907, -0.0435, -0.1372,  0.0127,  0.0141, -0.0989, -0.0621, -0.0531,\n",
       "                       -0.0580,  0.0153, -0.0366, -0.0077, -0.0493, -0.0109, -0.0431, -0.0447,\n",
       "                       -0.0221, -0.0244, -0.0872, -0.0071,  0.0063,  0.0002, -0.0353,  0.0168,\n",
       "                        0.0344, -0.0611,  0.0184,  0.0070, -0.0159,  0.0205, -0.0527,  0.0091,\n",
       "                       -0.0083, -0.0774,  0.0144, -0.0462, -0.0878, -0.1268,  0.0068, -0.0978,\n",
       "                        0.0203, -0.0633,  0.0146, -0.0808,  0.0025, -0.0428, -0.0878, -0.0151,\n",
       "                       -0.0306, -0.0166, -0.0670, -0.0377,  0.0277, -0.0749, -0.0158, -0.0921,\n",
       "                       -0.0272,  0.0064, -0.0540, -0.0725,  0.0087, -0.0723, -0.0150, -0.0473,\n",
       "                       -0.0230, -0.0574, -0.0285,  0.0066,  0.0236, -0.0721,  0.0057, -0.0318,\n",
       "                        0.0003, -0.0384,  0.0008, -0.0347,  0.0023, -0.0344, -0.0673, -0.0613,\n",
       "                       -0.1366, -0.0020, -0.0048, -0.0271,  0.0099, -0.0128,  0.0019, -0.0473],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn2.running_mean',\n",
       "               tensor([-8.6547e-03, -5.8950e-03, -4.2593e-03, -8.5481e-03, -6.2940e-03,\n",
       "                       -1.6473e-05, -1.1101e-02, -6.1158e-03, -6.1807e-03, -4.7480e-03,\n",
       "                       -2.1907e-04, -6.4523e-05,  7.6442e-03, -8.6769e-04, -1.3681e-02,\n",
       "                        6.7352e-03, -1.4975e-02, -4.3253e-03, -2.9032e-02, -1.2910e-02,\n",
       "                       -1.1199e-02, -1.0564e-02,  2.1376e-03, -7.9140e-03, -1.3416e-02,\n",
       "                       -1.3144e-02, -1.5767e-02,  3.5228e-03, -7.8508e-04, -1.0394e-02,\n",
       "                       -1.5505e-02, -2.2792e-03, -8.6486e-03, -5.6052e-45, -9.3224e-03,\n",
       "                       -9.8193e-03, -1.1882e-02,  2.2308e-04, -3.4902e-03, -1.1349e-02,\n",
       "                       -1.5346e-02, -7.9309e-03, -3.3161e-03, -1.2161e-02, -9.0012e-03,\n",
       "                       -1.3937e-04,  7.7978e-04, -2.0584e-02, -4.2107e-03,  3.1556e-04,\n",
       "                       -9.3987e-03, -4.4743e-04,  9.0612e-03,  2.7168e-03, -6.6323e-03,\n",
       "                       -1.1793e-02, -4.1267e-03, -4.3962e-03,  1.4292e-03, -7.7346e-03,\n",
       "                        7.9882e-05,  2.7837e-05, -9.3239e-03, -2.0095e-03, -4.5039e-03,\n",
       "                       -1.4884e-02, -6.8745e-03, -5.8444e-03, -9.2488e-05, -1.5707e-02,\n",
       "                       -6.1087e-03, -4.0228e-04, -1.1805e-03, -5.9765e-03, -9.4532e-03,\n",
       "                       -3.7282e-03, -2.2456e-03, -8.5755e-03, -3.6206e-03, -3.3103e-04,\n",
       "                       -4.4855e-03, -5.3267e-03, -9.1736e-03, -7.7187e-03, -1.4155e-02,\n",
       "                       -1.1509e-03, -7.6808e-03, -3.4094e-03, -2.3006e-02, -9.9420e-04,\n",
       "                       -6.3784e-04, -1.6506e-02, -9.5307e-03, -7.8299e-03, -8.6456e-04,\n",
       "                       -2.2986e-03, -8.1570e-03, -9.3763e-04, -9.4694e-03, -1.4216e-02,\n",
       "                       -9.3809e-03, -6.2380e-04, -5.8752e-03, -9.7749e-06, -5.6052e-45,\n",
       "                       -1.8824e-02, -2.5620e-03, -9.2253e-03,  2.1686e-03, -2.7389e-04,\n",
       "                       -6.6813e-03, -3.8155e-04, -9.9971e-03, -1.1360e-03, -3.3040e-03,\n",
       "                       -2.5494e-04, -1.2532e-02, -7.0047e-03, -6.8632e-03, -1.9849e-03,\n",
       "                       -2.2278e-03, -3.1050e-03, -9.4249e-03, -1.2164e-02, -1.2070e-02,\n",
       "                       -3.8120e-04, -2.9171e-03, -1.4879e-03, -8.8935e-03, -1.9032e-04,\n",
       "                       -3.1389e-03, -4.9865e-03, -5.0677e-04, -9.6679e-03, -1.6741e-04,\n",
       "                       -9.0979e-03, -1.0709e-04, -3.5020e-03, -9.4566e-03, -3.4441e-03,\n",
       "                       -2.1478e-04, -6.7040e-03, -7.0598e-03, -5.6947e-04, -1.1776e-02,\n",
       "                       -7.8582e-03, -7.7861e-03, -7.6972e-03, -7.4957e-03, -7.6136e-03,\n",
       "                       -9.5742e-03, -1.5973e-03, -1.5642e-04, -1.5819e-02, -9.1812e-03,\n",
       "                       -6.2326e-03, -1.7392e-03,  9.3501e-04, -2.5598e-03, -3.1833e-04,\n",
       "                       -5.6075e-03, -1.3963e-03, -2.6053e-03, -1.4895e-02, -3.5912e-03,\n",
       "                       -3.2297e-03, -5.9263e-03, -2.5890e-02, -1.1884e-02, -6.4348e-03,\n",
       "                       -9.1846e-03,  7.3747e-04, -7.0067e-03, -8.7661e-03,  7.1748e-03,\n",
       "                       -3.2564e-03, -2.7033e-03,  2.0980e-03, -5.1768e-04, -1.3980e-04,\n",
       "                       -1.1377e-02,  9.4070e-03, -5.6104e-03, -2.5842e-03, -1.5136e-02,\n",
       "                       -2.9022e-03, -1.4389e-02, -4.8623e-03, -7.9062e-03, -8.0595e-03,\n",
       "                       -1.1722e-02,  3.9152e-04, -6.1542e-03, -4.4737e-03, -5.5524e-03,\n",
       "                       -8.3389e-04, -8.4310e-03,  1.0015e-03, -6.2646e-03,  2.0412e-03,\n",
       "                       -5.3964e-03, -9.2573e-03, -1.0245e-02, -5.8782e-03, -2.1235e-03,\n",
       "                       -1.7421e-02, -4.6716e-04, -3.1230e-03, -1.1718e-03, -1.4319e-02,\n",
       "                       -4.4027e-04, -9.0403e-03, -6.9468e-05, -1.3627e-03, -8.3140e-03,\n",
       "                       -3.2828e-03, -7.1087e-03, -9.2550e-03, -1.1017e-02, -6.8639e-03,\n",
       "                       -6.8571e-03, -6.5868e-03, -9.8390e-03, -8.4270e-03, -1.0077e-02,\n",
       "                       -9.5472e-03, -5.2182e-03, -5.8425e-03, -9.0018e-03, -6.6153e-04,\n",
       "                       -1.3283e-02, -9.5117e-03, -1.7344e-02, -1.0943e-02, -5.5657e-03,\n",
       "                       -1.5115e-03, -1.7579e-03, -6.2537e-04, -1.2472e-04, -8.8152e-03,\n",
       "                       -4.7437e-03, -8.0165e-03, -2.2979e-02, -6.1924e-03, -9.6399e-05,\n",
       "                       -1.4015e-02, -8.4303e-03, -5.2843e-03, -1.7751e-02, -8.9946e-03,\n",
       "                       -2.2151e-05, -1.3305e-03,  3.1546e-04, -1.0272e-02, -4.6637e-03,\n",
       "                       -8.9227e-03], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn2.running_var',\n",
       "               tensor([1.7110e-04, 7.5747e-05, 1.0629e-04, 1.0681e-04, 1.2422e-04, 8.3691e-09,\n",
       "                       1.3052e-04, 1.4093e-04, 1.5751e-04, 3.2826e-05, 1.0774e-06, 1.0152e-07,\n",
       "                       1.4540e-04, 3.6420e-04, 2.5437e-04, 7.5706e-05, 2.0078e-04, 9.8814e-05,\n",
       "                       4.7469e-04, 1.1454e-04, 1.4816e-04, 1.9386e-04, 1.9242e-05, 2.4364e-04,\n",
       "                       3.0366e-04, 3.7687e-04, 3.0932e-04, 1.4917e-04, 4.9376e-05, 2.1359e-04,\n",
       "                       1.3834e-04, 2.2667e-05, 7.8147e-05, 5.6052e-45, 9.5959e-05, 1.7260e-04,\n",
       "                       3.8798e-04, 1.0382e-05, 3.9953e-05, 1.4993e-04, 2.1361e-04, 8.7835e-05,\n",
       "                       2.7436e-05, 1.9836e-04, 1.8886e-04, 8.3962e-08, 3.9765e-05, 2.9103e-04,\n",
       "                       7.7936e-05, 4.6660e-05, 2.3719e-04, 1.9735e-05, 8.4324e-05, 4.9463e-05,\n",
       "                       1.1885e-04, 7.1277e-05, 3.2429e-05, 4.4616e-05, 5.7068e-05, 2.3222e-04,\n",
       "                       9.5873e-08, 2.4845e-08, 8.2647e-05, 1.4991e-04, 3.0931e-04, 2.2479e-04,\n",
       "                       1.5044e-04, 6.3291e-05, 8.4726e-08, 3.8598e-04, 1.2590e-04, 2.8263e-06,\n",
       "                       1.4162e-05, 3.6483e-05, 1.7878e-04, 7.3448e-05, 3.0538e-05, 1.4378e-04,\n",
       "                       4.4641e-05, 3.9766e-05, 1.1981e-04, 1.0979e-04, 8.2233e-05, 6.8701e-05,\n",
       "                       2.9194e-04, 1.3415e-05, 1.3447e-04, 4.4590e-05, 4.3297e-04, 3.7510e-05,\n",
       "                       1.1324e-05, 2.7092e-04, 4.1380e-04, 1.9208e-04, 2.8430e-05, 1.7948e-05,\n",
       "                       1.6524e-04, 4.1538e-06, 1.3284e-04, 1.8257e-04, 7.2874e-05, 9.6653e-05,\n",
       "                       6.4113e-05, 1.1881e-07, 5.6052e-45, 3.3071e-04, 2.9317e-05, 1.2725e-04,\n",
       "                       1.5269e-04, 9.1162e-06, 7.7056e-05, 1.4197e-06, 3.0280e-04, 5.6772e-06,\n",
       "                       1.9449e-04, 3.1504e-07, 9.2806e-05, 9.2875e-05, 6.4269e-05, 1.5477e-04,\n",
       "                       9.2032e-05, 3.7419e-05, 2.2302e-04, 2.2717e-04, 1.7373e-04, 1.4068e-05,\n",
       "                       9.0491e-05, 1.1145e-04, 2.2443e-04, 2.4459e-08, 6.2620e-05, 7.2925e-05,\n",
       "                       5.8468e-05, 8.2732e-05, 2.6257e-07, 1.1027e-04, 6.3830e-08, 5.5000e-05,\n",
       "                       1.2986e-04, 5.3419e-05, 4.9441e-07, 1.3794e-04, 3.9660e-05, 1.8275e-06,\n",
       "                       9.3078e-05, 1.0893e-04, 6.7834e-05, 7.5425e-05, 1.8476e-04, 1.1156e-04,\n",
       "                       1.6271e-04, 2.7466e-05, 3.3625e-07, 2.6799e-04, 1.4766e-04, 7.5064e-05,\n",
       "                       6.7003e-05, 3.1241e-05, 8.3553e-05, 1.3010e-06, 1.1049e-04, 9.1655e-05,\n",
       "                       5.4253e-05, 1.6854e-04, 1.7793e-04, 4.7846e-05, 1.3454e-04, 5.8834e-04,\n",
       "                       1.2435e-04, 9.7511e-05, 3.7426e-04, 2.2065e-05, 9.4524e-05, 2.0087e-04,\n",
       "                       6.4718e-05, 3.7175e-05, 1.0381e-04, 6.8687e-05, 1.0799e-05, 8.9448e-05,\n",
       "                       1.2426e-04, 1.0689e-04, 5.7688e-05, 4.3123e-05, 1.6105e-04, 1.0358e-04,\n",
       "                       1.6678e-04, 2.0683e-04, 1.4735e-04, 1.1818e-04, 2.8087e-04, 8.0067e-06,\n",
       "                       5.5055e-05, 1.5902e-04, 5.6677e-05, 6.9224e-06, 9.6073e-05, 4.6355e-05,\n",
       "                       1.5327e-04, 1.8472e-04, 1.4958e-04, 1.2772e-04, 1.2804e-04, 7.8554e-05,\n",
       "                       7.6362e-05, 2.2098e-04, 1.1180e-04, 1.9341e-04, 1.5071e-05, 1.3126e-04,\n",
       "                       5.5428e-07, 1.0384e-04, 1.6790e-07, 5.1173e-06, 6.8200e-05, 3.9852e-05,\n",
       "                       1.1440e-04, 1.1899e-04, 1.6214e-04, 8.4254e-05, 4.9355e-05, 9.9387e-05,\n",
       "                       1.0351e-04, 1.6460e-04, 1.0014e-04, 1.0896e-04, 3.2175e-05, 1.8280e-04,\n",
       "                       1.1662e-04, 1.5307e-04, 3.3816e-04, 1.3357e-04, 4.5348e-04, 1.6271e-04,\n",
       "                       8.4523e-05, 5.0874e-06, 3.6871e-05, 4.7321e-05, 1.4701e-07, 1.2967e-04,\n",
       "                       8.6450e-05, 8.3457e-05, 3.7783e-04, 7.1444e-05, 3.0723e-07, 1.1166e-04,\n",
       "                       2.4962e-04, 6.6357e-05, 3.5759e-04, 1.2489e-04, 2.8791e-04, 6.6090e-05,\n",
       "                       6.5545e-05, 1.3109e-04, 3.9577e-05, 1.7556e-04], device='cuda:0')),\n",
       "              ('model.model.layer3.1.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer4.0.conv1.weight',\n",
       "               tensor([[[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0018, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0492,  0.0239]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0199,  0.0068],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0143],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0116,  0.0100],\n",
       "                         [ 0.0000,  0.0000,  0.0748]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0073],\n",
       "                         [ 0.0000,  0.0000, -0.0382]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0481, -0.0000],\n",
       "                         [-0.0000,  0.0495,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0181,  0.0325],\n",
       "                         [-0.0000,  0.0200,  0.0286]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0141]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0347, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0127,  0.0424],\n",
       "                         [ 0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0381],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 1., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn1.weight',\n",
       "               tensor([0.1399, 0.0925, 0.1151, 0.1527, 0.1397, 0.1544, 0.1235, 0.1393, 0.1257,\n",
       "                       0.1120, 0.1063, 0.1100, 0.1249, 0.1253, 0.1035, 0.1430, 0.1246, 0.0156,\n",
       "                       0.1372, 0.1284, 0.1339, 0.1404, 0.1016, 0.1115, 0.0986, 0.1060, 0.1253,\n",
       "                       0.1436, 0.1002, 0.1516, 0.1161, 0.1363, 0.1557, 0.1268, 0.1050, 0.1360,\n",
       "                       0.1238, 0.1246, 0.1577, 0.1235, 0.1011, 0.1533, 0.1207, 0.1501, 0.1575,\n",
       "                       0.1524, 0.1131, 0.1408, 0.0976, 0.1343, 0.1062, 0.0972, 0.1123, 0.1664,\n",
       "                       0.1270, 0.1424, 0.1067, 0.1502, 0.1317, 0.1380, 0.1394, 0.1131, 0.1321,\n",
       "                       0.1111, 0.0923, 0.1506, 0.1038, 0.0995, 0.1376, 0.1281, 0.0718, 0.1446,\n",
       "                       0.1175, 0.1461, 0.1258, 0.1214, 0.1383, 0.0997, 0.1554, 0.1459, 0.1147,\n",
       "                       0.1342, 0.1566, 0.1413, 0.1178, 0.1133, 0.1279, 0.1169, 0.1364, 0.1272,\n",
       "                       0.0729, 0.1304, 0.1798, 0.1516, 0.0999, 0.1129, 0.0990, 0.1545, 0.0823,\n",
       "                       0.1046, 0.1481, 0.1335, 0.1503, 0.1531, 0.1318, 0.1269, 0.1574, 0.1196,\n",
       "                       0.1215, 0.1040, 0.1451, 0.1494, 0.1326, 0.0751, 0.1143, 0.1503, 0.1217,\n",
       "                       0.1073, 0.1213, 0.1147, 0.1410, 0.1309, 0.1466, 0.1377, 0.1088, 0.1312,\n",
       "                       0.1347, 0.0687, 0.1283, 0.1665, 0.0291, 0.1204, 0.1518, 0.1214, 0.1009,\n",
       "                       0.1254, 0.1487, 0.1439, 0.1349, 0.1199, 0.1488, 0.1232, 0.1548, 0.1262,\n",
       "                       0.1493, 0.1361, 0.1392, 0.1408, 0.1236, 0.1327, 0.0885, 0.1033, 0.1319,\n",
       "                       0.0846, 0.1108, 0.1275, 0.1358, 0.1261, 0.0874, 0.1027, 0.1336, 0.1411,\n",
       "                       0.1479, 0.1212, 0.1259, 0.1432, 0.0938, 0.1544, 0.1202, 0.1160, 0.1123,\n",
       "                       0.1691, 0.1328, 0.1176, 0.1345, 0.1265, 0.1429, 0.0887, 0.0937, 0.1494,\n",
       "                       0.1461, 0.1453, 0.1201, 0.1041, 0.1524, 0.1278, 0.1366, 0.1410, 0.0950,\n",
       "                       0.1024, 0.0771, 0.1482, 0.1507, 0.1164, 0.1206, 0.1397, 0.1407, 0.1395,\n",
       "                       0.1329, 0.1159, 0.1336, 0.0866, 0.1441, 0.1305, 0.1180, 0.1250, 0.1417,\n",
       "                       0.1188, 0.0956, 0.0746, 0.1355, 0.0962, 0.1223, 0.1134, 0.1177, 0.1081,\n",
       "                       0.1170, 0.1527, 0.1417, 0.1161, 0.1564, 0.1320, 0.0764, 0.1441, 0.1218,\n",
       "                       0.1252, 0.1558, 0.1343, 0.1388, 0.1227, 0.1289, 0.0855, 0.1146, 0.1340,\n",
       "                       0.1198, 0.1467, 0.0973, 0.1517, 0.1317, 0.1323, 0.1260, 0.1188, 0.1596,\n",
       "                       0.1024, 0.1449, 0.1058, 0.1227, 0.1330, 0.1493, 0.1371, 0.1101, 0.1558,\n",
       "                       0.1059, 0.1346, 0.1646, 0.1345, 0.1402, 0.1197, 0.1390, 0.0965, 0.0947,\n",
       "                       0.1614, 0.1518, 0.1431, 0.1028, 0.1191, 0.1295, 0.1631, 0.1059, 0.1260,\n",
       "                       0.1034, 0.1658, 0.1046, 0.1482, 0.1525, 0.1235, 0.0900, 0.1503, 0.0981,\n",
       "                       0.1459, 0.1279, 0.1257, 0.1501, 0.1255, 0.1019, 0.0820, 0.1276, 0.1014,\n",
       "                       0.1085, 0.1438, 0.1313, 0.1385, 0.0521, 0.1200, 0.0687, 0.1136, 0.1312,\n",
       "                       0.1405, 0.1362, 0.1051, 0.1207, 0.1341, 0.1489, 0.1322, 0.1173, 0.1279,\n",
       "                       0.1095, 0.0950, 0.1108, 0.0920, 0.0700, 0.1385, 0.1047, 0.0805, 0.1283,\n",
       "                       0.0962, 0.1734, 0.1341, 0.1221, 0.1503, 0.1586, 0.1076, 0.0842, 0.1485,\n",
       "                       0.1427, 0.1088, 0.0880, 0.1086, 0.1732, 0.1357, 0.1567, 0.1102, 0.0951,\n",
       "                       0.1691, 0.1180, 0.1241, 0.1017, 0.0778, 0.0984, 0.1406, 0.0896, 0.1351,\n",
       "                       0.1150, 0.1504, 0.1959, 0.1287, 0.1313, 0.1334, 0.0681, 0.0984, 0.1238,\n",
       "                       0.0764, 0.1201, 0.0907, 0.1385, 0.1066, 0.1310, 0.1101, 0.1330, 0.0932,\n",
       "                       0.1321, 0.0654, 0.1082, 0.0833, 0.0774, 0.1135, 0.1115, 0.0913, 0.1064,\n",
       "                       0.1439, 0.1282, 0.1135, 0.1118, 0.1116, 0.1224, 0.1011, 0.1460, 0.0979,\n",
       "                       0.1510, 0.1670, 0.1003, 0.1412, 0.1387, 0.1228, 0.1215, 0.1321, 0.1215,\n",
       "                       0.1028, 0.1029, 0.1161, 0.1012, 0.1393, 0.1703, 0.1230, 0.1122, 0.1288,\n",
       "                       0.1389, 0.1084, 0.1222, 0.1396, 0.0827, 0.1309, 0.1118, 0.1270, 0.1133,\n",
       "                       0.1223, 0.1545, 0.1263, 0.1015, 0.1238, 0.1375, 0.1435, 0.1212, 0.1372,\n",
       "                       0.1130, 0.1501, 0.1196, 0.1117, 0.1488, 0.1411, 0.1257, 0.1327, 0.1281,\n",
       "                       0.1353, 0.1210, 0.1450, 0.1205, 0.1133, 0.0745, 0.1117, 0.1149, 0.1355,\n",
       "                       0.1343, 0.0777, 0.0727, 0.1304, 0.1317, 0.1308, 0.0746, 0.1255, 0.1324,\n",
       "                       0.1518, 0.1259, 0.1225, 0.1428, 0.1398, 0.1114, 0.1124, 0.0893, 0.1314,\n",
       "                       0.1266, 0.1921, 0.1209, 0.0852, 0.1440, 0.1209, 0.1090, 0.1189, 0.1547,\n",
       "                       0.1284, 0.1288, 0.1420, 0.0180, 0.1372, 0.0979, 0.1127, 0.1225, 0.1305,\n",
       "                       0.1410, 0.1241, 0.0970, 0.1442, 0.1561, 0.1156, 0.0949, 0.1323, 0.1611,\n",
       "                       0.0910, 0.1373, 0.1207, 0.1411, 0.0915, 0.1103, 0.1122, 0.1310, 0.0600,\n",
       "                       0.1494, 0.1250, 0.1018, 0.0102, 0.1155, 0.1342, 0.1230, 0.1252, 0.0956,\n",
       "                       0.1326, 0.1395, 0.1280, 0.1158, 0.1288, 0.1066, 0.1062, 0.1209, 0.1637,\n",
       "                       0.1189, 0.0762, 0.1220, 0.0992, 0.1195, 0.1461, 0.1420, 0.1408],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn1.bias',\n",
       "               tensor([-8.9157e-02, -3.9858e-02, -7.3783e-02, -8.4438e-02, -6.0912e-02,\n",
       "                       -8.6252e-02, -6.8163e-02, -9.4933e-02, -5.0892e-02, -5.9931e-02,\n",
       "                       -5.1029e-02, -3.1067e-02, -6.0726e-02, -7.6543e-02, -9.4375e-02,\n",
       "                       -1.0256e-01, -9.2744e-02, -7.1217e-02, -9.6689e-02, -1.0565e-01,\n",
       "                       -8.2784e-02, -9.3513e-02, -5.7273e-02, -1.0012e-01, -2.7651e-02,\n",
       "                       -5.1441e-02, -1.0484e-01, -4.5744e-02, -4.5303e-02, -8.5163e-02,\n",
       "                       -1.0104e-01, -6.5821e-02, -8.3031e-02, -5.2746e-02, -6.2295e-02,\n",
       "                       -3.5290e-02, -8.0995e-02, -8.8655e-02, -8.5096e-02, -9.1786e-02,\n",
       "                       -1.0566e-04, -9.3713e-02, -3.2539e-02, -1.0189e-01, -1.6923e-01,\n",
       "                       -1.3674e-01, -7.7540e-02, -1.2019e-01, -8.8803e-02, -5.4502e-02,\n",
       "                       -5.8379e-02, -9.7773e-02, -6.3639e-02, -9.4705e-02, -9.8146e-02,\n",
       "                       -8.7211e-02, -5.1164e-02, -1.6778e-01, -7.6726e-02, -7.1536e-02,\n",
       "                       -3.1726e-02, -7.5070e-02, -9.7649e-02, -6.4116e-02, -5.2675e-02,\n",
       "                       -7.7528e-02, -7.4132e-02, -3.9910e-02, -1.2738e-01, -7.9121e-02,\n",
       "                       -4.1745e-02, -9.8613e-02, -7.6040e-02, -1.1451e-01, -1.2122e-01,\n",
       "                       -3.8692e-02, -5.3140e-02, -6.5770e-02, -8.4298e-02, -8.5313e-02,\n",
       "                       -6.3879e-02, -1.3737e-01, -8.3981e-02, -8.7632e-02, -5.3627e-02,\n",
       "                       -3.7982e-02, -9.2751e-02, -1.0694e-01, -1.1725e-01, -7.6204e-02,\n",
       "                       -3.7972e-02, -3.8625e-02, -1.0537e-01, -1.5032e-01, -5.8050e-02,\n",
       "                       -3.9173e-02, -5.2861e-02, -1.0761e-01, -1.7572e-02, -3.0142e-02,\n",
       "                       -1.4356e-01, -7.8630e-02, -1.0326e-01, -8.9756e-02, -1.1115e-01,\n",
       "                       -6.9402e-02, -7.4099e-02, -1.1776e-01, -9.2292e-02, -5.8322e-02,\n",
       "                       -1.1730e-01, -9.9790e-02, -7.4034e-02, -3.8999e-02, -8.2386e-02,\n",
       "                       -1.0595e-01, -5.4015e-02, -7.1763e-02, -5.7786e-02, -4.2992e-02,\n",
       "                       -1.0056e-01, -7.5199e-02, -1.6063e-01, -8.2630e-02, -5.0027e-02,\n",
       "                       -9.0232e-02, -6.9271e-02, -4.3277e-02, -5.8523e-02, -9.4526e-02,\n",
       "                       -3.5155e-02, -7.7523e-02, -8.2726e-02, -4.1591e-02, -7.1466e-02,\n",
       "                       -8.8337e-02, -7.4898e-02, -1.0469e-01, -8.8957e-02, -1.1719e-01,\n",
       "                       -9.9271e-02, -1.0963e-01, -1.1957e-01, -1.0939e-03, -1.0250e-01,\n",
       "                       -9.4791e-02, -8.5477e-02, -8.0134e-02, -6.2769e-02, -1.0292e-01,\n",
       "                       -2.3425e-02, -6.2344e-02, -1.1703e-01, -1.5566e-02, -7.4413e-02,\n",
       "                       -6.8242e-02, -1.0095e-01, -9.7241e-02, -5.7527e-02, -5.5086e-02,\n",
       "                       -1.3177e-01, -9.1475e-02, -7.5150e-02, -7.6466e-02, -1.0653e-01,\n",
       "                       -6.9733e-02, -6.0200e-02, -9.5097e-02, -7.8192e-02, -5.1351e-02,\n",
       "                       -2.8444e-02, -1.7247e-01, -6.8842e-02, -3.6478e-02, -1.4066e-01,\n",
       "                       -3.8167e-02, -1.0251e-01, -5.4293e-02, -3.1267e-02, -5.8162e-02,\n",
       "                       -4.5727e-02, -1.0847e-01, -5.7884e-02, -5.9719e-02, -9.6138e-02,\n",
       "                       -7.7544e-02, -8.5196e-02, -1.0708e-01, -2.1154e-02, -6.1151e-02,\n",
       "                       -3.2670e-02, -8.4646e-02, -9.3761e-02, -4.4452e-02,  1.1258e-02,\n",
       "                       -8.9996e-02, -8.4880e-02, -1.1423e-01, -6.7164e-02, -4.7753e-02,\n",
       "                       -8.2789e-02, -4.3576e-02, -6.4554e-02, -8.6040e-02, -3.4612e-02,\n",
       "                       -8.7322e-02, -6.0749e-02, -7.2453e-02, -5.9586e-02, -6.4349e-02,\n",
       "                       -6.5177e-02, -8.1790e-02, -6.3181e-02, -3.8114e-02, -7.7592e-02,\n",
       "                       -7.2300e-02, -3.9701e-02, -6.5852e-02, -9.8883e-02, -7.5704e-02,\n",
       "                       -4.9789e-02, -1.1742e-01, -8.3410e-02, -1.2530e-01, -9.3595e-02,\n",
       "                       -9.7492e-02, -1.1790e-01, -7.3437e-02, -7.8939e-02, -8.4837e-02,\n",
       "                       -9.9550e-02, -5.6404e-02, -1.5482e-02, -9.1972e-02, -6.4453e-02,\n",
       "                       -1.0905e-01, -3.2747e-02, -1.1020e-01, -9.4484e-02, -1.2010e-01,\n",
       "                       -4.6656e-02, -7.2499e-02, -1.0933e-01, -4.3051e-02, -1.0602e-01,\n",
       "                       -3.8803e-02, -6.2634e-02, -8.1030e-02, -1.2583e-01, -1.1272e-01,\n",
       "                       -6.4070e-02, -1.2395e-01, -2.9493e-02, -1.3445e-01, -6.8168e-02,\n",
       "                       -6.4709e-02, -6.6386e-02, -5.0103e-02, -2.3799e-02, -2.0501e-02,\n",
       "                        2.3966e-02, -1.1329e-01, -5.0806e-02, -6.8978e-02, -4.9573e-02,\n",
       "                       -5.7679e-02, -5.5586e-02, -1.3570e-01, -5.3470e-02, -5.8958e-02,\n",
       "                       -8.8756e-02, -1.2855e-01, -6.2207e-02, -8.3592e-02, -1.0587e-01,\n",
       "                       -7.6957e-02, -3.3679e-02, -1.6427e-01, -2.9233e-02, -8.7315e-02,\n",
       "                       -1.9661e-02, -1.0367e-01, -8.3267e-02, -1.0568e-01, -4.8903e-02,\n",
       "                       -5.3789e-02, -9.4032e-02, -4.6527e-02, -7.1598e-02, -1.3545e-01,\n",
       "                       -3.2872e-02, -6.6032e-02, -4.8806e-02, -2.2887e-02, -1.0161e-01,\n",
       "                       -1.0176e-01, -5.8431e-02, -3.7031e-02, -1.2280e-01, -2.3062e-02,\n",
       "                       -1.0138e-01, -6.2426e-02, -1.3258e-01, -5.6581e-02, -7.2027e-02,\n",
       "                       -4.1172e-02, -3.0196e-02,  1.4340e-02, -1.4178e-02, -3.0181e-02,\n",
       "                       -2.3895e-02, -8.4098e-02, -3.2038e-02, -9.8257e-03, -9.9467e-02,\n",
       "                       -5.2890e-02, -1.3150e-01, -5.9400e-02, -7.1449e-02, -1.2504e-01,\n",
       "                       -1.0721e-01, -7.3100e-02, -3.8392e-02, -8.7406e-02, -6.5852e-02,\n",
       "                       -1.0799e-01, -4.0522e-02, -8.1987e-02, -1.6828e-01, -8.4707e-02,\n",
       "                       -6.2990e-02, -7.1241e-02, -7.3524e-02, -1.5775e-01, -7.3403e-02,\n",
       "                       -4.5506e-02, -5.8470e-02, -4.4412e-02, -3.9616e-02, -6.2635e-02,\n",
       "                       -5.8940e-02, -8.2260e-02, -1.1374e-01, -4.6188e-02, -1.1591e-01,\n",
       "                       -6.8695e-02, -1.2464e-01, -7.4430e-02, -1.0064e-02, -4.1194e-02,\n",
       "                       -8.6309e-02, -8.5067e-02, -9.4185e-02, -6.2623e-02, -1.1761e-01,\n",
       "                       -8.4716e-02, -6.5542e-02, -8.6713e-02, -9.7074e-02, -1.8434e-02,\n",
       "                       -1.0031e-01, -4.2391e-02, -1.2009e-01, -7.1212e-02, -3.2497e-02,\n",
       "                       -7.2368e-02, -8.3763e-02, -5.2279e-03, -5.2526e-02, -1.2781e-01,\n",
       "                       -9.2429e-02, -3.6967e-02, -2.2891e-02, -2.2304e-02, -6.7420e-02,\n",
       "                       -3.2553e-02, -8.5815e-02, -2.7114e-02, -8.5722e-02, -1.0085e-01,\n",
       "                       -1.0382e-02, -9.1218e-02, -8.8498e-02, -1.2149e-01, -5.8132e-02,\n",
       "                       -9.9490e-02, -4.9955e-02, -5.4282e-02, -9.1280e-02, -4.9584e-02,\n",
       "                       -5.1045e-02, -9.8659e-02, -1.3390e-01, -7.3613e-02, -5.2443e-02,\n",
       "                       -7.2239e-02, -5.8767e-02, -2.0146e-02, -6.7415e-02, -7.2233e-02,\n",
       "                       -6.6673e-02, -1.4128e-01, -3.3444e-02, -8.8504e-02, -1.0391e-01,\n",
       "                       -8.4592e-02, -1.2124e-01, -3.8845e-02, -6.7106e-03, -5.2204e-02,\n",
       "                       -1.0111e-01, -1.4560e-01, -8.2732e-02, -9.6669e-02, -4.9362e-02,\n",
       "                       -1.3365e-01, -8.1165e-02, -8.0011e-02, -1.4151e-01, -5.8696e-02,\n",
       "                       -5.2106e-02, -1.1542e-01, -8.0669e-02, -1.0279e-01, -1.0094e-01,\n",
       "                       -9.4846e-02, -6.3589e-02, -3.7961e-02, -7.0854e-02, -6.5652e-02,\n",
       "                       -1.0553e-01, -6.1056e-02, -8.1564e-02, -2.8414e-02, -4.4442e-02,\n",
       "                       -7.4803e-03, -1.4018e-01, -8.0837e-02, -4.6417e-02, -8.2448e-02,\n",
       "                       -9.3222e-02, -8.8774e-02, -1.1547e-01, -5.9612e-02, -1.2522e-01,\n",
       "                       -3.4966e-02, -7.2838e-02, -7.0724e-02, -3.7647e-02, -6.4828e-02,\n",
       "                       -5.0874e-02, -1.4080e-01, -1.0806e-01, -4.6915e-02, -6.1218e-02,\n",
       "                       -1.2258e-02, -2.5163e-02, -7.2073e-02, -1.1129e-01, -1.9012e-01,\n",
       "                       -1.2369e-01, -9.6689e-02, -6.4129e-02, -1.1258e-01, -1.7652e-02,\n",
       "                       -3.3817e-02, -1.0946e-01, -1.2659e-01, -9.1353e-02, -8.9551e-02,\n",
       "                       -1.6558e-02, -8.7156e-02, -6.0756e-02, -1.5190e-02,  2.3197e-02,\n",
       "                       -1.1508e-01, -1.2309e-01, -3.9894e-02, -1.0146e-01, -7.5740e-02,\n",
       "                       -9.4302e-02, -5.7951e-02, -5.5404e-02, -5.5797e-02, -1.2750e-01,\n",
       "                       -2.4571e-02, -4.4008e-02, -8.8181e-02, -2.0850e-02, -4.2982e-02,\n",
       "                       -4.5448e-02, -8.9284e-02, -1.9676e-02, -3.8856e-02, -4.5808e-02,\n",
       "                       -8.2954e-02, -4.8785e-02, -1.0197e-01, -6.5734e-02, -9.6583e-02,\n",
       "                       -7.3005e-02, -5.7967e-02, -1.7990e-02, -1.3680e-01, -9.6146e-04,\n",
       "                       -7.0281e-02, -7.4259e-02, -4.9949e-02,  7.4352e-03, -1.3637e-01,\n",
       "                       -7.7666e-02, -6.4829e-02], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn1.running_mean',\n",
       "               tensor([-2.8264e-02,  4.1652e-02,  4.2750e-03,  5.6055e-02, -4.4784e-03,\n",
       "                       -4.1618e-02,  3.1126e-03,  1.8383e-02,  5.5977e-02,  4.7863e-02,\n",
       "                        6.4787e-02, -1.0585e-01, -2.3886e-02,  4.4713e-03, -3.5395e-03,\n",
       "                        7.5193e-02,  5.4012e-02,  1.0260e-03,  1.0189e-01,  3.6804e-02,\n",
       "                       -3.4835e-02,  2.0132e-02,  1.7801e-02, -2.2886e-02,  4.1021e-02,\n",
       "                        4.0293e-02,  2.9891e-02, -1.3485e-02,  2.2352e-02,  7.7865e-02,\n",
       "                        8.9512e-02,  1.7231e-02,  3.8631e-02, -8.7913e-04,  3.8649e-02,\n",
       "                       -5.9443e-03,  8.6907e-03,  1.0754e-02,  1.4584e-02,  1.2133e-02,\n",
       "                        3.9721e-02,  1.9139e-02,  3.2737e-02,  1.1411e-02, -6.7721e-03,\n",
       "                       -9.2522e-02, -1.1671e-01,  9.6469e-02, -3.0484e-03, -7.0991e-02,\n",
       "                        1.0457e-02,  1.4524e-02,  5.4260e-03, -9.1909e-02,  3.4776e-02,\n",
       "                        7.7020e-02, -1.2520e-02,  4.4902e-02, -2.8629e-02, -2.0312e-02,\n",
       "                        8.9946e-03,  4.0653e-02,  3.9903e-02,  2.4278e-02,  3.6403e-02,\n",
       "                       -8.0004e-02, -3.1318e-02, -5.7165e-02,  6.3609e-02,  6.2681e-03,\n",
       "                        2.8354e-02, -1.1447e-02,  5.0939e-03,  5.3318e-02, -2.5271e-02,\n",
       "                       -5.0914e-02,  7.7912e-03, -1.7702e-03, -2.2388e-02,  7.5776e-05,\n",
       "                       -2.1432e-02, -1.1605e-02,  6.2252e-02, -1.1345e-02,  7.4552e-02,\n",
       "                       -2.1205e-02, -3.5657e-02, -7.5228e-02,  5.2053e-02,  5.0036e-02,\n",
       "                        1.8121e-02, -6.4039e-03,  1.1585e-01, -9.3791e-02,  5.3185e-03,\n",
       "                        1.6207e-02, -5.7490e-03,  1.2105e-01,  1.4390e-02, -7.4581e-03,\n",
       "                       -1.1627e-02,  1.2917e-02,  5.9115e-02,  7.6975e-02,  5.7289e-02,\n",
       "                       -6.7484e-03,  2.3703e-02, -2.4400e-02,  6.4851e-02,  2.7893e-02,\n",
       "                        4.9136e-02,  1.0037e-02,  3.4212e-02, -2.6673e-02,  6.4597e-02,\n",
       "                       -6.7843e-03, -1.8912e-02, -4.3104e-03,  3.2313e-02, -4.3840e-02,\n",
       "                        1.1432e-02,  1.0131e-02,  4.1272e-02,  4.2010e-02, -2.6866e-02,\n",
       "                        5.0062e-03,  5.0379e-02,  2.1262e-02, -1.8226e-02,  1.9060e-02,\n",
       "                        5.6052e-45,  1.4391e-02,  4.4609e-02, -4.5259e-02,  5.7795e-02,\n",
       "                        1.0586e-03, -2.0910e-02,  4.9958e-02, -1.4947e-02, -3.4462e-02,\n",
       "                       -3.7493e-02,  4.9645e-02,  6.5505e-02, -6.7119e-02,  4.5263e-02,\n",
       "                        1.2292e-02,  3.3926e-02,  2.4706e-03,  1.0310e-02,  3.7564e-02,\n",
       "                        2.0471e-02, -2.7070e-02,  5.9300e-03,  5.5940e-02,  1.5167e-02,\n",
       "                        5.9317e-02, -6.1718e-02,  1.0349e-01,  2.7814e-02,  3.2995e-02,\n",
       "                        1.8288e-02,  8.1447e-02, -1.0859e-02,  7.5190e-02,  1.7343e-02,\n",
       "                       -1.8518e-02,  5.0866e-03,  2.8178e-02,  4.5928e-04,  2.8473e-02,\n",
       "                        2.7414e-03,  2.9675e-02, -2.3511e-02,  3.6110e-02,  3.2421e-02,\n",
       "                        2.4056e-03,  7.8014e-03,  2.0346e-02,  1.7535e-02,  2.5947e-02,\n",
       "                       -1.6698e-02,  4.4060e-03,  3.7374e-02, -1.9278e-02,  1.5646e-02,\n",
       "                       -4.4899e-02,  4.6707e-02,  5.8607e-02,  1.5697e-02,  7.2050e-02,\n",
       "                        1.8880e-02,  4.0509e-02,  4.1468e-02, -1.6306e-02, -4.4599e-02,\n",
       "                        2.2086e-02, -6.6614e-02,  5.5432e-02,  8.1461e-02, -1.0683e-02,\n",
       "                       -9.2395e-04,  5.1766e-02,  3.4868e-02, -1.0382e-02, -4.6259e-02,\n",
       "                       -1.2780e-02, -9.0679e-02,  7.6872e-02,  1.1897e-02,  2.1247e-02,\n",
       "                       -3.8487e-02,  2.4631e-02,  3.0681e-02,  1.9962e-02,  4.9345e-02,\n",
       "                        1.9164e-02, -3.1862e-02, -4.6043e-02,  2.7865e-02, -2.2065e-02,\n",
       "                       -1.6746e-03,  6.4255e-02,  9.6313e-03,  1.6295e-02,  9.7226e-03,\n",
       "                       -1.4662e-02,  9.5818e-02, -4.6313e-02, -2.9650e-02,  5.1654e-02,\n",
       "                        4.2330e-02,  1.5001e-02, -5.1974e-03,  4.0364e-02,  4.9807e-02,\n",
       "                        8.8415e-02,  7.5945e-03, -3.5929e-02,  8.2018e-02,  2.2320e-02,\n",
       "                       -2.2866e-02,  1.7671e-02,  3.0041e-02,  5.6810e-02,  8.1086e-02,\n",
       "                       -3.2505e-02, -3.1868e-03,  6.3721e-02,  2.2205e-02,  7.3417e-02,\n",
       "                        1.4566e-02,  6.8820e-02,  9.9544e-02,  5.5136e-02,  8.0748e-02,\n",
       "                       -1.6989e-02,  6.0460e-02,  2.0756e-02, -4.6596e-02,  6.4477e-03,\n",
       "                       -9.7607e-02,  7.5888e-02,  2.3624e-02, -1.1320e-02,  1.9546e-02,\n",
       "                        2.3621e-02,  1.3848e-02,  6.8195e-02,  2.4988e-02,  6.2912e-02,\n",
       "                        1.6557e-02, -3.4325e-02,  7.9595e-03,  6.5897e-03, -5.3025e-02,\n",
       "                       -2.7860e-02,  2.9383e-02, -1.8028e-02,  2.6903e-02, -3.0958e-02,\n",
       "                        1.2929e-02, -1.0185e-02,  1.1087e-03,  2.7265e-02,  3.8608e-02,\n",
       "                        1.9902e-02,  5.6348e-02,  6.0980e-02, -1.4003e-02,  2.9149e-02,\n",
       "                        2.9741e-02,  1.0570e-01,  6.1046e-03,  8.9693e-03,  3.9154e-02,\n",
       "                        5.0904e-02,  3.4472e-02, -1.0126e-01,  3.6652e-02, -2.5933e-02,\n",
       "                       -2.1126e-03,  1.8023e-02,  5.5073e-02,  9.6380e-03, -2.7191e-02,\n",
       "                       -3.1128e-02, -3.0506e-02, -2.1507e-02, -5.7850e-02, -5.3261e-02,\n",
       "                        1.4698e-02,  3.2903e-02, -2.2633e-02,  1.8577e-02, -1.6162e-02,\n",
       "                        2.8831e-02, -6.5618e-03,  5.1058e-02,  1.0102e-01,  1.1375e-01,\n",
       "                        8.1054e-03,  3.2868e-02,  3.0767e-02,  4.5447e-02,  1.4656e-02,\n",
       "                        4.6681e-02,  1.9939e-02, -2.6298e-03, -9.1023e-02, -8.3395e-03,\n",
       "                        6.3969e-02,  2.8333e-02,  4.5274e-03,  7.4109e-02,  9.4877e-03,\n",
       "                       -1.0416e-02,  4.8028e-02, -6.6013e-03, -1.5007e-03,  4.5347e-03,\n",
       "                        6.0309e-02,  3.5568e-02,  3.0400e-02,  2.3813e-02,  1.0804e-01,\n",
       "                       -1.0416e-02,  3.6437e-03,  3.3322e-03, -2.2698e-03, -1.0595e-02,\n",
       "                        3.8835e-02,  3.4834e-02,  4.0782e-04,  5.5308e-02,  1.9046e-02,\n",
       "                       -3.3588e-02, -1.5418e-03,  2.6110e-02, -7.7899e-02, -6.1217e-02,\n",
       "                       -7.4064e-02,  2.6579e-02,  5.4433e-02,  3.3791e-02,  3.5959e-02,\n",
       "                       -5.8106e-02, -1.8509e-02, -1.7165e-02, -2.2093e-02, -1.1833e-02,\n",
       "                       -4.4223e-02,  4.8821e-02,  1.1744e-02, -5.6614e-02, -6.0559e-02,\n",
       "                        2.4745e-02,  1.3525e-02,  8.2672e-03, -2.4711e-02,  1.2316e-03,\n",
       "                        3.7416e-02,  3.3223e-02,  8.7843e-02,  3.7689e-02, -3.4341e-02,\n",
       "                        1.2480e-01,  3.5026e-02, -4.7511e-03,  4.5234e-02,  1.7825e-02,\n",
       "                        2.1992e-02,  1.4417e-03,  1.1203e-01,  6.3031e-02, -2.7572e-03,\n",
       "                        4.0412e-03,  4.8849e-02,  4.6815e-02,  5.3105e-02, -1.1357e-03,\n",
       "                        3.3354e-02, -3.9401e-02,  4.1724e-02,  8.1216e-02,  9.8488e-03,\n",
       "                        6.4868e-02, -1.0913e-02, -2.5116e-02, -2.5297e-02,  1.7697e-02,\n",
       "                        2.8827e-02,  1.3159e-01, -4.5109e-02, -1.9220e-02,  5.0453e-04,\n",
       "                        8.0175e-02,  1.9442e-02,  4.1103e-03,  1.2420e-02, -5.7428e-03,\n",
       "                       -4.5450e-02,  2.2479e-02, -2.3156e-02,  2.6978e-02,  7.3357e-02,\n",
       "                       -4.3780e-02,  1.5014e-02, -2.5983e-04,  1.7362e-02, -7.7354e-04,\n",
       "                        4.2443e-02, -7.9544e-02, -6.8142e-02,  1.2216e-02,  1.0093e-02,\n",
       "                       -5.3163e-02,  6.1273e-02,  2.1038e-02,  1.1921e-02, -1.4324e-02,\n",
       "                        6.6212e-02,  2.1912e-02,  6.8789e-02,  2.5143e-02, -1.3197e-02,\n",
       "                       -6.9826e-02,  8.5589e-03,  3.4890e-03,  5.9968e-02, -4.9503e-02,\n",
       "                       -4.0765e-03, -6.0773e-02,  1.8632e-02,  3.2659e-02,  4.3247e-03,\n",
       "                       -5.1844e-02, -5.6540e-03,  2.0485e-02,  3.2903e-02,  1.0205e-02,\n",
       "                        8.2269e-02,  5.4614e-02,  6.5974e-03,  7.9498e-02, -1.2898e-02,\n",
       "                        1.3508e-03,  8.3854e-02, -6.7095e-03,  3.2673e-02,  3.6799e-02,\n",
       "                       -1.0144e-03,  4.6993e-02,  2.7285e-02,  5.5860e-02, -1.0531e-01,\n",
       "                        1.0757e-01,  8.9435e-02, -3.1283e-03, -7.7752e-03,  4.6770e-03,\n",
       "                        2.8912e-03,  2.4001e-02, -1.1882e-01,  3.2490e-02, -2.4962e-02,\n",
       "                        1.6322e-02,  6.9859e-02,  3.8627e-02, -3.6446e-02,  1.3920e-02,\n",
       "                       -2.3666e-02,  6.6646e-02, -7.4650e-02, -2.3283e-02, -1.8316e-04,\n",
       "                        6.7135e-02,  3.1375e-02, -1.2486e-02, -6.6589e-02, -5.4863e-02,\n",
       "                        4.8900e-02,  3.3555e-02, -7.1211e-02,  5.1400e-02, -4.7750e-02,\n",
       "                       -7.5029e-03, -4.3712e-03,  3.3755e-02,  2.3882e-02,  3.3299e-02,\n",
       "                        3.2463e-02, -9.5788e-03], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn1.running_var',\n",
       "               tensor([4.5213e-03, 1.4141e-03, 2.9794e-03, 4.0507e-03, 5.2696e-03, 6.6802e-03,\n",
       "                       4.6974e-03, 4.1531e-03, 4.6257e-03, 2.0276e-03, 1.8837e-03, 6.2466e-03,\n",
       "                       7.8870e-03, 4.0861e-03, 2.2833e-03, 5.1483e-03, 3.7167e-03, 1.5091e-03,\n",
       "                       4.2959e-03, 2.3036e-03, 3.2530e-03, 1.6304e-03, 2.4673e-03, 2.6293e-03,\n",
       "                       1.9327e-03, 1.2523e-03, 1.9690e-03, 7.9721e-03, 3.7481e-03, 9.1502e-03,\n",
       "                       4.0027e-03, 4.8971e-03, 8.1995e-03, 3.1234e-03, 1.6805e-03, 1.4667e-03,\n",
       "                       6.8283e-03, 4.2602e-03, 5.2075e-03, 2.8174e-03, 6.9401e-03, 6.7248e-03,\n",
       "                       2.3963e-03, 5.8159e-03, 5.2938e-03, 8.5414e-03, 3.9080e-03, 4.3440e-03,\n",
       "                       3.9301e-03, 1.0242e-02, 2.4920e-03, 1.9322e-03, 3.3666e-03, 7.8086e-03,\n",
       "                       5.4002e-03, 5.4063e-03, 1.5566e-03, 5.0281e-03, 6.9527e-03, 9.9609e-03,\n",
       "                       4.4986e-03, 3.2537e-03, 4.3007e-03, 3.8491e-03, 7.3132e-04, 6.4798e-03,\n",
       "                       1.5899e-03, 2.4274e-03, 4.0206e-03, 4.3436e-03, 5.4806e-04, 6.1911e-03,\n",
       "                       2.6794e-03, 4.6033e-03, 3.5220e-03, 4.0072e-03, 5.3782e-03, 1.4971e-03,\n",
       "                       6.0624e-03, 4.1066e-03, 1.7480e-03, 3.8635e-03, 2.6243e-03, 5.8830e-03,\n",
       "                       4.2003e-03, 2.1287e-03, 5.3327e-03, 7.0485e-03, 1.7339e-03, 2.1919e-03,\n",
       "                       4.8749e-04, 6.2882e-03, 9.4076e-03, 3.8489e-03, 3.0186e-03, 2.1636e-03,\n",
       "                       1.2572e-03, 8.5802e-03, 1.0218e-03, 4.3057e-03, 7.8977e-03, 2.5067e-03,\n",
       "                       7.5510e-03, 5.6674e-03, 3.1052e-03, 7.1061e-03, 8.9405e-03, 3.7326e-03,\n",
       "                       3.4312e-03, 3.7683e-03, 3.0289e-03, 6.9050e-03, 2.3386e-03, 5.7544e-04,\n",
       "                       2.0797e-03, 9.9451e-03, 2.1481e-03, 2.3620e-03, 3.5692e-03, 3.0552e-03,\n",
       "                       5.9181e-03, 3.9927e-03, 6.5047e-03, 3.4682e-03, 8.3173e-04, 5.0895e-03,\n",
       "                       3.5171e-03, 2.5912e-04, 4.3035e-03, 1.1256e-02, 5.6052e-45, 3.4683e-03,\n",
       "                       1.4564e-02, 4.2252e-03, 1.7201e-03, 2.9529e-03, 7.2126e-03, 5.7709e-03,\n",
       "                       4.4839e-03, 4.3529e-03, 5.1603e-03, 3.7656e-03, 7.2076e-03, 8.0758e-03,\n",
       "                       7.1855e-03, 4.2546e-03, 6.4592e-03, 2.2815e-03, 3.9065e-03, 3.3461e-03,\n",
       "                       1.5792e-03, 9.4246e-04, 2.9409e-03, 1.3878e-03, 2.0640e-03, 1.6518e-03,\n",
       "                       3.5605e-03, 3.1082e-03, 8.8831e-04, 8.0137e-04, 3.7223e-03, 5.9863e-03,\n",
       "                       6.9093e-03, 2.6906e-03, 1.7533e-03, 5.8173e-03, 1.4244e-03, 4.2882e-03,\n",
       "                       2.6112e-03, 1.2895e-03, 2.9590e-03, 3.9285e-03, 4.0609e-03, 3.9775e-03,\n",
       "                       5.7247e-03, 2.1838e-03, 7.0342e-03, 5.2427e-04, 2.3256e-03, 4.3150e-03,\n",
       "                       7.1437e-03, 6.7981e-03, 3.7466e-03, 2.1863e-03, 4.5686e-03, 2.3496e-03,\n",
       "                       6.0547e-03, 1.0426e-02, 1.8733e-03, 5.4003e-03, 7.0649e-04, 7.2459e-03,\n",
       "                       4.5328e-03, 3.8962e-03, 9.4472e-03, 4.0847e-03, 8.1204e-03, 3.0865e-03,\n",
       "                       7.9444e-03, 3.3827e-03, 2.9768e-03, 1.0243e-03, 6.4357e-03, 3.1924e-03,\n",
       "                       3.7584e-03, 1.1463e-03, 5.5189e-03, 7.4458e-03, 1.9029e-03, 2.4702e-04,\n",
       "                       4.9019e-03, 1.9985e-03, 2.6707e-03, 1.8538e-03, 2.4133e-03, 3.2864e-03,\n",
       "                       5.9089e-03, 7.1001e-03, 3.2845e-03, 2.8994e-03, 1.6106e-02, 3.1485e-03,\n",
       "                       5.4967e-04, 3.4420e-03, 3.4277e-03, 3.2170e-03, 4.4987e-03, 1.3023e-02,\n",
       "                       5.0116e-03, 3.2310e-03, 6.5854e-03, 8.0311e-04, 1.1987e-02, 5.2530e-03,\n",
       "                       2.9620e-03, 5.3715e-03, 1.3835e-03, 5.3403e-03, 3.2558e-03, 3.4071e-03,\n",
       "                       3.9025e-03, 4.0452e-03, 7.8550e-03, 9.0343e-04, 4.3197e-03, 3.6235e-03,\n",
       "                       3.8628e-03, 7.7734e-03, 6.6938e-03, 5.4455e-03, 1.2347e-03, 1.1565e-02,\n",
       "                       4.5930e-03, 4.3000e-03, 7.1630e-03, 2.1540e-03, 6.5728e-03, 4.3583e-03,\n",
       "                       1.2073e-02, 1.7773e-03, 1.9469e-03, 6.0233e-03, 6.2259e-03, 7.6324e-03,\n",
       "                       2.2063e-03, 4.2859e-03, 4.5356e-03, 1.0123e-02, 1.9508e-03, 2.5957e-03,\n",
       "                       1.7810e-03, 7.1225e-03, 1.8345e-03, 4.3959e-03, 8.0902e-03, 5.0435e-03,\n",
       "                       9.0159e-04, 3.5841e-03, 1.4812e-03, 6.8096e-03, 9.1494e-03, 3.6587e-03,\n",
       "                       4.0860e-03, 4.2325e-03, 6.8939e-04, 9.1726e-04, 1.9440e-03, 4.1320e-03,\n",
       "                       3.3044e-03, 5.6622e-03, 5.8090e-03, 5.1247e-03, 1.0852e-04, 2.5045e-03,\n",
       "                       7.8281e-04, 2.0137e-03, 7.7023e-03, 1.0002e-02, 3.8833e-03, 1.0513e-03,\n",
       "                       1.8310e-03, 2.4305e-03, 9.9516e-03, 6.2477e-03, 4.2968e-03, 2.4628e-03,\n",
       "                       3.7683e-03, 4.5875e-03, 5.3752e-03, 3.5704e-03, 1.3030e-04, 5.4234e-03,\n",
       "                       2.7175e-03, 1.2337e-03, 4.2415e-03, 3.7498e-03, 8.9414e-03, 3.1121e-03,\n",
       "                       2.7543e-03, 5.4863e-03, 7.6765e-03, 1.7008e-03, 9.3274e-04, 3.3353e-03,\n",
       "                       3.6102e-03, 2.3177e-03, 5.4944e-04, 3.6133e-03, 7.5408e-03, 3.7193e-03,\n",
       "                       2.1940e-02, 2.1419e-03, 2.9756e-03, 7.9761e-03, 2.1487e-03, 3.9990e-03,\n",
       "                       5.1817e-03, 5.9482e-04, 9.0816e-04, 2.4477e-03, 5.4143e-04, 4.7272e-03,\n",
       "                       2.0863e-03, 8.8334e-03, 1.0393e-02, 6.5958e-03, 4.8084e-03, 4.3092e-03,\n",
       "                       5.0935e-04, 3.2871e-03, 1.5270e-03, 9.0906e-04, 5.9531e-03, 2.4689e-03,\n",
       "                       4.8922e-03, 2.0000e-03, 2.9683e-03, 1.1751e-03, 6.6003e-03, 3.2067e-03,\n",
       "                       3.8390e-03, 6.2324e-04, 2.3166e-03, 9.0032e-04, 6.3122e-04, 5.6548e-03,\n",
       "                       4.7161e-03, 8.7142e-04, 2.8223e-03, 2.4582e-03, 2.9352e-03, 1.4704e-03,\n",
       "                       3.2305e-03, 6.4131e-03, 4.8266e-03, 2.6731e-03, 8.3963e-03, 7.4233e-04,\n",
       "                       1.3618e-02, 6.0113e-03, 3.5250e-03, 3.7368e-03, 2.7853e-03, 4.5739e-03,\n",
       "                       4.7130e-03, 3.6247e-03, 5.4329e-03, 1.1714e-03, 1.7114e-03, 2.0915e-03,\n",
       "                       1.6231e-03, 4.1411e-03, 6.5102e-03, 4.0202e-03, 2.5303e-03, 4.2584e-03,\n",
       "                       4.4900e-03, 2.4062e-03, 5.0274e-03, 2.9586e-03, 1.4261e-03, 3.6375e-03,\n",
       "                       2.5302e-03, 6.3013e-03, 2.4535e-03, 1.5979e-03, 4.8684e-03, 6.6172e-03,\n",
       "                       2.6832e-03, 3.1307e-03, 5.1297e-03, 4.1153e-03, 2.7449e-03, 4.8638e-03,\n",
       "                       1.4749e-03, 5.2297e-03, 2.3603e-03, 1.0299e-03, 6.5643e-03, 6.0120e-03,\n",
       "                       5.7986e-03, 2.9585e-03, 2.6652e-03, 5.5526e-03, 5.7288e-03, 8.2320e-03,\n",
       "                       3.8158e-03, 1.4142e-03, 6.8231e-04, 1.4088e-03, 2.3827e-03, 3.2222e-03,\n",
       "                       7.7169e-03, 1.3738e-03, 1.8062e-04, 6.4711e-03, 5.0850e-03, 3.9855e-03,\n",
       "                       1.1299e-03, 2.9035e-03, 3.6412e-03, 3.8337e-03, 2.9387e-03, 2.1350e-03,\n",
       "                       5.6126e-03, 7.9655e-03, 2.0928e-03, 9.4342e-04, 3.3635e-03, 5.4034e-03,\n",
       "                       2.4316e-03, 1.4553e-02, 3.8588e-03, 7.6220e-04, 5.4324e-03, 5.3064e-03,\n",
       "                       2.5093e-03, 3.6057e-03, 6.8409e-03, 3.5390e-03, 3.7593e-03, 3.2017e-03,\n",
       "                       4.7662e-05, 3.6629e-03, 2.8418e-03, 1.6296e-03, 4.1386e-03, 4.8107e-03,\n",
       "                       4.9615e-03, 5.5167e-03, 2.2465e-03, 3.2694e-03, 6.1904e-03, 6.6617e-03,\n",
       "                       7.4285e-03, 4.6446e-03, 8.4004e-03, 1.2807e-03, 3.6691e-03, 5.1889e-03,\n",
       "                       5.5899e-03, 1.2249e-03, 5.1927e-03, 1.7497e-03, 3.7236e-03, 1.4582e-04,\n",
       "                       4.0824e-03, 2.3928e-03, 3.8197e-03, 1.4901e-04, 5.0845e-03, 3.1807e-03,\n",
       "                       1.3480e-02, 2.6585e-03, 1.0218e-03, 4.5771e-03, 8.4200e-03, 4.0265e-03,\n",
       "                       4.4765e-03, 3.9740e-03, 3.5092e-03, 1.8726e-03, 6.8240e-03, 6.3089e-03,\n",
       "                       6.2523e-03, 6.5183e-04, 2.3635e-03, 2.2010e-03, 1.0723e-02, 6.1782e-03,\n",
       "                       5.8664e-03, 6.4088e-03], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer4.0.conv2.weight',\n",
       "               tensor([[[[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0163, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0361, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0212,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0628,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0203, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0079, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0318, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0139, -0.0044],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0102, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0108, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0249, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0057, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0079, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0207, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0030,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0215, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0043]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0127,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 1.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn2.weight',\n",
       "               tensor([0.1378, 0.1538, 0.1666, 0.1285, 0.1653, 0.1253, 0.1243, 0.1825, 0.1279,\n",
       "                       0.1331, 0.0985, 0.1015, 0.1408, 0.1823, 0.1339, 0.0894, 0.1311, 0.1123,\n",
       "                       0.1388, 0.1300, 0.1586, 0.1147, 0.1077, 0.0906, 0.1393, 0.1747, 0.1331,\n",
       "                       0.1237, 0.1283, 0.1162, 0.0935, 0.1309, 0.1117, 0.1324, 0.0966, 0.1430,\n",
       "                       0.1221, 0.1070, 0.1411, 0.1509, 0.1084, 0.1244, 0.1183, 0.1246, 0.1148,\n",
       "                       0.1575, 0.1054, 0.1411, 0.0969, 0.1107, 0.1242, 0.1173, 0.1413, 0.1291,\n",
       "                       0.1120, 0.1347, 0.1450, 0.1540, 0.1168, 0.1766, 0.0909, 0.1255, 0.1270,\n",
       "                       0.1282, 0.0966, 0.0977, 0.1334, 0.1192, 0.1496, 0.1203, 0.1666, 0.0597,\n",
       "                       0.0982, 0.0479, 0.1257, 0.1000, 0.1474, 0.0988, 0.1387, 0.1325, 0.1184,\n",
       "                       0.1162, 0.1071, 0.0533, 0.1189, 0.1468, 0.1638, 0.1100, 0.2460, 0.1276,\n",
       "                       0.1141, 0.1128, 0.1658, 0.1111, 0.1040, 0.1159, 0.1486, 0.1592, 0.1427,\n",
       "                       0.1235, 0.1316, 0.2243, 0.1301, 0.1031, 0.1161, 0.1139, 0.0911, 0.1194,\n",
       "                       0.1036, 0.1072, 0.1178, 0.1597, 0.1338, 0.1662, 0.1254, 0.1343, 0.1381,\n",
       "                       0.1242, 0.1331, 0.1146, 0.0984, 0.1227, 0.0498, 0.1196, 0.1257, 0.1501,\n",
       "                       0.1127, 0.0723, 0.1780, 0.1298, 0.0989, 0.1192, 0.0932, 0.1060, 0.1289,\n",
       "                       0.1783, 0.0995, 0.1463, 0.1212, 0.1235, 0.1343, 0.1110, 0.1221, 0.1016,\n",
       "                       0.1767, 0.1157, 0.1132, 0.1182, 0.1441, 0.1180, 0.1372, 0.1059, 0.1187,\n",
       "                       0.1409, 0.1185, 0.0822, 0.0957, 0.0961, 0.1172, 0.1668, 0.1135, 0.1504,\n",
       "                       0.1374, 0.0743, 0.1714, 0.1487, 0.1575, 0.1063, 0.1247, 0.0804, 0.1537,\n",
       "                       0.1296, 0.1630, 0.1356, 0.1575, 0.1156, 0.0865, 0.1209, 0.1315, 0.1331,\n",
       "                       0.1333, 0.1376, 0.1500, 0.1186, 0.1626, 0.1403, 0.1140, 0.1003, 0.1111,\n",
       "                       0.1463, 0.2269, 0.0970, 0.1089, 0.1645, 0.0885, 0.1285, 0.1295, 0.1430,\n",
       "                       0.1149, 0.1533, 0.0452, 0.0954, 0.1156, 0.1203, 0.1339, 0.1282, 0.1343,\n",
       "                       0.1127, 0.1314, 0.1112, 0.1122, 0.1260, 0.1298, 0.1282, 0.1077, 0.0614,\n",
       "                       0.1130, 0.1304, 0.1188, 0.1568, 0.1573, 0.1218, 0.1539, 0.1585, 0.1081,\n",
       "                       0.0923, 0.1362, 0.1586, 0.1333, 0.1185, 0.1036, 0.1582, 0.1087, 0.1756,\n",
       "                       0.1728, 0.0943, 0.1334, 0.1100, 0.1203, 0.1408, 0.1311, 0.0882, 0.1390,\n",
       "                       0.1149, 0.1354, 0.1359, 0.1127, 0.0981, 0.1155, 0.1824, 0.1782, 0.1586,\n",
       "                       0.1489, 0.1132, 0.1233, 0.1259, 0.1338, 0.1263, 0.1027, 0.1370, 0.1557,\n",
       "                       0.0797, 0.0966, 0.1353, 0.1244, 0.1918, 0.1263, 0.1613, 0.1394, 0.1568,\n",
       "                       0.1864, 0.0512, 0.1709, 0.1263, 0.1532, 0.0973, 0.1405, 0.0960, 0.1262,\n",
       "                       0.1219, 0.1432, 0.1457, 0.1209, 0.1381, 0.0812, 0.1224, 0.1377, 0.1314,\n",
       "                       0.1412, 0.1290, 0.1069, 0.1229, 0.1142, 0.1085, 0.1357, 0.1050, 0.1436,\n",
       "                       0.1441, 0.0886, 0.1660, 0.0883, 0.1372, 0.1167, 0.0888, 0.1154, 0.1429,\n",
       "                       0.1381, 0.1600, 0.1358, 0.1105, 0.1183, 0.1026, 0.1330, 0.1292, 0.1518,\n",
       "                       0.1267, 0.1074, 0.0941, 0.1114, 0.1731, 0.1752, 0.0908, 0.1446, 0.1212,\n",
       "                       0.1655, 0.1036, 0.0945, 0.1230, 0.1225, 0.1608, 0.1442, 0.1684, 0.1352,\n",
       "                       0.0909, 0.1334, 0.1263, 0.1545, 0.1229, 0.1037, 0.1377, 0.1397, 0.1395,\n",
       "                       0.1212, 0.1103, 0.1410, 0.1098, 0.1293, 0.1208, 0.1448, 0.1217, 0.1241,\n",
       "                       0.1486, 0.0822, 0.1164, 0.0931, 0.1516, 0.1104, 0.1559, 0.1582, 0.1295,\n",
       "                       0.1133, 0.1329, 0.1239, 0.1231, 0.1083, 0.1284, 0.1208, 0.0895, 0.1042,\n",
       "                       0.1452, 0.1519, 0.0851, 0.0801, 0.1160, 0.1774, 0.0942, 0.1641, 0.1053,\n",
       "                       0.1251, 0.1556, 0.1206, 0.1223, 0.1382, 0.1181, 0.1241, 0.1433, 0.1205,\n",
       "                       0.1146, 0.1321, 0.1301, 0.1396, 0.1481, 0.1511, 0.1159, 0.1473, 0.1312,\n",
       "                       0.1462, 0.1255, 0.1117, 0.1228, 0.1371, 0.1243, 0.1283, 0.1066, 0.1589,\n",
       "                       0.0889, 0.1068, 0.1567, 0.1055, 0.1141, 0.1311, 0.1535, 0.1033, 0.1126,\n",
       "                       0.1074, 0.1269, 0.0515, 0.1261, 0.1500, 0.1586, 0.1275, 0.1407, 0.2274,\n",
       "                       0.1407, 0.1108, 0.1094, 0.0862, 0.1363, 0.1391, 0.1228, 0.1288, 0.1094,\n",
       "                       0.0995, 0.1087, 0.0824, 0.1444, 0.1303, 0.1078, 0.1491, 0.1097, 0.0532,\n",
       "                       0.1275, 0.1308, 0.0907, 0.1369, 0.1048, 0.1302, 0.1280, 0.1797, 0.1327,\n",
       "                       0.1522, 0.1390, 0.1447, 0.0869, 0.1184, 0.0951, 0.1415, 0.1362, 0.1078,\n",
       "                       0.1377, 0.1340, 0.1160, 0.1504, 0.1016, 0.1237, 0.1357, 0.1341, 0.1417,\n",
       "                       0.1506, 0.0984, 0.1318, 0.1331, 0.1078, 0.1745, 0.1246, 0.1380, 0.1338,\n",
       "                       0.1032, 0.1194, 0.1375, 0.1227, 0.1739, 0.0833, 0.1171, 0.1135, 0.1415,\n",
       "                       0.0824, 0.1424, 0.1339, 0.1260, 0.1243, 0.1305, 0.1512, 0.0981, 0.1438,\n",
       "                       0.1305, 0.1263, 0.1572, 0.1242, 0.1271, 0.1096, 0.1552, 0.1327, 0.1374,\n",
       "                       0.1513, 0.1091, 0.1601, 0.1338, 0.1267, 0.1413, 0.1249, 0.1186],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn2.bias',\n",
       "               tensor([-0.0515, -0.0480, -0.0139, -0.0420, -0.0184, -0.0500, -0.0633, -0.0939,\n",
       "                       -0.0413, -0.0679, -0.0201, -0.0312, -0.0406, -0.0312, -0.0182, -0.0414,\n",
       "                       -0.0580, -0.0492, -0.0340, -0.0289, -0.0559, -0.0731, -0.0391, -0.0387,\n",
       "                       -0.0825, -0.0918, -0.0313, -0.0554, -0.0641, -0.0537, -0.0367, -0.0581,\n",
       "                       -0.0251, -0.0533, -0.0356, -0.0392, -0.0371, -0.0358, -0.0637, -0.0342,\n",
       "                       -0.0407, -0.0331, -0.0429, -0.0083, -0.0528, -0.0741, -0.0438, -0.0638,\n",
       "                       -0.0212, -0.0312, -0.0351, -0.0332, -0.0595, -0.0748, -0.0201, -0.0531,\n",
       "                       -0.0360, -0.0578, -0.0254, -0.0055, -0.0596, -0.0607, -0.0498, -0.0445,\n",
       "                       -0.0348,  0.0004, -0.0121, -0.0756, -0.0626, -0.0640, -0.0571, -0.0162,\n",
       "                       -0.0517, -0.0170, -0.0484, -0.0416, -0.0585, -0.0425, -0.0254, -0.0467,\n",
       "                       -0.0482, -0.0570, -0.0255, -0.0127, -0.0537, -0.0292, -0.0279, -0.0478,\n",
       "                       -0.0278, -0.0030, -0.0405, -0.0470, -0.0224, -0.0485, -0.0096, -0.0442,\n",
       "                       -0.0527, -0.0924, -0.0601, -0.0627, -0.0546, -0.0539, -0.0100, -0.0337,\n",
       "                       -0.0647, -0.0345, -0.0160, -0.0432, -0.0501, -0.0306, -0.0413, -0.0804,\n",
       "                       -0.0514, -0.0863, -0.0170, -0.0467, -0.0527, -0.0487, -0.0458, -0.0645,\n",
       "                       -0.0220, -0.0552, -0.0030, -0.0290, -0.0514, -0.0988, -0.0348, -0.0120,\n",
       "                       -0.0703, -0.0317, -0.0486, -0.0625, -0.0287, -0.0645, -0.0595, -0.0450,\n",
       "                       -0.0225, -0.0549, -0.0598, -0.0734, -0.0513, -0.0662, -0.0447, -0.0480,\n",
       "                       -0.0173, -0.0456, -0.0487, -0.0248, -0.0596, -0.0315, -0.0481, -0.0201,\n",
       "                       -0.0300, -0.0504, -0.0403, -0.0244, -0.0427, -0.0407, -0.0642, -0.0429,\n",
       "                       -0.0730, -0.0572, -0.0259,  0.0677, -0.1200, -0.0403, -0.0451, -0.0145,\n",
       "                       -0.0683, -0.0439, -0.0875, -0.0214, -0.0629, -0.0810, -0.0467, -0.0707,\n",
       "                       -0.0340, -0.0514, -0.0266, -0.0303, -0.0692, -0.0398, -0.0399, -0.0508,\n",
       "                       -0.0160, -0.0151, -0.0233, -0.0342, -0.0505, -0.0655, -0.0540, -0.0597,\n",
       "                       -0.0234, -0.0356, -0.0399, -0.0759, -0.0442, -0.0371, -0.0376, -0.0316,\n",
       "                       -0.0156, -0.0481, -0.0565, -0.0650, -0.0545, -0.0622, -0.0343, -0.0644,\n",
       "                       -0.0374, -0.0473, -0.0469, -0.0568, -0.0585, -0.0217, -0.0210, -0.0148,\n",
       "                       -0.0377, -0.0687, -0.0458, -0.0909, -0.0832, -0.0523, -0.0509, -0.0528,\n",
       "                       -0.0339, -0.0458, -0.0354, -0.0771, -0.0494, -0.0374, -0.0451, -0.0371,\n",
       "                       -0.0752, -0.0279, -0.0510, -0.0421, -0.0457, -0.0297, -0.0487, -0.0313,\n",
       "                       -0.0467, -0.0237, -0.0574, -0.0306, -0.0739, -0.0406, -0.0460, -0.0296,\n",
       "                       -0.0146, -0.0471, -0.0524, -0.0223, -0.0836, -0.0199,  0.0042, -0.0287,\n",
       "                       -0.0343, -0.0656, -0.0478, -0.0622, -0.0267, -0.0307, -0.0217,  0.0234,\n",
       "                       -0.0170, -0.0390, -0.0667, -0.0825, -0.0788, -0.0300, -0.0458, -0.0312,\n",
       "                       -0.0675, -0.0808, -0.0982, -0.0173, -0.0628, -0.0483, -0.0910, -0.0319,\n",
       "                       -0.0431, -0.0547, -0.0453, -0.0280,  0.0006, -0.0382, -0.0483, -0.0740,\n",
       "                       -0.0231, -0.0617, -0.0484, -0.0356, -0.0160, -0.0401, -0.0265, -0.0445,\n",
       "                       -0.0278, -0.0353, -0.0248, -0.0286, -0.0398, -0.0332, -0.0369, -0.0221,\n",
       "                       -0.0394, -0.0339, -0.0544, -0.0680, -0.0927, -0.0248, -0.0603, -0.0545,\n",
       "                       -0.0346, -0.0355, -0.0449, -0.0546, -0.0403, -0.0460, -0.0695,  0.0054,\n",
       "                       -0.0358, -0.0624, -0.0563, -0.0589, -0.0649, -0.0506, -0.0444, -0.0761,\n",
       "                       -0.0649, -0.0753, -0.0358, -0.0670, -0.0227, -0.0558, -0.0364, -0.0566,\n",
       "                       -0.0257, -0.0558, -0.0250, -0.0511, -0.0534, -0.0182,  0.0113, -0.0533,\n",
       "                       -0.0730, -0.0494, -0.0655, -0.0363, -0.0228, -0.0361, -0.0671, -0.0713,\n",
       "                       -0.0259, -0.0725, -0.0370, -0.0529, -0.0523, -0.0442, -0.0252, -0.0511,\n",
       "                       -0.0239, -0.0379, -0.0310, -0.0407, -0.0246, -0.0530, -0.0434, -0.0465,\n",
       "                       -0.0418, -0.0746, -0.0455, -0.0282, -0.0293, -0.0595, -0.0360, -0.0429,\n",
       "                       -0.0590, -0.0528, -0.0504, -0.0554, -0.0148, -0.0362, -0.0333, -0.0440,\n",
       "                       -0.0561, -0.0517, -0.0291, -0.0863, -0.0259, -0.0400, -0.0527, -0.0503,\n",
       "                       -0.0642, -0.0306, -0.0556, -0.0902, -0.0273, -0.0627, -0.0277, -0.0431,\n",
       "                       -0.0337, -0.0638, -0.0232, -0.0261, -0.0665, -0.0194, -0.0560, -0.0985,\n",
       "                       -0.0467, -0.0609, -0.0382, -0.0770, -0.0124, -0.0630, -0.0402, -0.0675,\n",
       "                       -0.0372, -0.0365, -0.0714, -0.0577, -0.0077, -0.0409, -0.0486, -0.0320,\n",
       "                       -0.0236, -0.0406, -0.0311, -0.0312, -0.0167, -0.0571, -0.0657, -0.0262,\n",
       "                       -0.0592, -0.0734, -0.0300, -0.0551, -0.0287, -0.0546, -0.0342, -0.0551,\n",
       "                       -0.0157, -0.0311, -0.0546, -0.0421, -0.0548, -0.0276, -0.0388, -0.0592,\n",
       "                       -0.1035, -0.0512, -0.0634, -0.0754, -0.0414, -0.0288, -0.0349, -0.0192,\n",
       "                       -0.0605, -0.0605, -0.0501, -0.0388, -0.0178, -0.0429, -0.0455, -0.0397,\n",
       "                       -0.0323, -0.0456, -0.0851, -0.0557, -0.0300, -0.0367, -0.0400, -0.0483,\n",
       "                       -0.0273, -0.0783, -0.0392, -0.0810, -0.0418, -0.0267, -0.0620, -0.0703,\n",
       "                       -0.0291, -0.0329, -0.0439, -0.0526, -0.0566, -0.0390, -0.0326, -0.0212,\n",
       "                       -0.0389, -0.0346, -0.0492, -0.0170, -0.0531, -0.0429, -0.0812, -0.0565,\n",
       "                       -0.0285, -0.1069, -0.0482, -0.0686, -0.0026, -0.0463, -0.0375, -0.0531,\n",
       "                       -0.0623, -0.0419, -0.0477, -0.0979, -0.0728, -0.0650, -0.0605, -0.0402],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn2.running_mean',\n",
       "               tensor([-4.9342e-03,  2.8813e-03, -9.7351e-04, -4.4269e-03,  3.8982e-03,\n",
       "                       -3.2001e-03, -2.1520e-02, -3.3708e-02,  5.7738e-03, -8.9014e-03,\n",
       "                        1.4417e-02, -4.5658e-03, -1.1634e-02, -5.6321e-03,  1.3996e-03,\n",
       "                        7.3185e-03, -1.8743e-02, -9.6414e-03, -1.8192e-02, -1.3828e-02,\n",
       "                       -6.1679e-03,  1.5803e-03,  2.5825e-03, -1.0924e-02, -2.8205e-02,\n",
       "                       -3.4775e-03, -1.2801e-02, -3.3076e-04,  5.3386e-03,  1.3599e-03,\n",
       "                        2.0046e-03, -1.9913e-02, -6.4788e-03,  5.9797e-03, -4.4810e-03,\n",
       "                       -1.3356e-02,  5.6204e-03, -4.0501e-04,  1.4132e-02, -1.2580e-02,\n",
       "                       -7.1645e-03, -5.0516e-03, -1.8444e-02, -2.1133e-02, -5.4259e-03,\n",
       "                       -2.8654e-02, -1.2658e-02, -2.1425e-02,  6.3542e-03, -7.7920e-03,\n",
       "                       -1.1630e-02, -6.5751e-03,  9.7026e-03, -1.6868e-02, -1.2506e-03,\n",
       "                       -1.8634e-03, -6.6736e-03, -4.0553e-03, -6.6688e-03, -2.1708e-02,\n",
       "                       -5.3950e-03, -7.4290e-03, -1.4245e-02, -8.1353e-03, -7.3725e-03,\n",
       "                       -1.1106e-03, -2.2129e-02, -3.1765e-03, -3.0154e-02, -1.0447e-02,\n",
       "                       -1.0455e-02,  5.0566e-04, -3.0935e-03,  2.0484e-03, -8.0236e-03,\n",
       "                       -5.9858e-03, -1.7682e-02, -4.2045e-03, -1.5770e-02, -2.0705e-02,\n",
       "                       -4.3199e-03, -3.2243e-03, -6.2181e-03,  4.5298e-03, -1.9495e-02,\n",
       "                       -7.6258e-03,  3.7054e-03, -4.9260e-03, -1.1992e-02,  7.5549e-03,\n",
       "                       -1.0068e-03, -5.0615e-03, -6.9406e-03, -2.0408e-03, -1.0504e-02,\n",
       "                        8.2423e-03,  1.0274e-02, -6.0016e-03, -2.0909e-02,  3.1138e-03,\n",
       "                       -2.4906e-03, -2.8951e-02, -1.6458e-02, -1.1130e-02,  1.5421e-03,\n",
       "                       -8.4131e-03,  8.2625e-03, -9.1732e-03,  2.7096e-03, -1.5244e-02,\n",
       "                       -1.2853e-02, -1.6570e-02,  4.5305e-03, -3.3838e-02, -1.6939e-02,\n",
       "                       -2.7934e-03, -1.6943e-02, -1.1419e-02, -1.4361e-02,  1.0024e-03,\n",
       "                        4.6273e-03, -4.6756e-03,  1.3580e-04, -1.9593e-02,  8.5348e-05,\n",
       "                       -1.0809e-02, -2.2549e-02,  5.5362e-04, -1.1648e-02, -3.0169e-02,\n",
       "                       -1.0099e-03, -8.1114e-03,  4.1633e-03, -5.0091e-04, -1.5500e-02,\n",
       "                        1.3524e-03, -1.9352e-03, -4.6420e-04, -1.2491e-02, -1.6822e-02,\n",
       "                       -2.5870e-02,  3.6467e-04, -3.2903e-03,  9.7629e-03, -1.5539e-02,\n",
       "                       -1.1039e-02, -6.9942e-03, -2.8318e-02, -1.1607e-02, -1.8950e-02,\n",
       "                       -4.9002e-03,  1.0658e-02, -1.8995e-02, -2.5593e-02, -6.1676e-03,\n",
       "                        1.7964e-03, -1.1274e-02,  1.0580e-02,  4.9701e-03, -1.7670e-02,\n",
       "                        3.1712e-03,  9.5165e-04, -2.9517e-03, -1.9350e-02, -1.5718e-02,\n",
       "                       -2.2097e-02, -2.4999e-02,  1.7407e-03, -1.1036e-02, -6.8355e-03,\n",
       "                       -1.8036e-02, -1.4624e-02,  1.9931e-03,  4.8118e-03, -7.1959e-03,\n",
       "                       -2.3127e-02, -1.3025e-02, -1.3239e-02, -4.3662e-03, -4.6626e-03,\n",
       "                       -2.2870e-04, -9.7238e-03,  1.6929e-04, -3.6067e-03, -2.5581e-02,\n",
       "                       -1.4176e-02, -7.6186e-03, -1.2372e-02,  6.5228e-03, -1.5996e-02,\n",
       "                       -2.1799e-02,  3.9948e-03, -2.2980e-02, -1.4553e-02, -2.0792e-03,\n",
       "                       -9.6261e-03, -1.3047e-02, -8.3578e-03,  1.8676e-03,  8.4808e-04,\n",
       "                        1.7333e-03, -2.8310e-03, -1.8526e-02, -2.2196e-03, -1.8714e-02,\n",
       "                       -5.3981e-03, -3.7800e-03, -8.3345e-03, -4.1431e-02,  2.4256e-03,\n",
       "                       -2.1609e-03, -1.7689e-02, -1.3499e-02, -1.9716e-02, -1.4082e-02,\n",
       "                        3.1333e-03,  7.6501e-03, -9.0007e-03, -1.4733e-02, -9.6943e-03,\n",
       "                       -2.1011e-02, -8.2820e-03, -2.5325e-04,  7.0149e-03, -1.1876e-02,\n",
       "                       -8.7053e-03, -1.1672e-02, -8.3615e-05, -6.8763e-03, -1.0015e-02,\n",
       "                       -2.1503e-03, -1.2286e-02, -2.8395e-02,  3.1533e-02, -1.7395e-02,\n",
       "                       -1.2043e-02, -6.6187e-03,  3.0465e-02, -2.0362e-02, -4.9114e-03,\n",
       "                        1.4257e-02,  7.4321e-04,  1.4230e-02, -2.8732e-03, -1.8548e-02,\n",
       "                       -1.4064e-02,  4.2730e-03, -6.2971e-03, -1.8280e-02,  8.3063e-03,\n",
       "                       -1.6524e-02, -8.6147e-03, -5.4051e-03,  9.7921e-04, -2.5865e-02,\n",
       "                       -7.2426e-03, -2.8763e-02, -1.2108e-02,  5.6031e-03, -3.1132e-02,\n",
       "                       -1.5514e-02,  3.8316e-03, -6.1916e-03, -2.5826e-02, -5.9884e-03,\n",
       "                        2.5537e-03, -1.7419e-02, -1.8112e-02, -3.6964e-03,  8.1692e-04,\n",
       "                       -1.7890e-02,  6.2529e-04, -8.3154e-03,  5.4266e-03, -3.1794e-02,\n",
       "                        9.6492e-04, -3.1345e-02,  3.2758e-04,  4.7815e-03,  1.7611e-03,\n",
       "                       -5.1099e-03, -2.3038e-02, -7.0638e-03, -1.7539e-02, -1.8335e-02,\n",
       "                        2.0178e-03, -5.9229e-04, -2.9060e-03, -1.6068e-02,  2.3172e-03,\n",
       "                       -1.9576e-02, -3.4052e-03, -9.2178e-03, -4.6536e-03, -1.1823e-02,\n",
       "                       -2.4512e-04,  8.6191e-03, -2.3377e-02, -9.4171e-03, -1.5249e-02,\n",
       "                       -4.8672e-03, -1.5357e-02, -2.1561e-03, -8.2470e-03, -6.6970e-03,\n",
       "                        1.2767e-04,  6.7807e-03,  4.3157e-03,  1.4250e-02, -1.4075e-02,\n",
       "                       -1.7204e-02, -1.5389e-02, -2.1948e-03,  6.6721e-03,  2.7041e-03,\n",
       "                        2.1002e-03, -6.0664e-03,  1.7695e-03, -1.5101e-02, -1.8759e-02,\n",
       "                       -2.3774e-03,  2.0001e-03, -4.8607e-03, -1.4234e-02, -6.1389e-03,\n",
       "                       -1.7446e-02,  5.5738e-03, -9.5396e-04,  1.6297e-03, -1.3812e-02,\n",
       "                        5.3056e-04, -3.1454e-02, -3.6649e-03, -1.7561e-03, -1.1448e-02,\n",
       "                       -1.0786e-02, -1.2079e-02,  4.3609e-03, -8.7264e-03, -6.7960e-04,\n",
       "                        4.6609e-03, -3.1966e-02, -1.1523e-02,  4.5470e-04, -1.6142e-02,\n",
       "                        1.5239e-03,  6.9268e-04, -1.1232e-02, -1.8823e-02,  2.2595e-03,\n",
       "                       -2.0868e-03, -1.4224e-02, -9.2531e-03, -1.0358e-02,  4.8322e-03,\n",
       "                       -1.7771e-04, -8.2963e-03, -2.8378e-02, -2.6927e-03,  1.0215e-02,\n",
       "                       -1.2633e-02, -2.5819e-02, -3.3913e-03, -1.9128e-02, -2.8648e-03,\n",
       "                       -1.3335e-02,  4.2310e-03, -1.4246e-02, -9.1719e-05,  9.5990e-04,\n",
       "                        3.5177e-03, -5.4934e-03,  1.4406e-03, -4.4589e-03,  3.8380e-04,\n",
       "                        4.4329e-04, -1.2068e-02, -2.2040e-03, -1.1463e-02,  7.5353e-04,\n",
       "                       -2.7323e-02, -1.4742e-02,  1.1976e-02, -3.7870e-03, -1.6765e-02,\n",
       "                        1.5106e-03, -1.8590e-02,  7.3754e-03, -1.6376e-02, -8.1959e-04,\n",
       "                       -9.6603e-04,  9.7097e-03,  1.4153e-02, -8.4632e-03, -8.2435e-03,\n",
       "                       -5.0368e-03, -6.6076e-03, -1.4295e-02, -1.7041e-02, -1.6333e-02,\n",
       "                        6.3013e-03,  1.7774e-02, -2.2391e-02, -2.0953e-02, -5.3214e-03,\n",
       "                        1.2944e-02,  4.7106e-03,  1.0586e-02, -1.0229e-02, -9.6988e-03,\n",
       "                       -2.7813e-02, -2.2006e-02, -1.6479e-02, -4.1685e-03,  2.8328e-03,\n",
       "                       -9.2724e-03,  1.7477e-03, -4.9732e-03, -6.3569e-03,  3.0336e-03,\n",
       "                       -1.7709e-02, -1.3643e-03, -3.0348e-02,  6.4127e-03, -6.1343e-03,\n",
       "                        4.2396e-03, -6.2336e-03,  9.4173e-03, -8.1014e-03, -6.3961e-03,\n",
       "                        9.4912e-03, -8.2012e-03, -9.1313e-03, -3.5776e-03, -4.1429e-03,\n",
       "                       -1.9954e-02, -1.0346e-02, -1.4769e-03, -2.3224e-03,  9.2381e-04,\n",
       "                        1.8736e-03, -1.3548e-02, -4.0844e-03,  1.0972e-03, -6.2526e-04,\n",
       "                       -5.2476e-03, -2.1553e-03,  1.6801e-02, -2.9052e-02, -1.8387e-02,\n",
       "                        2.5610e-03, -1.4760e-02, -6.4824e-04,  5.0712e-03, -2.2855e-02,\n",
       "                       -1.5093e-03, -1.4107e-02,  2.3570e-03, -1.1536e-02,  6.2975e-03,\n",
       "                       -1.6477e-02, -2.8556e-02, -2.1892e-02,  2.9888e-03, -2.1060e-02,\n",
       "                       -2.9236e-03, -9.9834e-03, -1.8517e-03,  2.7559e-03,  1.6771e-05,\n",
       "                        1.1996e-04, -1.1720e-02, -1.4008e-02, -1.5426e-02, -1.6055e-02,\n",
       "                       -8.9012e-03, -2.0601e-02, -5.0482e-03, -5.0428e-03, -6.0099e-03,\n",
       "                        2.0377e-04,  3.8250e-03, -1.3952e-02, -1.6907e-02, -9.5108e-03,\n",
       "                       -1.6733e-02, -1.0946e-02, -3.6867e-02, -2.0729e-02, -1.0114e-02,\n",
       "                       -1.7785e-02, -1.5871e-02, -5.2068e-03,  1.0998e-02,  1.4563e-02,\n",
       "                       -6.4536e-03, -1.1535e-02, -1.3728e-03, -2.6595e-03, -4.7747e-03,\n",
       "                       -3.2767e-02,  1.3796e-02,  1.4934e-02,  1.1994e-02, -5.7782e-03,\n",
       "                       -1.3829e-02,  8.1374e-04, -5.5317e-03,  1.6966e-03, -1.3862e-02,\n",
       "                       -1.5499e-02, -6.0481e-03], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn2.running_var',\n",
       "               tensor([4.1595e-04, 4.6145e-04, 5.9797e-04, 4.9395e-04, 5.7970e-04, 2.5797e-04,\n",
       "                       4.2006e-04, 8.1153e-04, 4.3289e-04, 7.9525e-04, 1.2115e-04, 1.0807e-04,\n",
       "                       8.3351e-04, 7.3398e-04, 6.1838e-04, 1.7575e-04, 3.6416e-04, 1.6791e-04,\n",
       "                       6.5508e-04, 4.5349e-04, 4.5882e-04, 2.0353e-04, 2.5390e-04, 1.9204e-04,\n",
       "                       5.3913e-04, 6.8967e-04, 4.9033e-04, 3.8339e-04, 4.6042e-04, 2.2205e-04,\n",
       "                       1.1468e-04, 3.7889e-04, 2.1836e-04, 2.7734e-04, 9.4711e-05, 5.6644e-04,\n",
       "                       2.2619e-04, 4.1216e-04, 4.1144e-04, 5.2990e-04, 2.6760e-04, 3.0631e-04,\n",
       "                       5.2381e-04, 5.8359e-04, 2.2186e-04, 5.2947e-04, 2.1659e-04, 6.6521e-04,\n",
       "                       8.1118e-05, 2.3897e-04, 3.8276e-04, 5.1646e-04, 5.1828e-04, 6.4659e-04,\n",
       "                       2.3792e-04, 4.7603e-04, 6.4529e-04, 7.2026e-04, 4.8519e-04, 1.4675e-03,\n",
       "                       2.3023e-04, 4.3685e-04, 3.0782e-04, 4.3146e-04, 1.4037e-04, 3.7424e-04,\n",
       "                       7.2306e-04, 1.7012e-04, 1.0173e-03, 3.0427e-04, 5.6006e-04, 2.1035e-05,\n",
       "                       1.0291e-04, 1.6741e-06, 4.6643e-04, 1.6667e-04, 5.2802e-04, 2.3659e-04,\n",
       "                       1.1500e-03, 3.8717e-04, 1.8698e-04, 5.0543e-04, 2.3044e-04, 1.2986e-05,\n",
       "                       3.6296e-04, 3.5424e-04, 5.6345e-04, 1.8349e-04, 1.2436e-03, 5.4940e-05,\n",
       "                       2.8676e-04, 1.9385e-04, 9.2041e-04, 2.1372e-04, 2.4254e-04, 3.6222e-04,\n",
       "                       6.3177e-04, 5.2326e-04, 7.2110e-04, 2.4317e-04, 3.4045e-04, 9.4613e-04,\n",
       "                       6.6514e-04, 3.5597e-04, 3.1976e-04, 2.6064e-04, 5.7782e-05, 3.0845e-04,\n",
       "                       1.6739e-04, 4.8000e-04, 3.6437e-04, 6.9834e-04, 4.4849e-04, 4.8821e-04,\n",
       "                       4.6443e-04, 5.1812e-04, 4.6521e-04, 6.4223e-04, 7.5709e-04, 3.6141e-04,\n",
       "                       1.3410e-04, 3.1346e-04, 9.1254e-06, 3.1967e-04, 2.7912e-04, 4.1732e-04,\n",
       "                       2.7231e-04, 5.7774e-05, 1.8876e-03, 4.3459e-04, 1.1860e-04, 1.5401e-04,\n",
       "                       2.5501e-04, 1.8901e-04, 5.5564e-04, 1.0767e-03, 2.3861e-04, 6.4975e-04,\n",
       "                       3.2615e-04, 4.5070e-04, 5.9121e-04, 3.0026e-04, 3.1469e-04, 1.5698e-04,\n",
       "                       7.6154e-04, 2.7097e-04, 1.4327e-04, 7.0221e-04, 3.5115e-04, 6.9124e-04,\n",
       "                       3.9842e-04, 1.8812e-04, 5.8095e-04, 7.4054e-04, 2.4158e-04, 6.0187e-05,\n",
       "                       2.1660e-04, 1.2977e-04, 3.3342e-04, 5.5665e-04, 3.9276e-04, 7.2059e-04,\n",
       "                       4.5064e-04, 7.3584e-04, 7.9732e-04, 3.6993e-04, 1.4310e-03, 1.5094e-04,\n",
       "                       4.8975e-04, 1.8841e-04, 6.3661e-04, 8.2287e-04, 5.4951e-04, 3.7196e-04,\n",
       "                       4.7468e-04, 3.0172e-04, 1.7125e-04, 6.8998e-04, 3.7433e-04, 3.7636e-04,\n",
       "                       5.0380e-04, 6.7536e-04, 5.6696e-04, 3.6857e-04, 1.1800e-03, 7.0777e-04,\n",
       "                       1.8365e-04, 3.1555e-04, 1.1390e-04, 5.1645e-04, 1.7887e-03, 1.5472e-04,\n",
       "                       3.5730e-04, 1.2171e-03, 1.1633e-04, 2.2337e-04, 3.4828e-04, 3.6140e-04,\n",
       "                       3.6913e-04, 4.6379e-04, 4.7324e-06, 1.2175e-04, 8.9722e-04, 3.4879e-04,\n",
       "                       6.3322e-04, 2.1870e-04, 3.0110e-04, 3.2194e-04, 9.3735e-04, 4.1207e-04,\n",
       "                       2.7540e-04, 4.9406e-04, 3.3178e-04, 4.3303e-04, 3.4887e-04, 2.1893e-05,\n",
       "                       2.1919e-04, 5.8750e-04, 3.4742e-04, 7.0832e-04, 9.7507e-04, 9.4946e-04,\n",
       "                       2.3427e-04, 5.0184e-04, 4.4247e-04, 9.8847e-05, 3.9281e-04, 4.6934e-04,\n",
       "                       4.2071e-04, 2.4848e-04, 2.4812e-04, 6.6828e-04, 5.1990e-04, 4.7297e-04,\n",
       "                       7.5155e-04, 4.2882e-04, 3.8733e-04, 2.0874e-04, 5.0070e-04, 3.5189e-04,\n",
       "                       4.4533e-04, 4.6217e-05, 4.2988e-04, 3.6752e-04, 2.9909e-04, 3.1695e-04,\n",
       "                       2.8370e-04, 3.1248e-04, 2.9321e-04, 6.2454e-04, 8.3022e-04, 7.8558e-04,\n",
       "                       6.6672e-04, 3.9447e-04, 5.6578e-04, 2.9707e-04, 9.3105e-04, 5.4685e-04,\n",
       "                       1.4538e-04, 5.2056e-04, 3.5160e-04, 4.1854e-05, 7.8682e-05, 1.3382e-03,\n",
       "                       1.3087e-03, 6.1557e-04, 4.6014e-04, 5.3466e-04, 4.3482e-04, 4.4409e-04,\n",
       "                       1.5537e-03, 1.1329e-05, 7.4616e-04, 3.1907e-04, 6.8093e-04, 1.6310e-04,\n",
       "                       4.6133e-04, 1.4708e-04, 4.1323e-04, 1.4603e-04, 2.5113e-04, 8.1350e-04,\n",
       "                       3.6710e-04, 9.1188e-04, 2.5474e-04, 1.6376e-04, 6.2130e-04, 5.6138e-04,\n",
       "                       3.7165e-04, 1.6441e-04, 2.8721e-04, 3.8248e-04, 5.2160e-04, 2.0045e-04,\n",
       "                       8.6093e-04, 3.0196e-04, 2.5277e-04, 8.0609e-04, 1.4488e-04, 7.2443e-04,\n",
       "                       1.4246e-04, 3.6608e-04, 2.1027e-04, 7.3686e-05, 3.1908e-04, 1.0278e-03,\n",
       "                       2.3717e-04, 6.6158e-04, 3.1646e-04, 4.5659e-04, 3.4452e-04, 6.0532e-04,\n",
       "                       4.0363e-04, 5.0531e-04, 6.7450e-04, 4.7957e-04, 3.5283e-04, 1.5953e-04,\n",
       "                       4.2822e-04, 9.7583e-04, 5.8457e-04, 2.1429e-04, 6.0679e-04, 3.0736e-04,\n",
       "                       8.1189e-04, 2.1764e-04, 7.1941e-05, 3.3032e-04, 2.3890e-04, 6.6454e-04,\n",
       "                       4.8282e-04, 1.2545e-03, 1.9817e-04, 1.5036e-04, 2.8181e-04, 4.1292e-04,\n",
       "                       7.8883e-04, 3.6006e-04, 3.7972e-04, 3.7194e-04, 3.9395e-04, 5.8875e-04,\n",
       "                       1.2758e-03, 2.2338e-04, 5.8950e-04, 1.6729e-04, 2.9665e-04, 3.5219e-04,\n",
       "                       6.3966e-04, 3.8838e-04, 3.7175e-04, 5.4958e-04, 6.0052e-04, 2.5678e-04,\n",
       "                       1.5503e-04, 6.3204e-04, 3.0371e-04, 8.9171e-04, 7.2095e-04, 5.1927e-04,\n",
       "                       1.5722e-04, 4.2336e-04, 3.0622e-04, 5.0732e-04, 2.7114e-04, 5.9133e-04,\n",
       "                       1.6052e-04, 1.0225e-04, 2.1341e-04, 4.2147e-04, 2.8397e-04, 9.2814e-05,\n",
       "                       4.9885e-05, 2.3756e-04, 7.3765e-04, 2.9383e-04, 7.8728e-04, 2.3107e-04,\n",
       "                       3.6604e-04, 4.1715e-04, 3.7413e-04, 4.2737e-04, 5.1487e-04, 4.5297e-04,\n",
       "                       6.2433e-04, 1.3064e-03, 2.9930e-04, 3.0775e-04, 4.2791e-04, 4.0134e-04,\n",
       "                       4.6706e-04, 5.3304e-04, 5.1835e-04, 6.7266e-04, 6.8649e-04, 4.1280e-04,\n",
       "                       3.2658e-04, 5.1621e-04, 3.3196e-04, 3.4585e-04, 4.5286e-04, 2.8756e-04,\n",
       "                       6.3960e-04, 2.3343e-04, 7.5935e-04, 1.2451e-04, 9.6333e-04, 4.7887e-04,\n",
       "                       4.0700e-04, 2.3494e-04, 4.1658e-04, 6.0186e-04, 3.3918e-04, 2.2094e-04,\n",
       "                       1.8317e-04, 5.2342e-04, 5.9718e-05, 4.3085e-04, 6.1413e-04, 4.9237e-04,\n",
       "                       3.0641e-04, 4.2254e-04, 2.1964e-03, 2.4030e-04, 1.8331e-04, 2.5994e-04,\n",
       "                       9.0516e-05, 3.5522e-04, 5.1784e-04, 1.4430e-04, 2.9912e-04, 2.2695e-04,\n",
       "                       4.5545e-04, 2.4101e-04, 2.0717e-04, 4.1756e-04, 2.9053e-04, 2.8261e-04,\n",
       "                       8.4353e-04, 1.7719e-04, 4.8038e-06, 5.0188e-04, 1.9019e-04, 1.7774e-04,\n",
       "                       5.4655e-04, 2.4038e-04, 2.8067e-04, 4.0500e-04, 7.8924e-04, 4.6945e-04,\n",
       "                       4.3303e-04, 3.5856e-04, 4.4558e-04, 1.5941e-04, 5.6437e-04, 8.6170e-05,\n",
       "                       4.9317e-04, 3.3540e-04, 3.2943e-04, 8.5292e-04, 1.1655e-03, 2.9261e-04,\n",
       "                       7.9986e-04, 1.2058e-04, 5.5704e-04, 2.6146e-04, 5.4435e-04, 4.6274e-04,\n",
       "                       5.1907e-04, 1.5705e-04, 3.6095e-04, 6.2862e-04, 2.7348e-04, 1.1640e-03,\n",
       "                       6.2344e-04, 7.6447e-04, 4.5921e-04, 2.3080e-04, 2.3860e-04, 2.0772e-04,\n",
       "                       3.7733e-04, 6.6590e-04, 1.7240e-04, 3.4477e-04, 2.7954e-04, 3.5036e-04,\n",
       "                       1.2944e-04, 9.6598e-04, 8.1765e-04, 3.2605e-04, 7.3396e-04, 8.1652e-04,\n",
       "                       5.9062e-04, 1.5364e-04, 6.2092e-04, 5.2105e-04, 3.7512e-04, 6.7330e-04,\n",
       "                       2.0020e-04, 2.5216e-04, 6.6522e-04, 4.2008e-04, 7.2668e-04, 2.7748e-04,\n",
       "                       5.1701e-04, 6.4762e-04, 4.6937e-04, 6.4002e-04, 5.3471e-04, 3.5761e-04,\n",
       "                       3.3386e-04, 3.2050e-04], device='cuda:0')),\n",
       "              ('model.model.layer4.0.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.0.weight',\n",
       "               tensor([[[[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0280]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]],\n",
       "               \n",
       "                        [[0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.0454]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[0.0000]],\n",
       "               \n",
       "                        [[-0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.0.weight_mask',\n",
       "               tensor([[[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]],\n",
       "               \n",
       "                        [[0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.1.weight',\n",
       "               tensor([ 0.0465,  0.1191,  0.1184,  0.1273,  0.0714,  0.0530,  0.1022,  0.0938,\n",
       "                        0.0703,  0.0929,  0.0323,  0.0303,  0.1168,  0.0498,  0.1048,  0.0547,\n",
       "                        0.0795,  0.0421,  0.1028,  0.0891,  0.0351,  0.0778,  0.1073,  0.0837,\n",
       "                        0.0715,  0.0043,  0.0863,  0.0774,  0.0801,  0.0253,  0.0229,  0.0799,\n",
       "                        0.0638,  0.0445,  0.0664,  0.0935,  0.0588,  0.1290,  0.0793,  0.0601,\n",
       "                        0.0488,  0.0583,  0.0991,  0.1005,  0.0828,  0.0733,  0.0591,  0.1157,\n",
       "                        0.0762,  0.0741,  0.0607,  0.1033,  0.0638,  0.0930,  0.0456,  0.0768,\n",
       "                        0.1646,  0.0591,  0.0609,  0.1118,  0.0812,  0.0966,  0.0326,  0.0240,\n",
       "                        0.0592,  0.0816,  0.1039,  0.0415,  0.1437,  0.0600,  0.0676,  0.0210,\n",
       "                        0.0154,  0.0169,  0.0914,  0.0512,  0.0772,  0.1007,  0.1503,  0.0795,\n",
       "                        0.0774,  0.1024,  0.0656,  0.0234,  0.0782,  0.1092,  0.0765,  0.0521,\n",
       "                        0.0864,  0.0277,  0.0701,  0.0448,  0.1182,  0.0723,  0.0555,  0.0857,\n",
       "                        0.1243,  0.0616,  0.0657,  0.0457,  0.0516,  0.1436,  0.1012,  0.1250,\n",
       "                        0.1037,  0.0770,  0.0173,  0.0318,  0.0236,  0.0996,  0.0494,  0.0658,\n",
       "                        0.0710,  0.0848,  0.0568,  0.0858,  0.0654,  0.1621,  0.1157,  0.0798,\n",
       "                        0.0184,  0.0619,  0.0143,  0.0878,  0.0311,  0.0463,  0.0543,  0.0529,\n",
       "                        0.1744,  0.0747,  0.0511,  0.0595,  0.0491,  0.0983,  0.1385,  0.1477,\n",
       "                        0.0345,  0.1217,  0.0835,  0.1126,  0.0737,  0.0995,  0.0663,  0.0623,\n",
       "                        0.1457,  0.0633,  0.0549,  0.1191,  0.0356,  0.1370,  0.0225,  0.0231,\n",
       "                        0.1088,  0.0775,  0.0855,  0.0319,  0.0503,  0.0444,  0.0400,  0.1023,\n",
       "                        0.1090,  0.1320,  0.0899,  0.1542,  0.1052,  0.0379,  0.1686,  0.0248,\n",
       "                        0.1090,  0.0677,  0.0898,  0.1165,  0.0670,  0.0643,  0.1103,  0.0807,\n",
       "                        0.0846,  0.1240,  0.0478,  0.0269,  0.0887,  0.1603,  0.1437,  0.0251,\n",
       "                        0.1735,  0.1366,  0.0546,  0.0752,  0.0285,  0.0713,  0.2187,  0.1184,\n",
       "                        0.0867,  0.1558,  0.0612,  0.0587,  0.0619,  0.0663,  0.0984,  0.0920,\n",
       "                        0.0171,  0.0543,  0.1792,  0.0758,  0.1274,  0.0383,  0.1062,  0.0999,\n",
       "                        0.1610,  0.1117,  0.0794,  0.0711,  0.0323,  0.1045,  0.0845,  0.0247,\n",
       "                        0.0434,  0.1010,  0.0828,  0.0777,  0.1480,  0.1915,  0.0554,  0.1183,\n",
       "                        0.0812,  0.0671,  0.0845,  0.0391,  0.0900,  0.0356,  0.0566,  0.0749,\n",
       "                        0.0731,  0.0520,  0.1546,  0.1179,  0.0322,  0.0326,  0.0974,  0.0157,\n",
       "                        0.0738,  0.0160,  0.1092,  0.0710,  0.0211,  0.0409,  0.0834,  0.0776,\n",
       "                        0.0333,  0.0892,  0.1146,  0.1307,  0.1483,  0.0886,  0.1072,  0.0531,\n",
       "                        0.0841,  0.1163,  0.0195,  0.0826,  0.0568,  0.0426,  0.0165,  0.1559,\n",
       "                        0.1594,  0.1213,  0.0888,  0.0547,  0.0748,  0.0539,  0.1226,  0.0267,\n",
       "                        0.1228,  0.1047,  0.0903,  0.0655,  0.0951,  0.0696,  0.1017,  0.0199,\n",
       "                        0.0193,  0.1057,  0.0676,  0.0950, -0.0092,  0.0225,  0.1254,  0.0747,\n",
       "                        0.0542,  0.0549,  0.0631,  0.0815,  0.0690,  0.0441,  0.0769,  0.0949,\n",
       "                        0.0266,  0.1195,  0.0495,  0.0803,  0.0768,  0.0679,  0.0625,  0.0198,\n",
       "                        0.0646,  0.1803,  0.0131,  0.0721,  0.0511,  0.0682,  0.0602,  0.0635,\n",
       "                        0.0504,  0.1282,  0.0218,  0.0696,  0.0597,  0.0846,  0.0821, -0.0339,\n",
       "                        0.0982,  0.0595,  0.1510,  0.0837,  0.0826,  0.0766,  0.0174,  0.0869,\n",
       "                        0.0320,  0.0920,  0.0636,  0.1068,  0.0367,  0.0277,  0.0603,  0.1109,\n",
       "                        0.0897,  0.0628,  0.0607,  0.0165,  0.0234,  0.0044,  0.1602,  0.0826,\n",
       "                        0.0893,  0.0234,  0.0376,  0.0904,  0.0755,  0.0189,  0.0589,  0.0827,\n",
       "                        0.1231,  0.0917,  0.0610,  0.1110,  0.0900,  0.0999,  0.1268,  0.0717,\n",
       "                        0.0158,  0.0725,  0.0123,  0.0611,  0.0573,  0.1050,  0.0380,  0.0656,\n",
       "                        0.0619,  0.0216,  0.0270,  0.0257,  0.0229,  0.0198,  0.0903,  0.0577,\n",
       "                        0.0771,  0.0746,  0.0805,  0.0919,  0.0624,  0.0667,  0.0530,  0.1391,\n",
       "                        0.1055,  0.1448,  0.0616,  0.0824,  0.0570,  0.0732,  0.0818,  0.1703,\n",
       "                        0.1476,  0.1106,  0.1451,  0.0670,  0.0459,  0.0914,  0.0753,  0.0704,\n",
       "                        0.1239,  0.0766,  0.1056,  0.0452,  0.1049,  0.0654,  0.1266,  0.0199,\n",
       "                        0.0874,  0.0771,  0.0513,  0.0740,  0.1363,  0.0508,  0.0667,  0.0862,\n",
       "                        0.0868,  0.0408,  0.0522,  0.0859,  0.0616,  0.1367,  0.1619,  0.0597,\n",
       "                        0.0534,  0.0725,  0.0303,  0.0692,  0.0284,  0.0252,  0.0593,  0.0339,\n",
       "                        0.1293,  0.0513,  0.1031,  0.0537,  0.0255,  0.1063,  0.1376,  0.0198,\n",
       "                        0.0232,  0.0914,  0.0258,  0.0885,  0.0491,  0.0654,  0.0570,  0.0747,\n",
       "                        0.0722,  0.0828,  0.0696,  0.0825,  0.0058,  0.0723,  0.0978,  0.0413,\n",
       "                        0.0744,  0.0595,  0.0825,  0.1502,  0.1493,  0.0476,  0.0730,  0.0139,\n",
       "                        0.1145,  0.0232,  0.0656,  0.0793,  0.1475,  0.0456,  0.0510,  0.1206,\n",
       "                        0.0655,  0.1443,  0.1259,  0.0729,  0.0628,  0.0699,  0.0600,  0.0515,\n",
       "                        0.0927,  0.0619,  0.0781,  0.0863,  0.0485,  0.0462,  0.0611,  0.1428,\n",
       "                        0.1049,  0.0602,  0.1222,  0.1540,  0.0483,  0.0555,  0.0836,  0.1240,\n",
       "                        0.1018,  0.0500,  0.0452,  0.0674,  0.1278,  0.0797,  0.1728,  0.0637,\n",
       "                        0.0542,  0.1429,  0.0895,  0.0978,  0.0881,  0.0700,  0.1185,  0.0511],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.1.bias',\n",
       "               tensor([-0.0515, -0.0480, -0.0139, -0.0420, -0.0184, -0.0500, -0.0633, -0.0939,\n",
       "                       -0.0413, -0.0679, -0.0201, -0.0312, -0.0406, -0.0312, -0.0182, -0.0414,\n",
       "                       -0.0580, -0.0492, -0.0340, -0.0289, -0.0559, -0.0731, -0.0391, -0.0387,\n",
       "                       -0.0825, -0.0918, -0.0313, -0.0554, -0.0641, -0.0537, -0.0367, -0.0581,\n",
       "                       -0.0251, -0.0533, -0.0356, -0.0392, -0.0371, -0.0358, -0.0637, -0.0342,\n",
       "                       -0.0407, -0.0331, -0.0429, -0.0083, -0.0528, -0.0741, -0.0438, -0.0638,\n",
       "                       -0.0212, -0.0312, -0.0351, -0.0332, -0.0595, -0.0748, -0.0201, -0.0531,\n",
       "                       -0.0360, -0.0578, -0.0254, -0.0055, -0.0596, -0.0607, -0.0498, -0.0445,\n",
       "                       -0.0348,  0.0004, -0.0121, -0.0756, -0.0626, -0.0640, -0.0571, -0.0162,\n",
       "                       -0.0517, -0.0170, -0.0484, -0.0416, -0.0585, -0.0425, -0.0254, -0.0467,\n",
       "                       -0.0482, -0.0570, -0.0255, -0.0127, -0.0537, -0.0292, -0.0279, -0.0478,\n",
       "                       -0.0278, -0.0030, -0.0405, -0.0470, -0.0224, -0.0485, -0.0096, -0.0442,\n",
       "                       -0.0527, -0.0924, -0.0601, -0.0627, -0.0546, -0.0539, -0.0100, -0.0337,\n",
       "                       -0.0647, -0.0345, -0.0160, -0.0432, -0.0501, -0.0306, -0.0413, -0.0804,\n",
       "                       -0.0514, -0.0863, -0.0170, -0.0467, -0.0527, -0.0487, -0.0458, -0.0645,\n",
       "                       -0.0220, -0.0552, -0.0030, -0.0290, -0.0514, -0.0988, -0.0348, -0.0120,\n",
       "                       -0.0703, -0.0317, -0.0486, -0.0625, -0.0287, -0.0645, -0.0595, -0.0450,\n",
       "                       -0.0225, -0.0549, -0.0598, -0.0734, -0.0513, -0.0662, -0.0447, -0.0480,\n",
       "                       -0.0173, -0.0456, -0.0487, -0.0248, -0.0596, -0.0315, -0.0481, -0.0201,\n",
       "                       -0.0300, -0.0504, -0.0403, -0.0244, -0.0427, -0.0407, -0.0642, -0.0429,\n",
       "                       -0.0730, -0.0572, -0.0259,  0.0677, -0.1200, -0.0403, -0.0451, -0.0145,\n",
       "                       -0.0683, -0.0439, -0.0875, -0.0214, -0.0629, -0.0810, -0.0467, -0.0707,\n",
       "                       -0.0340, -0.0514, -0.0266, -0.0303, -0.0692, -0.0398, -0.0399, -0.0508,\n",
       "                       -0.0160, -0.0151, -0.0233, -0.0342, -0.0505, -0.0655, -0.0540, -0.0597,\n",
       "                       -0.0234, -0.0356, -0.0399, -0.0759, -0.0442, -0.0371, -0.0376, -0.0316,\n",
       "                       -0.0156, -0.0481, -0.0565, -0.0650, -0.0545, -0.0622, -0.0343, -0.0644,\n",
       "                       -0.0374, -0.0473, -0.0469, -0.0568, -0.0585, -0.0217, -0.0210, -0.0148,\n",
       "                       -0.0377, -0.0687, -0.0458, -0.0909, -0.0832, -0.0523, -0.0509, -0.0528,\n",
       "                       -0.0339, -0.0458, -0.0354, -0.0771, -0.0494, -0.0374, -0.0451, -0.0371,\n",
       "                       -0.0752, -0.0279, -0.0510, -0.0421, -0.0457, -0.0297, -0.0487, -0.0313,\n",
       "                       -0.0467, -0.0237, -0.0574, -0.0306, -0.0739, -0.0406, -0.0460, -0.0296,\n",
       "                       -0.0146, -0.0471, -0.0524, -0.0223, -0.0836, -0.0199,  0.0042, -0.0287,\n",
       "                       -0.0343, -0.0656, -0.0478, -0.0622, -0.0267, -0.0307, -0.0217,  0.0234,\n",
       "                       -0.0170, -0.0390, -0.0667, -0.0825, -0.0788, -0.0300, -0.0458, -0.0312,\n",
       "                       -0.0675, -0.0808, -0.0982, -0.0173, -0.0628, -0.0483, -0.0910, -0.0319,\n",
       "                       -0.0431, -0.0547, -0.0453, -0.0280,  0.0006, -0.0382, -0.0483, -0.0740,\n",
       "                       -0.0231, -0.0617, -0.0484, -0.0356, -0.0160, -0.0401, -0.0265, -0.0445,\n",
       "                       -0.0278, -0.0353, -0.0248, -0.0286, -0.0398, -0.0332, -0.0369, -0.0221,\n",
       "                       -0.0394, -0.0339, -0.0544, -0.0680, -0.0927, -0.0248, -0.0603, -0.0545,\n",
       "                       -0.0346, -0.0355, -0.0449, -0.0546, -0.0403, -0.0460, -0.0695,  0.0054,\n",
       "                       -0.0358, -0.0624, -0.0563, -0.0589, -0.0649, -0.0506, -0.0444, -0.0761,\n",
       "                       -0.0649, -0.0753, -0.0358, -0.0670, -0.0227, -0.0558, -0.0364, -0.0566,\n",
       "                       -0.0257, -0.0558, -0.0250, -0.0511, -0.0534, -0.0182,  0.0113, -0.0533,\n",
       "                       -0.0730, -0.0494, -0.0655, -0.0363, -0.0228, -0.0361, -0.0671, -0.0713,\n",
       "                       -0.0259, -0.0725, -0.0370, -0.0529, -0.0523, -0.0442, -0.0252, -0.0511,\n",
       "                       -0.0239, -0.0379, -0.0310, -0.0407, -0.0246, -0.0530, -0.0434, -0.0465,\n",
       "                       -0.0418, -0.0746, -0.0455, -0.0282, -0.0293, -0.0595, -0.0360, -0.0429,\n",
       "                       -0.0590, -0.0528, -0.0504, -0.0554, -0.0148, -0.0362, -0.0333, -0.0440,\n",
       "                       -0.0561, -0.0517, -0.0291, -0.0863, -0.0259, -0.0400, -0.0527, -0.0503,\n",
       "                       -0.0642, -0.0306, -0.0556, -0.0902, -0.0273, -0.0627, -0.0277, -0.0431,\n",
       "                       -0.0337, -0.0638, -0.0232, -0.0261, -0.0665, -0.0194, -0.0560, -0.0985,\n",
       "                       -0.0467, -0.0609, -0.0382, -0.0770, -0.0124, -0.0630, -0.0402, -0.0675,\n",
       "                       -0.0372, -0.0365, -0.0714, -0.0577, -0.0077, -0.0409, -0.0486, -0.0320,\n",
       "                       -0.0236, -0.0406, -0.0311, -0.0312, -0.0167, -0.0571, -0.0657, -0.0262,\n",
       "                       -0.0592, -0.0734, -0.0300, -0.0551, -0.0287, -0.0546, -0.0342, -0.0551,\n",
       "                       -0.0157, -0.0311, -0.0546, -0.0421, -0.0548, -0.0276, -0.0388, -0.0592,\n",
       "                       -0.1035, -0.0512, -0.0634, -0.0754, -0.0414, -0.0288, -0.0349, -0.0192,\n",
       "                       -0.0605, -0.0605, -0.0501, -0.0388, -0.0178, -0.0429, -0.0455, -0.0397,\n",
       "                       -0.0323, -0.0456, -0.0851, -0.0557, -0.0300, -0.0367, -0.0400, -0.0483,\n",
       "                       -0.0273, -0.0783, -0.0392, -0.0810, -0.0418, -0.0267, -0.0620, -0.0703,\n",
       "                       -0.0291, -0.0329, -0.0439, -0.0526, -0.0566, -0.0390, -0.0326, -0.0212,\n",
       "                       -0.0389, -0.0346, -0.0492, -0.0170, -0.0531, -0.0429, -0.0812, -0.0565,\n",
       "                       -0.0285, -0.1069, -0.0482, -0.0686, -0.0026, -0.0463, -0.0375, -0.0531,\n",
       "                       -0.0623, -0.0419, -0.0477, -0.0979, -0.0728, -0.0650, -0.0605, -0.0402],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.1.running_mean',\n",
       "               tensor([ 7.4538e-05,  4.0140e-02,  4.3585e-02,  3.9091e-02,  3.5555e-02,\n",
       "                        1.1581e-02, -1.3181e-02,  1.3030e-02,  2.0722e-02,  3.7896e-02,\n",
       "                        2.5031e-03,  5.6052e-45,  1.5307e-02,  4.1331e-03, -2.0699e-02,\n",
       "                       -1.1731e-02,  1.7672e-02, -1.9338e-02, -2.5887e-02, -1.0041e-02,\n",
       "                        1.8029e-02,  7.4556e-04,  2.6906e-02, -1.0693e-02, -1.6154e-02,\n",
       "                        4.5548e-03,  1.5344e-02,  1.1001e-02, -4.3513e-03,  6.3886e-03,\n",
       "                       -1.2468e-04,  2.3548e-02,  3.9384e-03, -1.4299e-03,  1.3384e-03,\n",
       "                       -2.8939e-02,  7.1171e-03, -1.5532e-02,  1.6213e-02,  9.3713e-03,\n",
       "                       -1.7723e-02, -1.0671e-02, -2.8290e-02, -2.4453e-02,  8.6609e-03,\n",
       "                        3.7318e-03, -7.1149e-03,  1.7947e-03,  9.0062e-03, -4.2479e-03,\n",
       "                        2.5849e-02, -2.7089e-02, -9.1853e-03, -1.0673e-02, -1.9525e-02,\n",
       "                       -2.0786e-03,  8.4854e-03,  3.4854e-02,  8.7188e-03, -2.1275e-02,\n",
       "                       -1.2067e-02, -1.9594e-02,  1.2586e-03,  1.6473e-02,  5.1608e-03,\n",
       "                       -2.9612e-02, -5.0139e-03,  6.5889e-03, -2.8404e-02,  6.9861e-03,\n",
       "                        1.3551e-02, -5.6052e-45,  7.9549e-04, -5.6052e-45, -2.6330e-02,\n",
       "                        4.6120e-03, -1.3917e-02,  8.7529e-03, -2.8546e-02,  1.0311e-02,\n",
       "                        3.4129e-03,  1.8416e-02, -1.1986e-02, -5.6052e-45,  9.2492e-03,\n",
       "                        1.8344e-02,  1.2787e-02, -3.8750e-03,  3.8219e-03, -5.6052e-45,\n",
       "                        4.0523e-03,  5.7128e-03,  3.0678e-02,  8.0129e-03,  2.1341e-02,\n",
       "                        1.2812e-02,  5.9976e-03, -1.7840e-02, -1.4518e-02,  1.3396e-03,\n",
       "                        7.3009e-03,  4.4014e-02, -8.6730e-03, -6.2618e-03,  1.9983e-02,\n",
       "                       -3.6913e-03, -5.6052e-45,  1.0661e-03,  5.6052e-45,  7.8154e-03,\n",
       "                       -1.6496e-02,  3.0777e-03, -8.6363e-03,  6.0784e-03, -4.1448e-03,\n",
       "                       -4.4140e-03,  1.1158e-02,  8.9719e-03,  4.9186e-03,  1.7495e-02,\n",
       "                       -5.6052e-45,  1.0704e-03, -5.6052e-45,  4.8081e-03,  7.0691e-03,\n",
       "                        3.0073e-02,  4.5364e-03,  5.4612e-03, -2.6208e-02, -5.5557e-03,\n",
       "                        1.1054e-02,  8.7420e-03,  1.0539e-02, -3.1374e-03, -3.2225e-02,\n",
       "                        2.7760e-02,  4.7043e-03, -2.4528e-02,  7.5691e-03,  1.1541e-02,\n",
       "                        3.4655e-02, -2.4971e-02, -4.7117e-03, -5.2211e-03,  2.5614e-02,\n",
       "                        6.3549e-03,  7.2818e-03,  3.5146e-02,  7.9637e-03, -3.4029e-02,\n",
       "                       -3.9720e-03, -5.6052e-45, -1.4286e-02,  1.6809e-02,  1.3290e-02,\n",
       "                       -5.6052e-45, -7.4545e-03, -5.0059e-03, -7.1471e-03, -1.1196e-02,\n",
       "                        1.1329e-02, -2.5844e-02, -5.7433e-03, -5.2692e-02, -8.6341e-03,\n",
       "                        2.7277e-03,  9.1693e-03, -5.6052e-45,  1.6395e-02, -1.0631e-03,\n",
       "                        1.3326e-02,  6.6769e-02,  2.6634e-03,  1.3424e-02,  1.4875e-02,\n",
       "                       -2.3017e-02, -2.1863e-02, -1.7363e-03, -1.0872e-02, -5.6052e-45,\n",
       "                       -5.7555e-03,  1.1747e-02,  1.4900e-02,  9.3409e-03,  1.0165e-02,\n",
       "                        2.1596e-03, -2.3507e-03, -1.6083e-02, -5.6052e-45, -2.5339e-03,\n",
       "                        4.3613e-02,  2.9260e-02,  9.5306e-03,  2.9039e-03,  3.3242e-03,\n",
       "                       -1.4379e-02, -6.0708e-03, -1.3631e-02,  1.9838e-02,  3.2890e-02,\n",
       "                       -5.6052e-45,  1.2980e-02, -1.9053e-03,  3.5151e-02,  1.9440e-02,\n",
       "                        2.5159e-03,  2.1127e-02, -1.7940e-02,  7.7742e-03,  1.2528e-02,\n",
       "                        2.7946e-02,  7.7092e-03,  1.7143e-03,  4.2209e-02,  6.8680e-04,\n",
       "                        5.6052e-45,  3.3875e-03,  8.0588e-04, -2.6061e-02,  2.3087e-02,\n",
       "                       -2.0963e-02, -5.3635e-02,  4.4515e-03,  2.5971e-02,  4.6631e-03,\n",
       "                        6.0492e-03, -1.3810e-02,  1.9667e-03,  4.6330e-02,  1.7771e-02,\n",
       "                       -1.0530e-02,  5.4553e-05, -2.8160e-03, -4.2427e-03, -4.2161e-03,\n",
       "                       -1.6687e-02,  1.2504e-02,  2.8935e-03, -2.0588e-02, -5.6052e-45,\n",
       "                        1.9268e-02, -5.6052e-45,  3.5715e-02, -5.2038e-03,  3.7092e-03,\n",
       "                        5.5838e-03, -1.3395e-02,  6.8806e-03,  1.9499e-03,  4.4208e-02,\n",
       "                        1.5060e-02, -2.1139e-03, -5.4473e-03,  1.8787e-02, -1.0866e-02,\n",
       "                       -4.1302e-03, -2.3987e-02, -6.1581e-03,  5.6052e-45,  1.4792e-02,\n",
       "                        1.0850e-02,  3.9827e-03, -5.6052e-45, -4.3321e-02, -2.9475e-02,\n",
       "                        2.3192e-02, -4.1267e-03,  1.1386e-02,  5.0893e-03,  1.6196e-02,\n",
       "                       -1.2105e-02, -5.6052e-45, -2.0641e-02,  3.9005e-02, -4.0418e-02,\n",
       "                       -8.6255e-03, -1.5072e-02,  5.1248e-03, -2.6827e-03,  2.6868e-03,\n",
       "                       -5.6052e-45, -1.0188e-02,  2.2924e-02,  2.9960e-02, -5.6052e-45,\n",
       "                        4.1026e-04,  1.8074e-02,  1.4263e-02,  6.2300e-03,  1.1477e-02,\n",
       "                        1.4004e-02,  1.3324e-02, -3.2915e-02,  5.6221e-03,  3.2198e-03,\n",
       "                        1.0476e-02, -5.1883e-04, -4.8600e-02,  3.5645e-03,  2.5119e-02,\n",
       "                        1.4706e-02, -2.0636e-02,  7.9262e-03,  4.9765e-03,  6.1403e-03,\n",
       "                       -3.5938e-02, -5.6052e-45,  1.5878e-02,  1.8189e-02, -7.6638e-03,\n",
       "                        5.1513e-02, -1.3379e-02,  6.5905e-03,  3.2815e-02,  7.3542e-03,\n",
       "                        4.7070e-02,  1.7960e-02,  1.3509e-02, -1.9447e-02,  3.3552e-03,\n",
       "                        2.4506e-02, -9.2152e-03,  5.5041e-02, -4.2091e-03,  2.7403e-02,\n",
       "                       -1.8908e-02, -5.6052e-45, -1.1019e-02,  1.4707e-02,  4.5319e-02,\n",
       "                        2.4913e-02, -3.0231e-03,  4.0963e-03,  1.6678e-03,  1.7904e-02,\n",
       "                       -2.0674e-02, -1.9381e-02, -6.7697e-03, -1.4645e-02, -4.1406e-03,\n",
       "                        1.3292e-02, -9.4151e-04, -1.9470e-02, -1.1718e-02, -5.9916e-03,\n",
       "                       -5.6052e-45, -1.7057e-03,  2.3874e-02, -2.4513e-02,  5.6052e-45,\n",
       "                        2.3558e-02, -2.7166e-02, -6.2925e-02,  3.4708e-02, -2.1908e-03,\n",
       "                        7.4113e-03,  9.3097e-03, -1.8388e-02,  1.1233e-02,  4.7664e-02,\n",
       "                       -5.6052e-45, -2.3670e-02,  8.5928e-03, -1.1727e-02,  1.4212e-02,\n",
       "                        4.8112e-03,  6.5036e-03,  1.4396e-02,  1.4594e-02, -1.8234e-03,\n",
       "                        8.9150e-03,  5.6052e-45, -1.1949e-03,  5.6052e-45,  3.9032e-02,\n",
       "                        1.3675e-02,  7.6527e-03,  1.8135e-02,  1.0648e-02,  2.1011e-03,\n",
       "                       -1.5954e-02, -2.1775e-02,  3.2308e-02, -1.2790e-02,  2.5475e-02,\n",
       "                       -1.8385e-02, -1.6285e-02,  5.0832e-03,  2.7321e-04,  8.9868e-04,\n",
       "                        5.3320e-02,  1.6446e-02,  1.5426e-02,  1.4327e-02, -3.3102e-02,\n",
       "                        1.4898e-03,  1.8147e-02,  2.3390e-02,  1.2244e-02,  2.7399e-02,\n",
       "                        4.2308e-03,  9.9678e-03, -1.5554e-04,  3.7352e-03,  5.4478e-03,\n",
       "                        1.8580e-03, -1.1294e-02,  1.8208e-02,  3.2930e-02,  5.9521e-03,\n",
       "                       -1.5166e-03, -2.4446e-02, -1.2633e-02,  3.6586e-03,  5.3365e-03,\n",
       "                       -4.1682e-03,  1.4896e-02,  1.6669e-02, -7.8359e-03,  1.8874e-02,\n",
       "                        9.1026e-03, -2.2589e-02,  7.7565e-03,  7.0699e-04,  4.3676e-04,\n",
       "                        5.4771e-03,  1.4324e-03, -2.1405e-03, -5.6052e-45,  1.4514e-03,\n",
       "                       -3.3034e-03,  4.6799e-03, -3.4823e-02, -1.6344e-02,  8.0619e-03,\n",
       "                        1.4292e-02,  5.6052e-45,  4.8378e-02, -1.8731e-02, -5.6052e-45,\n",
       "                       -5.6052e-45,  2.3693e-02,  3.2621e-03,  1.3895e-02,  1.8304e-02,\n",
       "                        4.0845e-03,  1.4962e-02,  1.9867e-02, -2.4524e-02,  2.8062e-02,\n",
       "                        2.5206e-02,  9.1117e-03,  1.3839e-03,  7.2343e-03,  2.2909e-03,\n",
       "                        1.3195e-03, -4.3723e-03,  1.0314e-02, -3.7781e-02,  4.3572e-02,\n",
       "                       -2.7413e-02,  2.6038e-03, -1.8183e-02,  1.6431e-04, -4.5399e-02,\n",
       "                        8.9205e-03, -9.8229e-03,  2.6601e-02,  2.7795e-02, -6.1849e-03,\n",
       "                       -9.0097e-03, -1.5219e-02, -3.7097e-03, -3.3809e-02,  3.6581e-02,\n",
       "                        2.2771e-02,  3.1543e-03,  1.3244e-02, -2.4674e-04,  2.3914e-03,\n",
       "                        2.3520e-02,  9.5469e-03,  1.3975e-02, -9.0903e-03,  1.0009e-02,\n",
       "                        1.9457e-02,  1.5749e-02, -4.2565e-02,  3.4170e-02,  1.5424e-02,\n",
       "                       -4.1686e-03,  3.5842e-02, -8.6971e-03,  2.1967e-02, -6.7268e-03,\n",
       "                        2.0889e-02,  1.7740e-03,  4.5280e-03, -9.7466e-03,  5.7259e-03,\n",
       "                       -5.3739e-02,  6.8313e-03,  7.3516e-03, -3.0080e-03, -3.5117e-02,\n",
       "                       -5.2623e-04,  8.1149e-03,  2.8721e-03,  8.7284e-03,  2.3920e-02,\n",
       "                        3.7489e-02,  1.4361e-02], device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.1.running_var',\n",
       "               tensor([1.3647e-04, 8.7413e-04, 5.9837e-04, 1.9117e-03, 5.9945e-04, 1.6749e-04,\n",
       "                       1.6126e-03, 1.5531e-03, 4.3613e-04, 1.5997e-03, 1.2470e-05, 5.6052e-45,\n",
       "                       2.9517e-03, 3.4542e-05, 1.5967e-03, 5.1968e-05, 6.4313e-04, 1.6377e-04,\n",
       "                       2.1289e-03, 1.7443e-03, 1.1908e-04, 4.2281e-04, 3.5972e-04, 4.5078e-04,\n",
       "                       4.3689e-04, 9.5692e-06, 1.4369e-03, 3.4163e-04, 6.6354e-04, 4.7747e-05,\n",
       "                       2.3150e-05, 5.4834e-04, 2.5346e-04, 8.0654e-05, 7.3437e-05, 1.3513e-03,\n",
       "                       1.1263e-04, 9.9731e-04, 3.3488e-04, 6.4457e-04, 2.4875e-04, 2.0613e-04,\n",
       "                       1.1866e-03, 2.5626e-03, 2.9065e-04, 8.1197e-04, 1.8829e-04, 2.8347e-03,\n",
       "                       5.6787e-05, 2.6487e-04, 3.7065e-04, 2.1380e-03, 4.9053e-04, 6.5392e-04,\n",
       "                       2.1389e-04, 8.7197e-04, 2.1694e-03, 6.1276e-04, 7.4340e-04, 3.1253e-03,\n",
       "                       5.0241e-04, 1.0043e-03, 6.1821e-05, 8.9741e-05, 3.0144e-05, 1.2393e-03,\n",
       "                       1.6830e-03, 5.2984e-05, 3.5007e-03, 3.2962e-04, 1.4985e-04, 5.6052e-45,\n",
       "                       9.1976e-06, 5.6052e-45, 1.0601e-03, 3.2345e-05, 5.7541e-04, 6.7713e-04,\n",
       "                       3.4622e-03, 7.6134e-04, 3.3948e-04, 1.4732e-03, 4.9352e-04, 5.6052e-45,\n",
       "                       4.7868e-04, 5.5878e-04, 5.3141e-04, 2.0132e-04, 5.7960e-04, 5.6052e-45,\n",
       "                       1.8547e-04, 9.4910e-05, 2.2985e-03, 4.2626e-04, 2.7842e-04, 4.8146e-04,\n",
       "                       1.2461e-03, 4.3942e-04, 7.1456e-04, 1.2420e-04, 1.4555e-04, 1.7094e-03,\n",
       "                       1.2362e-03, 1.7190e-03, 1.4128e-03, 4.4009e-04, 5.6052e-45, 2.6736e-04,\n",
       "                       5.6052e-45, 1.3009e-03, 5.0185e-04, 5.0490e-04, 2.4559e-04, 1.0565e-03,\n",
       "                       3.2862e-04, 7.0088e-04, 3.7141e-04, 2.9900e-03, 3.4198e-03, 7.9387e-04,\n",
       "                       5.6052e-45, 3.7684e-04, 5.6052e-45, 7.0795e-04, 6.0947e-05, 3.7786e-04,\n",
       "                       2.9560e-05, 3.8352e-05, 7.9581e-03, 2.5057e-04, 9.3336e-05, 1.2642e-04,\n",
       "                       1.6913e-04, 2.6357e-04, 2.5256e-03, 2.5693e-03, 3.0995e-05, 8.6931e-04,\n",
       "                       2.8149e-04, 1.3530e-03, 7.8578e-04, 1.0351e-03, 2.4155e-04, 2.9835e-05,\n",
       "                       1.3788e-03, 2.9973e-04, 2.1731e-04, 3.3095e-03, 5.2510e-05, 3.3931e-03,\n",
       "                       4.0797e-05, 5.6052e-45, 1.4284e-03, 9.1658e-04, 6.5129e-04, 5.6052e-45,\n",
       "                       1.4814e-04, 3.5501e-05, 2.0188e-04, 6.5875e-04, 9.9327e-04, 1.3462e-03,\n",
       "                       3.9076e-04, 8.8243e-03, 1.5918e-03, 5.1914e-05, 7.9064e-03, 5.6052e-45,\n",
       "                       1.2547e-03, 2.7900e-04, 7.7647e-04, 2.1041e-03, 3.5960e-04, 2.8262e-04,\n",
       "                       6.5974e-04, 7.8686e-04, 4.5723e-04, 2.3756e-03, 3.8966e-04, 5.6052e-45,\n",
       "                       9.0646e-04, 2.9891e-03, 1.6641e-03, 8.0402e-05, 7.2827e-03, 3.0717e-03,\n",
       "                       1.0821e-04, 5.7008e-04, 5.6052e-45, 6.3779e-04, 5.4409e-03, 1.0703e-03,\n",
       "                       7.3265e-04, 5.1137e-03, 4.0943e-05, 2.0171e-04, 3.9019e-04, 2.6403e-04,\n",
       "                       7.9118e-04, 7.3556e-04, 5.6052e-45, 1.8373e-04, 5.1643e-03, 4.7101e-04,\n",
       "                       2.5397e-03, 8.2779e-05, 3.1389e-04, 9.0221e-04, 3.9400e-03, 1.2227e-03,\n",
       "                       3.9532e-04, 4.6104e-04, 1.4771e-05, 1.2490e-03, 7.4313e-04, 5.6052e-45,\n",
       "                       2.5218e-05, 8.6710e-04, 4.4124e-04, 8.3359e-04, 3.2794e-03, 7.0562e-03,\n",
       "                       4.4425e-05, 1.2823e-03, 9.8659e-04, 1.5078e-04, 5.9432e-04, 1.2192e-04,\n",
       "                       1.2562e-03, 1.7624e-04, 1.4713e-04, 4.8722e-04, 6.2124e-04, 2.9279e-05,\n",
       "                       2.5023e-03, 1.4878e-03, 1.4293e-04, 1.3619e-05, 1.4141e-03, 5.6052e-45,\n",
       "                       4.3026e-04, 5.6052e-45, 1.2076e-03, 4.2756e-04, 5.7086e-05, 1.1530e-04,\n",
       "                       2.2081e-04, 5.6715e-04, 7.1072e-05, 9.7970e-04, 9.2263e-04, 3.2088e-03,\n",
       "                       3.1010e-03, 7.4160e-04, 1.2747e-03, 1.5755e-04, 1.3118e-03, 1.5334e-03,\n",
       "                       5.6052e-45, 1.8224e-03, 1.1577e-04, 2.3558e-05, 5.6052e-45, 6.8130e-03,\n",
       "                       5.7430e-03, 4.9618e-04, 4.7986e-04, 5.2194e-04, 7.4090e-04, 1.5252e-04,\n",
       "                       2.9578e-03, 5.6052e-45, 1.3519e-03, 1.1241e-03, 8.1835e-04, 6.7683e-05,\n",
       "                       6.8289e-04, 6.4728e-05, 4.4636e-04, 3.8638e-05, 5.6052e-45, 1.7683e-03,\n",
       "                       2.2733e-04, 2.6652e-03, 5.6052e-45, 9.0402e-05, 1.8552e-03, 7.0751e-04,\n",
       "                       2.8550e-04, 1.3347e-04, 4.2353e-04, 5.4126e-04, 1.3241e-03, 7.9923e-05,\n",
       "                       9.8881e-04, 6.3362e-04, 5.6105e-05, 2.0788e-03, 8.5437e-05, 6.8883e-04,\n",
       "                       3.2873e-04, 2.4171e-04, 4.7398e-05, 3.6605e-05, 4.2701e-04, 7.2291e-03,\n",
       "                       5.6052e-45, 9.1401e-04, 1.8715e-04, 7.9536e-04, 5.4258e-04, 7.9166e-04,\n",
       "                       4.2611e-04, 1.4571e-03, 9.3481e-05, 9.9933e-04, 2.6831e-04, 3.2917e-04,\n",
       "                       4.7129e-04, 1.0755e-04, 5.7493e-04, 1.6824e-04, 1.4041e-03, 7.2709e-04,\n",
       "                       8.4785e-04, 4.9517e-04, 5.6052e-45, 4.8690e-04, 7.7021e-05, 1.0487e-03,\n",
       "                       4.4337e-04, 2.0819e-03, 2.9637e-05, 6.8146e-06, 1.9132e-04, 6.7028e-04,\n",
       "                       7.7313e-04, 6.7185e-04, 7.8568e-04, 5.3134e-05, 1.3360e-04, 1.1437e-06,\n",
       "                       7.5575e-03, 2.3546e-04, 1.3102e-03, 5.6052e-45, 7.7793e-05, 9.0974e-04,\n",
       "                       5.2978e-04, 5.6052e-45, 2.4343e-04, 4.9270e-04, 3.3751e-03, 4.1400e-04,\n",
       "                       9.0806e-05, 1.8971e-03, 4.1386e-04, 1.8611e-03, 2.1591e-03, 8.4909e-04,\n",
       "                       5.6052e-45, 8.2876e-04, 6.9835e-05, 5.8991e-04, 2.0515e-04, 2.1624e-03,\n",
       "                       6.8009e-05, 2.0725e-04, 2.5786e-04, 1.0495e-05, 1.1049e-04, 5.6052e-45,\n",
       "                       5.6396e-06, 5.6052e-45, 8.6354e-04, 5.3752e-04, 1.3255e-03, 3.5423e-04,\n",
       "                       6.3939e-04, 6.1609e-04, 3.6138e-04, 8.1516e-04, 5.8450e-04, 1.9915e-03,\n",
       "                       1.9824e-03, 3.1394e-03, 4.9141e-04, 3.9281e-04, 9.1834e-04, 6.3719e-04,\n",
       "                       1.1281e-03, 2.0194e-03, 1.7326e-03, 2.2161e-03, 2.1503e-03, 4.5070e-04,\n",
       "                       4.0043e-04, 8.6380e-04, 2.3642e-04, 6.0726e-04, 1.3796e-03, 4.1052e-04,\n",
       "                       2.2138e-03, 1.0718e-04, 9.4697e-04, 1.4026e-05, 2.7189e-03, 2.2110e-04,\n",
       "                       7.0453e-04, 2.7598e-04, 5.4638e-04, 1.0582e-03, 2.0767e-03, 5.6711e-05,\n",
       "                       3.6119e-05, 1.0073e-03, 1.6616e-04, 2.6558e-04, 1.7742e-04, 7.6266e-04,\n",
       "                       1.3460e-04, 7.6725e-04, 6.3431e-03, 1.1416e-04, 1.0096e-04, 3.2858e-04,\n",
       "                       1.5267e-05, 1.5427e-04, 5.6052e-45, 2.2286e-05, 1.1681e-04, 2.0264e-05,\n",
       "                       1.4995e-03, 1.4358e-04, 7.0121e-04, 2.6817e-04, 5.6052e-45, 8.5058e-04,\n",
       "                       3.1273e-03, 5.6052e-45, 5.6052e-45, 5.2417e-04, 1.3871e-05, 6.4405e-04,\n",
       "                       4.2510e-04, 1.8699e-04, 2.6917e-04, 4.1908e-04, 1.7966e-03, 1.0485e-03,\n",
       "                       7.7610e-04, 6.2885e-04, 3.9105e-06, 7.3309e-05, 1.5457e-03, 9.6420e-06,\n",
       "                       7.6139e-04, 2.5712e-04, 6.0451e-04, 2.9900e-03, 4.5838e-03, 6.8325e-05,\n",
       "                       4.9912e-04, 1.3992e-05, 1.9279e-03, 9.6514e-05, 3.8888e-04, 1.2674e-03,\n",
       "                       9.1925e-04, 1.6219e-04, 1.3562e-04, 1.6470e-03, 7.9066e-05, 3.8552e-03,\n",
       "                       3.4758e-03, 7.5257e-04, 4.1370e-04, 2.5737e-04, 1.9015e-04, 6.1723e-05,\n",
       "                       9.3502e-04, 4.0394e-04, 2.8155e-04, 3.3927e-04, 3.6695e-04, 1.3500e-04,\n",
       "                       6.6315e-04, 3.2674e-03, 2.7985e-03, 2.7780e-04, 3.0550e-03, 3.4376e-03,\n",
       "                       2.0108e-04, 2.1176e-04, 3.4008e-04, 1.1283e-03, 7.8817e-04, 3.5011e-04,\n",
       "                       2.3491e-04, 1.4590e-04, 1.7948e-03, 4.6951e-04, 3.8953e-03, 1.6235e-04,\n",
       "                       6.5933e-04, 4.3272e-03, 5.1211e-04, 1.9245e-03, 7.6184e-04, 5.0875e-04,\n",
       "                       9.4740e-04, 2.5491e-04], device='cuda:0')),\n",
       "              ('model.model.layer4.0.downsample.1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer4.1.conv1.weight',\n",
       "               tensor([[[[-0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0065, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0134, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0106, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0004, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0114, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0040, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0160,  0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0071,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0312,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0038],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.1.conv1.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 1., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 1.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn1.weight',\n",
       "               tensor([0.0929, 0.0432, 0.0547, 0.0950, 0.0243, 0.0205, 0.0796, 0.1157, 0.0778,\n",
       "                       0.0536, 0.0874, 0.0236, 0.0821, 0.0738, 0.1217, 0.0543, 0.0744, 0.0751,\n",
       "                       0.0968, 0.0941, 0.0715, 0.0272, 0.0960, 0.0885, 0.0514, 0.0504, 0.0705,\n",
       "                       0.0654, 0.1132, 0.0585, 0.0619, 0.0907, 0.0743, 0.1116, 0.0586, 0.0443,\n",
       "                       0.0788, 0.1275, 0.1274, 0.0860, 0.0883, 0.1028, 0.1186, 0.0787, 0.0839,\n",
       "                       0.0554, 0.0745, 0.0679, 0.0463, 0.0755, 0.0898, 0.1184, 0.0662, 0.0809,\n",
       "                       0.1015, 0.0400, 0.0736, 0.0757, 0.0860, 0.1029, 0.0726, 0.0997, 0.0698,\n",
       "                       0.0727, 0.0562, 0.0543, 0.0925, 0.1003, 0.0780, 0.0781, 0.0952, 0.0773,\n",
       "                       0.0586, 0.0808, 0.0515, 0.0613, 0.0572, 0.0906, 0.0853, 0.0487, 0.0804,\n",
       "                       0.0807, 0.0771, 0.0899, 0.0771, 0.1077, 0.0544, 0.0923, 0.1240, 0.1090,\n",
       "                       0.1028, 0.0827, 0.0554, 0.0982, 0.0437, 0.0930, 0.0794, 0.0671, 0.0830,\n",
       "                       0.0386, 0.0765, 0.0924, 0.0154, 0.0686, 0.0755, 0.1064, 0.1462, 0.1054,\n",
       "                       0.0801, 0.0721, 0.0794, 0.1032, 0.0935, 0.0630, 0.1074, 0.1006, 0.0645,\n",
       "                       0.1349, 0.0562, 0.0835, 0.0909, 0.0708, 0.0539, 0.1353, 0.1065, 0.0952,\n",
       "                       0.0681, 0.0664, 0.0748, 0.1255, 0.0769, 0.0675, 0.1072, 0.1017, 0.0721,\n",
       "                       0.0603, 0.0948, 0.0640, 0.0449, 0.0670, 0.0822, 0.1184, 0.1089, 0.0832,\n",
       "                       0.0544, 0.0677, 0.0779, 0.0671, 0.1148, 0.0993, 0.1000, 0.0703, 0.0832,\n",
       "                       0.1329, 0.0908, 0.0947, 0.0731, 0.0674, 0.0704, 0.1046, 0.0523, 0.0577,\n",
       "                       0.0625, 0.0480, 0.0831, 0.0704, 0.1071, 0.0968, 0.0704, 0.0810, 0.0409,\n",
       "                       0.0742, 0.0597, 0.0411, 0.0585, 0.0559, 0.0465, 0.0630, 0.1032, 0.0935,\n",
       "                       0.0288, 0.1023, 0.1270, 0.0905, 0.0659, 0.0965, 0.0568, 0.0985, 0.0761,\n",
       "                       0.0906, 0.0969, 0.0530, 0.0497, 0.0545, 0.0864, 0.0794, 0.0995, 0.1129,\n",
       "                       0.1164, 0.1030, 0.1060, 0.1178, 0.0809, 0.0940, 0.0805, 0.0467, 0.0794,\n",
       "                       0.0165, 0.0717, 0.1066, 0.0731, 0.0830, 0.1026, 0.0373, 0.0639, 0.0967,\n",
       "                       0.0699, 0.1008, 0.0861, 0.0822, 0.0787, 0.0649, 0.0740, 0.1083, 0.0986,\n",
       "                       0.0986, 0.0642, 0.0639, 0.0822, 0.0942, 0.1611, 0.0357, 0.1174, 0.0572,\n",
       "                       0.0503, 0.0743, 0.0708, 0.0737, 0.0870, 0.0590, 0.0906, 0.1068, 0.1162,\n",
       "                       0.0583, 0.0850, 0.0710, 0.0440, 0.0731, 0.0863, 0.1073, 0.0722, 0.1061,\n",
       "                       0.1017, 0.0872, 0.0628, 0.0180, 0.0743, 0.0939, 0.0775, 0.0947, 0.1017,\n",
       "                       0.0548, 0.0711, 0.0984, 0.0651, 0.0479, 0.0439, 0.0619, 0.1070, 0.1346,\n",
       "                       0.0468, 0.1109, 0.1000, 0.0704, 0.0407, 0.0869, 0.0813, 0.0305, 0.0970,\n",
       "                       0.0361, 0.0458, 0.0453, 0.0658, 0.0467, 0.0704, 0.0839, 0.1020, 0.0564,\n",
       "                       0.0518, 0.0367, 0.0805, 0.0856, 0.0671, 0.1033, 0.0550, 0.1037, 0.0713,\n",
       "                       0.0492, 0.0284, 0.0798, 0.1122, 0.0755, 0.0564, 0.0812, 0.0586, 0.0779,\n",
       "                       0.0593, 0.0461, 0.0765, 0.0881, 0.0707, 0.0948, 0.0649, 0.0876, 0.0524,\n",
       "                       0.0369, 0.0731, 0.0559, 0.1326, 0.0417, 0.0662, 0.0597, 0.0845, 0.0481,\n",
       "                       0.0524, 0.0915, 0.1128, 0.1042, 0.0888, 0.0802, 0.0553, 0.0846, 0.0840,\n",
       "                       0.0950, 0.0891, 0.0922, 0.0655, 0.0137, 0.0564, 0.1143, 0.0812, 0.0502,\n",
       "                       0.1009, 0.0870, 0.0744, 0.0649, 0.0900, 0.0639, 0.1064, 0.0654, 0.0643,\n",
       "                       0.0389, 0.0511, 0.0857, 0.0644, 0.0463, 0.0567, 0.0677, 0.0772, 0.0602,\n",
       "                       0.0633, 0.0678, 0.0600, 0.0988, 0.0958, 0.1054, 0.0709, 0.0826, 0.0868,\n",
       "                       0.0875, 0.0816, 0.0468, 0.0705, 0.0758, 0.0422, 0.0910, 0.0829, 0.0452,\n",
       "                       0.0524, 0.0614, 0.0689, 0.0892, 0.0988, 0.0832, 0.0443, 0.1044, 0.0278,\n",
       "                       0.0888, 0.0550, 0.0825, 0.0562, 0.0725, 0.0602, 0.0384, 0.0636, 0.1054,\n",
       "                       0.1079, 0.1111, 0.0588, 0.0597, 0.1453, 0.0856, 0.0890, 0.0893, 0.0685,\n",
       "                       0.0749, 0.1188, 0.1012, 0.0696, 0.1018, 0.0736, 0.1010, 0.1022, 0.0897,\n",
       "                       0.0781, 0.0234, 0.0781, 0.0620, 0.0907, 0.0911, 0.0688, 0.0555, 0.0813,\n",
       "                       0.0721, 0.0953, 0.0970, 0.0611, 0.0795, 0.0676, 0.0867, 0.0863, 0.0922,\n",
       "                       0.0710, 0.0466, 0.0497, 0.1316, 0.0504, 0.0486, 0.1227, 0.0791, 0.0823,\n",
       "                       0.0833, 0.0739, 0.0726, 0.0778, 0.0777, 0.0682, 0.0772, 0.1009, 0.0887,\n",
       "                       0.1030, 0.0915, 0.0483, 0.0849, 0.0848, 0.1360, 0.0542, 0.0681, 0.0581,\n",
       "                       0.0862, 0.0848, 0.0538, 0.0977, 0.0940, 0.0684, 0.0755, 0.0790, 0.1012,\n",
       "                       0.0924, 0.0860, 0.0545, 0.0546, 0.1067, 0.0736, 0.0947, 0.0657, 0.0742,\n",
       "                       0.0635, 0.0736, 0.0802, 0.0739, 0.0681, 0.0803, 0.0878, 0.1079, 0.0818,\n",
       "                       0.0955, 0.0731, 0.0701, 0.0893, 0.0822, 0.0824, 0.0520, 0.0725, 0.0940,\n",
       "                       0.0828, 0.0887, 0.0363, 0.0493, 0.1325, 0.0939, 0.0879, 0.0477, 0.0801,\n",
       "                       0.0698, 0.0656, 0.0589, 0.0769, 0.0824, 0.0746, 0.1153, 0.0902],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn1.bias',\n",
       "               tensor([-1.6980e-02, -3.3157e-02, -1.2498e-02, -5.0338e-02, -1.6404e-02,\n",
       "                       -2.4659e-02, -4.8953e-02, -3.4103e-02, -3.1443e-02, -3.2526e-02,\n",
       "                       -4.6931e-02, -2.9647e-02, -8.8828e-03,  1.6076e-03, -1.0364e-01,\n",
       "                       -3.1923e-02, -2.4848e-02, -9.4465e-02, -7.7896e-03, -3.8932e-02,\n",
       "                       -2.8780e-02, -2.3609e-02, -4.7545e-02, -7.1571e-02, -4.1332e-02,\n",
       "                       -6.2429e-03, -5.6409e-02, -1.5776e-02, -9.7161e-02, -3.3513e-02,\n",
       "                       -3.3863e-02, -1.8472e-02, -2.6991e-02, -7.3408e-02, -5.3141e-02,\n",
       "                       -2.5866e-02, -5.8303e-02,  4.7521e-03, -1.0341e-02, -3.1983e-02,\n",
       "                       -5.8969e-02, -7.8250e-02,  2.2104e-02, -1.4036e-02, -4.6806e-02,\n",
       "                       -3.5658e-02, -8.6040e-02, -5.3381e-02, -4.6942e-02, -3.4499e-02,\n",
       "                        1.8470e-02,  7.0968e-03, -3.2217e-02, -4.5193e-02, -1.1811e-02,\n",
       "                       -2.1705e-02, -5.4073e-02, -3.7799e-02, -5.8267e-02, -1.5459e-02,\n",
       "                       -5.5710e-02, -4.9925e-03, -4.0701e-02, -4.2257e-02, -4.4895e-02,\n",
       "                       -3.5929e-02, -6.2615e-03, -2.3004e-02, -5.2999e-02, -5.3139e-02,\n",
       "                       -1.8781e-02, -5.9540e-02, -1.9570e-02, -3.5398e-02, -1.5184e-02,\n",
       "                       -1.9032e-02, -4.0516e-02, -6.8887e-02, -6.2166e-02, -2.9672e-02,\n",
       "                       -3.0794e-02, -8.1346e-02, -3.7442e-02, -1.1706e-02, -8.7623e-02,\n",
       "                       -8.3053e-02, -2.9596e-02, -9.8586e-02, -2.1229e-03, -5.3409e-02,\n",
       "                       -6.8228e-02, -6.8749e-02, -1.8351e-02, -2.2730e-02, -3.3817e-02,\n",
       "                       -6.8559e-02, -5.2867e-02, -6.1319e-02, -6.6884e-02, -2.7140e-02,\n",
       "                       -9.0268e-04,  1.3885e-02, -3.8861e-02, -3.1428e-02, -3.4779e-03,\n",
       "                       -2.9145e-03, -1.9881e-02, -9.4940e-02, -5.8552e-02, -2.2966e-02,\n",
       "                       -4.7696e-02, -1.4059e-02, -2.0724e-03, -4.6399e-02, -5.7800e-02,\n",
       "                       -3.3823e-02,  1.0614e-02, -4.3711e-03, -3.5394e-02, -1.3461e-02,\n",
       "                       -5.1834e-02,  2.4107e-02, -3.9258e-02,  2.5824e-02, -5.2273e-02,\n",
       "                       -6.9341e-03, -3.0208e-02, -3.9162e-02, -6.9550e-03, -8.6394e-02,\n",
       "                       -2.5083e-02, -3.8639e-02,  1.5852e-02,  3.6654e-02, -3.3338e-02,\n",
       "                       -4.6585e-02, -5.2327e-02, -3.3801e-02, -2.9959e-02, -1.5723e-02,\n",
       "                        1.3768e-03, -3.1855e-02, -9.8492e-02, -2.5804e-02, -1.4468e-02,\n",
       "                       -3.7859e-02, -6.1597e-02, -5.7722e-02,  6.5732e-03, -1.3872e-02,\n",
       "                        5.4503e-03, -9.2260e-04, -5.2341e-02, -4.8969e-02, -1.0845e-02,\n",
       "                       -7.0691e-02, -1.7799e-02, -3.6204e-02, -1.6199e-02, -7.8660e-02,\n",
       "                       -2.2248e-02, -1.8231e-02, -5.2700e-02, -2.2547e-02, -5.0275e-02,\n",
       "                       -2.8293e-02, -5.2646e-02, -6.5717e-02, -5.0970e-02, -8.4567e-02,\n",
       "                       -9.0328e-03, -5.7577e-02, -3.0107e-02, -4.5024e-02, -2.7484e-02,\n",
       "                       -2.7132e-02, -3.2136e-02, -3.6297e-02,  9.2207e-03, -1.9489e-02,\n",
       "                       -3.2874e-02, -2.5240e-02, -5.9638e-02, -2.5342e-03, -4.7944e-02,\n",
       "                       -3.1644e-02, -2.8456e-02, -7.1297e-02, -4.8473e-02, -9.0878e-02,\n",
       "                       -4.8038e-02, -3.7182e-02, -4.2109e-02, -3.9432e-02, -1.0908e-02,\n",
       "                       -2.8555e-02,  9.2471e-04, -2.1283e-02, -2.6795e-02, -5.8926e-02,\n",
       "                        1.2530e-02,  8.2098e-03, -1.0167e-02, -5.5788e-02, -4.9877e-02,\n",
       "                       -3.6948e-02, -3.4116e-02, -3.7026e-02, -7.5567e-02, -1.7047e-02,\n",
       "                       -5.1116e-02, -2.6570e-02, -6.8179e-02, -2.3444e-02, -5.0661e-02,\n",
       "                       -6.4497e-03, -5.2461e-02,  3.2416e-02, -2.2512e-02, -5.2013e-02,\n",
       "                       -2.7238e-02, -1.7094e-02, -5.3667e-02, -2.3645e-02, -6.3365e-02,\n",
       "                       -7.5291e-02, -5.4913e-02,  8.3690e-04, -5.8743e-02, -7.2809e-02,\n",
       "                        4.2620e-03, -2.9825e-02, -5.0617e-03, -2.2216e-02, -4.4592e-02,\n",
       "                       -3.8901e-02, -6.9218e-02, -3.9197e-03, -4.8893e-02, -4.5306e-03,\n",
       "                       -3.9178e-02, -1.8432e-02, -1.4034e-02, -3.2825e-02, -8.1144e-02,\n",
       "                       -4.1626e-02, -3.7597e-02, -4.7018e-02, -7.7100e-03,  1.7013e-02,\n",
       "                       -7.9626e-02, -9.8804e-02, -2.3606e-02, -5.6399e-02, -9.1264e-03,\n",
       "                       -2.5141e-02, -4.9336e-02, -1.8460e-02,  8.7167e-03, -2.0148e-03,\n",
       "                       -3.8007e-02, -2.5651e-02, -5.0007e-02, -6.2452e-02, -4.4318e-02,\n",
       "                       -2.8168e-02, -2.6254e-02, -2.5867e-02, -6.7260e-02, -2.2536e-02,\n",
       "                       -9.0634e-03, -9.8432e-03, -6.9473e-02, -1.4975e-02, -1.1869e-02,\n",
       "                       -5.4362e-02, -5.5270e-02, -3.4038e-02, -5.0733e-02, -3.2729e-02,\n",
       "                       -5.2928e-02, -6.4721e-03, -4.6756e-02, -1.7943e-02, -5.3207e-02,\n",
       "                       -7.1637e-02, -3.6711e-03, -2.4266e-02, -2.7153e-02, -1.3685e-02,\n",
       "                       -5.5791e-02,  8.5614e-03, -1.9365e-02, -7.6197e-02, -1.6073e-02,\n",
       "                       -5.1540e-02, -2.1267e-02,  3.4227e-03, -2.5200e-02, -4.1308e-02,\n",
       "                        2.7717e-02, -9.3526e-02, -8.4369e-03, -1.2762e-03, -2.5456e-02,\n",
       "                       -2.5510e-02, -2.5714e-02, -2.5386e-02, -1.9631e-05, -5.2014e-02,\n",
       "                       -5.2851e-02, -6.3459e-02, -2.0931e-02, -3.7358e-02, -2.2225e-02,\n",
       "                       -1.4572e-02, -6.6566e-02, -3.6342e-02, -1.5987e-02, -1.9292e-02,\n",
       "                       -4.6216e-02, -2.0609e-02, -1.9671e-02, -1.4790e-02, -5.2708e-02,\n",
       "                       -4.4367e-02,  1.7881e-02,  2.3532e-03, -3.7620e-02, -3.1256e-02,\n",
       "                       -4.6094e-02, -8.9495e-02, -8.3030e-03, -2.1232e-02, -7.6288e-03,\n",
       "                       -1.0016e-01, -4.2419e-02, -3.6509e-02, -3.9388e-02, -1.5741e-03,\n",
       "                       -3.9034e-03, -3.2851e-02, -9.2567e-02, -1.1132e-01, -5.8970e-02,\n",
       "                       -3.0343e-02, -1.9624e-02, -1.7062e-02, -7.3373e-03, -3.0094e-02,\n",
       "                       -7.6704e-02, -1.5727e-02, -3.4948e-02, -3.3850e-02, -5.7229e-02,\n",
       "                       -1.7714e-02, -2.2941e-02,  7.1172e-03, -5.0575e-02, -9.0438e-03,\n",
       "                       -5.0771e-02, -4.7396e-03, -2.5014e-02, -6.8537e-02, -4.0488e-02,\n",
       "                       -1.6240e-03, -4.1989e-02, -6.4320e-02, -1.9399e-02, -7.2216e-02,\n",
       "                       -2.3566e-02, -2.9101e-02, -3.8434e-02, -6.5869e-02, -2.4157e-02,\n",
       "                        9.6347e-03, -6.4220e-02, -1.3693e-02, -1.3283e-02, -2.2815e-03,\n",
       "                       -2.5932e-02, -5.1607e-02, -4.1610e-02, -7.1777e-02, -1.8171e-02,\n",
       "                       -4.7880e-02, -3.0980e-02, -1.1288e-01, -5.5456e-02, -8.8860e-02,\n",
       "                       -1.9449e-02, -1.5320e-02, -5.3344e-02, -2.0332e-02, -2.1452e-02,\n",
       "                       -7.7850e-02, -4.3416e-04,  1.2928e-02, -3.8159e-02, -3.1512e-02,\n",
       "                       -1.8182e-02, -2.3391e-02, -1.4446e-02, -3.5670e-02, -3.0521e-02,\n",
       "                       -6.0842e-02, -8.6976e-02, -6.7660e-03, -2.1065e-03,  1.7039e-02,\n",
       "                       -4.4406e-02, -4.1335e-02, -1.0587e-01, -4.0689e-02, -2.3611e-02,\n",
       "                       -3.0816e-02,  8.5862e-03, -3.2540e-02, -3.5495e-02, -2.7826e-02,\n",
       "                       -2.8696e-02, -3.8789e-02, -3.6944e-02, -6.9312e-02, -8.1444e-02,\n",
       "                       -1.0124e-01, -4.0055e-03, -2.7784e-02, -5.6821e-02, -8.4783e-02,\n",
       "                       -1.0779e-01, -9.3041e-02, -6.2829e-02, -3.0940e-02, -3.6660e-02,\n",
       "                        3.0240e-02, -2.6156e-02, -2.4960e-02, -1.4848e-02, -2.9937e-02,\n",
       "                       -6.0150e-02, -4.1267e-02, -5.8913e-03, -4.7932e-02, -3.9467e-02,\n",
       "                       -3.7818e-02, -2.8049e-02,  2.0779e-02, -1.0426e-01, -2.5022e-02,\n",
       "                        1.3009e-02, -3.9741e-02, -2.8549e-02, -5.6948e-02, -8.2189e-02,\n",
       "                        3.0077e-04, -2.2762e-02, -2.5460e-02, -3.8584e-02, -1.5558e-02,\n",
       "                       -4.3664e-02,  3.2470e-02, -6.9494e-02,  2.7728e-03, -3.3476e-02,\n",
       "                       -1.0215e-02, -3.1796e-03, -8.6047e-02, -4.2217e-02,  5.4652e-03,\n",
       "                       -2.2984e-02, -3.4233e-02, -8.3211e-02, -8.2016e-03, -4.1148e-03,\n",
       "                       -5.3885e-02, -6.5775e-02, -7.4131e-03, -6.0358e-02, -2.3203e-02,\n",
       "                       -2.2756e-02, -4.4747e-02, -6.2474e-02, -5.8453e-02, -8.8716e-02,\n",
       "                       -1.9068e-03, -5.6603e-02, -7.0651e-02, -3.0424e-02, -5.0061e-02,\n",
       "                       -5.4700e-02, -2.5970e-02, -4.3267e-02, -6.5114e-03, -1.3547e-02,\n",
       "                       -4.6283e-02, -6.0310e-02, -2.9668e-02, -2.7838e-02,  3.7798e-02,\n",
       "                       -5.8184e-02, -4.6147e-02, -1.9978e-04, -9.1398e-02, -3.7818e-02,\n",
       "                       -4.2215e-02, -2.8923e-02, -6.6819e-02, -5.4014e-02, -6.3271e-02,\n",
       "                       -3.2566e-02, -2.0662e-02], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn1.running_mean',\n",
       "               tensor([-4.8171e-02, -8.1449e-03, -3.9831e-02, -1.2138e-02, -6.8022e-04,\n",
       "                       -7.8335e-04, -3.6032e-02, -8.7068e-02, -2.5378e-02,  1.2479e-03,\n",
       "                       -1.6065e-02, -9.8267e-04, -5.1865e-02, -8.7080e-02, -3.8673e-02,\n",
       "                       -2.5888e-03, -2.5828e-02, -1.6664e-02, -4.1021e-02,  3.3467e-02,\n",
       "                       -2.4838e-02,  2.9026e-03, -9.1524e-03, -2.6583e-02, -3.8557e-03,\n",
       "                        5.8859e-03,  1.4202e-03, -1.9889e-02, -2.8562e-03, -2.0050e-02,\n",
       "                       -4.0454e-03, -4.4903e-02, -2.4478e-03, -4.9848e-02, -2.0434e-02,\n",
       "                       -1.5260e-03, -7.1638e-04, -6.5363e-02, -1.0078e-01,  3.0332e-02,\n",
       "                       -2.5346e-02, -3.5837e-02, -8.4300e-02, -4.5219e-02, -1.6898e-02,\n",
       "                       -2.1179e-02, -3.7976e-02,  2.3862e-03, -1.7059e-02, -4.6777e-02,\n",
       "                       -6.6784e-02, -9.7431e-02, -1.6954e-02, -2.0075e-02, -5.1335e-02,\n",
       "                       -8.2549e-04, -9.6189e-03, -3.1288e-02, -4.3300e-02, -6.7180e-02,\n",
       "                       -3.5426e-02, -5.1385e-02, -2.2638e-02, -3.2009e-02, -1.6964e-04,\n",
       "                        1.0071e-02, -3.2287e-02, -5.9720e-02, -2.1506e-02, -4.9004e-02,\n",
       "                       -5.0238e-02, -3.0611e-04, -6.9688e-03, -1.8545e-02, -6.7145e-03,\n",
       "                       -2.9175e-02, -1.5921e-02,  1.2638e-02, -6.9490e-02,  2.8756e-03,\n",
       "                       -2.1751e-02, -5.8792e-02, -3.3141e-02, -4.0503e-02, -4.3391e-02,\n",
       "                       -8.1183e-04, -5.2950e-03, -3.5992e-02, -8.7047e-02, -8.1044e-02,\n",
       "                       -8.4544e-02, -2.6939e-02, -1.7835e-02, -9.0120e-02, -7.4598e-03,\n",
       "                       -7.5261e-03, -5.1370e-02, -5.4259e-03, -2.5826e-02,  4.1311e-03,\n",
       "                       -4.2725e-02, -7.9702e-02, -2.7304e-03, -1.3818e-02, -5.0028e-02,\n",
       "                       -6.2177e-02, -8.3465e-02, -3.1262e-02, -1.1144e-03, -3.5952e-02,\n",
       "                       -2.3003e-02, -6.0816e-02, -4.4227e-02, -1.1893e-02, -1.1771e-01,\n",
       "                        3.4115e-02, -3.3279e-02, -7.0985e-02, -4.1816e-03, -5.5101e-02,\n",
       "                       -2.9906e-02, -2.9493e-02, -4.2461e-03, -7.8096e-02, -1.5106e-02,\n",
       "                       -7.0193e-02, -4.9473e-03, -3.5331e-02, -5.1201e-02,  7.1932e-03,\n",
       "                        2.3151e-02, -1.4201e-03, -6.8284e-02, -9.9392e-02,  2.9935e-03,\n",
       "                       -1.2900e-03, -3.8764e-02, -2.2320e-02, -3.7714e-03, -5.9796e-03,\n",
       "                       -4.8145e-02, -4.4202e-02, -6.8902e-02, -1.9510e-02,  1.9147e-03,\n",
       "                       -8.1836e-03, -4.8742e-02, -2.6331e-02, -8.1738e-02, -7.9725e-02,\n",
       "                       -4.8299e-02, -2.2595e-02, -3.8857e-02, -3.2644e-02, -5.3030e-02,\n",
       "                       -3.7002e-02, -9.9325e-03, -2.1204e-02, -3.3142e-02, -7.0873e-02,\n",
       "                        8.8085e-03, -2.0926e-02, -1.9047e-02, -2.3975e-03, -3.7319e-02,\n",
       "                       -1.9515e-02,  3.9511e-02, -4.0024e-02, -3.7860e-03, -3.5115e-02,\n",
       "                        1.8625e-03, -6.3958e-03, -3.1819e-03, -5.7256e-03, -1.9345e-02,\n",
       "                       -3.6633e-02, -3.5094e-03, -1.2402e-02, -6.3019e-02, -3.8663e-02,\n",
       "                       -1.6393e-03, -5.8058e-02,  3.4742e-03, -4.0661e-02, -1.5419e-03,\n",
       "                       -4.8332e-02,  9.1403e-03, -6.7962e-02, -5.5541e-02, -8.7451e-03,\n",
       "                       -3.4678e-02, -8.0834e-03,  1.6558e-04, -1.5874e-02, -4.0306e-02,\n",
       "                       -3.2759e-02, -5.9790e-02, -1.1857e-01, -3.6318e-02, -4.8022e-02,\n",
       "                       -7.1113e-02, -6.9250e-02, -5.7646e-02, -5.9643e-03, -2.9734e-02,\n",
       "                       -1.5981e-02, -1.4996e-02, -1.9901e-03, -1.7083e-02, -5.8769e-02,\n",
       "                       -5.1449e-02, -3.3582e-02, -2.4401e-02,  1.5383e-03, -1.4486e-02,\n",
       "                       -1.6459e-02,  1.0182e-02, -6.3011e-02, -3.5825e-02, -4.3551e-02,\n",
       "                       -2.0647e-03, -1.7904e-02, -2.3428e-02, -5.8690e-02, -4.1893e-02,\n",
       "                       -3.5242e-02,  7.3167e-05,  6.0387e-03, -1.1136e-02, -7.3543e-02,\n",
       "                       -1.1131e-01, -6.2349e-03, -4.9543e-02, -1.0008e-02, -1.2243e-02,\n",
       "                       -2.8534e-03, -2.5268e-03, -2.3873e-03, -7.5933e-03, -2.9523e-02,\n",
       "                       -6.1866e-02, -1.1401e-01, -5.6670e-02, -6.3015e-03, -3.6694e-02,\n",
       "                       -3.3412e-02, -1.0247e-02, -4.2215e-02, -5.1257e-02, -1.1027e-01,\n",
       "                       -2.1127e-03, -6.4858e-02, -1.6692e-02, -1.3300e-02,  2.9441e-03,\n",
       "                        9.8162e-04, -3.2518e-02, -7.1118e-02, -5.0801e-02, -5.0900e-02,\n",
       "                       -2.7167e-02, -2.1731e-02,  4.6338e-03, -1.4502e-02, -3.3046e-03,\n",
       "                       -1.3742e-02, -1.8514e-02, -2.0884e-02,  1.7418e-02, -7.6066e-02,\n",
       "                       -5.1258e-03, -5.5731e-02, -6.2740e-02,  3.3354e-03, -5.7296e-03,\n",
       "                       -2.8238e-02,  1.4144e-02,  1.2918e-03, -5.3163e-02, -7.6143e-03,\n",
       "                        4.2908e-04, -3.6858e-02, -4.2268e-04, -1.6622e-02, -1.0036e-02,\n",
       "                       -4.2133e-02, -9.3303e-02, -3.9425e-02, -1.0421e-02, -4.3157e-03,\n",
       "                       -1.6808e-02, -6.1932e-02, -1.0722e-02,  1.5465e-02, -6.1793e-03,\n",
       "                       -5.7289e-02, -2.6144e-02, -2.7348e-02, -3.0606e-04, -3.7860e-02,\n",
       "                       -1.2601e-01, -1.8807e-02, -1.4908e-02, -1.3536e-02,  3.5189e-03,\n",
       "                       -3.4359e-02, -1.9456e-02, -9.0411e-03, -5.3820e-02, -4.3906e-03,\n",
       "                       -9.4860e-03, -2.3012e-02, -1.4181e-02, -4.6471e-02,  5.7845e-03,\n",
       "                       -8.3641e-03, -3.2900e-02, -5.5436e-03, -1.0897e-01, -8.5692e-03,\n",
       "                       -2.2030e-02, -4.7159e-02, -1.5094e-02, -2.7881e-02,  4.0177e-04,\n",
       "                       -1.5475e-02, -7.2112e-02, -7.7283e-02, -2.3956e-02, -2.2794e-02,\n",
       "                       -1.9958e-02, -1.4520e-02, -3.0542e-02, -3.8289e-02, -4.9241e-02,\n",
       "                       -6.0425e-03, -3.5685e-03, -5.2756e-04, -6.2057e-04, -9.9295e-02,\n",
       "                       -2.7542e-02, -1.4972e-02, -4.3617e-02, -4.1393e-02,  1.7374e-03,\n",
       "                       -2.5508e-02, -6.6858e-02, -2.8363e-02, -9.5963e-02, -2.1369e-02,\n",
       "                       -2.7754e-02, -7.8498e-03, -6.6714e-05, -7.1328e-03, -4.3082e-03,\n",
       "                        1.0731e-02, -3.0612e-02, -3.5505e-02,  2.3376e-02, -3.5471e-02,\n",
       "                       -1.9892e-02, -3.2588e-02, -2.1304e-02, -5.3033e-02, -5.9653e-02,\n",
       "                        4.1433e-02, -3.1056e-02, -1.7458e-02, -1.6969e-02, -3.3242e-02,\n",
       "                        1.1198e-02, -1.2350e-02, -5.6883e-03, -2.6063e-02, -7.7779e-03,\n",
       "                       -4.7545e-02, -6.5121e-03,  3.2214e-03, -2.1061e-02, -4.5291e-02,\n",
       "                       -2.7308e-04, -5.2571e-02, -2.5767e-02, -2.4004e-02, -7.0980e-03,\n",
       "                       -6.1933e-02,  9.0554e-04, -4.1426e-02,  7.9229e-03, -3.1072e-02,\n",
       "                        9.7231e-03, -3.2094e-02, -5.5145e-03, -7.3107e-03, -1.4327e-02,\n",
       "                       -4.2417e-02, -8.4786e-02, -1.0472e-01,  1.3671e-03, -3.6140e-02,\n",
       "                       -3.9722e-02, -2.0396e-02, -6.2780e-02,  2.1159e-02, -1.0751e-02,\n",
       "                        1.0429e-03, -6.5258e-02, -6.0182e-02, -2.8428e-03, -1.0062e-01,\n",
       "                       -4.1165e-03, -3.9541e-02, -7.7357e-02,  1.7113e-02, -1.4025e-02,\n",
       "                       -2.8076e-03, -2.2335e-02, -3.2192e-02, -5.4584e-02, -7.4238e-02,\n",
       "                       -8.5383e-03, -1.2826e-02,  2.3102e-02,  1.4182e-02, -5.3432e-02,\n",
       "                       -1.9299e-02, -1.4313e-02, -3.4260e-02, -5.4179e-03, -3.6533e-02,\n",
       "                       -4.2136e-02, -3.2329e-02, -1.4630e-02,  2.3361e-03,  5.4045e-03,\n",
       "                       -1.2381e-01, -1.7905e-02,  2.0551e-03, -7.9282e-02, -1.6914e-02,\n",
       "                       -3.9285e-03, -4.1777e-02, -4.5734e-02, -1.8592e-02, -4.3790e-02,\n",
       "                        1.0565e-02, -5.2507e-03, -5.2244e-02, -5.3860e-02, -6.8374e-02,\n",
       "                       -8.1838e-02, -2.0087e-02, -2.0218e-02, -2.3466e-02, -2.0727e-02,\n",
       "                       -7.0495e-02, -1.0595e-02, -1.3766e-02, -1.1760e-02, -5.5753e-02,\n",
       "                        6.8373e-03, -3.3073e-02, -3.6083e-02, -1.1006e-01, -5.3755e-02,\n",
       "                       -3.8308e-02, -3.0809e-02, -9.9590e-02, -4.5970e-02, -7.3646e-02,\n",
       "                       -4.9029e-03, -8.8275e-03, -4.7259e-02, -4.0762e-02, -6.6360e-02,\n",
       "                       -2.5965e-03,  5.7257e-03, -2.6063e-02, -5.2027e-02, -2.1213e-02,\n",
       "                       -3.9246e-02, -1.7154e-02, -2.9501e-02, -1.1502e-02, -5.5944e-02,\n",
       "                       -4.7230e-02, -4.4699e-02, -3.2396e-02, -1.6290e-03, -8.8375e-03,\n",
       "                        1.5500e-02,  3.4484e-03, -1.2349e-02, -3.4769e-02, -8.4957e-02,\n",
       "                       -1.9647e-02, -4.5785e-02, -1.7216e-02, -1.7749e-02, -9.9839e-02,\n",
       "                       -3.1158e-02, -1.1273e-02,  6.5144e-03, -3.6391e-02, -5.2715e-02,\n",
       "                       -8.5448e-03, -1.5944e-02, -7.5405e-03, -3.0733e-02, -8.1790e-03,\n",
       "                       -9.4880e-02, -4.2520e-02], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn1.running_var',\n",
       "               tensor([2.5032e-03, 5.0285e-05, 5.2717e-04, 3.4797e-04, 4.6097e-06, 3.7199e-06,\n",
       "                       1.0015e-03, 2.6215e-03, 3.2188e-04, 8.1059e-05, 6.2962e-04, 1.1508e-05,\n",
       "                       1.4410e-03, 2.5279e-03, 1.1540e-03, 3.0935e-05, 1.6735e-03, 2.9399e-04,\n",
       "                       2.5766e-03, 1.1784e-03, 3.5265e-04, 2.2517e-05, 7.7426e-04, 3.4836e-04,\n",
       "                       3.8301e-05, 1.2986e-04, 3.5155e-04, 5.8644e-04, 1.3510e-03, 2.5716e-04,\n",
       "                       2.3510e-04, 1.6945e-03, 5.7269e-04, 1.1040e-03, 1.7003e-04, 7.4381e-05,\n",
       "                       2.8554e-04, 8.6389e-03, 1.4567e-02, 4.2376e-04, 6.1244e-04, 8.9515e-04,\n",
       "                       8.9278e-03, 9.8461e-04, 4.0315e-04, 2.2601e-04, 4.3545e-04, 2.9730e-04,\n",
       "                       8.1375e-05, 5.6932e-04, 5.9513e-03, 9.6640e-03, 6.6705e-04, 1.7202e-04,\n",
       "                       3.5743e-03, 4.0529e-05, 2.6366e-04, 3.4378e-04, 4.1208e-04, 3.6415e-03,\n",
       "                       3.4127e-04, 3.4077e-03, 5.3603e-04, 6.8286e-04, 1.0401e-04, 3.7799e-05,\n",
       "                       1.5261e-03, 3.0084e-03, 3.8201e-04, 5.9484e-04, 1.8858e-03, 1.6717e-04,\n",
       "                       1.0019e-04, 7.3859e-04, 9.1235e-05, 4.2407e-04, 9.7228e-05, 6.4107e-04,\n",
       "                       6.8417e-04, 5.6996e-05, 1.1749e-03, 8.7305e-04, 6.5536e-04, 2.4132e-03,\n",
       "                       5.3630e-04, 6.5024e-04, 2.1418e-04, 4.2014e-04, 8.7843e-03, 2.9215e-03,\n",
       "                       2.9193e-03, 4.5614e-04, 1.4596e-04, 4.2540e-03, 2.7455e-05, 6.0809e-04,\n",
       "                       7.3962e-04, 2.1685e-04, 9.3327e-04, 1.5230e-05, 1.0263e-03, 4.0991e-03,\n",
       "                       3.9759e-05, 1.7441e-04, 2.4869e-03, 2.7935e-03, 8.5611e-03, 7.7192e-04,\n",
       "                       2.6277e-04, 1.0004e-03, 2.2356e-04, 4.2534e-03, 1.7947e-03, 1.6605e-04,\n",
       "                       3.0275e-03, 6.5148e-04, 1.0663e-03, 7.0446e-03, 6.2168e-05, 3.2119e-03,\n",
       "                       3.0666e-04, 2.2510e-03, 1.0386e-04, 1.6913e-02, 1.4549e-03, 3.5540e-03,\n",
       "                       1.8367e-04, 5.6492e-04, 1.7596e-03, 1.1207e-03, 6.2483e-04, 1.0869e-04,\n",
       "                       1.0481e-02, 8.9356e-03, 4.3881e-04, 6.8071e-05, 8.3114e-04, 2.2490e-04,\n",
       "                       2.9980e-05, 4.3827e-04, 2.3088e-03, 2.6465e-03, 9.9460e-04, 9.6516e-04,\n",
       "                       5.8099e-05, 4.3319e-04, 5.6353e-04, 4.1002e-04, 5.6929e-03, 5.3273e-03,\n",
       "                       5.1676e-03, 6.0667e-04, 4.3192e-04, 2.4123e-03, 2.7070e-03, 1.2050e-03,\n",
       "                       6.3273e-04, 2.6245e-04, 7.3439e-04, 9.6120e-04, 1.6675e-04, 2.0285e-04,\n",
       "                       1.6703e-04, 9.3386e-05, 7.5716e-04, 4.8356e-04, 1.2379e-03, 6.1105e-04,\n",
       "                       2.5221e-04, 5.4949e-04, 2.5114e-05, 3.5023e-04, 1.2279e-04, 5.1790e-05,\n",
       "                       5.7966e-04, 1.1187e-03, 4.7385e-05, 2.1523e-04, 3.6138e-03, 1.3180e-03,\n",
       "                       6.4651e-06, 1.8579e-03, 6.0939e-04, 3.2233e-03, 2.1359e-04, 1.5862e-03,\n",
       "                       6.3119e-05, 1.0423e-03, 1.3131e-03, 7.1646e-04, 1.1128e-03, 3.3551e-05,\n",
       "                       8.1294e-05, 1.0033e-04, 1.7163e-03, 1.0133e-03, 4.8016e-03, 5.2194e-03,\n",
       "                       3.7617e-03, 1.4198e-03, 7.8874e-03, 8.6143e-03, 1.2547e-03, 5.2888e-04,\n",
       "                       2.6475e-04, 1.2009e-04, 3.2838e-04, 2.0611e-05, 2.4578e-04, 1.5328e-03,\n",
       "                       5.7353e-04, 1.2081e-03, 1.1287e-03, 1.1200e-05, 1.3724e-04, 3.1910e-03,\n",
       "                       1.9325e-04, 8.6862e-03, 5.5206e-04, 3.2142e-04, 4.9216e-04, 5.0615e-04,\n",
       "                       3.9578e-04, 5.3948e-03, 9.5297e-04, 5.8933e-04, 1.5059e-04, 4.6741e-04,\n",
       "                       9.4505e-04, 1.3910e-03, 1.9627e-02, 3.4962e-05, 4.0183e-03, 1.2336e-04,\n",
       "                       1.2884e-04, 1.8715e-04, 4.8095e-04, 7.2002e-04, 4.9185e-04, 9.5826e-04,\n",
       "                       1.5979e-03, 5.9451e-03, 3.6726e-03, 4.5395e-05, 6.7628e-04, 3.0909e-04,\n",
       "                       1.6079e-04, 3.0247e-04, 1.9229e-03, 8.5289e-03, 3.0834e-04, 1.0477e-03,\n",
       "                       2.1760e-03, 4.7283e-04, 3.7382e-04, 2.7463e-06, 3.9247e-04, 2.9051e-03,\n",
       "                       3.9480e-03, 4.8976e-03, 1.3213e-03, 3.3818e-04, 2.3135e-04, 9.3352e-04,\n",
       "                       1.2490e-04, 5.9025e-05, 1.8700e-04, 2.4131e-04, 8.8772e-04, 7.6802e-03,\n",
       "                       1.2340e-04, 3.7896e-03, 9.0540e-04, 2.5291e-04, 1.0518e-04, 5.8824e-04,\n",
       "                       6.2272e-04, 1.1461e-05, 9.9339e-04, 4.5591e-05, 1.2275e-04, 5.6238e-04,\n",
       "                       1.2517e-04, 2.9978e-04, 1.9593e-04, 6.7452e-04, 4.4104e-03, 7.9740e-04,\n",
       "                       1.1154e-04, 4.6108e-05, 4.4927e-04, 2.8867e-03, 5.2160e-04, 1.1616e-03,\n",
       "                       1.1613e-04, 1.7406e-03, 4.3029e-04, 2.7731e-04, 8.8096e-06, 8.0010e-04,\n",
       "                       6.9092e-03, 3.4288e-04, 3.3610e-04, 8.9545e-04, 7.3772e-05, 9.7457e-04,\n",
       "                       2.5191e-04, 6.9633e-05, 2.1768e-03, 4.1628e-04, 1.2919e-04, 1.0417e-03,\n",
       "                       2.0742e-04, 1.0002e-03, 6.1316e-05, 2.1323e-05, 3.2143e-04, 8.3811e-05,\n",
       "                       1.3815e-02, 1.2329e-04, 2.9348e-04, 5.3480e-04, 1.2511e-03, 5.9583e-04,\n",
       "                       9.5201e-05, 6.3676e-04, 7.4762e-03, 4.4022e-03, 9.4205e-04, 9.2808e-04,\n",
       "                       1.7024e-04, 4.0980e-04, 2.0849e-03, 1.4081e-03, 2.5013e-03, 5.4064e-04,\n",
       "                       2.2065e-04, 2.0093e-05, 4.4115e-05, 7.4474e-03, 1.0091e-03, 1.5331e-04,\n",
       "                       4.5130e-04, 5.3334e-04, 4.2040e-04, 3.1512e-04, 2.8003e-03, 3.7584e-04,\n",
       "                       5.8831e-03, 5.0013e-04, 4.9407e-04, 1.4033e-04, 8.8589e-05, 5.0943e-04,\n",
       "                       1.4415e-04, 1.3448e-04, 9.3141e-04, 1.4344e-03, 4.8197e-04, 9.8343e-04,\n",
       "                       4.1835e-04, 6.0155e-04, 3.8656e-04, 1.1615e-03, 1.9846e-03, 1.8118e-03,\n",
       "                       2.7315e-04, 4.0700e-04, 2.0592e-03, 4.2663e-04, 1.9716e-04, 1.1123e-04,\n",
       "                       2.3106e-04, 3.6771e-04, 2.6256e-05, 2.1454e-03, 4.8671e-04, 1.3126e-04,\n",
       "                       5.0630e-04, 8.9385e-04, 3.8264e-04, 8.5061e-04, 2.6183e-03, 3.2447e-04,\n",
       "                       1.2179e-04, 1.5450e-03, 7.4642e-06, 8.5216e-04, 1.7031e-04, 4.2073e-04,\n",
       "                       9.2977e-05, 9.4623e-04, 2.4314e-04, 4.6818e-05, 7.9476e-04, 1.0917e-03,\n",
       "                       9.0102e-03, 9.4448e-03, 1.3430e-04, 8.8905e-04, 6.2588e-03, 4.9229e-04,\n",
       "                       2.1275e-03, 2.9562e-04, 6.7564e-04, 1.3836e-04, 1.0572e-03, 4.0452e-03,\n",
       "                       6.1467e-04, 5.6717e-03, 3.2421e-04, 2.2711e-03, 1.1399e-03, 3.5554e-04,\n",
       "                       3.8074e-04, 1.0822e-05, 9.9722e-04, 3.3076e-04, 1.7198e-03, 3.8464e-03,\n",
       "                       5.2516e-04, 8.5888e-05, 2.9989e-04, 3.9634e-04, 7.1865e-04, 1.1147e-03,\n",
       "                       4.4764e-04, 2.8917e-04, 1.4691e-04, 4.5878e-04, 4.1710e-04, 7.0257e-04,\n",
       "                       4.0748e-04, 6.1578e-05, 3.5076e-05, 1.3051e-02, 3.3390e-04, 7.9322e-05,\n",
       "                       7.4265e-03, 4.5755e-04, 6.8913e-04, 7.7524e-04, 1.2209e-03, 4.1285e-04,\n",
       "                       9.6163e-04, 3.3405e-04, 1.9393e-04, 1.8028e-03, 7.0280e-04, 2.2167e-03,\n",
       "                       4.8355e-03, 5.0684e-04, 9.6888e-05, 7.0464e-04, 5.5187e-04, 7.1006e-03,\n",
       "                       1.7682e-04, 5.9662e-04, 1.2918e-04, 3.1591e-03, 1.5398e-04, 1.2325e-03,\n",
       "                       9.5073e-04, 6.0281e-03, 8.8304e-04, 2.0131e-03, 1.2536e-03, 2.3317e-03,\n",
       "                       9.0232e-04, 6.0447e-03, 1.0083e-04, 1.7097e-04, 1.0613e-03, 1.7115e-03,\n",
       "                       3.2156e-03, 1.8677e-04, 3.0880e-04, 6.1882e-04, 6.6528e-04, 9.8947e-04,\n",
       "                       8.9440e-04, 2.8551e-04, 1.1929e-03, 5.7372e-04, 1.0015e-03, 1.2936e-03,\n",
       "                       9.1274e-04, 3.8774e-04, 2.1290e-04, 4.4467e-04, 5.7921e-04, 5.6292e-04,\n",
       "                       1.4242e-04, 6.3164e-04, 4.2147e-03, 3.6677e-04, 4.0764e-04, 1.2905e-04,\n",
       "                       1.0173e-04, 1.5003e-02, 8.0118e-04, 3.3227e-04, 1.5399e-04, 4.2262e-04,\n",
       "                       1.0884e-03, 1.5497e-04, 1.1205e-04, 2.0625e-04, 3.1174e-04, 2.5978e-04,\n",
       "                       5.2052e-03, 1.9142e-03], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn1.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.layer4.1.conv2.weight',\n",
       "               tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0029, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0101,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0040, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0042,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0027],\n",
       "                         [ 0.0033,  0.0000,  0.0033],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0028, -0.0000, -0.0028],\n",
       "                         [-0.0031,  0.0034, -0.0000],\n",
       "                         [-0.0031, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0031, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0120, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0194,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [ 0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0072,  0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000,  0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0046, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[ 0.0000, -0.0000,  0.0000],\n",
       "                         [ 0.0000, -0.0096, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000]],\n",
       "               \n",
       "                        [[-0.0000, -0.0000, -0.0000],\n",
       "                         [-0.0000, -0.0000, -0.0000],\n",
       "                         [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.1.conv2.weight_mask',\n",
       "               tensor([[[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 1.],\n",
       "                         [1., 0., 1.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[1., 0., 1.],\n",
       "                         [1., 1., 0.],\n",
       "                         [1., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]],\n",
       "               \n",
       "               \n",
       "                       [[[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 1., 0.],\n",
       "                         [0., 0., 0.]],\n",
       "               \n",
       "                        [[0., 0., 0.],\n",
       "                         [0., 0., 0.],\n",
       "                         [0., 0., 0.]]]], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn2.weight',\n",
       "               tensor([0.1858, 0.3781, 0.4917, 0.4758, 0.3700, 0.1767, 0.1915, 0.5052, 0.3364,\n",
       "                       0.3732, 0.1642, 0.1690, 0.3746, 0.4111, 0.3425, 0.1750, 0.1806, 0.2255,\n",
       "                       0.4483, 0.3246, 0.1736, 0.1863, 0.3444, 0.1913, 0.1819, 0.3531, 0.3587,\n",
       "                       0.3097, 0.1795, 0.1776, 0.1693, 0.1877, 0.1769, 0.1722, 0.1730, 0.1808,\n",
       "                       0.1817, 0.5069, 0.1677, 0.1756, 0.1760, 0.1673, 0.3496, 0.4611, 0.1671,\n",
       "                       0.1783, 0.1853, 0.3744, 0.1847, 0.1807, 0.1830, 0.3553, 0.1708, 0.1820,\n",
       "                       0.1713, 0.1864, 0.5295, 0.1842, 0.3545, 0.6688, 0.1755, 0.1938, 0.1814,\n",
       "                       0.1874, 0.1792, 0.4654, 0.3290, 0.1863, 0.4119, 0.1804, 0.3793, 0.1636,\n",
       "                       0.1667, 0.1799, 0.3122, 0.1717, 0.1744, 0.1785, 0.4882, 0.1832, 0.1692,\n",
       "                       0.1732, 0.1894, 0.2108, 0.1731, 0.3722, 0.5080, 0.1615, 0.5998, 0.3614,\n",
       "                       0.1643, 0.1863, 0.3867, 0.1794, 0.3425, 0.3328, 0.3731, 0.1759, 0.1810,\n",
       "                       0.1681, 0.3489, 0.5180, 0.5269, 0.3779, 0.1719, 0.1553, 0.1856, 0.1901,\n",
       "                       0.1722, 0.2684, 0.1910, 0.3488, 0.1765, 0.1771, 0.1878, 0.3487, 0.1672,\n",
       "                       0.4191, 0.5593, 0.1808, 0.1688, 0.1788, 0.1754, 0.3084, 0.1786, 0.1672,\n",
       "                       0.1765, 0.1862, 0.5565, 0.1871, 0.1768, 0.1835, 0.1866, 0.1851, 0.3369,\n",
       "                       0.4402, 0.1759, 0.3956, 0.3820, 0.1791, 0.1689, 0.1866, 0.1746, 0.1852,\n",
       "                       0.6191, 0.1755, 0.1760, 0.1868, 0.1865, 0.4592, 0.1661, 0.3435, 0.3546,\n",
       "                       0.3211, 0.1678, 0.1855, 0.1812, 0.1954, 0.1748, 0.4152, 0.3581, 0.3675,\n",
       "                       0.3698, 0.5462, 0.3737, 0.1905, 0.4614, 0.1820, 0.1769, 0.1726, 0.1852,\n",
       "                       0.3796, 0.1861, 0.1924, 0.4144, 0.1626, 0.1802, 0.3400, 0.1909, 0.1784,\n",
       "                       0.1870, 0.4013, 0.5572, 0.1787, 0.5531, 0.5046, 0.1917, 0.1872, 0.1745,\n",
       "                       0.1884, 0.6349, 0.1770, 0.1893, 0.5909, 0.1712, 0.1772, 0.1792, 0.1665,\n",
       "                       0.3581, 0.4797, 0.1638, 0.1831, 0.4467, 0.1655, 0.5322, 0.1859, 0.3827,\n",
       "                       0.3273, 0.4879, 0.3708, 0.1695, 0.1653, 0.1882, 0.3355, 0.1918, 0.1759,\n",
       "                       0.1720, 0.3370, 0.1708, 0.3241, 0.4834, 0.5447, 0.3872, 0.4371, 0.1693,\n",
       "                       0.1734, 0.3753, 0.1893, 0.3457, 0.1758, 0.1780, 0.3542, 0.1781, 0.1814,\n",
       "                       0.4048, 0.3172, 0.1779, 0.1871, 0.1586, 0.1923, 0.1764, 0.1684, 0.3984,\n",
       "                       0.1590, 0.1702, 0.2061, 0.1885, 0.1745, 0.1814, 0.3886, 0.5353, 0.5243,\n",
       "                       0.4130, 0.3297, 0.3420, 0.2177, 0.3284, 0.2049, 0.1895, 0.3284, 0.3576,\n",
       "                       0.1876, 0.1821, 0.3767, 0.5104, 0.4544, 0.1870, 0.3410, 0.1715, 0.1807,\n",
       "                       0.3876, 0.1754, 0.3804, 0.1645, 0.1927, 0.1692, 0.1794, 0.1663, 0.1854,\n",
       "                       0.1735, 0.2224, 0.3651, 0.1743, 0.3410, 0.1842, 0.1765, 0.4023, 0.1846,\n",
       "                       0.1870, 0.1912, 0.1649, 0.1689, 0.1791, 0.1947, 0.3884, 0.1865, 0.3239,\n",
       "                       0.4092, 0.1720, 0.3317, 0.1772, 0.3999, 0.1649, 0.1697, 0.1889, 0.6853,\n",
       "                       0.1758, 0.3653, 0.1649, 0.1883, 0.1847, 0.1789, 0.1956, 0.3433, 0.1736,\n",
       "                       0.1811, 0.1938, 0.1644, 0.1864, 0.3511, 0.3902, 0.1736, 0.4265, 0.1806,\n",
       "                       0.3583, 0.1647, 0.1755, 0.1808, 0.1774, 0.1845, 0.1782, 0.5032, 0.1823,\n",
       "                       0.1769, 0.1809, 0.3654, 0.4389, 0.1833, 0.1874, 0.1805, 0.1681, 0.1715,\n",
       "                       0.4661, 0.1844, 0.3156, 0.1817, 0.1908, 0.3571, 0.3114, 0.1837, 0.3171,\n",
       "                       0.1715, 0.1832, 0.1763, 0.1729, 0.3428, 0.1762, 0.4037, 0.4371, 0.1773,\n",
       "                       0.1722, 0.3832, 0.1828, 0.1695, 0.1839, 0.1838, 0.1855, 0.1797, 0.1818,\n",
       "                       0.3568, 0.1806, 0.1844, 0.1902, 0.1702, 0.4047, 0.1912, 0.1810, 0.1737,\n",
       "                       0.1890, 0.3853, 0.1889, 0.3714, 0.1793, 0.3915, 0.3862, 0.5394, 0.1778,\n",
       "                       0.2002, 0.3163, 0.3307, 0.1840, 0.4131, 0.3889, 0.3778, 0.4656, 0.3330,\n",
       "                       0.3346, 0.1752, 0.3350, 0.1943, 0.3640, 0.1790, 0.3282, 0.1702, 0.4915,\n",
       "                       0.1748, 0.4890, 0.1864, 0.1703, 0.1909, 0.3262, 0.1844, 0.3608, 0.1731,\n",
       "                       0.1863, 0.1952, 0.1637, 0.1795, 0.1788, 0.3580, 0.3710, 0.3649, 0.5204,\n",
       "                       0.3402, 0.1804, 0.1888, 0.1810, 0.4139, 0.3620, 0.1828, 0.1808, 0.1894,\n",
       "                       0.1742, 0.1785, 0.1896, 0.3499, 0.3456, 0.1743, 0.5589, 0.1732, 0.1760,\n",
       "                       0.1836, 0.1794, 0.1703, 0.1806, 0.1822, 0.1881, 0.1854, 0.3759, 0.3355,\n",
       "                       0.1683, 0.1867, 0.1752, 0.1769, 0.3567, 0.1794, 0.1786, 0.1889, 0.1742,\n",
       "                       0.5259, 0.5882, 0.1783, 0.4074, 0.1815, 0.1902, 0.1705, 0.1889, 0.1823,\n",
       "                       0.4491, 0.1894, 0.1753, 0.3737, 0.1780, 0.5588, 0.3583, 0.1905, 0.1829,\n",
       "                       0.1711, 0.1874, 0.1656, 0.3377, 0.3490, 0.1602, 0.1715, 0.1785, 0.2030,\n",
       "                       0.1867, 0.3973, 0.3911, 0.1801, 0.4389, 0.5116, 0.1903, 0.1833, 0.1703,\n",
       "                       0.3852, 0.1615, 0.1858, 0.1820, 0.1811, 0.5180, 0.3271, 0.4295, 0.1697,\n",
       "                       0.1718, 0.4714, 0.4079, 0.1860, 0.3403, 0.1688, 0.3928, 0.1924],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn2.bias',\n",
       "               tensor([0.0492, 0.0411, 0.0421, 0.0766, 0.0537, 0.0362, 0.0406, 0.0373, 0.0895,\n",
       "                       0.0628, 0.0344, 0.0305, 0.0403, 0.0344, 0.0790, 0.0417, 0.0468, 0.0723,\n",
       "                       0.0730, 0.0414, 0.0332, 0.0494, 0.0240, 0.0463, 0.0402, 0.0469, 0.0653,\n",
       "                       0.0579, 0.0268, 0.0354, 0.0417, 0.0221, 0.0340, 0.0340, 0.0295, 0.0313,\n",
       "                       0.0367, 0.0960, 0.0343, 0.0478, 0.0397, 0.0390, 0.0451, 0.0648, 0.0339,\n",
       "                       0.0428, 0.0361, 0.0711, 0.0431, 0.0328, 0.0442, 0.0523, 0.0345, 0.0373,\n",
       "                       0.0469, 0.0362, 0.0732, 0.0477, 0.0906, 0.1201, 0.0312, 0.0457, 0.0434,\n",
       "                       0.0345, 0.0432, 0.0399, 0.0143, 0.0231, 0.0388, 0.0236, 0.0228, 0.0270,\n",
       "                       0.0367, 0.0241, 0.0675, 0.0393, 0.0363, 0.0460, 0.0618, 0.0421, 0.0355,\n",
       "                       0.0417, 0.0608, 0.0633, 0.0322, 0.0262, 0.0987, 0.0317, 0.1037, 0.0204,\n",
       "                       0.0381, 0.0413, 0.0452, 0.0358, 0.0640, 0.0667, 0.0351, 0.0423, 0.0387,\n",
       "                       0.0443, 0.0155, 0.0498, 0.0567, 0.0811, 0.0452, 0.0334, 0.0379, 0.0364,\n",
       "                       0.0313, 0.1049, 0.0494, 0.0866, 0.0385, 0.0395, 0.0440, 0.0779, 0.0452,\n",
       "                       0.0297, 0.1288, 0.0432, 0.0414, 0.0410, 0.0327, 0.0183, 0.0388, 0.0240,\n",
       "                       0.0299, 0.0371, 0.0683, 0.0403, 0.0256, 0.0353, 0.0339, 0.0389, 0.0610,\n",
       "                       0.0225, 0.0376, 0.0284, 0.1221, 0.0431, 0.0439, 0.0343, 0.0412, 0.0317,\n",
       "                       0.0531, 0.0251, 0.0271, 0.0580, 0.0380, 0.0504, 0.0345, 0.0515, 0.0281,\n",
       "                       0.0613, 0.0402, 0.0300, 0.0352, 0.0433, 0.0331, 0.0421, 0.0233, 0.0878,\n",
       "                       0.0481, 0.1614, 0.0165, 0.0384, 0.0408, 0.0314, 0.0363, 0.0450, 0.0353,\n",
       "                       0.0211, 0.0285, 0.0297, 0.0327, 0.0430, 0.0330, 0.0459, 0.0503, 0.0373,\n",
       "                       0.0324, 0.0155, 0.0420, 0.0458, 0.0399, 0.0771, 0.0356, 0.0437, 0.0342,\n",
       "                       0.0506, 0.0646, 0.0342, 0.0498, 0.0565, 0.0311, 0.0362, 0.0441, 0.0325,\n",
       "                       0.0683, 0.0524, 0.0315, 0.0313, 0.0104, 0.0384, 0.1055, 0.0372, 0.0134,\n",
       "                       0.0584, 0.0710, 0.0300, 0.0321, 0.0314, 0.0415, 0.0588, 0.0403, 0.0340,\n",
       "                       0.0379, 0.0648, 0.0287, 0.0538, 0.0481, 0.0736, 0.0255, 0.0162, 0.0411,\n",
       "                       0.0327, 0.0311, 0.0318, 0.0525, 0.0369, 0.0350, 0.0503, 0.0330, 0.0466,\n",
       "                       0.0381, 0.0668, 0.0506, 0.0384, 0.0310, 0.0427, 0.0417, 0.0312, 0.0737,\n",
       "                       0.0354, 0.0381, 0.0347, 0.0431, 0.0382, 0.0519, 0.0555, 0.0771, 0.1124,\n",
       "                       0.0052, 0.0446, 0.0774, 0.0655, 0.0719, 0.0515, 0.0385, 0.0741, 0.0555,\n",
       "                       0.0327, 0.0438, 0.1318, 0.0280, 0.0182, 0.0344, 0.0253, 0.0439, 0.0367,\n",
       "                       0.0055, 0.0217, 0.0451, 0.0388, 0.0272, 0.0390, 0.0413, 0.0326, 0.0381,\n",
       "                       0.0251, 0.0617, 0.0528, 0.0405, 0.0582, 0.0362, 0.0300, 0.0335, 0.0488,\n",
       "                       0.0355, 0.0280, 0.0401, 0.0382, 0.0339, 0.0311, 0.0680, 0.0323, 0.0361,\n",
       "                       0.0699, 0.0386, 0.0492, 0.0358, 0.0076, 0.0331, 0.0404, 0.0583, 0.0447,\n",
       "                       0.0241, 0.0425, 0.0317, 0.0357, 0.0475, 0.0382, 0.0627, 0.0401, 0.0369,\n",
       "                       0.0454, 0.0390, 0.0348, 0.0486, 0.0382, 0.0577, 0.0400, 0.0601, 0.0332,\n",
       "                       0.0269, 0.0420, 0.0423, 0.0275, 0.0375, 0.0400, 0.0454, 0.0645, 0.0336,\n",
       "                       0.0310, 0.0382, 0.0335, 0.1282, 0.0442, 0.0463, 0.0412, 0.0263, 0.0293,\n",
       "                       0.1290, 0.0589, 0.0280, 0.0355, 0.0322, 0.0487, 0.0249, 0.0342, 0.0297,\n",
       "                       0.0354, 0.0495, 0.0397, 0.0344, 0.0527, 0.0395, 0.0484, 0.0517, 0.0288,\n",
       "                       0.0326, 0.0090, 0.0321, 0.0365, 0.0514, 0.0435, 0.0495, 0.0453, 0.0402,\n",
       "                       0.0036, 0.0512, 0.0353, 0.0313, 0.0372, 0.0454, 0.0447, 0.0523, 0.0411,\n",
       "                       0.0442, 0.0407, 0.0594, 0.0429, 0.0497, 0.0300, 0.0684, 0.1003, 0.0422,\n",
       "                       0.0445, 0.0114, 0.0681, 0.0396, 0.0045, 0.0174, 0.1089, 0.0388, 0.0140,\n",
       "                       0.0431, 0.0435, 0.0115, 0.0407, 0.0578, 0.0520, 0.0344, 0.0380, 0.0828,\n",
       "                       0.0325, 0.0768, 0.0351, 0.0401, 0.0461, 0.0391, 0.0393, 0.0291, 0.0353,\n",
       "                       0.0564, 0.0397, 0.0372, 0.0325, 0.0292, 0.0152, 0.0365, 0.0234, 0.0466,\n",
       "                       0.0492, 0.0464, 0.0282, 0.0383, 0.0352, 0.0114, 0.0380, 0.0338, 0.0390,\n",
       "                       0.0286, 0.0391, 0.0368, 0.0366, 0.0356, 0.0422, 0.0990, 0.0371, 0.0302,\n",
       "                       0.0365, 0.0351, 0.0504, 0.0494, 0.0343, 0.0503, 0.0355, 0.0411, 0.0182,\n",
       "                       0.0376, 0.0342, 0.0372, 0.0360, 0.0653, 0.0364, 0.0464, 0.0411, 0.0381,\n",
       "                       0.0684, 0.1241, 0.0284, 0.0244, 0.0343, 0.0559, 0.0319, 0.0355, 0.0387,\n",
       "                       0.0459, 0.0389, 0.0389, 0.0641, 0.0426, 0.1170, 0.0458, 0.0433, 0.0461,\n",
       "                       0.0319, 0.0311, 0.0351, 0.0362, 0.0430, 0.0360, 0.0320, 0.0567, 0.0504,\n",
       "                       0.0506, 0.0536, 0.0315, 0.0382, 0.0598, 0.0625, 0.0372, 0.0357, 0.0232,\n",
       "                       0.0483, 0.0355, 0.0428, 0.0396, 0.0322, 0.0899, 0.0372, 0.0471, 0.0401,\n",
       "                       0.0335, 0.1117, 0.0276, 0.0401, 0.0775, 0.0368, 0.0222, 0.0396],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn2.running_mean',\n",
       "               tensor([-1.0846e-02, -1.0767e-02, -2.2198e-02, -2.8363e-02, -1.1828e-02,\n",
       "                       -1.7343e-03, -6.6134e-03, -3.3621e-02, -1.3464e-02, -3.5137e-03,\n",
       "                       -4.2488e-03, -7.7919e-04, -1.5504e-02, -7.8141e-03, -6.2532e-03,\n",
       "                       -4.0155e-03, -6.8700e-03, -2.6895e-03, -1.0600e-02, -1.0079e-02,\n",
       "                       -3.3195e-03, -2.8979e-03, -8.5471e-03, -3.9214e-03, -2.9264e-03,\n",
       "                       -6.4581e-03, -6.6718e-03, -1.0072e-02, -4.3196e-03, -7.0486e-03,\n",
       "                       -3.9913e-03, -3.0547e-03, -4.0627e-03, -1.4963e-03, -3.5954e-03,\n",
       "                       -5.3335e-03, -4.0211e-03, -7.2888e-03, -1.3014e-03, -6.9367e-03,\n",
       "                       -1.8244e-03, -4.5594e-03, -1.1498e-02,  5.2773e-04, -1.9463e-03,\n",
       "                       -7.1383e-03, -7.2601e-03, -1.7442e-02, -5.6910e-03, -3.7089e-03,\n",
       "                       -3.4619e-03, -1.1812e-02, -3.2101e-03, -4.4867e-03, -5.6531e-03,\n",
       "                       -2.3712e-03, -2.8888e-02, -5.5566e-03, -1.5393e-02, -2.7104e-02,\n",
       "                       -4.4118e-03, -3.1088e-03, -4.3412e-03, -4.6378e-03, -1.6074e-03,\n",
       "                       -2.0413e-02, -1.8206e-02, -4.1416e-03, -1.8020e-02, -4.8037e-04,\n",
       "                       -1.3188e-02, -3.1579e-03, -1.9715e-03, -5.3210e-03, -8.9081e-03,\n",
       "                       -2.8647e-03, -3.9469e-03, -4.8209e-03, -2.0260e-03, -6.1327e-03,\n",
       "                       -6.1233e-03, -4.5177e-03, -2.0924e-03, -3.5224e-03, -3.1780e-03,\n",
       "                       -1.9338e-03, -1.9301e-02, -1.5938e-03, -5.0706e-02, -6.4538e-03,\n",
       "                       -5.1153e-03, -4.1805e-03, -1.6588e-02, -2.7569e-03, -1.8689e-02,\n",
       "                       -7.3462e-03, -2.3192e-02, -6.0933e-03, -5.4487e-03, -6.7868e-03,\n",
       "                       -5.9141e-03, -3.2498e-02, -1.3692e-02, -2.4801e-02, -5.4406e-03,\n",
       "                       -5.3829e-03, -5.5056e-03,  1.0910e-03,  1.6911e-04, -2.3522e-02,\n",
       "                       -9.5374e-03, -1.7648e-02, -6.9468e-03, -3.9303e-03, -2.5329e-03,\n",
       "                       -1.6414e-02, -4.6576e-03, -1.7179e-02, -7.8904e-03, -1.7511e-03,\n",
       "                       -3.3571e-03, -9.4123e-03, -8.9066e-04, -2.3635e-03, -6.6747e-05,\n",
       "                       -4.3306e-03, -2.5101e-03, -5.2454e-03, -3.6005e-02, -7.3515e-03,\n",
       "                       -2.3808e-03, -2.7215e-03, -7.8720e-03, -5.3770e-03, -1.8653e-02,\n",
       "                       -1.6433e-02, -5.7147e-03, -1.2870e-02, -2.3824e-02, -5.1162e-03,\n",
       "                       -2.3123e-03, -4.6887e-03, -5.0105e-03, -5.5790e-03, -3.3759e-02,\n",
       "                       -3.0621e-03, -3.8798e-04, -2.0770e-03, -5.2589e-03, -2.4210e-02,\n",
       "                       -4.9117e-03, -8.4510e-03, -1.0754e-02, -6.5790e-03, -1.2558e-03,\n",
       "                       -6.1412e-04, -6.0584e-03, -2.4334e-03, -2.8081e-03, -1.1324e-02,\n",
       "                       -1.4177e-02, -6.2984e-03, -1.0571e-02, -5.3751e-02, -2.0089e-02,\n",
       "                       -7.2278e-03, -1.9700e-02, -2.5752e-04, -9.7979e-04, -1.9321e-03,\n",
       "                       -3.2549e-03, -7.2641e-03, -4.0706e-03, -5.3819e-03,  3.5919e-03,\n",
       "                        2.7983e-04, -3.5178e-03, -1.6984e-02, -9.4116e-04, -2.3932e-03,\n",
       "                       -4.0278e-03, -1.8701e-02, -4.0816e-02, -1.1051e-04, -3.0665e-02,\n",
       "                       -3.4695e-03, -4.1178e-03, -3.7831e-03, -3.7941e-03, -7.2834e-03,\n",
       "                       -3.5198e-02, -4.7419e-03, -4.4704e-03, -3.1642e-02, -6.7382e-04,\n",
       "                       -7.9902e-04, -1.5185e-03, -5.5322e-03, -1.6271e-02, -2.3546e-02,\n",
       "                       -3.4037e-03, -2.1763e-03, -7.8435e-03, -7.5062e-03, -1.8676e-02,\n",
       "                       -9.9221e-04, -2.9829e-03, -7.9768e-03, -1.7332e-02, -1.4221e-02,\n",
       "                       -4.2423e-03, -3.1465e-03, -4.0589e-03, -8.3357e-03, -4.6068e-03,\n",
       "                        1.1417e-04, -1.6274e-03, -6.3036e-03, -2.6059e-03, -9.4188e-03,\n",
       "                       -2.8178e-02, -2.2389e-02, -1.2434e-02, -1.5974e-02,  1.7286e-03,\n",
       "                       -4.9756e-03, -2.0522e-02, -3.8986e-03, -1.5511e-02, -3.9344e-03,\n",
       "                       -5.3180e-03, -1.9582e-02, -3.3796e-03, -7.8363e-05, -2.6301e-02,\n",
       "                       -1.2808e-02, -5.4879e-03, -6.8511e-03, -1.4160e-03, -2.5348e-03,\n",
       "                       -3.6332e-03, -2.3331e-03, -6.4910e-03,  1.6394e-03, -3.3060e-03,\n",
       "                       -6.0307e-03, -2.0088e-03, -1.0954e-03, -4.4988e-03, -1.3218e-02,\n",
       "                       -1.8597e-02, -1.4590e-02, -7.3502e-03, -1.1906e-02, -3.0094e-03,\n",
       "                        1.7462e-04, -9.5384e-03, -1.0247e-03, -5.6730e-03, -5.7688e-03,\n",
       "                       -1.0868e-02, -3.8983e-03, -6.4606e-04, -2.3112e-02, -2.8385e-02,\n",
       "                       -2.9244e-02, -7.5118e-03, -1.1691e-02, -3.0623e-03, -5.6779e-03,\n",
       "                       -1.9343e-02, -1.5742e-03, -1.6555e-02, -2.2692e-03, -6.2481e-03,\n",
       "                       -2.0448e-03, -4.7486e-04, -4.4580e-03, -9.4502e-05, -2.5171e-03,\n",
       "                       -5.7344e-03, -3.5437e-03, -4.4190e-03, -1.6343e-02, -6.6527e-03,\n",
       "                       -2.6332e-03, -7.9192e-03, -8.1130e-03, -3.6512e-03, -1.9259e-03,\n",
       "                       -2.8069e-03, -3.8324e-03, -5.8769e-03, -4.0966e-03, -1.3145e-02,\n",
       "                       -5.2714e-03, -1.2654e-02, -7.2431e-03, -2.1483e-03, -1.0899e-02,\n",
       "                       -5.7678e-03, -7.5870e-03, -4.1935e-03, -4.4758e-03, -5.6095e-03,\n",
       "                       -4.5277e-02, -4.0879e-03, -2.5304e-02, -5.0131e-03, -8.2367e-04,\n",
       "                       -4.8720e-03, -3.7120e-03, -4.1229e-03, -9.1360e-03, -3.8653e-03,\n",
       "                       -4.4771e-03, -9.9790e-04, -3.3899e-03, -4.1635e-03, -1.3468e-02,\n",
       "                       -6.3336e-03, -7.4664e-03, -1.4932e-02, -2.6075e-03, -7.0163e-03,\n",
       "                       -1.7521e-03, -5.5187e-03, -1.5963e-03, -2.2870e-03, -7.7571e-03,\n",
       "                       -5.9153e-03, -2.1871e-02, -6.2533e-03, -5.5185e-03, -5.0571e-04,\n",
       "                       -7.8805e-03, -4.0664e-02, -1.6514e-03, -3.9270e-03, -1.7734e-03,\n",
       "                       -4.1338e-03, -4.2923e-03, -2.3799e-02, -2.2804e-03, -5.8345e-03,\n",
       "                        1.6573e-03, -2.7665e-03, -1.1541e-02, -1.1756e-02, -6.9874e-04,\n",
       "                       -4.8980e-03, -1.1462e-03, -8.4562e-04, -4.5237e-03, -4.7769e-03,\n",
       "                       -1.0105e-02, -4.1844e-03, -7.6463e-03, -1.6844e-02, -3.9389e-03,\n",
       "                       -4.6852e-03, -1.9767e-02, -3.0509e-03, -6.5443e-03, -6.2613e-03,\n",
       "                       -6.9205e-03, -5.3090e-04, -6.2166e-03, -5.7356e-03, -1.7006e-02,\n",
       "                       -3.9141e-03, -6.6342e-03,  1.1174e-03, -4.1786e-03, -2.1896e-02,\n",
       "                       -7.8867e-03,  1.3760e-03, -5.3074e-03, -4.3629e-03, -1.8011e-02,\n",
       "                       -4.8100e-03, -1.1534e-02, -6.2717e-03, -1.7855e-02, -1.9978e-02,\n",
       "                       -1.5405e-02, -7.1958e-03, -7.3031e-03, -1.1240e-02, -1.1342e-02,\n",
       "                       -2.2205e-03, -8.6458e-03, -1.0372e-02, -6.4906e-03, -2.8176e-02,\n",
       "                       -1.2352e-02, -1.1296e-02, -9.9105e-03, -1.3434e-02, -5.9578e-03,\n",
       "                       -1.8555e-02, -1.8498e-03, -9.6499e-03, -3.6235e-03, -1.8226e-02,\n",
       "                       -2.1832e-03, -2.7803e-02, -3.3275e-03, -2.4029e-03, -9.4612e-03,\n",
       "                       -1.0703e-02, -3.0855e-03, -7.0311e-03, -5.2409e-03, -2.8828e-03,\n",
       "                       -4.0830e-03, -5.5974e-03, -6.7009e-03, -5.3139e-03, -5.0844e-03,\n",
       "                       -1.5564e-02, -4.5332e-03, -3.0157e-02, -7.1342e-03, -4.8753e-03,\n",
       "                       -3.9493e-03, -1.3861e-03, -2.4836e-02,  3.3220e-03, -2.7935e-03,\n",
       "                        3.2037e-04, -7.6257e-03, -3.8577e-03, -8.5388e-04, -3.8741e-03,\n",
       "                       -4.2661e-03, -8.3224e-03, -4.3681e-03, -1.3160e-02,  1.5535e-03,\n",
       "                       -6.7057e-03, -7.9402e-03, -6.1630e-03, -3.5144e-03, -2.3009e-03,\n",
       "                       -2.5281e-03,  5.4462e-04, -1.2270e-03, -1.5047e-02, -1.9960e-02,\n",
       "                       -2.4980e-03, -4.6096e-03,  2.1655e-03, -1.2257e-03, -1.0200e-02,\n",
       "                       -2.7125e-03, -2.8963e-03, -5.6031e-03, -1.7780e-03, -2.5845e-02,\n",
       "                       -3.6551e-02, -4.5652e-04,  1.6507e-03, -1.9668e-03, -4.5202e-03,\n",
       "                       -1.3112e-03, -2.7264e-03, -1.6773e-03, -2.5878e-02, -4.6075e-03,\n",
       "                       -6.0513e-03, -1.5941e-02, -3.5310e-03, -2.0368e-02, -1.1712e-02,\n",
       "                       -3.6013e-03, -1.0859e-02, -1.7994e-03, -5.1441e-03, -6.0531e-04,\n",
       "                       -1.2271e-02, -1.6401e-02, -1.6048e-03, -3.4971e-03, -5.6713e-03,\n",
       "                       -6.1796e-03, -3.4034e-03, -1.9835e-02, -1.4185e-02, -2.0457e-03,\n",
       "                       -1.9074e-02, -3.0111e-02, -4.2676e-03, -8.2841e-03, -5.1964e-03,\n",
       "                       -1.2429e-02, -2.7663e-03, -6.0949e-03, -3.1519e-03, -3.7622e-03,\n",
       "                       -1.9733e-02, -1.2980e-02, -1.0978e-02, -3.1941e-03, -4.5416e-03,\n",
       "                       -1.4493e-02, -1.0046e-03, -2.7268e-03, -1.2521e-02, -4.2695e-04,\n",
       "                       -2.5974e-03, -8.1041e-05], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn2.running_var',\n",
       "               tensor([2.2167e-05, 2.8461e-04, 3.4953e-04, 5.3503e-04, 1.6597e-04, 2.7641e-05,\n",
       "                       4.0057e-05, 3.6816e-04, 2.9913e-04, 4.0200e-04, 1.5306e-05, 2.1436e-05,\n",
       "                       4.1217e-04, 3.3535e-04, 3.2215e-04, 2.8810e-05, 4.1478e-05, 5.2290e-05,\n",
       "                       4.8071e-04, 1.6694e-04, 2.3755e-05, 2.8409e-05, 1.9480e-04, 3.7920e-05,\n",
       "                       3.5208e-05, 3.0141e-04, 2.9697e-04, 2.4519e-04, 2.2263e-05, 2.7188e-05,\n",
       "                       2.6058e-05, 1.8492e-05, 2.6799e-05, 1.9287e-05, 1.8735e-05, 2.9549e-05,\n",
       "                       3.0266e-05, 5.0821e-04, 2.2064e-05, 3.7741e-05, 2.8931e-05, 2.4381e-05,\n",
       "                       2.5953e-04, 4.3546e-04, 2.5141e-05, 3.4522e-05, 2.0937e-05, 3.2853e-04,\n",
       "                       3.4117e-05, 3.2865e-05, 3.4977e-05, 3.2497e-04, 2.2425e-05, 3.3517e-05,\n",
       "                       2.9373e-05, 2.5688e-05, 4.9644e-04, 3.8594e-05, 3.9719e-04, 1.2397e-03,\n",
       "                       2.1556e-05, 3.6952e-05, 3.6003e-05, 3.7837e-05, 3.8754e-05, 4.2532e-04,\n",
       "                       2.3331e-04, 1.2219e-05, 4.4783e-04, 2.0184e-05, 3.0444e-04, 1.5914e-05,\n",
       "                       1.8928e-05, 1.6979e-05, 2.5733e-04, 1.5917e-05, 3.3359e-05, 3.4312e-05,\n",
       "                       5.2197e-04, 3.6775e-05, 1.8397e-05, 2.5643e-05, 5.4226e-05, 4.4961e-05,\n",
       "                       3.2890e-05, 2.5842e-04, 4.7362e-04, 1.6882e-05, 4.4808e-04, 1.2184e-04,\n",
       "                       2.7397e-05, 3.4026e-05, 2.5984e-04, 2.3713e-05, 3.0478e-04, 2.4116e-04,\n",
       "                       1.5649e-04, 4.1425e-05, 3.8552e-05, 2.9349e-05, 2.6671e-04, 6.1082e-04,\n",
       "                       4.1419e-04, 3.2646e-04, 2.9960e-05, 2.0327e-05, 3.1131e-05, 4.6988e-05,\n",
       "                       2.3041e-05, 8.6314e-05, 3.5834e-05, 3.4247e-04, 2.2342e-05, 3.0474e-05,\n",
       "                       3.2705e-05, 3.3050e-04, 2.9627e-05, 3.6545e-04, 8.2287e-04, 3.5609e-05,\n",
       "                       2.2395e-05, 2.9176e-05, 2.7611e-05, 1.7567e-04, 2.6415e-05, 7.5158e-06,\n",
       "                       2.4846e-05, 3.0161e-05, 6.1901e-04, 2.4187e-05, 1.9043e-05, 2.6963e-05,\n",
       "                       2.9299e-05, 3.6077e-05, 3.1656e-04, 2.0927e-04, 2.1079e-05, 2.2734e-04,\n",
       "                       3.6140e-04, 3.1129e-05, 3.1086e-05, 2.6780e-05, 3.7686e-05, 2.3740e-05,\n",
       "                       6.5758e-04, 1.8533e-05, 1.8851e-05, 4.8251e-05, 3.1378e-05, 4.4179e-04,\n",
       "                       2.3359e-05, 3.0896e-04, 2.7415e-04, 2.2935e-04, 2.9954e-05, 2.7393e-05,\n",
       "                       1.6946e-05, 3.4450e-05, 2.5291e-05, 3.1907e-04, 2.3205e-04, 3.7715e-04,\n",
       "                       1.6857e-04, 9.2740e-04, 2.8045e-04, 3.3961e-05, 5.7405e-04, 2.5270e-05,\n",
       "                       3.4239e-05, 2.4822e-05, 3.5738e-05, 3.2351e-04, 4.0165e-05, 4.3383e-05,\n",
       "                       3.4458e-04, 3.0223e-05, 2.5313e-05, 2.5719e-04, 3.4043e-05, 3.0848e-05,\n",
       "                       3.7381e-05, 3.4898e-04, 5.9553e-04, 3.4748e-05, 5.8241e-04, 4.9604e-04,\n",
       "                       3.1201e-05, 3.7744e-05, 2.7109e-05, 3.7482e-05, 5.4942e-04, 3.6045e-05,\n",
       "                       3.8040e-05, 6.0590e-04, 2.2779e-05, 2.6928e-05, 2.8612e-05, 2.8993e-05,\n",
       "                       3.2341e-04, 3.5855e-04, 1.5835e-05, 1.8469e-05, 5.2326e-04, 1.6290e-05,\n",
       "                       7.9298e-04, 3.8607e-05, 2.4010e-04, 2.6513e-04, 3.8113e-04, 2.7212e-04,\n",
       "                       3.4258e-05, 2.6913e-05, 3.1734e-05, 2.0384e-04, 3.0002e-05, 2.3747e-05,\n",
       "                       2.0590e-05, 3.0944e-04, 2.0915e-05, 2.6403e-04, 4.1767e-04, 6.1377e-04,\n",
       "                       3.0562e-04, 3.5663e-04, 2.2235e-05, 2.6104e-05, 3.1898e-04, 3.6432e-05,\n",
       "                       2.2616e-04, 2.8361e-05, 2.7745e-05, 2.3957e-04, 2.2562e-05, 2.4241e-05,\n",
       "                       2.6251e-04, 2.6266e-04, 3.4696e-05, 2.0931e-05, 1.6064e-05, 3.5749e-05,\n",
       "                       4.0568e-05, 1.6125e-05, 4.6540e-04, 1.7678e-05, 1.9614e-05, 3.7662e-05,\n",
       "                       4.3254e-05, 3.7513e-05, 3.8600e-05, 3.0663e-04, 4.4801e-04, 6.3301e-04,\n",
       "                       3.8347e-04, 2.7322e-04, 2.9259e-04, 5.3045e-05, 2.4467e-04, 5.0726e-05,\n",
       "                       2.7996e-05, 3.5627e-04, 1.8938e-04, 2.7495e-05, 3.6296e-05, 5.2757e-04,\n",
       "                       5.7251e-04, 3.2827e-04, 2.9641e-05, 2.4725e-04, 3.4686e-05, 2.1635e-05,\n",
       "                       2.5044e-04, 1.4153e-05, 2.3318e-04, 2.1446e-05, 3.1788e-05, 2.3212e-05,\n",
       "                       3.3714e-05, 1.6117e-05, 2.7384e-05, 2.0755e-05, 2.5969e-05, 2.5027e-04,\n",
       "                       3.4260e-05, 3.1936e-04, 3.0599e-05, 2.5737e-05, 3.6703e-04, 4.8340e-05,\n",
       "                       3.2686e-05, 3.6765e-05, 2.6586e-05, 3.0701e-05, 2.0875e-05, 3.5489e-05,\n",
       "                       4.2540e-04, 3.0823e-05, 1.9426e-04, 3.1805e-04, 2.6175e-05, 2.0848e-04,\n",
       "                       2.8960e-05, 2.6388e-04, 2.2251e-05, 2.2698e-05, 4.2653e-05, 1.1058e-03,\n",
       "                       1.5000e-05, 3.3853e-04, 1.2134e-05, 4.3150e-05, 3.1147e-05, 2.5731e-05,\n",
       "                       5.3632e-05, 1.9048e-04, 3.2718e-05, 4.1607e-05, 3.8241e-05, 2.1066e-05,\n",
       "                       3.8487e-05, 2.3479e-04, 2.4791e-04, 1.4002e-05, 2.4758e-04, 2.2429e-05,\n",
       "                       2.3890e-04, 2.6093e-05, 2.4830e-05, 1.9090e-05, 3.0703e-05, 3.2657e-05,\n",
       "                       3.6230e-05, 4.7920e-04, 2.3756e-05, 1.5651e-05, 2.4113e-05, 2.1538e-04,\n",
       "                       6.1231e-04, 3.9063e-05, 3.3626e-05, 2.7064e-05, 1.0371e-05, 3.6225e-05,\n",
       "                       5.7578e-04, 3.8621e-05, 2.0529e-04, 3.2808e-05, 3.1333e-05, 2.9318e-04,\n",
       "                       1.5594e-04, 4.0286e-05, 1.8335e-04, 2.4576e-05, 3.8367e-05, 3.6651e-05,\n",
       "                       1.9830e-05, 2.4714e-04, 3.6145e-05, 3.1699e-04, 3.4237e-04, 2.5110e-05,\n",
       "                       2.0504e-05, 2.7525e-04, 2.8464e-05, 2.2109e-05, 2.7400e-05, 2.8709e-05,\n",
       "                       3.6247e-05, 3.5643e-05, 2.6907e-05, 2.5456e-04, 3.1858e-05, 1.9734e-05,\n",
       "                       2.9747e-05, 1.6293e-05, 3.1687e-04, 3.2672e-05, 4.1121e-05, 2.5862e-05,\n",
       "                       4.0413e-05, 2.7783e-04, 4.2716e-05, 3.1034e-04, 3.1533e-05, 2.7156e-04,\n",
       "                       3.3021e-04, 7.4076e-04, 2.6170e-05, 3.4258e-05, 1.8914e-04, 1.9694e-04,\n",
       "                       3.4263e-05, 3.5293e-04, 2.5800e-04, 4.8339e-04, 6.6499e-04, 1.9639e-04,\n",
       "                       2.0746e-04, 3.2914e-05, 2.1935e-04, 4.4182e-05, 3.6930e-04, 2.6787e-05,\n",
       "                       2.7704e-04, 2.0058e-05, 5.1443e-04, 2.4907e-05, 5.5300e-04, 4.1708e-05,\n",
       "                       3.4544e-05, 3.8548e-05, 2.0030e-04, 3.3546e-05, 2.2787e-04, 2.7893e-05,\n",
       "                       3.5555e-05, 3.0415e-05, 1.6553e-05, 2.7538e-05, 2.3229e-05, 2.0303e-04,\n",
       "                       2.4700e-04, 2.3966e-04, 4.9021e-04, 2.4111e-04, 3.2343e-05, 3.3275e-05,\n",
       "                       2.4056e-05, 2.4048e-04, 2.1224e-04, 2.2756e-05, 3.8320e-05, 3.6355e-05,\n",
       "                       2.2485e-05, 3.3055e-05, 2.9872e-05, 2.1989e-04, 1.8896e-04, 3.8615e-05,\n",
       "                       7.0481e-04, 2.8084e-05, 1.4704e-05, 3.5491e-05, 2.9001e-05, 2.8003e-05,\n",
       "                       4.4361e-05, 1.2242e-05, 3.8108e-05, 3.3326e-05, 3.3518e-04, 1.9124e-04,\n",
       "                       2.5823e-05, 2.1553e-05, 2.0182e-05, 2.5906e-05, 3.4456e-04, 2.8177e-05,\n",
       "                       3.1362e-05, 3.3259e-05, 2.4409e-05, 4.6869e-04, 7.9933e-04, 2.5413e-05,\n",
       "                       3.0719e-04, 2.8860e-05, 6.4632e-05, 2.8539e-05, 3.2871e-05, 3.2056e-05,\n",
       "                       3.0233e-04, 3.4338e-05, 2.9847e-05, 3.0625e-04, 3.2183e-05, 8.0183e-04,\n",
       "                       2.8407e-04, 4.8771e-05, 2.8279e-05, 2.7715e-05, 2.9031e-05, 2.5868e-05,\n",
       "                       1.6800e-04, 2.0864e-04, 1.8023e-05, 2.3604e-05, 2.8055e-05, 4.9785e-05,\n",
       "                       4.5449e-05, 4.0715e-04, 2.9875e-04, 3.1029e-05, 4.5559e-04, 4.9068e-04,\n",
       "                       3.2081e-05, 2.3693e-05, 1.5187e-05, 3.6296e-04, 1.9763e-05, 3.0539e-05,\n",
       "                       2.8454e-05, 2.2146e-05, 6.4269e-04, 2.5952e-04, 4.2831e-04, 2.8571e-05,\n",
       "                       1.9075e-05, 4.9306e-04, 3.4219e-04, 2.3701e-05, 2.7351e-04, 2.8636e-05,\n",
       "                       2.3835e-04, 3.5331e-05], device='cuda:0')),\n",
       "              ('model.model.layer4.1.bn2.num_batches_tracked',\n",
       "               tensor(224904, device='cuda:0')),\n",
       "              ('model.model.fc.weight',\n",
       "               tensor([[0.0000, -0.0000, 0.0000,  ..., 0.0000, 0.0000, -0.0000],\n",
       "                       [0.0000, 0.0000, 0.0000,  ..., 0.0000, -0.0000, -0.0000],\n",
       "                       [0.0000, -0.0000, -0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                       ...,\n",
       "                       [-0.0000, -0.0000, -0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "                       [-0.0000, 0.0000, 0.3621,  ..., 0.0000, 0.3686, -0.0000],\n",
       "                       [0.0000, 0.3962, 0.3730,  ..., -0.0000, -0.0000, 0.0000]],\n",
       "                      device='cuda:0')),\n",
       "              ('model.model.fc.bias',\n",
       "               tensor([ 0.0257, -0.0886,  0.0585,  0.2736, -0.0221,  0.1453, -0.0710,  0.0056,\n",
       "                       -0.1864, -0.1135], device='cuda:0')),\n",
       "              ('model.model.fc.weight_mask',\n",
       "               tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "                       [0., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0'))]),\n",
       " 'epoch': 25}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_factor=.95\n",
    "\n",
    "student_path_template = \"checkpoints_student/T=10, alpha=0.5, dropout_hidden=0.0, dropout_input=0.0, lr=0.01, lr_decay=0.95, momentum=0.9, weight_decay=0.001_pruning_{scaling_factor}_final.tar\"\n",
    "student_path = student_path_template.format(scaling_factor=pruning_factor)\n",
    "checkpoint = torch.load(student_path)\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.ao.nn.quantized.modules.conv.Conv2d.weight' object to parameter 'weight_orig' (torch.nn.Parameter or None required)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m student_model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Check for QAT modules that include Conv2d\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (nn\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mConv2d, nn\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mLinear)):\n\u001b[1;32m----> 7\u001b[0m         \u001b[43mprune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_unstructured\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.05\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:929\u001b[0m, in \u001b[0;36ml1_unstructured\u001b[1;34m(module, name, amount, importance_scores)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ml1_unstructured\u001b[39m(module, name, amount, importance_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Prune tensor by removing units with the lowest L1-norm.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    Prunes tensor corresponding to parameter called ``name`` in ``module``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m        odict_keys(['bias', 'weight_orig', 'weight_mask'])\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m     \u001b[43mL1Unstructured\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportance_scores\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:553\u001b[0m, in \u001b[0;36mL1Unstructured.apply\u001b[1;34m(cls, module, name, amount, importance_scores)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mcls\u001b[39m, module, name, amount, importance_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    533\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Add pruning on the fly and reparametrization of a tensor.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03m    Adds the forward pre-hook that enables pruning on the fly and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m            If unspecified or None, the module parameter will be used in its place.\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportance_scores\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:168\u001b[0m, in \u001b[0;36mBasePruningMethod.apply\u001b[1;34m(cls, module, name, importance_scores, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# If this is the first time pruning is applied, take care of moving\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# the original tensor to a new parameter called name + '_orig' and\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# and deleting the original parameter\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(method, PruningContainer):\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# copy `module[name]` to `module[name + '_orig']`\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_orig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# temporarily delete `module[name]`\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_parameters[name]\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:613\u001b[0m, in \u001b[0;36mModule.register_parameter\u001b[1;34m(self, name, param)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, Parameter):\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(param)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object to parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None required)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad_fn:\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign non-leaf Tensor to parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters must be created explicitly. To express \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas a function of another Tensor, compute the value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe forward() method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.ao.nn.quantized.modules.conv.Conv2d.weight' object to parameter 'weight_orig' (torch.nn.Parameter or None required)"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "for name, module in student_model.named_modules():\n",
    "    # Check for QAT modules that include Conv2d\n",
    "    if isinstance(module, (nn.quantized.Conv2d, nn.quantized.Linear)):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 11.309782\n"
     ]
    }
   ],
   "source": [
    "from utils import print_size_of_model\n",
    "\n",
    "print_size_of_model(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.028818974271416664, zero_point=0, padding=(3, 3))\n",
       "  (bn1): Identity()\n",
       "  (relu): Identity()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.010436596348881721, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.029334038496017456, zero_point=61, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.03929217904806137, zero_point=35\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.014016865752637386, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03678586333990097, zero_point=76, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.04718213900923729, zero_point=47\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.011604868806898594, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.021898584440350533, zero_point=52, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.02307654544711113, zero_point=53)\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.02595202811062336, zero_point=50\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.010178747586905956, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.023663491010665894, zero_point=58, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.032937273383140564, zero_point=51\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010753863491117954, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.013492907397449017, zero_point=59, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.00791059248149395, zero_point=55)\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.013665317557752132, zero_point=58\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.003959656227380037, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.011112019419670105, zero_point=64, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.014378649182617664, zero_point=61\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.0038561017718166113, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.008980989456176758, zero_point=48, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.008590689860284328, zero_point=58)\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.01442408375442028, zero_point=46\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizedBasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.004589454736560583, zero_point=0, padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): Identity()\n",
       "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0158607866615057, zero_point=54, padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (add_func): QFunctional(\n",
       "        scale=0.027871860191226006, zero_point=49\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.11037641763687134, zero_point=29, qscheme=torch.per_channel_affine)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight, Shape: (64, 3, 7, 7)\n",
      "Layer: layer1.0.conv1.weight, Shape: (64, 64, 3, 3)\n",
      "Layer: layer1.0.conv2.weight, Shape: (64, 64, 3, 3)\n",
      "Layer: layer1.1.conv1.weight, Shape: (64, 64, 3, 3)\n",
      "Layer: layer1.1.conv2.weight, Shape: (64, 64, 3, 3)\n",
      "Layer: layer2.0.conv1.weight, Shape: (128, 64, 3, 3)\n",
      "Layer: layer2.0.conv2.weight, Shape: (128, 128, 3, 3)\n",
      "Layer: layer2.0.downsample.0.weight, Shape: (128, 64, 1, 1)\n",
      "Layer: layer2.1.conv1.weight, Shape: (128, 128, 3, 3)\n",
      "Layer: layer2.1.conv2.weight, Shape: (128, 128, 3, 3)\n",
      "Layer: layer3.0.conv1.weight, Shape: (256, 128, 3, 3)\n",
      "Layer: layer3.0.conv2.weight, Shape: (256, 256, 3, 3)\n",
      "Layer: layer3.0.downsample.0.weight, Shape: (256, 128, 1, 1)\n",
      "Layer: layer3.1.conv1.weight, Shape: (256, 256, 3, 3)\n",
      "Layer: layer3.1.conv2.weight, Shape: (256, 256, 3, 3)\n",
      "Layer: layer4.0.conv1.weight, Shape: (512, 256, 3, 3)\n",
      "Layer: layer4.0.conv2.weight, Shape: (512, 512, 3, 3)\n",
      "Layer: layer4.0.downsample.0.weight, Shape: (512, 256, 1, 1)\n",
      "Layer: layer4.1.conv1.weight, Shape: (512, 512, 3, 3)\n",
      "Layer: layer4.1.conv2.weight, Shape: (512, 512, 3, 3)\n",
      "Layer: fc.weight, Shape: (10, 512)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all layers and extract weights\n",
    "layer_weights = {}\n",
    "\n",
    "# Check if the model has parameters and handle quantized layers\n",
    "for name, module in student_model.model.model.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):  # Regular layers\n",
    "        if module.weight is not None:\n",
    "            layer_weights[f\"{name}.weight\"] = module.weight.detach().cpu().numpy()\n",
    "        if module.bias is not None:\n",
    "            layer_weights[f\"{name}.bias\"] = module.bias.detach().cpu().numpy()\n",
    "    elif isinstance(module, (nn.quantized.Conv2d, nn.quantized.Linear)):  # Quantized layers\n",
    "        weight = module._weight_bias()[0]\n",
    "        layer_weights[f\"{name}.weight\"] = weight.dequantize().cpu().numpy()\n",
    "\n",
    "# Print the layer names and their corresponding weight shapes\n",
    "for layer_name, weights in layer_weights.items():\n",
    "    print(f\"Layer: {layer_name}, Shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the quantized model: 11.08%\n"
     ]
    }
   ],
   "source": [
    "def get_sparsity_quantized_model(model):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity (fraction of zero weights) in a quantized model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The quantized model.\n",
    "    \n",
    "    Returns:\n",
    "        float: Overall sparsity of the model.\n",
    "    \"\"\"\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        # Check for quantized layers\n",
    "        if isinstance(module, (nn.quantized.Conv2d, nn.quantized.Linear)):\n",
    "            # Access quantized weights\n",
    "            weight = module._weight_bias()[0]\n",
    "\n",
    "            # Dequantize weights to check for zeros\n",
    "            dequantized_weight = weight.dequantize()\n",
    "            \n",
    "            # Count total and zero weights\n",
    "            total_weights += dequantized_weight.numel()\n",
    "            zero_weights += (dequantized_weight == 0).sum().item()\n",
    "\n",
    "    # Calculate sparsity\n",
    "    sparsity = zero_weights / total_weights if total_weights > 0 else 0\n",
    "    return sparsity\n",
    "\n",
    "# Example Usage\n",
    "sparsity = get_sparsity_quantized_model(student_model)\n",
    "print(f\"Sparsity of the quantized model: {sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the total number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose parameters need to be counted.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum((p.data != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_zero_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable parameters that are exactly zero in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose zero parameters need to be counted.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of trainable parameters that are exactly zero.\n",
    "    \"\"\"\n",
    "    return sum((p.data == 0).sum().item() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_35960\\49978029.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9224\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "teacher_net = networks.TeacherNetwork50()\n",
    "teacher_net = teacher_net.to(fast_device)\n",
    "\n",
    "checkpoint = torch.load('resnet50_cifar10_pretrained.bin')\n",
    "\n",
    "\n",
    "teacher_net.model.load_state_dict(checkpoint)\n",
    "teacher_net.to(fast_device)\n",
    "\n",
    "# pre-trained teacher accuracy\n",
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(teacher_net, test_loader, fast_device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_63100\\77459327.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_375.tar')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for StudentNetwork:\n\tMissing key(s) in state_dict: \"model.model.bn1.weight\", \"model.model.bn1.bias\", \"model.model.bn1.running_mean\", \"model.model.bn1.running_var\", \"model.model.layer1.0.bn1.weight\", \"model.model.layer1.0.bn1.bias\", \"model.model.layer1.0.bn1.running_mean\", \"model.model.layer1.0.bn1.running_var\", \"model.model.layer1.0.bn2.weight\", \"model.model.layer1.0.bn2.bias\", \"model.model.layer1.0.bn2.running_mean\", \"model.model.layer1.0.bn2.running_var\", \"model.model.layer1.1.bn1.weight\", \"model.model.layer1.1.bn1.bias\", \"model.model.layer1.1.bn1.running_mean\", \"model.model.layer1.1.bn1.running_var\", \"model.model.layer1.1.bn2.weight\", \"model.model.layer1.1.bn2.bias\", \"model.model.layer1.1.bn2.running_mean\", \"model.model.layer1.1.bn2.running_var\", \"model.model.layer2.0.bn1.weight\", \"model.model.layer2.0.bn1.bias\", \"model.model.layer2.0.bn1.running_mean\", \"model.model.layer2.0.bn1.running_var\", \"model.model.layer2.0.bn2.weight\", \"model.model.layer2.0.bn2.bias\", \"model.model.layer2.0.bn2.running_mean\", \"model.model.layer2.0.bn2.running_var\", \"model.model.layer2.0.downsample.1.weight\", \"model.model.layer2.0.downsample.1.bias\", \"model.model.layer2.0.downsample.1.running_mean\", \"model.model.layer2.0.downsample.1.running_var\", \"model.model.layer2.1.bn1.weight\", \"model.model.layer2.1.bn1.bias\", \"model.model.layer2.1.bn1.running_mean\", \"model.model.layer2.1.bn1.running_var\", \"model.model.layer2.1.bn2.weight\", \"model.model.layer2.1.bn2.bias\", \"model.model.layer2.1.bn2.running_mean\", \"model.model.layer2.1.bn2.running_var\", \"model.model.layer3.0.bn1.weight\", \"model.model.layer3.0.bn1.bias\", \"model.model.layer3.0.bn1.running_mean\", \"model.model.layer3.0.bn1.running_var\", \"model.model.layer3.0.bn2.weight\", \"model.model.layer3.0.bn2.bias\", \"model.model.layer3.0.bn2.running_mean\", \"model.model.layer3.0.bn2.running_var\", \"model.model.layer3.0.downsample.1.weight\", \"model.model.layer3.0.downsample.1.bias\", \"model.model.layer3.0.downsample.1.running_mean\", \"model.model.layer3.0.downsample.1.running_var\", \"model.model.layer3.1.bn1.weight\", \"model.model.layer3.1.bn1.bias\", \"model.model.layer3.1.bn1.running_mean\", \"model.model.layer3.1.bn1.running_var\", \"model.model.layer3.1.bn2.weight\", \"model.model.layer3.1.bn2.bias\", \"model.model.layer3.1.bn2.running_mean\", \"model.model.layer3.1.bn2.running_var\", \"model.model.layer4.0.bn1.weight\", \"model.model.layer4.0.bn1.bias\", \"model.model.layer4.0.bn1.running_mean\", \"model.model.layer4.0.bn1.running_var\", \"model.model.layer4.0.bn2.weight\", \"model.model.layer4.0.bn2.bias\", \"model.model.layer4.0.bn2.running_mean\", \"model.model.layer4.0.bn2.running_var\", \"model.model.layer4.0.downsample.1.weight\", \"model.model.layer4.0.downsample.1.bias\", \"model.model.layer4.0.downsample.1.running_mean\", \"model.model.layer4.0.downsample.1.running_var\", \"model.model.layer4.1.bn1.weight\", \"model.model.layer4.1.bn1.bias\", \"model.model.layer4.1.bn1.running_mean\", \"model.model.layer4.1.bn1.running_var\", \"model.model.layer4.1.bn2.weight\", \"model.model.layer4.1.bn2.bias\", \"model.model.layer4.1.bn2.running_mean\", \"model.model.layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"model.model.conv1.bn.weight\", \"model.model.conv1.bn.bias\", \"model.model.conv1.bn.running_mean\", \"model.model.conv1.bn.running_var\", \"model.model.conv1.bn.num_batches_tracked\", \"model.model.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.conv1.weight_fake_quant.observer_enabled\", \"model.model.conv1.weight_fake_quant.scale\", \"model.model.conv1.weight_fake_quant.zero_point\", \"model.model.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.conv1.activation_post_process.fake_quant_enabled\", \"model.model.conv1.activation_post_process.observer_enabled\", \"model.model.conv1.activation_post_process.scale\", \"model.model.conv1.activation_post_process.zero_point\", \"model.model.conv1.activation_post_process.activation_post_process.eps\", \"model.model.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.0.add_func.activation_post_process.scale\", \"model.model.layer1.0.add_func.activation_post_process.zero_point\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv1.bn.weight\", \"model.model.layer1.0.conv1.bn.bias\", \"model.model.layer1.0.conv1.bn.running_mean\", \"model.model.layer1.0.conv1.bn.running_var\", \"model.model.layer1.0.conv1.bn.num_batches_tracked\", \"model.model.layer1.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.scale\", \"model.model.layer1.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv1.activation_post_process.scale\", \"model.model.layer1.0.conv1.activation_post_process.zero_point\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv2.bn.weight\", \"model.model.layer1.0.conv2.bn.bias\", \"model.model.layer1.0.conv2.bn.running_mean\", \"model.model.layer1.0.conv2.bn.running_var\", \"model.model.layer1.0.conv2.bn.num_batches_tracked\", \"model.model.layer1.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.scale\", \"model.model.layer1.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv2.activation_post_process.scale\", \"model.model.layer1.0.conv2.activation_post_process.zero_point\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.1.add_func.activation_post_process.scale\", \"model.model.layer1.1.add_func.activation_post_process.zero_point\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv1.bn.weight\", \"model.model.layer1.1.conv1.bn.bias\", \"model.model.layer1.1.conv1.bn.running_mean\", \"model.model.layer1.1.conv1.bn.running_var\", \"model.model.layer1.1.conv1.bn.num_batches_tracked\", \"model.model.layer1.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.scale\", \"model.model.layer1.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv1.activation_post_process.scale\", \"model.model.layer1.1.conv1.activation_post_process.zero_point\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv2.bn.weight\", \"model.model.layer1.1.conv2.bn.bias\", \"model.model.layer1.1.conv2.bn.running_mean\", \"model.model.layer1.1.conv2.bn.running_var\", \"model.model.layer1.1.conv2.bn.num_batches_tracked\", \"model.model.layer1.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.scale\", \"model.model.layer1.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv2.activation_post_process.scale\", \"model.model.layer1.1.conv2.activation_post_process.zero_point\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.0.add_func.activation_post_process.scale\", \"model.model.layer2.0.add_func.activation_post_process.zero_point\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv1.bn.weight\", \"model.model.layer2.0.conv1.bn.bias\", \"model.model.layer2.0.conv1.bn.running_mean\", \"model.model.layer2.0.conv1.bn.running_var\", \"model.model.layer2.0.conv1.bn.num_batches_tracked\", \"model.model.layer2.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.scale\", \"model.model.layer2.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv1.activation_post_process.scale\", \"model.model.layer2.0.conv1.activation_post_process.zero_point\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv2.bn.weight\", \"model.model.layer2.0.conv2.bn.bias\", \"model.model.layer2.0.conv2.bn.running_mean\", \"model.model.layer2.0.conv2.bn.running_var\", \"model.model.layer2.0.conv2.bn.num_batches_tracked\", \"model.model.layer2.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.scale\", \"model.model.layer2.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv2.activation_post_process.scale\", \"model.model.layer2.0.conv2.activation_post_process.zero_point\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.bn.weight\", \"model.model.layer2.0.downsample.0.bn.bias\", \"model.model.layer2.0.downsample.0.bn.running_mean\", \"model.model.layer2.0.downsample.0.bn.running_var\", \"model.model.layer2.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer2.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.scale\", \"model.model.layer2.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.1.add_func.activation_post_process.scale\", \"model.model.layer2.1.add_func.activation_post_process.zero_point\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv1.bn.weight\", \"model.model.layer2.1.conv1.bn.bias\", \"model.model.layer2.1.conv1.bn.running_mean\", \"model.model.layer2.1.conv1.bn.running_var\", \"model.model.layer2.1.conv1.bn.num_batches_tracked\", \"model.model.layer2.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.scale\", \"model.model.layer2.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv1.activation_post_process.scale\", \"model.model.layer2.1.conv1.activation_post_process.zero_point\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv2.bn.weight\", \"model.model.layer2.1.conv2.bn.bias\", \"model.model.layer2.1.conv2.bn.running_mean\", \"model.model.layer2.1.conv2.bn.running_var\", \"model.model.layer2.1.conv2.bn.num_batches_tracked\", \"model.model.layer2.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.scale\", \"model.model.layer2.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv2.activation_post_process.scale\", \"model.model.layer2.1.conv2.activation_post_process.zero_point\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.0.add_func.activation_post_process.scale\", \"model.model.layer3.0.add_func.activation_post_process.zero_point\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv1.bn.weight\", \"model.model.layer3.0.conv1.bn.bias\", \"model.model.layer3.0.conv1.bn.running_mean\", \"model.model.layer3.0.conv1.bn.running_var\", \"model.model.layer3.0.conv1.bn.num_batches_tracked\", \"model.model.layer3.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.scale\", \"model.model.layer3.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv1.activation_post_process.scale\", \"model.model.layer3.0.conv1.activation_post_process.zero_point\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv2.bn.weight\", \"model.model.layer3.0.conv2.bn.bias\", \"model.model.layer3.0.conv2.bn.running_mean\", \"model.model.layer3.0.conv2.bn.running_var\", \"model.model.layer3.0.conv2.bn.num_batches_tracked\", \"model.model.layer3.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.scale\", \"model.model.layer3.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv2.activation_post_process.scale\", \"model.model.layer3.0.conv2.activation_post_process.zero_point\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.bn.weight\", \"model.model.layer3.0.downsample.0.bn.bias\", \"model.model.layer3.0.downsample.0.bn.running_mean\", \"model.model.layer3.0.downsample.0.bn.running_var\", \"model.model.layer3.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer3.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.scale\", \"model.model.layer3.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.1.add_func.activation_post_process.scale\", \"model.model.layer3.1.add_func.activation_post_process.zero_point\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv1.bn.weight\", \"model.model.layer3.1.conv1.bn.bias\", \"model.model.layer3.1.conv1.bn.running_mean\", \"model.model.layer3.1.conv1.bn.running_var\", \"model.model.layer3.1.conv1.bn.num_batches_tracked\", \"model.model.layer3.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.scale\", \"model.model.layer3.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv1.activation_post_process.scale\", \"model.model.layer3.1.conv1.activation_post_process.zero_point\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv2.bn.weight\", \"model.model.layer3.1.conv2.bn.bias\", \"model.model.layer3.1.conv2.bn.running_mean\", \"model.model.layer3.1.conv2.bn.running_var\", \"model.model.layer3.1.conv2.bn.num_batches_tracked\", \"model.model.layer3.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.scale\", \"model.model.layer3.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv2.activation_post_process.scale\", \"model.model.layer3.1.conv2.activation_post_process.zero_point\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.0.add_func.activation_post_process.scale\", \"model.model.layer4.0.add_func.activation_post_process.zero_point\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv1.bn.weight\", \"model.model.layer4.0.conv1.bn.bias\", \"model.model.layer4.0.conv1.bn.running_mean\", \"model.model.layer4.0.conv1.bn.running_var\", \"model.model.layer4.0.conv1.bn.num_batches_tracked\", \"model.model.layer4.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.scale\", \"model.model.layer4.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv1.activation_post_process.scale\", \"model.model.layer4.0.conv1.activation_post_process.zero_point\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv2.bn.weight\", \"model.model.layer4.0.conv2.bn.bias\", \"model.model.layer4.0.conv2.bn.running_mean\", \"model.model.layer4.0.conv2.bn.running_var\", \"model.model.layer4.0.conv2.bn.num_batches_tracked\", \"model.model.layer4.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.scale\", \"model.model.layer4.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv2.activation_post_process.scale\", \"model.model.layer4.0.conv2.activation_post_process.zero_point\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.bn.weight\", \"model.model.layer4.0.downsample.0.bn.bias\", \"model.model.layer4.0.downsample.0.bn.running_mean\", \"model.model.layer4.0.downsample.0.bn.running_var\", \"model.model.layer4.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer4.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.scale\", \"model.model.layer4.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.1.add_func.activation_post_process.scale\", \"model.model.layer4.1.add_func.activation_post_process.zero_point\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv1.bn.weight\", \"model.model.layer4.1.conv1.bn.bias\", \"model.model.layer4.1.conv1.bn.running_mean\", \"model.model.layer4.1.conv1.bn.running_var\", \"model.model.layer4.1.conv1.bn.num_batches_tracked\", \"model.model.layer4.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.scale\", \"model.model.layer4.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv1.activation_post_process.scale\", \"model.model.layer4.1.conv1.activation_post_process.zero_point\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv2.bn.weight\", \"model.model.layer4.1.conv2.bn.bias\", \"model.model.layer4.1.conv2.bn.running_mean\", \"model.model.layer4.1.conv2.bn.running_var\", \"model.model.layer4.1.conv2.bn.num_batches_tracked\", \"model.model.layer4.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.scale\", \"model.model.layer4.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv2.activation_post_process.scale\", \"model.model.layer4.1.conv2.activation_post_process.zero_point\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.fc.weight_fake_quant.fake_quant_enabled\", \"model.model.fc.weight_fake_quant.observer_enabled\", \"model.model.fc.weight_fake_quant.scale\", \"model.model.fc.weight_fake_quant.zero_point\", \"model.model.fc.weight_fake_quant.activation_post_process.eps\", \"model.model.fc.weight_fake_quant.activation_post_process.min_val\", \"model.model.fc.weight_fake_quant.activation_post_process.max_val\", \"model.model.fc.activation_post_process.fake_quant_enabled\", \"model.model.fc.activation_post_process.observer_enabled\", \"model.model.fc.activation_post_process.scale\", \"model.model.fc.activation_post_process.zero_point\", \"model.model.fc.activation_post_process.activation_post_process.eps\", \"model.model.fc.activation_post_process.activation_post_process.min_val\", \"model.model.fc.activation_post_process.activation_post_process.max_val\", \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m student_net \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mStudentNetwork(pruning_factor, teacher_net, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, qat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dif_arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_BiT_adam\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mT=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_375.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mstudent_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m student_net \u001b[38;5;241m=\u001b[39m student_net\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m      5\u001b[0m student_net\u001b[38;5;241m.\u001b[39mto(fast_device)\n",
      "File \u001b[1;32mc:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for StudentNetwork:\n\tMissing key(s) in state_dict: \"model.model.bn1.weight\", \"model.model.bn1.bias\", \"model.model.bn1.running_mean\", \"model.model.bn1.running_var\", \"model.model.layer1.0.bn1.weight\", \"model.model.layer1.0.bn1.bias\", \"model.model.layer1.0.bn1.running_mean\", \"model.model.layer1.0.bn1.running_var\", \"model.model.layer1.0.bn2.weight\", \"model.model.layer1.0.bn2.bias\", \"model.model.layer1.0.bn2.running_mean\", \"model.model.layer1.0.bn2.running_var\", \"model.model.layer1.1.bn1.weight\", \"model.model.layer1.1.bn1.bias\", \"model.model.layer1.1.bn1.running_mean\", \"model.model.layer1.1.bn1.running_var\", \"model.model.layer1.1.bn2.weight\", \"model.model.layer1.1.bn2.bias\", \"model.model.layer1.1.bn2.running_mean\", \"model.model.layer1.1.bn2.running_var\", \"model.model.layer2.0.bn1.weight\", \"model.model.layer2.0.bn1.bias\", \"model.model.layer2.0.bn1.running_mean\", \"model.model.layer2.0.bn1.running_var\", \"model.model.layer2.0.bn2.weight\", \"model.model.layer2.0.bn2.bias\", \"model.model.layer2.0.bn2.running_mean\", \"model.model.layer2.0.bn2.running_var\", \"model.model.layer2.0.downsample.1.weight\", \"model.model.layer2.0.downsample.1.bias\", \"model.model.layer2.0.downsample.1.running_mean\", \"model.model.layer2.0.downsample.1.running_var\", \"model.model.layer2.1.bn1.weight\", \"model.model.layer2.1.bn1.bias\", \"model.model.layer2.1.bn1.running_mean\", \"model.model.layer2.1.bn1.running_var\", \"model.model.layer2.1.bn2.weight\", \"model.model.layer2.1.bn2.bias\", \"model.model.layer2.1.bn2.running_mean\", \"model.model.layer2.1.bn2.running_var\", \"model.model.layer3.0.bn1.weight\", \"model.model.layer3.0.bn1.bias\", \"model.model.layer3.0.bn1.running_mean\", \"model.model.layer3.0.bn1.running_var\", \"model.model.layer3.0.bn2.weight\", \"model.model.layer3.0.bn2.bias\", \"model.model.layer3.0.bn2.running_mean\", \"model.model.layer3.0.bn2.running_var\", \"model.model.layer3.0.downsample.1.weight\", \"model.model.layer3.0.downsample.1.bias\", \"model.model.layer3.0.downsample.1.running_mean\", \"model.model.layer3.0.downsample.1.running_var\", \"model.model.layer3.1.bn1.weight\", \"model.model.layer3.1.bn1.bias\", \"model.model.layer3.1.bn1.running_mean\", \"model.model.layer3.1.bn1.running_var\", \"model.model.layer3.1.bn2.weight\", \"model.model.layer3.1.bn2.bias\", \"model.model.layer3.1.bn2.running_mean\", \"model.model.layer3.1.bn2.running_var\", \"model.model.layer4.0.bn1.weight\", \"model.model.layer4.0.bn1.bias\", \"model.model.layer4.0.bn1.running_mean\", \"model.model.layer4.0.bn1.running_var\", \"model.model.layer4.0.bn2.weight\", \"model.model.layer4.0.bn2.bias\", \"model.model.layer4.0.bn2.running_mean\", \"model.model.layer4.0.bn2.running_var\", \"model.model.layer4.0.downsample.1.weight\", \"model.model.layer4.0.downsample.1.bias\", \"model.model.layer4.0.downsample.1.running_mean\", \"model.model.layer4.0.downsample.1.running_var\", \"model.model.layer4.1.bn1.weight\", \"model.model.layer4.1.bn1.bias\", \"model.model.layer4.1.bn1.running_mean\", \"model.model.layer4.1.bn1.running_var\", \"model.model.layer4.1.bn2.weight\", \"model.model.layer4.1.bn2.bias\", \"model.model.layer4.1.bn2.running_mean\", \"model.model.layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"model.model.conv1.bn.weight\", \"model.model.conv1.bn.bias\", \"model.model.conv1.bn.running_mean\", \"model.model.conv1.bn.running_var\", \"model.model.conv1.bn.num_batches_tracked\", \"model.model.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.conv1.weight_fake_quant.observer_enabled\", \"model.model.conv1.weight_fake_quant.scale\", \"model.model.conv1.weight_fake_quant.zero_point\", \"model.model.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.conv1.activation_post_process.fake_quant_enabled\", \"model.model.conv1.activation_post_process.observer_enabled\", \"model.model.conv1.activation_post_process.scale\", \"model.model.conv1.activation_post_process.zero_point\", \"model.model.conv1.activation_post_process.activation_post_process.eps\", \"model.model.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.0.add_func.activation_post_process.scale\", \"model.model.layer1.0.add_func.activation_post_process.zero_point\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv1.bn.weight\", \"model.model.layer1.0.conv1.bn.bias\", \"model.model.layer1.0.conv1.bn.running_mean\", \"model.model.layer1.0.conv1.bn.running_var\", \"model.model.layer1.0.conv1.bn.num_batches_tracked\", \"model.model.layer1.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv1.weight_fake_quant.scale\", \"model.model.layer1.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv1.activation_post_process.scale\", \"model.model.layer1.0.conv1.activation_post_process.zero_point\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.0.conv2.bn.weight\", \"model.model.layer1.0.conv2.bn.bias\", \"model.model.layer1.0.conv2.bn.running_mean\", \"model.model.layer1.0.conv2.bn.running_var\", \"model.model.layer1.0.conv2.bn.num_batches_tracked\", \"model.model.layer1.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.0.conv2.weight_fake_quant.scale\", \"model.model.layer1.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.0.conv2.activation_post_process.scale\", \"model.model.layer1.0.conv2.activation_post_process.zero_point\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer1.1.add_func.activation_post_process.scale\", \"model.model.layer1.1.add_func.activation_post_process.zero_point\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv1.bn.weight\", \"model.model.layer1.1.conv1.bn.bias\", \"model.model.layer1.1.conv1.bn.running_mean\", \"model.model.layer1.1.conv1.bn.running_var\", \"model.model.layer1.1.conv1.bn.num_batches_tracked\", \"model.model.layer1.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv1.weight_fake_quant.scale\", \"model.model.layer1.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv1.activation_post_process.scale\", \"model.model.layer1.1.conv1.activation_post_process.zero_point\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer1.1.conv2.bn.weight\", \"model.model.layer1.1.conv2.bn.bias\", \"model.model.layer1.1.conv2.bn.running_mean\", \"model.model.layer1.1.conv2.bn.running_var\", \"model.model.layer1.1.conv2.bn.num_batches_tracked\", \"model.model.layer1.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer1.1.conv2.weight_fake_quant.scale\", \"model.model.layer1.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer1.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer1.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer1.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer1.1.conv2.activation_post_process.scale\", \"model.model.layer1.1.conv2.activation_post_process.zero_point\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer1.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.0.add_func.activation_post_process.scale\", \"model.model.layer2.0.add_func.activation_post_process.zero_point\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv1.bn.weight\", \"model.model.layer2.0.conv1.bn.bias\", \"model.model.layer2.0.conv1.bn.running_mean\", \"model.model.layer2.0.conv1.bn.running_var\", \"model.model.layer2.0.conv1.bn.num_batches_tracked\", \"model.model.layer2.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv1.weight_fake_quant.scale\", \"model.model.layer2.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv1.activation_post_process.scale\", \"model.model.layer2.0.conv1.activation_post_process.zero_point\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.conv2.bn.weight\", \"model.model.layer2.0.conv2.bn.bias\", \"model.model.layer2.0.conv2.bn.running_mean\", \"model.model.layer2.0.conv2.bn.running_var\", \"model.model.layer2.0.conv2.bn.num_batches_tracked\", \"model.model.layer2.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.conv2.weight_fake_quant.scale\", \"model.model.layer2.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.0.conv2.activation_post_process.scale\", \"model.model.layer2.0.conv2.activation_post_process.zero_point\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.bn.weight\", \"model.model.layer2.0.downsample.0.bn.bias\", \"model.model.layer2.0.downsample.0.bn.running_mean\", \"model.model.layer2.0.downsample.0.bn.running_var\", \"model.model.layer2.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer2.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer2.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer2.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer2.0.downsample.0.activation_post_process.scale\", \"model.model.layer2.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer2.1.add_func.activation_post_process.scale\", \"model.model.layer2.1.add_func.activation_post_process.zero_point\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv1.bn.weight\", \"model.model.layer2.1.conv1.bn.bias\", \"model.model.layer2.1.conv1.bn.running_mean\", \"model.model.layer2.1.conv1.bn.running_var\", \"model.model.layer2.1.conv1.bn.num_batches_tracked\", \"model.model.layer2.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv1.weight_fake_quant.scale\", \"model.model.layer2.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv1.activation_post_process.scale\", \"model.model.layer2.1.conv1.activation_post_process.zero_point\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer2.1.conv2.bn.weight\", \"model.model.layer2.1.conv2.bn.bias\", \"model.model.layer2.1.conv2.bn.running_mean\", \"model.model.layer2.1.conv2.bn.running_var\", \"model.model.layer2.1.conv2.bn.num_batches_tracked\", \"model.model.layer2.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer2.1.conv2.weight_fake_quant.scale\", \"model.model.layer2.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer2.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer2.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer2.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer2.1.conv2.activation_post_process.scale\", \"model.model.layer2.1.conv2.activation_post_process.zero_point\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer2.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.0.add_func.activation_post_process.scale\", \"model.model.layer3.0.add_func.activation_post_process.zero_point\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv1.bn.weight\", \"model.model.layer3.0.conv1.bn.bias\", \"model.model.layer3.0.conv1.bn.running_mean\", \"model.model.layer3.0.conv1.bn.running_var\", \"model.model.layer3.0.conv1.bn.num_batches_tracked\", \"model.model.layer3.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv1.weight_fake_quant.scale\", \"model.model.layer3.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv1.activation_post_process.scale\", \"model.model.layer3.0.conv1.activation_post_process.zero_point\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.conv2.bn.weight\", \"model.model.layer3.0.conv2.bn.bias\", \"model.model.layer3.0.conv2.bn.running_mean\", \"model.model.layer3.0.conv2.bn.running_var\", \"model.model.layer3.0.conv2.bn.num_batches_tracked\", \"model.model.layer3.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.conv2.weight_fake_quant.scale\", \"model.model.layer3.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.0.conv2.activation_post_process.scale\", \"model.model.layer3.0.conv2.activation_post_process.zero_point\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.bn.weight\", \"model.model.layer3.0.downsample.0.bn.bias\", \"model.model.layer3.0.downsample.0.bn.running_mean\", \"model.model.layer3.0.downsample.0.bn.running_var\", \"model.model.layer3.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer3.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer3.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer3.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer3.0.downsample.0.activation_post_process.scale\", \"model.model.layer3.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer3.1.add_func.activation_post_process.scale\", \"model.model.layer3.1.add_func.activation_post_process.zero_point\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv1.bn.weight\", \"model.model.layer3.1.conv1.bn.bias\", \"model.model.layer3.1.conv1.bn.running_mean\", \"model.model.layer3.1.conv1.bn.running_var\", \"model.model.layer3.1.conv1.bn.num_batches_tracked\", \"model.model.layer3.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv1.weight_fake_quant.scale\", \"model.model.layer3.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv1.activation_post_process.scale\", \"model.model.layer3.1.conv1.activation_post_process.zero_point\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer3.1.conv2.bn.weight\", \"model.model.layer3.1.conv2.bn.bias\", \"model.model.layer3.1.conv2.bn.running_mean\", \"model.model.layer3.1.conv2.bn.running_var\", \"model.model.layer3.1.conv2.bn.num_batches_tracked\", \"model.model.layer3.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer3.1.conv2.weight_fake_quant.scale\", \"model.model.layer3.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer3.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer3.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer3.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer3.1.conv2.activation_post_process.scale\", \"model.model.layer3.1.conv2.activation_post_process.zero_point\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer3.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.0.add_func.activation_post_process.scale\", \"model.model.layer4.0.add_func.activation_post_process.zero_point\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv1.bn.weight\", \"model.model.layer4.0.conv1.bn.bias\", \"model.model.layer4.0.conv1.bn.running_mean\", \"model.model.layer4.0.conv1.bn.running_var\", \"model.model.layer4.0.conv1.bn.num_batches_tracked\", \"model.model.layer4.0.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv1.weight_fake_quant.scale\", \"model.model.layer4.0.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv1.activation_post_process.scale\", \"model.model.layer4.0.conv1.activation_post_process.zero_point\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.conv2.bn.weight\", \"model.model.layer4.0.conv2.bn.bias\", \"model.model.layer4.0.conv2.bn.running_mean\", \"model.model.layer4.0.conv2.bn.running_var\", \"model.model.layer4.0.conv2.bn.num_batches_tracked\", \"model.model.layer4.0.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.conv2.weight_fake_quant.scale\", \"model.model.layer4.0.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.0.conv2.activation_post_process.scale\", \"model.model.layer4.0.conv2.activation_post_process.zero_point\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.bn.weight\", \"model.model.layer4.0.downsample.0.bn.bias\", \"model.model.layer4.0.downsample.0.bn.running_mean\", \"model.model.layer4.0.downsample.0.bn.running_var\", \"model.model.layer4.0.downsample.0.bn.num_batches_tracked\", \"model.model.layer4.0.downsample.0.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.observer_enabled\", \"model.model.layer4.0.downsample.0.weight_fake_quant.scale\", \"model.model.layer4.0.downsample.0.weight_fake_quant.zero_point\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.0.downsample.0.activation_post_process.fake_quant_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.observer_enabled\", \"model.model.layer4.0.downsample.0.activation_post_process.scale\", \"model.model.layer4.0.downsample.0.activation_post_process.zero_point\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.eps\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.0.downsample.0.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.add_func.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.add_func.activation_post_process.observer_enabled\", \"model.model.layer4.1.add_func.activation_post_process.scale\", \"model.model.layer4.1.add_func.activation_post_process.zero_point\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.add_func.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv1.bn.weight\", \"model.model.layer4.1.conv1.bn.bias\", \"model.model.layer4.1.conv1.bn.running_mean\", \"model.model.layer4.1.conv1.bn.running_var\", \"model.model.layer4.1.conv1.bn.num_batches_tracked\", \"model.model.layer4.1.conv1.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv1.weight_fake_quant.scale\", \"model.model.layer4.1.conv1.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv1.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv1.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv1.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv1.activation_post_process.scale\", \"model.model.layer4.1.conv1.activation_post_process.zero_point\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv1.activation_post_process.activation_post_process.max_val\", \"model.model.layer4.1.conv2.bn.weight\", \"model.model.layer4.1.conv2.bn.bias\", \"model.model.layer4.1.conv2.bn.running_mean\", \"model.model.layer4.1.conv2.bn.running_var\", \"model.model.layer4.1.conv2.bn.num_batches_tracked\", \"model.model.layer4.1.conv2.weight_fake_quant.fake_quant_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.observer_enabled\", \"model.model.layer4.1.conv2.weight_fake_quant.scale\", \"model.model.layer4.1.conv2.weight_fake_quant.zero_point\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.eps\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.min_val\", \"model.model.layer4.1.conv2.weight_fake_quant.activation_post_process.max_val\", \"model.model.layer4.1.conv2.activation_post_process.fake_quant_enabled\", \"model.model.layer4.1.conv2.activation_post_process.observer_enabled\", \"model.model.layer4.1.conv2.activation_post_process.scale\", \"model.model.layer4.1.conv2.activation_post_process.zero_point\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.eps\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.min_val\", \"model.model.layer4.1.conv2.activation_post_process.activation_post_process.max_val\", \"model.model.fc.weight_fake_quant.fake_quant_enabled\", \"model.model.fc.weight_fake_quant.observer_enabled\", \"model.model.fc.weight_fake_quant.scale\", \"model.model.fc.weight_fake_quant.zero_point\", \"model.model.fc.weight_fake_quant.activation_post_process.eps\", \"model.model.fc.weight_fake_quant.activation_post_process.min_val\", \"model.model.fc.weight_fake_quant.activation_post_process.max_val\", \"model.model.fc.activation_post_process.fake_quant_enabled\", \"model.model.fc.activation_post_process.observer_enabled\", \"model.model.fc.activation_post_process.scale\", \"model.model.fc.activation_post_process.zero_point\", \"model.model.fc.activation_post_process.activation_post_process.eps\", \"model.model.fc.activation_post_process.activation_post_process.min_val\", \"model.model.fc.activation_post_process.activation_post_process.max_val\", \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\". "
     ]
    }
   ],
   "source": [
    "\n",
    "student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=False, fuse=False, qat=False, dif_arch=True)\n",
    "checkpoint = torch.load('student_BiT_adam\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_375.tar')\n",
    "student_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "student_net = student_net.model.model\n",
    "student_net.to(fast_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\daniel\\anaconda3\\envs\\Classifier\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_46216\\707560430.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('student_BiT_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Accuracy for pruning factor 0: 0.9522\n",
=======
      "Layer indices to quantize [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Total number of layers to quantize 21\n",
      "\n",
      "Quantizing layer with index: 0\n",
      "Quantization progress: 0 out of 20\n",
      "\n",
      "shape of W: torch.Size([64, 3, 7, 7])\n",
      "shape of analog_layer_input: torch.Size([896, 147])\n",
      "shape of quantized_layer_input: torch.Size([896, 147])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 27/27 [00:00<00:00, 978.99it/s]\n"
=======
      "100%|██████████| 147/147 [00:00<00:00, 549.03it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 0 is 25.63368797302246.\n",
      "The relative quantization error of layer 0 is 0.07626139372587204.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 1\n",
      "Quantization progress: 1 out of 20\n",
      "\n",
      "shape of W: torch.Size([64, 64, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([384, 576])\n",
      "shape of quantized_layer_input: torch.Size([384, 576])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 576/576 [00:00<00:00, 3364.89it/s]\n"
=======
      "100%|██████████| 576/576 [00:00<00:00, 2098.73it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 1 is 3.9425508975982666.\n",
      "The relative quantization error of layer 1 is 0.05762189254164696.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 2\n",
      "Quantization progress: 2 out of 20\n",
      "\n",
      "shape of W: torch.Size([64, 64, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([384, 576])\n",
      "shape of quantized_layer_input: torch.Size([384, 576])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 576/576 [00:00<00:00, 3395.94it/s]\n"
=======
      "100%|██████████| 576/576 [00:00<00:00, 2564.26it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 2 is 1.7968082427978516.\n",
      "The relative quantization error of layer 2 is 0.08124750852584839.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 3\n",
      "Quantization progress: 3 out of 20\n",
      "\n",
      "shape of W: torch.Size([64, 64, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([384, 576])\n",
      "shape of quantized_layer_input: torch.Size([384, 576])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 576/576 [00:00<00:00, 2966.68it/s]\n"
=======
      "100%|██████████| 576/576 [00:00<00:00, 2740.14it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 3 is 4.898229598999023.\n",
      "The relative quantization error of layer 3 is 0.08185645192861557.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 4\n",
      "Quantization progress: 4 out of 20\n",
      "\n",
      "shape of W: torch.Size([64, 64, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([384, 576])\n",
      "shape of quantized_layer_input: torch.Size([384, 576])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 576/576 [00:00<00:00, 2834.49it/s]\n"
=======
      "100%|██████████| 576/576 [00:00<00:00, 2577.88it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 4 is 1.6156749725341797.\n",
      "The relative quantization error of layer 4 is 0.09929851442575455.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 5\n",
      "Quantization progress: 5 out of 20\n",
      "\n",
      "shape of W: torch.Size([128, 64, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([384, 576])\n",
      "shape of quantized_layer_input: torch.Size([384, 576])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 576/576 [00:00<00:00, 2891.18it/s]\n"
=======
      "100%|██████████| 576/576 [00:00<00:00, 2834.06it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 5 is 8.018511772155762.\n",
      "The relative quantization error of layer 5 is 0.11040127277374268.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 6\n",
      "Quantization progress: 6 out of 20\n",
      "\n",
      "shape of W: torch.Size([128, 128, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([256, 1152])\n",
      "shape of quantized_layer_input: torch.Size([256, 1152])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 1152/1152 [00:00<00:00, 3435.48it/s]\n"
=======
      "100%|██████████| 1152/1152 [00:00<00:00, 3003.33it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 6 is 1.252508521080017.\n",
      "The relative quantization error of layer 6 is 0.08935167640447617.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 7\n",
      "Quantization progress: 7 out of 20\n",
      "\n",
      "shape of W: torch.Size([128, 64, 1, 1])\n",
      "shape of analog_layer_input: torch.Size([2176, 64])\n",
      "shape of quantized_layer_input: torch.Size([2176, 64])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 64/64 [00:00<00:00, 1543.21it/s]\n"
=======
      "100%|██████████| 64/64 [00:00<00:00, 3126.14it/s]"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 7 is 13.434507369995117.\n",
      "The relative quantization error of layer 7 is 0.1808401495218277.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing layer with index: 8\n",
      "Quantization progress: 8 out of 20\n",
      "\n",
      "shape of W: torch.Size([128, 128, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([256, 1152])\n",
      "shape of quantized_layer_input: torch.Size([256, 1152])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 1152/1152 [00:00<00:00, 1699.32it/s]\n"
=======
      "100%|██████████| 1152/1152 [00:00<00:00, 3124.68it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 8 is 2.0917251110076904.\n",
      "The relative quantization error of layer 8 is 0.09697361290454865.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 9\n",
      "Quantization progress: 9 out of 20\n",
      "\n",
      "shape of W: torch.Size([128, 128, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([256, 1152])\n",
      "shape of quantized_layer_input: torch.Size([256, 1152])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 1152/1152 [00:00<00:00, 3339.43it/s]\n"
=======
      "100%|██████████| 1152/1152 [00:00<00:00, 3199.35it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 9 is 0.667126476764679.\n",
      "The relative quantization error of layer 9 is 0.09861595928668976.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 10\n",
      "Quantization progress: 10 out of 20\n",
      "\n",
      "shape of W: torch.Size([256, 128, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([256, 1152])\n",
      "shape of quantized_layer_input: torch.Size([256, 1152])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 1152/1152 [00:00<00:00, 3351.82it/s]\n"
=======
      "100%|██████████| 1152/1152 [00:00<00:00, 3223.77it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 10 is 3.0722508430480957.\n",
      "The relative quantization error of layer 10 is 0.12117790430784225.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 11\n",
      "Quantization progress: 11 out of 20\n",
      "\n",
      "shape of W: torch.Size([256, 256, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 2304])\n",
      "shape of quantized_layer_input: torch.Size([128, 2304])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 2304/2304 [00:00<00:00, 3301.74it/s]\n"
=======
      "100%|██████████| 2304/2304 [00:00<00:00, 3480.09it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 11 is 1.1908488273620605.\n",
      "The relative quantization error of layer 11 is 0.08112328499555588.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 12\n",
      "Quantization progress: 12 out of 20\n",
      "\n",
      "shape of W: torch.Size([256, 128, 1, 1])\n",
      "shape of analog_layer_input: torch.Size([640, 128])\n",
      "shape of quantized_layer_input: torch.Size([640, 128])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 128/128 [00:00<00:00, 2875.58it/s]\n"
=======
      "100%|██████████| 128/128 [00:00<00:00, 2684.54it/s]"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 12 is 4.296163082122803.\n",
      "The relative quantization error of layer 12 is 0.2490590214729309.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing layer with index: 13\n",
      "Quantization progress: 13 out of 20\n",
      "\n",
      "shape of W: torch.Size([256, 256, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 2304])\n",
      "shape of quantized_layer_input: torch.Size([128, 2304])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 2304/2304 [00:00<00:00, 3302.94it/s]\n"
=======
      "100%|██████████| 2304/2304 [00:00<00:00, 3344.41it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 13 is 0.8493529558181763.\n",
      "The relative quantization error of layer 13 is 0.10086272656917572.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 14\n",
      "Quantization progress: 14 out of 20\n",
      "\n",
      "shape of W: torch.Size([256, 256, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 2304])\n",
      "shape of quantized_layer_input: torch.Size([128, 2304])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 2304/2304 [00:00<00:00, 3257.71it/s]\n"
=======
      "100%|██████████| 2304/2304 [00:00<00:00, 3353.66it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 14 is 0.34594428539276123.\n",
      "The relative quantization error of layer 14 is 0.13153810799121857.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 15\n",
      "Quantization progress: 15 out of 20\n",
      "\n",
      "shape of W: torch.Size([512, 256, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 2304])\n",
      "shape of quantized_layer_input: torch.Size([128, 2304])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 2304/2304 [00:01<00:00, 1299.55it/s]\n"
=======
      "100%|██████████| 2304/2304 [00:00<00:00, 3389.00it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 15 is 1.9843748807907104.\n",
      "The relative quantization error of layer 15 is 0.1347113400697708.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 16\n",
      "Quantization progress: 16 out of 20\n",
      "\n",
      "shape of W: torch.Size([512, 512, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 4608])\n",
      "shape of quantized_layer_input: torch.Size([128, 4608])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 4608/4608 [00:04<00:00, 1145.88it/s]\n"
=======
      "100%|██████████| 4608/4608 [00:01<00:00, 3529.13it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 16 is 0.7867875099182129.\n",
      "The relative quantization error of layer 16 is 0.2127627283334732.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 17\n",
      "Quantization progress: 17 out of 20\n",
      "\n",
      "shape of W: torch.Size([512, 256, 1, 1])\n",
      "shape of analog_layer_input: torch.Size([256, 256])\n",
      "shape of quantized_layer_input: torch.Size([256, 256])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 256/256 [00:00<00:00, 1163.55it/s]\n"
=======
      "100%|██████████| 256/256 [00:00<00:00, 3344.99it/s]"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 17 is 2.458876609802246.\n",
      "The relative quantization error of layer 17 is 0.3303937017917633.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing layer with index: 18\n",
      "Quantization progress: 18 out of 20\n",
      "\n",
      "shape of W: torch.Size([512, 512, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 4608])\n",
      "shape of quantized_layer_input: torch.Size([128, 4608])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 4608/4608 [00:03<00:00, 1202.02it/s]\n"
=======
      "100%|██████████| 4608/4608 [00:01<00:00, 3574.08it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 18 is 1.88277268409729.\n",
      "The relative quantization error of layer 18 is 0.1320144683122635.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 19\n",
      "Quantization progress: 19 out of 20\n",
      "\n",
      "shape of W: torch.Size([512, 512, 3, 3])\n",
      "shape of analog_layer_input: torch.Size([128, 4608])\n",
      "shape of quantized_layer_input: torch.Size([128, 4608])\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 4608/4608 [00:03<00:00, 1203.64it/s]\n"
=======
      "100%|██████████| 4608/4608 [00:01<00:00, 3428.54it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "The quantization error of layer 19 is 0.6852036714553833.\n",
      "The relative quantization error of layer 19 is 0.14334264397621155.\n",
      "\n",
      "\n",
      "Quantizing layer with index: 20\n",
      "Quantization progress: 20 out of 20\n",
      "\n",
>>>>>>> QAT_experiments
      "The number of groups: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|██████████| 512/512 [00:00<00:00, 1181.22it/s]\n"
=======
      "100%|██████████| 512/512 [00:00<00:00, 3313.58it/s]\n"
>>>>>>> QAT_experiments
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Quantized Test Accuracy for pruning factor 0: 0.938\n"
=======
      "The quantization error of layer 20 is 20.633893966674805.\n",
      "The relative quantization error of layer 20 is 0.19272233545780182.\n",
      "\n"
>>>>>>> QAT_experiments
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from quantize_neural_net import QuantizeNeuralNet\n",
    "\n",
    "student_net = networks.StudentNetwork(pruning_factor, teacher_net, q=False, fuse=False, qat=False, dif_arch=True)\n",
    "checkpoint = torch.load('student_BiT_no_QAT\\T=5, alpha=1.0, dropout_hidden=0.0, dropout_input=0.0, lr=0.0005, lr_decay=0.95, momentum=0.9, weight_decay=0.0001_checkpoint_epoch_200.tar')\n",
    "student_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "student_net = student_net.model.model\n",
    "student_net.to(fast_device)\n",
    "\n",
    "# Evaluate pre-trained student accuracy\n",
    "_, pre_quantized_accuracy = utils.getLossAccuracyOnDataset(student_net, test_loader, fast_device)\n",
    "print(f\"Initial Test Accuracy for pruning factor {0}: {pre_quantized_accuracy}\")\n",
    "\n",
    "# Quantize the network\n",
    "quantizer = QuantizeNeuralNet(\n",
    "    student_net,\n",
    "    'resnet18',\n",
    "    batch_size=128,\n",
    "    data_loader=train_loader,\n",
    "    mlp_bits=8,\n",
    "    cnn_bits=8,\n",
    "    ignore_layers=[],\n",
    "    mlp_alphabet_scalar=1.16,\n",
    "    cnn_alphabet_scalar=1.16,\n",
    "    mlp_percentile=1,\n",
    "    cnn_percentile=1,\n",
    "    reg=None,\n",
    "    lamb=0.1,\n",
    "    retain_rate=0.25,\n",
    "    stochastic_quantization=False,\n",
    "    device=fast_device\n",
    ")\n",
    "\n",
    "quantized_model = quantizer.quantize_network(verbose=False)\n",
    "\n",
    "# Evaluate the quantized model's accuracy\n",
    "_, post_quantized_accuracy = utils.getLossAccuracyOnDataset(quantized_model, test_loader, fast_device)\n",
    "print(f\"Quantized Test Accuracy for pruning factor {0}: {post_quantized_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7154)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sparsity(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sparsity(teacher_net.model)"
=======
    "quantized_model = quantizer.quantize_network()"
>>>>>>> QAT_experiments
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
