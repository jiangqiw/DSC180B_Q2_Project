{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_weights\n",
    "import torch\n",
    "from resnetv2 import ResNetV2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "#sys.path.append('/content/KD')\n",
    "# Import the module\n",
    "import networks\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "use_gpu = True    # set use_gpu to True if system has gpu\n",
    "gpu_id = 0        # id of gpu to be used\n",
    "cpu_device = torch.device('cpu')\n",
    "# fast_device is where computation (training, inference) happens\n",
    "fast_device = torch.device('cpu')\n",
    "if use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'    # set visible devices depending on system configuration\n",
    "    fast_device = torch.device('cuda:' + str(gpu_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "    if use_gpu:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up transformations for CIFAR-10\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),  # Augment training data by padding 4 and random cropping\n",
    "        transforms.RandomHorizontalFlip(),     # Randomly flip images horizontally\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization for CIFAR-10\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_tx = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR10_dataset/', train=False,\n",
    "                                            download=True, transform=val_tx)\n",
    "\n",
    "# Split the training dataset into training and validation\n",
    "num_train = int(0.95 * len(train_val_dataset))  # 95% of the dataset for training\n",
    "num_val = len(train_val_dataset) - num_train  # Remaining 5% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
    "\n",
    "# DataLoader setup\n",
    "batch_size = 128\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_72488\\1898405000.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('BiT-M-R101x1_step5000.tar')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetV2(\n",
       "  (root): Sequential(\n",
       "    (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (padp): ConstantPad2d(padding=(1, 1, 1, 1), value=0)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (block1): Sequential(\n",
       "      (unit01): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (unit02): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit03): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (unit01): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (unit02): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit03): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit04): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (unit01): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (unit02): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit03): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit04): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit05): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit06): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit07): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit08): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit09): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit10): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit11): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit12): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit13): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit14): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit15): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit16): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit17): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit18): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit19): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit20): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit21): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit22): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit23): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (unit01): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): StdConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (unit02): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit03): PreActBottleneck(\n",
       "        (gn1): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
       "        (conv1): StdConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (gn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv2): StdConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (gn3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv3): StdConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avg): AdaptiveAvgPool2d(output_size=1)\n",
       "    (conv): Conv2d(2048, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNetV2(ResNetV2.BLOCK_UNITS['r101'], width_factor=1, head_size=10)  # NOTE: No new head.\n",
    "checkpoint = torch.load('BiT-M-R101x1_step5000.tar')\n",
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    new_key = key.replace(\"module.\", \"\")  # Remove \"module.\" prefix\n",
    "    new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.to(fast_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_72488\\267949.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet18_pretrained.bin'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laplace approximation set up with random final layer.\n",
      "Final layer param dim: 5130\n",
      "\n",
      "Random final layer => predictions are essentially random.\n",
      "Softmax preds (no sampling):\n",
      " tensor([[1.1620e-03, 2.0118e-05, 9.6503e-06,  ..., 2.5841e-04, 3.3065e-05,\n",
      "         2.2161e-04],\n",
      "        [7.8100e-05, 3.1191e-06, 5.9166e-06,  ..., 3.7626e-05, 2.1082e-05,\n",
      "         8.1085e-05],\n",
      "        [1.2208e-04, 5.6580e-06, 4.5456e-06,  ..., 7.0741e-05, 2.0582e-05,\n",
      "         1.1381e-04],\n",
      "        ...,\n",
      "        [1.0408e-04, 1.2221e-05, 4.0627e-06,  ..., 3.0567e-04, 2.9768e-05,\n",
      "         2.0887e-04],\n",
      "        [7.7724e-05, 5.2199e-06, 3.1230e-06,  ..., 9.8981e-05, 1.7024e-05,\n",
      "         1.2698e-04],\n",
      "        [8.9125e-04, 2.1019e-05, 1.0241e-05,  ..., 3.9522e-04, 4.2944e-05,\n",
      "         2.8842e-04]], device='cuda:0')\n",
      "Laplace preds (with sampling):\n",
      " tensor([[0.2414, 0.0988, 0.0712,  ..., 0.1310, 0.0063, 0.2337],\n",
      "        [0.2622, 0.1921, 0.1065,  ..., 0.0026, 0.0079, 0.1321],\n",
      "        [0.2593, 0.1138, 0.0257,  ..., 0.1131, 0.0585, 0.1520],\n",
      "        ...,\n",
      "        [0.1900, 0.1007, 0.0201,  ..., 0.1954, 0.0728, 0.1222],\n",
      "        [0.2043, 0.1015, 0.0095,  ..., 0.1810, 0.0773, 0.1210],\n",
      "        [0.2396, 0.0999, 0.0769,  ..., 0.1175, 0.0054, 0.2509]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model.load_state_dict(torch.load('resnet18_pretrained.bin'))\n",
    "model.to(fast_device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"fc.\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Put model in eval mode (we're not actually training, so no dropout/batchnorm updates).\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# 4. GET (RANDOM) LAST-LAYER WEIGHTS & APPROXIMATE HESSIAN\n",
    "# ------------------------\n",
    "# Because we replaced the last layer, it's randomly initialized at this point and \n",
    "# DOES NOT match the data at all. We'll compute cross-entropy loss and Hessian\n",
    "# on that mismatch. This is purely to demonstrate the Laplace pipeline.\n",
    "\n",
    "# We'll do a \"dummy\" pass with cross-entropy:\n",
    "criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Collect the final-layer parameters\n",
    "fc_params = list(model.fc.parameters())  # [weight, bias]\n",
    "assert len(fc_params) == 2, \"Expected the final layer to have weight & bias.\"\n",
    "\n",
    "# (a) Flatten them into a single vector for convenience\n",
    "with torch.no_grad():\n",
    "    w_mean_list = [p.flatten() for p in fc_params]\n",
    "w_mean = torch.cat(w_mean_list)  # This is the 'MAP' weights (randomly init, untrained)\n",
    "\n",
    "# (b) Hessian diagonal approximation via sum of grad^2\n",
    "hessian_diag = torch.zeros_like(w_mean)\n",
    "\n",
    "# We won't do any optimizer steps, just forward/backward to gather Hessian info.\n",
    "for batch_x, batch_y in train_loader:\n",
    "    # Forward\n",
    "    batch_x, batch_y = batch_x.to(fast_device), batch_y.to(fast_device)\n",
    "    batch_x.requires_grad = False  # we only want gradient w.r.t. final-layer weights\n",
    "    logits = model(batch_x)\n",
    "    loss_sum = criterion_sum(logits, batch_y)  # sum of NLL over the batch\n",
    "\n",
    "    # Zero grads for last layer\n",
    "    model.fc.zero_grad()\n",
    "    loss_sum.backward(retain_graph=True)\n",
    "\n",
    "    # Gather gradient\n",
    "    grad_list = [p.grad.detach().flatten() for p in fc_params]\n",
    "    grad_vec = torch.cat(grad_list)\n",
    "\n",
    "    # Accumulate squared gradients for diagonal approximation\n",
    "    hessian_diag += grad_vec**2\n",
    "\n",
    "# (c) Add prior precision (1 / sigma^2_0)\n",
    "sigma2_0 = 5.0\n",
    "hessian_diag += 1.0 / sigma2_0\n",
    "\n",
    "# (d) The diagonal of the posterior covariance\n",
    "cov_diag = 1.0 / hessian_diag\n",
    "\n",
    "print(\"\\nLaplace approximation set up with random final layer.\")\n",
    "print(\"Final layer param dim:\", w_mean.shape[0])\n",
    "\n",
    "# ------------------------\n",
    "# 5. DEFINE UTILS FOR SAMPLING & PREDICTION\n",
    "# ------------------------\n",
    "def resnet_penultimate(model, x):\n",
    "    \"\"\"\n",
    "    Runs ResNet up to the final average pool & flatten (i.e., penultimate features).\n",
    "    \"\"\"\n",
    "    # replicate the forward logic for ResNet\n",
    "    x = model.conv1(x)\n",
    "    x = model.bn1(x)\n",
    "    x = model.relu(x)\n",
    "    x = model.maxpool(x)\n",
    "\n",
    "    x = model.layer1(x)\n",
    "    x = model.layer2(x)\n",
    "    x = model.layer3(x)\n",
    "    x = model.layer4(x)\n",
    "\n",
    "    x = model.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    return x\n",
    "\n",
    "def predict_laplace(model, x, cov_diag, n_samples=10):\n",
    "    \"\"\"\n",
    "    Make a prediction by averaging softmax outputs over\n",
    "    multiple samples from N(w_MAP, diag(cov_diag)) for the final layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Extract MAP (random) final-layer parameters\n",
    "    fc_params = list(model.fc.parameters())\n",
    "    with torch.no_grad():\n",
    "        w_map_list = [p.detach().flatten() for p in fc_params]\n",
    "    w_map = torch.cat(w_map_list)\n",
    "\n",
    "    # Reshape info\n",
    "    W_shape = model.fc.weight.shape  # [num_classes, 2048]\n",
    "    b_shape = model.fc.bias.shape\n",
    "    W_numel = W_shape[0] * W_shape[1]\n",
    "    b_numel = b_shape[0]\n",
    "\n",
    "    # Compute penultimate features\n",
    "    with torch.no_grad():\n",
    "        feats = resnet_penultimate(model, x)  # shape [batch_size, 2048]\n",
    "\n",
    "    # Monte Carlo sample from last-layer posterior\n",
    "    logits_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        eps = torch.randn_like(w_map)\n",
    "        w_samp = w_map + eps * torch.sqrt(cov_diag)\n",
    "\n",
    "        # Extract W, b\n",
    "        W_samp = w_samp[:W_numel].reshape(W_shape)\n",
    "        b_samp = w_samp[W_numel:W_numel + b_numel]\n",
    "\n",
    "        # logits = feats @ W_samp.T + b_samp\n",
    "        logits_s = feats @ W_samp.t() + b_samp\n",
    "        logits_samples.append(logits_s.unsqueeze(0))\n",
    "\n",
    "    logits_mc = torch.cat(logits_samples, dim=0)  # [n_samples, batch_size, num_classes]\n",
    "    probs_mc = torch.softmax(logits_mc, dim=-1)   # [n_samples, batch_size, num_classes]\n",
    "    return probs_mc.mean(dim=0)\n",
    "\n",
    "\n",
    "batch_x_test, batch_y_test = next(iter(test_loader))  # get first batch\n",
    "batch_x_test = batch_x_test.to(fast_device)\n",
    "# Standard forward (random final layer)\n",
    "with torch.no_grad():\n",
    "    logits_test = model(batch_x_test)\n",
    "y_pred_test_softmax = torch.softmax(logits_test, dim=-1)\n",
    "\n",
    "# Laplace-based predictions\n",
    "y_pred_test_laplace = predict_laplace(model, batch_x_test, cov_diag, n_samples=10)\n",
    "\n",
    "print(\"\\nRandom final layer => predictions are essentially random.\")\n",
    "print(\"Softmax preds (no sampling):\\n\", y_pred_test_softmax)\n",
    "print(\"Laplace preds (with sampling):\\n\", y_pred_test_laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\AppData\\Local\\Temp\\ipykernel_70428\\4093372423.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_resnet50_cifar10.pth'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m\n\u001b[0;32m     80\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistogram of Top-Class Probabilities (Mixup Samples)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 84\u001b[0m \u001b[43mrun_mixup_and_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 73\u001b[0m, in \u001b[0;36mrun_mixup_and_histogram\u001b[1;34m(model, train_loader, alpha, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m         top_prob, pred_class \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;66;03m# Collect these probabilities into a list for histogram\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m         top_probs_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtop_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# --- Plot histogram ---\u001b[39;00m\n\u001b[0;32m     76\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "model = torchvision.models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model.load_state_dict(torch.load('best_resnet50_cifar10.pth'))\n",
    "\n",
    "#model = networks.TeacherNetworkBiT()\n",
    "\n",
    "model.to(fast_device)\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Applies Mixup to a batch of (x, y).\n",
    "    x: Tensor of shape [batch_size, channels, height, width]\n",
    "    y: Tensor of shape [batch_size] (class indices)\n",
    "    alpha: Mixup hyperparameter\n",
    "    device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "      mixed_x: [batch_size, channels, height, width]\n",
    "      y_a, y_b: the two different label sets (for the mixup loss)\n",
    "      lam: the mixup interpolation coefficient\n",
    "    \"\"\"\n",
    "    if alpha <= 0:\n",
    "        return x, y, y, 1.0  # No mixup\n",
    "    \n",
    "    # Sample the lambda from Beta(alpha, alpha)\n",
    "    lam = torch.distributions.uniform.Uniform(0, 1).sample([]).to(device)\n",
    "    \n",
    "    # Shuffle the batch\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    # Mix the inputs\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    \n",
    "    # Original labels y and permuted labels y[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Example of how to collect top-class predictions\n",
    "# for *mixed-up* samples and plot a histogram\n",
    "# --------------------------------------------------\n",
    "def run_mixup_and_histogram(model, train_loader, alpha=0.2, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    top_probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # --- Apply Mixup ---\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(batch_x, batch_y, alpha=alpha, device=device)\n",
    "            \n",
    "            # --- Forward pass on mixed samples ---\n",
    "            logits = model(mixed_x)\n",
    "            probs = F.softmax(logits, dim=1)  # shape [batch_size, num_classes]\n",
    "            \n",
    "            # --- Get top-class probability for each sample ---\n",
    "            # (max over classes -> returns (values, indices))\n",
    "            top_prob, pred_class = probs.max(dim=1)\n",
    "            \n",
    "            # Collect these probabilities into a list for histogram\n",
    "            top_probs_list.extend(top_prob.cpu().numpy())\n",
    "\n",
    "    # --- Plot histogram ---\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.hist(top_probs_list, bins=np.linspace(0, 1, 21), edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel(\"Top Class Probability\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Histogram of Top-Class Probabilities (Mixup Samples)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "run_mixup_and_histogram(model, test_loader, alpha=0.2, device=fast_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9803\n"
     ]
    }
   ],
   "source": [
    "reproducibilitySeed()\n",
    "_, test_accuracy = utils.getLossAccuracyOnDataset(model, test_loader, fast_device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
